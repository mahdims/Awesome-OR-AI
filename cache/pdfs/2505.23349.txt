--- Page 1 ---
Towards Reward Fairness in RLHF: From a Resource Allocation
Perspective
Sheng Ouyang1,2,3,4*, Yulan Hu4†, Ge Chen4,5∗, Qingyang Li4, Fuzheng Zhang4, Yong Liu1,2,3,4†
1Gaoling School of Artificial Intelligence, Renmin University of China
2Beijing Key Laboratory of Research on Large Models and Intelligent Governance
3Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE
4Kuaishou Technology
5University of Chinese Academy of Sciences
{ouyangsheng, liuyonggsai}@ruc.edu.cn, {huyulan, liqingyang, zhangfuzheng}@kuaishou.com, chenge221@mails.ucas.ac.cn
Abstract
Rewards serve as proxies for human prefer-
ences and play a crucial role in Reinforcement
Learning from Human Feedback (RLHF). How-
ever, if these rewards are inherently imperfect,
exhibiting various biases, they can adversely
affect the alignment of large language mod-
els (LLMs). In this paper, we collectively de-
fine the various biases present in rewards as
the problem of reward unfairness.
We pro-
pose a bias-agnostic method to address the
issue of reward fairness from a resource al-
location perspective, without specifically de-
signing for each type of bias, yet effectively
mitigating them. Specifically, we model pref-
erence learning as a resource allocation prob-
lem, treating rewards as resources to be allo-
cated while considering the trade-off between
utility and fairness in their distribution. We
propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness
in rewards. We apply our methods in both
verification and reinforcement learning sce-
narios to obtain a fairness reward model and
a policy model, respectively.
Experiments
conducted in these scenarios demonstrate that
our approach aligns LLMs with human pref-
erences in a more fair manner. Our data and
code are available at https://github.com/
shoyua/Towards-Reward-Fairness.
1
Introduction
RLHF (Ouyang et al., 2022; Kaufmann et al., 2023;
Dong et al., 2024) has significantly advanced the
alignment of LLM outputs with human preferences,
ensuring that the responses are helpful, harmless,
and honest (Bai et al., 2022; Huang et al., 2024).
The reward model (RM) (Stiennon et al., 2020;
Ouyang et al., 2022; Yan et al., 2024) plays a cru-
cial role in this process by providing a quantitative
metric that measures the degree to which the model
* Work done during an internship at Kuaishou Technology.
† Corresponding authors.
RM
Max 
Rewards
Policy
Reward Learning
Alignment
Figure 1: Rewards unfair problem in RLHF.
outputs align with human preferences. This metric
guides the LLM in producing outputs that are more
consistent with human preference.
One of the key reasons for the success of
RLHF is the assumption that the reward model
can accurately represent and measure actual pref-
erences (Kim and Seo, 2024). However, if the re-
ward model itself is biased (Son et al., 2024; Hayes
et al., 2024; Reber et al., 2024), it can lead to pol-
icy models exhibiting behaviors that do not align
with human preferences (Gao et al., 2023; Dubois
et al., 2024b). Consider Figure 1, where we denote
a preference pair data as (x, yw, yl), representing
a query, a preferred and dispreferred response, re-
spectively. When training a reward model follow-
ing the Bradley-Terry model (Bradley and Terry,
1952) with such preference data, the output appears
reasonable because for each x, the reward of yw is
greater than that of yl. However, using this reward
model to guide the training of a policy model can
be problematic. The policy model aims to maxi-
mize rewards, and in this scenario, the reward for
yl
1 is greater than the reward for yw
2 , even though yl
1
is dispreferred and yw
2 is preferred. From a general
perspective, if (x1, yw
1 , yl
1) and (x2, yw
2 , yl
2) come
from different data types, such bias in their reward
distribution can steer the model to favor one type of
data over another. We define this issue as “reward
unfairness”.
We interpret various reward biases from the per-
spective of reward unfairness, including length
bias (Shen et al., 2023; Park et al., 2024), category
1
arXiv:2505.23349v1  [cs.LG]  29 May 2025


--- Page 2 ---
bias (Padmakumar et al., 2024), and social bias (Li
et al., 2023). For instance, when the rewards distri-
bution varies significantly across data of different
lengths or categories, resulting in reward unfair-
ness, it manifests as length bias or category bias
respectively. Existing work (Park et al., 2024; Chen
et al., 2024) has addressed these biases by propos-
ing targeted methods to mitigate them. Park et al.
(2024); Chen et al. (2024); Yang et al. (2024); Pad-
makumar et al. (2024) have employed techniques
such as length regularization to mitigate the effects
of length bias. These methods adjust the reward
distribution to prevent models from favoring longer
responses, thereby ensuring more fair outputs. On
the other hand, category bias has not been as widely
acknowledged. However, some studies have implic-
itly addressed this issue. For instance, the work on
learning diverse preferences (Yang et al., 2024; Pad-
makumar et al., 2024) and model ensemble (Ramé
et al., 2024b,a) indirectly reduce the impact of cat-
egory bias. These work promote a more varied
and representative set of outputs, which helps mini-
mize the skewness introduced by category-specific
biases.
However, these works are specifically designed
to address particular biases and lack the ability
to transfer solutions across different types of bi-
ases. In this paper, we propose a unified perspec-
tive that considers these biases as manifestations of
a broader issue: reward unfairness. To address this
comprehensively, we introduce the reward fairness
framework. Firstly, we model preference learn-
ing as a resource allocation problem (Katoh and
Ibaraki, 1998). In this framework, we define the
rewards in preference learning as the resources to
be allocated. The extent to which these rewards re-
flect human preferences is defined as utility, while
the consistent distribution of rewards across the
data is defined as fairness. We employ a unified
fairness function to measure the fairness of the re-
wards distribution. This approach seeks to achieve
a trade-off between fairness and utility. We propose
two methods to obtain fairness rewards: Fairness
Regularization and Fairness Coefficient. We then
apply these methods in two scenarios: Fairness
Rewards for Verification and Fairness Rewards for
Reinforcement Learning (RL). We conclude our
contributions as following:
Unified Perspective from Reward Unfairness
We introduce a novel perspective that frames vari-
ous biases as specific instances of the broader prob-
lem of reward unfairness. This unified view fosters
a more comprehensive understanding and approach
to addressing these biases.
Reward Fairness Framework
We propose the
reward fairness framework from a resource alloca-
tion perspective to systematically address reward
unfairness, aiming to balance fairness and utility in
reward distribution.
Application to Verification and RL
We apply
our proposed methods in two scenarios: (a) Fair-
ness Rewards for Verification, which focuses on
training a fairness RM, and (b) Fairness Rewards
for RL, which aims to train a policy model that
implicitly incorporates fair rewards. Our fairness
rewards methods can be seamlessly integrated with
existing RM and RL methods.
2
Related Work
RLHF has become the standard approach for align-
ing LLMs with human preferences. RLHF can be
decomposed into two main components: Reward
Learning and RL Finetune.
2.1
Reward Learning
The reward model is a crucial component of RLHF,
providing a quantitative metric to guide alignment
with human preferences. Reward models typically
follow the Bradley-Terry model (Bradley and Terry,
1952), but there are also approaches based on re-
gression paradigms (Wang et al., 2024a,b) and the
“LLM as a judge” approach (Zhang et al., 2024;
Zheng et al., 2023). However, Hou et al. (2021);
Kim and Seo (2024); Reber et al. (2024) have iden-
tified that reward models are imperfect proxies for
human preferences, exhibiting various issues such
as length bias (Shen et al., 2023) and reward hack-
ing (Skalse et al., 2022). Shen et al. (2023); Chen
et al. (2024) have found that the results of reward
models are influenced by the length of the input,
and they have attempted to decouple this relation-
ship during training to mitigate its effects. Fast
RL (Li et al., 2024) is closest to our method, how-
ever Fast RL is an ensemble method that considers
fairness between different reward functions.
2.2
RL Finetune
RL Finetuning (Dong et al., 2024) generally in-
volves using reinforcement learning techniques,
guided by the reward model, to train the policy
model. Algorithms such as proximal policy opti-
mization (PPO) (Schulman et al., 2017) and group
2


--- Page 3 ---
relative policy optimization (GRPO) (Shao et al.,
2024) are commonly used. There is also a category
of work that omits the reward model and directly
learns preference, such as direct preference opti-
mization (DPO) (Rafailov et al., 2024), Kahneman-
Tversky optimization (KTO) (Ethayarajh et al.,
2024), and SimPO (Meng et al., 2024). These
methods are more efficient and stable compared to
PPO-based approaches. Although they do not in-
volve the reward model in training, they implicitly
fit rewards to align with human preferences. Lu
et al. (2024); Liu et al. (2024); Dubois et al. (2024a)
have observed that aligned models tend to generate
longer responses, which introduces a length bias.
To mitigate this issue, they have proposed methods
such as length regularization (Park et al., 2024).
3
Preliminaries
Reward Model
In RLHF, RM acts as a proxy for
human preferences to rate the quality of the model
output. Generally, the RM follows the Bradley-
Terry Model (Bradley and Terry, 1952) and can be
formulated as:
p (yw ≻yl | x) =
exp (rϕ (x, yw))
exp (rϕ (x, yw)) + exp (rϕ (x, yl)),
(1)
where (x, yw, yl) ∼D represent a prompt , a pre-
ferred response and an dispreferred response from
the preference dataset D, respectively. rϕ(x, y) de-
notes a reward function with the parameters ϕ, and
this is subsequently denoted as rϕ(y) for simplicity.
We can train a RM rϕ following the log-likelihood
maximization as:
max
rϕ
E(x,yw,yl)∼D [log σ (rϕ (yw) −rϕ (yl))] ,
(2)
where σ is the sigmoid function.
RL Finetune
During the RL phase (Jaques et al.,
2017), the learned RM is used to provide feedback
to the policy model πθ with the parameters θ. The
optimization is formulated as:
max
πθ
Ex∼D,y∼πθ(y|x) [rϕ(y)]
−βDKL [πθ(y | x)∥πref(y | x)] ,
(3)
where β is a hyperparameter controlling the KL
penalty and πref is the reference model.
DPO
DPO (Rafailov et al., 2024) is a method
used to directly optimize a policy based on pref-
erence data. The objective of DPO is to align the
policy πθ with human preferences by maximizing
the likelihood of preferred outcomes.
max
πθ
E(x,yw,yl)∼D

log σ

β log πθ(yw | x)
πref(yw | x)
−β log πθ(yl | x)
πref(yl | x)

,
(4)
where β is a scaling factor.
Utility
Fairness
RM
Figure 2: Objective of Fairness Rewards. Di and Dj
represent different data. Fairness rewards aim to obtain
rewards that consider the trade-off between utility and
fairness. Utility refers to the ability of the rewards to dis-
tinguish between preferred and dispreferred responses,
as illustrated in the top-right figure. Fairness refers to
the consistent distribution of rewards across different
data, as depicted in the bottom-right figure.
4
Fairness Rewards Allocation
In this section, we aim to obtain the fairness re-
wards, as shown in Figure 2. We model preference
learning as a resource allocation problem that maxi-
mizes utility while ensuring the fairness of rewards.
Resource Allocation
Resource allocation (Katoh
and Ibaraki, 1998) involves distributing resources
R among entities i to optimize overall utility U.
The trade-off between fairness and utility is a key
consideration. Utility maximization is given by:
max U(a)
s.t.
X
i
ai ≤R,
(5)
where a = [a1, a2, . . . , an] is an allocation vector
and ai denotes the resources allocated to the i-th
entity. Fairness can be incorporated by adding a
fairness constraint F(a):
max
U(a)
s.t.
X
i
ai ≤R
F(a) ≥η
,
(6)
3


--- Page 4 ---
where η represents the desired level of fairness.
Balancing these objectives requires careful consid-
eration of both fairness and utility to achieve an
optimal allocation strategy.
Fairness Rewards
In RLHF, rewards are quanti-
fied representations of human preferences, reflect-
ing the degree to which model outputs align with
human preferences. We model preference learning
as a resource allocation problem, where rewards are
considered as resources to be allocated. According
to Eq (5), our objective is to maximize the util-
ity of reward allocation. We use U(a) to measure
the extent to which the reward allocation vector
a aligns with human preferences, with higher val-
ues indicating greater utility. Concurrently, it is
imperative to ensure fairness in the distribution of
rewards. We use F(a) to measure the fairness of
the reward allocation vector, with larger values in-
dicating greater fairness. We expect F(a) to satisfy
following properties:
1. Continuity The fairness measure F(a) is con-
tinuous on Rn
+ for all integers n ≥1.
This property ensures that small changes in re-
source allocation result in only minor changes
to the fairness measure, thereby guaranteeing
the stability and consistency of the fairness
measure.
2. Homogeneity The fairness measure F(a) is
a homogeneous function of degree 0:
F(a) = F(t · a), ∀t > 0.
This property indicates that the fairness mea-
sure is independent of the scale of resource
allocation.
3. Monotonicity For n = 2 entities, the fairness
measure F(θ, 1−θ) is monotonically increas-
ing as the absolute difference between the two
elements (i.e., |1 −2θ|) shrinks to zero.
This property states that the fairness measure
increases as the resource allocation between
two entities becomes more equal.
There is a unified fairness metric proposed by
Lan et al. (2010) that satisfies three key properties:
fτ(a) = sign(1 −τ) ·


n
X
i=1
 
ai
P
j aj
!1−τ

1
τ
,
(7)
where τ ∈R is a constant, which allows for the
derivation of different fairness functions based on
its value. For instance, when τ = −1, fτ=−1(a) =
(P
i ai)2
P
i a2
i
= n · J(a) results in Jain’s index J(a)
(Jain et al., 1984), which is a famous metric for
measuring fairness in the resource allocation.
According to Eq (6), we consider the trade-off
between utility and fairness. We employ Eq (7) to
measure the fairness of reward allocation. Since
rewards can be viewed as an infinite resource, and
given the property of Homogeneity in fairness met-
rics, the fairness measure is independent of the unit
of measurement or the size of the resource alloca-
tion. We can eliminate the constraint on the total
amount of resources from Eq (6). Consequently,
we propose the following two methods to transform
Eq (6) into an unconstrained optimization problem.
• Fairness Regularization: We add the two
measures together,
max U(a) + αF(a),
(8)
where α is a hyperparameter that controls the
impact of the fairness regularization.
• Fairness Coefficient: We multiply the two
measures,
max U(a) · F(a)γ,
(9)
where γ is a hyperparameter that controls the
impact of the fairness coefficient.
By incorporating these methods, we aim to
achieve a trade-off between fairness and utility in
the rewards allocation process, ensuring that the
rewards not only reflect human preferences accu-
rately but also do so in a fair manner.
Clarification of Fairness
Finally, we clarify that
in the context of LLMs, “fairness” usually relates
to “social bias” (Li et al., 2023; Gallegos et al.,
2024). However, in this paper, we reformulate
preference learning from a resource allocation per-
spective, treating rewards as allocated resources.
Here, “fairness” refers to the fairness of reward
allocation, drawing from resource allocation lit-
erature (Kumar and Kleinberg, 2000; Lan et al.,
2010), which differs from social bias. As stated
in Section 1, we interpret various reward biases
through the lens of reward unfairness, addressing
them uniformly. For example, in length bias, “en-
tity” refers to data of varying lengths; for category
4


--- Page 5 ---
bias, it refers to different data categories, such as
“helpful” and “harmless”. In social bias cases, like
gender bias, “entity” denotes different genders.
5
Reward-Fairness RLHF
In this section, we discuss the application of fair-
ness rewards in two scenarios:
• Fairness Rewards for Verification (§5.1): We
introduce how to train a reward-fairness re-
ward model, which serves as a fair verifica-
tion.
• Fairness Rewards for RL (§5.2): We detail
the training of a reward-fairness policy model,
which aims to generate outputs that are fairer.
5.1
Fairness Rewards for Verification
The objective of the reward model is to act as a
proxy for human preferences. Typically, it takes
a pair of prompt and response (x, y) as input and
outputs a scalar score to verify the quality of the
pair. To develop a reward-fairness reward model,
we need to define the utility function U and the
fairness function F as per Eq (8) and (9).
For the Bradley-Terry reward model, which uses
Eq (2) as its training objective, the goal is to allo-
cate a higher reward to the preferred response yw
compared to the dispreferred response yl. There-
fore, we define the elements of the allocation vector
ai as ai = rϕ(yw) −rϕ(yl). With the allocation
vector a defined, we can directly take Eq (7) as the
fairness function, i.e., F(a) = fτ(a). The utility
function for the reward model is then defined as:
U(a) = Eai∈a [log σ(ai)] .
(10)
We define two types of reward models incor-
porating fairness: Reward Model with Fairness
Regularization (FR RM) and Reward Model with
Fairness Coefficient (FC RM). Their training ob-
jectives are as follows:
FR RM
The training objective combines the util-
ity and fairness measures additively:
LFR RM = −Eai∈a [log σ(ai)] −αF(a).
(11)
FC RM
The training objective combines the util-
ity and fairness measures multiplicatively:
LFC RM = −Eai∈a [log σ(ai)] · F(a)γ.
(12)
5.2
Fairness Rewards For RL
Although the training of DPO does not explicitly
involve a reward model, it implicitly fits a reward
model (Rafailov et al., 2024). We can interpret the
term β log πθ(y|x)
πref(y|x) as an implicit reward. Similar
to the reward model, we define the elements of the
allocation vector ai as:
ai = β log πθ(yw | x)
πref(yw | x) −β log πθ(yl | x)
πref(yl | x).
(13)
Consequently, we can derive the same utility func-
tion as for the reward model, as shown in Eq (10).
The utility function of the DPO has the same form
as that of the reward model, except that the meaning
of the allocation vector is slightly different, where
ai ∈a represents the difference in implicit rewards
between the preferred and less preferred responses.
With the allocation vector, we can directly use Eq
(7) as a fairness function.
We define two types of DPO models incorpo-
rating fairness: DPO with Fairness Regulariza-
tion (FR DPO) and DPO with Fairness Coefficient
(FC DPO). The training objectives of LFR DPO and
LFC DPO can be converted to the same form as Eq
(11) and (12) respectively.
By incorporating these fairness measures into
the DPO framework, we aim to ensure that the
model not only aligns with human preferences but
also allocate implicit rewards in a fair manner.
6
Experiments
In this section, we empirically investigate the fol-
lowing two research questions RQ:
• RQ1: how effective is our Fairness Rewards
approach in both verification and RL scenar-
ios?
• RQ2: how does the choice of Fairness Func-
tion impact performance?
Datasets & Baselines
In the verification sce-
nario, we conduct experiments on two benchmarks:
Reward Bench (Lambert et al., 2024) and HH-
RLHF (Bai et al., 2022). Our RMs are trained
using the training set from HH-RLHF, making HH-
RLHF an in-distribution (ID) benchmark, while Re-
ward Bench serves as an out-of-distribution (OOD)
benchmark. We report the accuracy on both bench-
marks. For the RL scenario, we evaluate our meth-
ods on the AlpacaEval2 (Dubois et al., 2024a) and
5


--- Page 6 ---
12
10
8
6
4
2
0
2
Rewards
0.00
0.05
0.10
0.15
0.20
0.25
Density
Helpful
Harmless
(a) BT rewards on HH-RLHF.
2
0
2
4
6
8
10
Rewards
0.00
0.05
0.10
0.15
0.20
0.25
Density
(b) FR rewards on HH-RLHF.
10
8
6
4
2
0
2
Rewards
0.00
0.05
0.10
0.15
0.20
0.25
Density
(c) FC rewards on HH-RLHF.
15.0
12.5
10.0
7.5
5.0
2.5
0.0
2.5
5.0
Rewards
0.00
0.05
0.10
0.15
0.20
Density
chat
safety
(d) BT rewards on Reward Bench.
2
0
2
4
6
8
10
12
Rewards
0.00
0.05
0.10
0.15
0.20
0.25
Density
(e) FR rewards on Reward Bench.
12
10
8
6
4
2
0
2
Rewards
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Density
(f) FC rewards on Reward Bench.
Figure 3: Rewards across ID and OOD data.
MT-Bench (Zheng et al., 2023) benchmarks. We
provide results for AlpacaEval2 in terms of Length-
controlled Win Rate (LC WR) and Win Rate (WR),
and for MT-Bench, we report the overall score. For
the policy model, we utilize the UltraFeedback Bi-
narized and SHP datasets for training. We train
the policy model using different methods such as
DPO (Rafailov et al., 2024), KTO (Ethayarajh et al.,
2024) and R-DPO (Park et al., 2024) with HALOs.
1
8
16
32
64
Number of Samples
15
16
17
18
19
LC WR
BT RM
FR RM
FC RM
Figure 4: Performance of LLaMA3-SFT on AlpacaE-
val2 using different verification strategies.
Implementation Details
For the reward model,
we train on the HH-RLHF training set for one
epoch with a learning rate of 2 × 10−6. For the pol-
icy model, we utilize the UltraFeedback Binarized
and SHP datasets, training for one epoch with a
learning rate of 5 × 10−6. During sampling with
the policy model, the temperature coefficient is set
to 1. All experiments are performed on an 8 ×
H800 machine. Both the reward models and the
policy models are trained using LLaMA3-SFT (a
base model developed by Dong et al. (2024)) and
Qwen2.5-SFT (a base model we trained following
Dong et al. (2024)). Further experimental details
can be found in Appendix B.
6.1
Main Results (RQ1)
6.1.1
Fairness Verification
Figure 3 illustrates the distribution of rewards from
different RMs on ID data (HH-RLHF) and OOD
data (Reward Bench). The first row of figures
shows the rewards distribution on the ID dataset
HH-RLHF. It is evident that the Bradley-Terry (BT)
RM exhibits a significant disparity in the distribu-
tion of rewards between Helpful and Harmless data,
indicating an unfair allocation of rewards. In con-
trast, Reward Model with Fairness Regularization
(FR RM) and Reward Model with Fairness Co-
efficient (FC RM) demonstrate a more consistent
rewards distribution across Helpful and Harmless
data, indicating that those two RMs are fairer. The
figures in the second row show the distribution
of rewards on the OOD data, and we can draw
the same conclusions as for the ID data. Table 1
presents the performance of the three RMs on the
Reward Bench and HH-RLHF. The results show
no significant performance difference between FR
RM, FC RM, and BT RM, suggesting that Fair
RMs achieve a good trade-off between fairness and
utility without sacrificing model performance. Ad-
ditionally,we provide the distribution of rewards on
6


--- Page 7 ---
4.5
4.0
3.5
3.0
2.5
2.0
Rewards
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Density
Sent More
Sent Less
(a) Rewards distribution of BT RM.
4
5
6
7
8
Rewards
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Density
(b) Rewards distribution of FR RM.
6
5
4
3
2
1
Rewards
0.0
0.1
0.2
0.3
0.4
0.5
Density
(c) Rewards distribution of FC RM.
Figure 5: Rewards distribution on CrowS-Pairs.
Table 1: Performance of different verification strategies on two reward model benchmarks.
Verifiers
Reward Bench
HH-RLHF
Chat
Chathard
Reasoning
Safety
Avg.
Helpful
Harmless
Avg.
BT RM
93.02
57.02
84.98
77.43
78.11
74.38
73.23
73.81
FR RM
94.41
57.02
83.86
78.24
78.38
73.49
73.62
73.55
FC RM
94.41
53.29
85.53
76.76
77.50
74.30
73.62
73.96
length in Appendix C.
Data Selection
We will show that this fair dis-
tribution of rewards will bring extra benefits in
data selection. Figure 4 shows the performance
on AlpacaEval2 when using different RMs to se-
lect samples. From the figure, we can draw two
conclusions: (1) When sampling the same num-
ber of samples, FR RM and FC RM can select
higher quality samples compared to BT RM. (2) To
achieve the same performance, FR RM and FC RM
require fewer samples, indicating higher sampling
efficiency.
Social Bias
We further validated our methods
on the CrowS-Pairs1 (Nangia et al., 2020) dataset,
which includes sentences with social biases. This
dataset contains two types of sentences: “sent
more", which is more stereotypical, and “sent less",
which is less stereotypical. It encompasses nine
types of biases, such as gender and nationality. As
shown in Figure 5, the BT RM tends to assign
higher rewards to the more stereotypical sentences,
resulting in a larger distributional difference be-
tween “sent more" and “sent less". This indicates
that the BT RM exhibits unfairness across various
social biases. In contrast, FR RM and FC RM show
smaller distributional differences, demonstrating
greater fairness across different social biases.
These findings highlight the effectiveness of our
Fair RMs in providing a fairer reward distribution
1https://github.com/nyu-mll/crows-pairs/
while maintaining high performance and sampling
efficiency.
6.1.2
Fairness Policy Model
Table 2: Performance of different policy models on
AlpacaEval2 and MT-Bench.
AlpacaEval2
MT-Bench
LC WR
WR
Overall
SFT
14.34
8.17
5.93
R-DPO
20.87
11.16
6.48
KTO
19.44
16.64
6.64
DPO
16.71
14.23
6.46
+FR
20.48
15.74
6.70
LLaMA3
+FC
21.10
16.96
6.58
SFT
13.47
8.11
5.69
R-DPO
19.95
10.15
7.05
KTO
17.81
14.39
6.72
DPO
18.93
13.18
6.59
+FR
21.05
15.25
7.24
Qwen2.5
+FC
19.72
14.53
7.00
Table 2 presents the results of different policy
models on AlpacaEval2 and MT-Bench. It can be
observed that our fairness reward methods, when
combined with DPO, consistently demonstrates su-
perior performance on AlpacaEval2 and MT-Bench
with both LLaMA3-SFT and Qwen2.5-SFT as base
models. This highlights the effectiveness of our
fairness rewards method. The success of our meth-
7


--- Page 8 ---
ods can be attributed to their ability to implicitly fit
a fairness RM during the training of policy models,
thereby generating higher quality outputs.
Combining Fair DPOs with Fair RM further en-
hances performance. We sample the policy model
1, 8, 16, 32, and 64 times, using Fair RM to se-
lect the best sample. We recorded the lengths and
performance of these samples and fitted a curve
to this data, as shown in Figure 6. It can be ob-
served that for the same model, performance grad-
ually increases with length, indicating a correlation
between performance and length. However, for
different models, aligned models produce higher
quality outputs compared to the SFT model, but
their outputs are also longer. Among the three
aligned models, Fair DPOs achieve better perfor-
mance than DPO while producing shorter outputs,
suggesting that our model can mitigate length bias
to some extent.
1100
1200
1300
1400
1500
1600
1700
1800
Avg. Length
14
16
18
20
22
24
26
LC Win Rate
SFT
DPO
FR DPO
FC DPO
Figure 6: Length and performance relationships of sam-
ples for different models.
6.2
Ablation Study (RQ2)
Table 3: Performance under different fairness functions.
Model
AlpacaEval2
MT-Bench
LC WR
WR
Overall Score
DPO
16.71
14.23
6.46
FR DPO
τ = −5
19.72
14.44
6.59
τ = −1
20.48
15.74
6.70
τ = 0.5
20.01
15.21
6.56
τ = 2
20.01
17.35
6.69
τ = 10
19.98
16.24
6.62
Impact of Fairness Function
Eq (7) presents a
unified metric for measuring fairness, from which
different fairness functions can be derived by vary-
ing τ. We aim to explore the impact of different
fairness functions on performance. We experiment
with various τ values within the range of [-5, 10],
and the results are summarized in Table 3. It can
be observed that the performance of FR DPO con-
sistently surpasses that of the native DPO across all
fairness functions. This indicates that our method is
robust to variations in τ. The reason for this robust-
ness is that all fairness functions derived from the
unified metric satisfy the three desired properties,
ensuring that the rewards obtained are fair.
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
17
18
19
20
21
AlpacaEval2
6.45
6.50
6.55
6.60
6.65
6.70
MT-Bench
AlpacaEval2
MT-Bench
Figure 7: Performance under different fairness contribu-
tion α.
Impact of Fairness Contribution α
We con-
duct experiments by fixing τ = −1 and varying
the fairness contribution α within the range of [0,
0.15], as illustrated in Figure 7. When α = 0,
the model reduces to the native DPO. As α in-
creases, the model’s performance on AlpacaEval2
and MT-Bench initially improves and then declines.
This trend occurs because, at lower values of α, en-
hancing fairness contributes to better output quality.
However, when α becomes too large, the excessive
emphasis on fairness leads to a compromise in util-
ity. Considering the trade-off between fairness and
utility, we typically set α = 0.1 in practical experi-
ments. We present the ablation experiments on the
Fairness Contribution γ in Appendix C.
7
Conclusion
In this paper, we tackle the critical issue of reward
unfairness in RLHF. We identify that length bias
and category bias are specific case of the broader
problem of reward unfairness.
To address this
comprehensively, we introduce the reward fairness
framework, which models preference learning as
a resource allocation problem to balance fairness
and utility in reward distribution. We propose two
methods to achieve fairness rewards: Fairness Reg-
ularization and Fairness Coefficient. These meth-
8


--- Page 9 ---
ods are applied in two key scenarios: training a
fairness RM for verification and training a policy
model for reinforcement learning that implicitly
incorporates fair rewards.
Limitation
We investigate the issue of rewards unfairness and
proposed a solution from the perspective of re-
source allocation, validating our approach in both
verification and RL scenarios. The limitations of
this study are summarized as follows: (1) Reward
unfairness is a broad concept, and this paper pri-
marily focuses on category bias and length bias,
with a simple validation on social bias. However,
reward unfairness may be related to various issues
in reward models, such as reward hacking. (2)
Our Fairness Rewards method can seamlessly in-
tegrate with RM and RL frameworks that utilize
RMs either explicitly or implicitly. We have only
validated it on BT models and DPO, but the Fair-
ness Rewards method has the potential for broader
applications.
Acknowledgements
We would like to express our sincere gratitude
to all the anonymous reviewers for their invalu-
able feedback that greatly improved this paper.
In particular, special thanks to Dr. Weiran Shen
for his insightful suggestions and invaluable as-
sistance in early-stage discussions. This research
was supported by National Natural Science Foun-
dation of China (No.62476277), National Key Re-
search and Development Program of China (NO.
2024YFE0203200), CCF-ALIMAMA TECH Kan-
garoo Fund(No.CCF-ALIMAMA OF 2024008),
and Huawei-Renmin University joint program on
Information Retrieval. We also acknowledge the
support provided by the fund for building world-
class universities (disciplines) of Renmin Univer-
sity of China and by the funds from Beijing Key
Laboratory of Big Data Management and Analy-
sis Methods, Gaoling School of Artificial Intelli-
gence, Renmin University of China, from Engineer-
ing Research Center of Next-Generation Intelligent
Search and Recommendation, Ministry of Educa-
tion, from Intelligent Social Governance Interdisci-
plinary Platform, Major Innovation & Planning In-
terdisciplinary Platform for the “DoubleFirst Class”
Initiative, Renmin University of China, from Pub-
lic Policy and Decision-making Research Lab of
Renmin University of China, and from Public Com-
puting Cloud, Renmin University of China.
References
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. Arxiv
Preprint Arxiv:2204.05862.
Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39:324–345.
Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia,
Tianyi Zhou, Tom Goldstein, Heng Huang, Moham-
mad Shoeybi, and Bryan Catanzaro. 2024. Odin: Dis-
entangled reward mitigates hacking in rlhf. In Forty-
first International Conference on Machine Learning.
Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,
Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,
Caiming Xiong, and Tong Zhang. 2024. Rlhf work-
flow: From reward modeling to online Rlhf. Trans-
actions on Machine Learning Research.
Yann Dubois, Balázs Galambosi, Percy Liang, and Tat-
sunori B Hashimoto. 2024a. Length-controlled al-
pacaeval: A simple way to debias automatic evalua-
tors. Arxiv Preprint Arxiv:2404.04475.
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi
Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,
Percy S Liang, and Tatsunori B Hashimoto. 2024b.
Alpacafarm: A simulation framework for methods
that learn from human feedback. Advances in Neural
Information Processing Systems, 36.
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Dan Jurafsky, and Douwe Kiela. 2024. Model align-
ment as prospect theoretic optimization. In Forty-first
International Conference on Machine Learning.
Isabel O Gallegos, Ryan A Rossi, Joe Barrow,
Md Mehrab Tanjim, Sungchul Kim, Franck Dernon-
court, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed.
2024. Bias and fairness in large language models: A
survey. Computational Linguistics, 50:1097–1179.
Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-
ing laws for reward model overoptimization. In Pro-
ceedings of the 40th International Conference on Ma-
chine Learning, volume 202, pages 10835–10866.
William M Hayes, Nicolas Yax, and Stefano Palminteri.
2024. Large language models are biased reinforce-
ment learners. Arxiv Preprint Arxiv:2405.11422.
Zhengxu Hou, Bang Liu, Ruihui Zhao, Zijing Ou, Yafei
Liu, Xi Chen, and Yefeng Zheng. 2021. Imperfect
also deserves reward: Multi-level and sequential re-
ward modeling for better dialog management. Arxiv
Preprint Arxiv:2104.04748.
9


--- Page 10 ---
Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu,
Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang,
Wenhan Lyu, Yixuan Zhang, Xiner Li, Hanchi Sun,
Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun
Zhang, Bertie Vidgen, Bhavya Kailkhura, Caim-
ing Xiong, Chaowei Xiao, Chunyuan Li, Eric P.
Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi
Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis,
Marinka Zitnik, Meng Jiang, Mohit Bansal, James
Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han,
Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin
Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-
Wei Chang, Lifang He, Lifu Huang, Michael Backes,
Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen,
Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji,
Suman Jana, Tianlong Chen, Tianming Liu, Tianyi
Zhou, William Yang Wang, Xiang Li, Xiangliang
Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu
Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen,
and Yue Zhao. 2024. Position: TrustLlm: Trustwor-
thiness in large language models. In Proceedings of
the 41st International Conference on Machine Learn-
ing, volume 235, pages 20166–20270.
Rajendra K Jain, Dah-Ming W Chiu, William R Hawe,
et al. 1984. A quantitative measure of fairness and
discrimination. Eastern Research Laboratory, Digi-
tal Equipment Corporation, Hudson, Ma, 21:1.
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau,
José Miguel Hernández-Lobato, Richard E Turner,
and Douglas Eck. 2017. Sequence tutor: Conserva-
tive fine-tuning of sequence generation models with
kl-control. In International Conference on Machine
Learning, pages 1645–1654.
Naoki Katoh and Toshihide Ibaraki. 1998. Resource
allocation problems. Handbook of Combinatorial
Optimization: Volume1–3, pages 905–1006.
Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke
Hüllermeier. 2023.
A survey of reinforcement
learning from human feedback.
Arxiv Preprint
Arxiv:2312.14925.
Sungdong Kim and Minjoon Seo. 2024. Rethinking the
role of proxy rewards in language model alignment.
In Proceedings of the 2024 Conference on Empiri-
cal Methods in Natural Language Processing, pages
20656–20674.
A Kumar and J Kleinberg. 2000. Fairness measures for
resource allocation. In Proceedings 41st Annual Sym-
posium on Foundations of Computer Science, pages
75–75.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
et al. 2024.
Rewardbench:
Evaluating reward
models for language modeling.
Arxiv Preprint
Arxiv:2403.13787.
Tian Lan, David Kao, Mung Chiang, and Ashutosh
Sabharwal. 2010. An axiomatic theory of fairness
in network resource allocation. In 2010 Proceedings
IEEE INFOCOM, pages 1–9.
Jiahui Li, Hanlin Zhang, Fengda Zhang, Tai-Wei Chang,
Kun Kuang, Long Chen, and Jun Zhou. 2024. Opti-
mizing language models with fair and stable reward
composition in reinforcement learning. In Proceed-
ings of the 2024 Conference on Empirical Methods in
Natural Language Processing, pages 10122–10140.
Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying
Wang. 2023. A survey on fairness in large language
models. Arxiv Preprint Arxiv:2308.10149.
Wei Liu, Yang Bai, Chengcheng Han, Rongxiang Weng,
Jun Xu, Xuezhi Cao, Jingang Wang, and Xunliang
Cai. 2024. Length desensitization in directed prefer-
ence optimization. Arxiv Preprint Arxiv:2409.06411.
Junru Lu, Jiazheng Li, Siyu An, Meng Zhao, Yulan
He, Di Yin, and Xing Sun. 2024. Eliminating bi-
ased length reliance of direct preference optimiza-
tion via down-sampled kl divergence. Arxiv Preprint
Arxiv:2406.10957.
Yu
Meng,
Mengzhou
Xia,
and
Danqi
Chen.
2024.
Simpo:
Simple preference optimization
with a reference-free reward.
Arxiv Preprint
Arxiv:2405.14734.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. Crows-Pairs: A Chal-
lenge Dataset for Measuring Social Biases in Masked
Language Models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback.
Advances in Neural
Information Processing Systems, 35:27730–27744.
Vishakh Padmakumar, Chuanyang Jin, Hannah Rose
Kirk, and He He. 2024. Beyond the binary: Captur-
ing diverse preferences with reward regularization.
Arxiv Preprint Arxiv:2412.03822.
Ryan Park, Rafael Rafailov, Stefano Ermon, and
Chelsea Finn. 2024. Disentangling length from qual-
ity in direct preference optimization. In Findings of
the Association for Computational Linguistics: ACL
2024, pages 4998–5017.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neu-
ral Information Processing Systems, 36.
Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert
Dadashi, Léonard Hussenot, Pierre-Louis Cedoz,
Pier Giuseppe Sessa, Sertan Girgin, Arthur Douil-
lard, and Olivier Bachem. 2024a. Warp: On the
benefits of weight averaged rewarded policies. Arxiv
Preprint Arxiv:2406.16768.
10


--- Page 11 ---
Alexandre Ramé, Nino Vieillard, Léonard Hussenot,
Robert Dadashi, Geoffrey Cideron, Olivier Bachem,
and Johan Ferret. 2024b. Warm: On the benefits
of weight averaged reward models. Arxiv Preprint
Arxiv:2401.12187.
David Reber, Sean Richardson, Todd Nief, Cristina Gar-
bacea, and Victor Veitch. 2024. Rate: Score reward
models with imperfect rewrites of rewrites. Arxiv
Preprint Arxiv:2410.11348.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017.
Proxi-
mal policy optimization algorithms. Arxiv Preprint
Arxiv:1707.06347.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath:
Pushing the limits of mathematical reasoning in open
language models. Arxiv Preprint Arxiv:2402.03300.
Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shi-
han Dou, Tao Gui, Qi Zhang, and Xuanjing Huang.
2023. Loose lips sink ships: Mitigating length bias
in reinforcement learning from human feedback. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023, pages 2859–2873.
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov,
and David Krueger. 2022. Defining and characteriz-
ing reward gaming. Advances in Neural Information
Processing Systems, 35:9460–9471.
Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim,
and Seunghyeok Hong. 2024. Llm-as-a-judge & re-
ward model: What they can and cannot do. Arxiv
Preprint Arxiv:2409.11239.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems, 33:3008–
3021.
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao,
and Tong Zhang. 2024a. Interpretable preferences
via multi-objective reward modeling and mixture-of-
experts. Arxiv Preprint Arxiv:2406.12845.
Zhilin Wang, Alexander Bukharin, Olivier Delal-
leau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Olek-
sii Kuchaiev, and Yi Dong. 2024b.
Helpsteer2-
preference: Complementing ratings with preferences.
Arxiv Preprint Arxiv:2410.01257.
Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian
Xie, Chao Yu, Yu Wang, Dong Yan, and Yuan Shen.
2024. Reward-robust rlhf in llms. Arxiv Preprint
Arxiv:2409.15360.
Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han
Zhong, Dong Yu, and Jianshu Chen. 2024. Rewards-
in-context: Multi-objective alignment of founda-
tion models with dynamic preference adjustment.
In Forty-first International Conference on Machine
Learning.
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran
Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024.
Generative verifiers: Reward modeling as next-token
prediction. Arxiv Preprint Arxiv:2408.15240.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems, 36:46595–46623.
11


--- Page 12 ---
A
Further Discussions
Further explanation of Figure 1
Someone may argue that “ yl
1 could be a better response than yw
2 ”.
Our objective is to achieve both utility and fairness in reward allocation in RLHF. Figure 2 in our paper
effectively illustrates our objective. Considering an example where four responses (y1, y2, y3, and y4)
are sampled for the same prompt, with rewards ranked as y1 > y2 > y3 > y4. For preference data pairs
(y1, y2) and (y3, y4), it is reasonable that y2 > y3. This does not conflict with our objective, as utility
measures the alignment of reward allocation with human preferences, meaning that a good response should
receive a higher reward than a poor one. Fairness focuses on the reward distribution across different types
of samples, typically from various domains. These absolute reward values are generally incomparable,
but we expect their distributions to be as fair as possible to avoid issues in downstream scenarios such
as rejected sampling and RL. Unfortunately, our experiments reveal that the commonly used BT model
exhibits pervasive reward unfairness, as shown in Figure 3 and Figure 8. This unfairness affects both
in-distribution and out-of-distribution data, leading to category and length biases. Additional, Table 4
presents the average rewards from the Bradley-Terry (BT) model on the HH-RLHF dataset. For both
“helpful” and “harmless” data, the rewards for “chosen” are greater than those for “rejected”, aligning with
the utility objective. However, the rewards for “helpful” are significantly higher than those for “harmless”,
which is unfair. When such unfair rewards are used in rejected sampling and RL, the model’s output
becomes more helpful but neglects harmlessness.
Table 4: Average rewards of BT model on the HH-RLHF dataset.
Helpful
Harmless
chosen
rejected
chosen
rejected
Avg. Reward
-1.39
-2.26
-4.15
-5.23
Fairness and Utility
Figure 2 illustrates our dual objectives: achieving fairness and utility in reward
allocation. The relationship between these objectives varies slightly between verification and reinforcement
learning scenarios. In the verification scenario, “fairness” aims to make the distribution of different types
of rewards more consistent, while “utility” ensures that for any given prompt, the reward for a good
response exceeds that for a bad response. These objectives are inherently independent and non-conflicting,
though balancing them necessitates a multi-objective optimization. In the reinforcement learning scenario,
"fairness" can even enhance "utility" by guiding the model’s output to be both more helpful and harmless.
B
Experiment Setting
Datasets & Baselines
In the verification scenario, we conduct experiments on two benchmarks: Reward
Bench (Lambert et al., 2024) and HH-RLHF (Bai et al., 2022). Our RMs are trained using the training
set from HH-RLHF2, making HH-RLHF an in-distribution (ID) benchmark, while Reward Bench serves
as an out-of-distribution (OOD) benchmark. We report the accuracy on both benchmarks. For the RL
scenario, we evaluate our methods on the AlpacaEval2 (Dubois et al., 2024a) and MT-Bench (Zheng et al.,
2023) benchmarks. We provide results for AlpacaEval2 in terms of Length-controlled Win Rate (LC WR)
and Win Rate (WR), and for MT-Bench, we report the overall score. For the policy model, we utilize
the UltraFeedback Binarized3 and SHP4 datasets for training. We train the policy model using different
methods such as DPO (Rafailov et al., 2024), KTO (Ethayarajh et al., 2024) and R-DPO (Park et al., 2024)
with HALOs5.
2https://huggingface.co/datasets/Anthropic/hh-rlhf
3https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized
4https://huggingface.co/datasets/stanfordnlp/SHP
5https://github.com/ContextualAI/HALOs/
12


--- Page 13 ---
Training Setting
For the reward model, we train on the HH-RLHF training set for one epoch with a
learning rate of 2×10−6 and a global batch size of 256. For the policy model, we utilize the UltraFeedback
Binarized and SHP datasets, training for one epoch with a learning rate of 5×10−6 and a global batch size
of 256. During sampling with the policy model, the temperature coefficient is set to 1. All experiments
are performed on an 8 × H800 machine. Both the reward models and the policy models are trained using
LLaMA3-SFT6 (a base model developed by Dong et al. (2024)) and Qwen2.5-SFT (a base model we
trained7 following Dong et al. (2024)). Qwen2.5-SFT is a model we trained based on the Qwen2.5-7B
base using the dataset from RLHFow8 for one epoch. The global batch size was set to 128, and the
learning rate was 2 × 10−5. For all policy models, the β parameter was uniformly set to 0.1. Additionally,
the desirable weight and undesirable weight for the KTO were both set to 1.
C
Supplement Experiment
Rewards on Length
The rewards distribution of BT RM, FR RM, and FC RM across different lengths
on the HH-RLHF dataset is illustrated in Figure 8. Our FR RM and FC RM exhibits a more consistent
rewards distribution across varying lengths, demonstrating that our method effectively mitigates length
bias.
12.5
10.0
7.5
5.0
2.5
0.0
2.5
Rewards
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Density
Tokens < 50 
Tokens >= 300
(a) BT rewards on length.
2
0
2
4
6
8
10
Rewadrs
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Density
(b) FR rewards on length.
10
8
6
4
2
0
2
Rewadrs
0.00
0.05
0.10
0.15
0.20
0.25
Density
(c) FC rewards on length.
Figure 8: Rewards on Length
Impact of Fairness Contribution γ
We conduct experiments by fixing τ = −1 and varying the fairness
contribution γ within the range of [0, 1.5], as illustrated in Figure 9. We obtain conclusions similar to
those with fairness contribution α. Considering the trade-off between fairness and utility, we typically set
γ = 0.5 in practical experiments.
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
17
18
19
20
21
AlpacaEval2
6.46
6.48
6.50
6.52
6.54
6.56
6.58
MT-Bench
AlpacaEval2
MT-Bench
Figure 9: Performance under different fairness contribution γ.
6https://huggingface.co/RLHFlow/LLaMA3-SFT
7https://github.com/RLHFlow/Online-RLHF/
8https://huggingface.co/datasets/RLHFlow/RLHFlow-SFT-Dataset-ver2
13
