--- Page 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
1
Hierarchical Online-Scheduling for Energy-Efficient
Split Inference with Progressive Transmission
Zengzipeng Tang, Student Member, IEEE, Yuxuan Sun, Member, IEEE, Wei Chen, Senior Member, IEEE,
Jianwen Ding, Bo Ai, Fellow, IEEE and Yulin Shao, Member, IEEE
Abstract—Device-edge collaborative inference with Deep Neu-
ral Networks (DNNs) faces fundamental trade-offs among ac-
curacy, latency and energy consumption. Current scheduling
exhibits two drawbacks: a granularity mismatch between coarse,
task-level decisions and fine-grained, packet-level channel dynam-
ics, and insufficient awareness of per-task complexity. Conse-
quently, scheduling solely at the task level leads to inefficient re-
source utilization. This paper proposes a novel ENergy-ACcuracy
Hierarchical optimization framework for split Inference, named
ENACHI, that jointly optimizes task- and packet-level scheduling
to maximize accuracy under energy and delay constraints. A
two-tier Lyapunov-based framework is developed for ENACHI,
with a progressive transmission technique further integrated
to enhance adaptivity. At the task level, an outer drift-plus-
penalty loop makes online decisions for DNN partitioning and
bandwidth allocation, and establishes a reference power budget
to manage the long-term energy-accuracy trade-off. At the packet
level, an uncertainty-aware progressive transmission mechanism
is employed to adaptively manage per-sample task complexity.
This is integrated with a nested inner control loop implementing
a novel reference-tracking policy, which dynamically adjusts per-
slot transmit power to adapt to fluctuating channel conditions.
Experiments on ImageNet dataset demonstrate that ENACHI
outperforms state-of-the-art benchmarks under varying dead-
lines and bandwidths, achieving a 43.12% gain in inference
accuracy with a 62.13% reduction in energy consumption under
stringent deadlines, and exhibits high scalability by maintaining
stable energy consumption in congested multi-user scenarios.
Index Terms—Edge AI, split inference, resource allocation,
energy efficiency, Lyapunov optimization
I. INTRODUCTION
T
HE deployment of Deep Neural Networks (DNNs) on
smart mobile devices is driving a fundamental shift
toward edge Artificial Intelligence (AI), essential for support-
ing real-time inference in applications like augmented reality
(AR), virtual reality (VR), and advanced perception networks
for the Industrial Internet of Things (IIoT). These applica-
tions involve continuous AI task generation and require high-
throughput visual perception, creating a significant demand
for on-device, context-aware intelligence [1]. Concurrently, the
rise of paradigms like Integrated Sensing and Edge AI (ISEA)
[2] further intensifies this trend, demanding an unprecedented
level of tight coupling between environmental sensing, real-
time computation, and ultra-reliable communication to deliver
Zengzipeng Tang, Yuxuan Sun (Corresponding Author), Wei Chen, Jianwen
Ding and Bo Ai are with the School of Electronic and Information Engineer-
ing, Beijing Jiaotong University, Beijing 100044, China. (e-mail: {zzptang,
yxsun, weich, jwding, boai}@bjtu.edu.cn)
Yulin Shao is with the Department of Electrical and Electronic Engineering,
The University of Hong Kong (e-mail: ylshao@hku.hk).
context-aware intelligence [3], which is increasingly viewed
as a key enabler for 6G intelligent perception. This trend,
however, faces a core conflict with the physical limitations of
edge devices. The pursuit of state-of-the-art performance has
led to an exponential increase in model size and computational
complexity [4]. This creates a major energy challenge for
mobile devices [5], which often have severe constraints on
battery capacity and processing power, hindering their ability
to independently run large-scale AI models.
To tackle this, the device-edge collaborative paradigm of
split inference, enabled by Edge Intelligence (EI) [6], has
emerged as a promising solution. By partitioning a DNN
between the device and the edge server, this architecture
offloads computation-intensive layers to the edge, thereby
alleviating the processing load of the device. However, it does
not fundamentally resolve the energy and latency challenges,
as both local computation and wireless transmission remain
power-consuming. Moreover, task success is often constrained
by strict real-time requirements. In latency-sensitive applica-
tions such as AR and industrial automation [7], inference
results lose their utility once deadlines are missed, turning
latency from a soft performance metric into a hard Quality-
of-Service (QoS) constraint.
Optimizing inference accuracy and energy performance for
deadline-critical AI tasks is highly challenging. Key opera-
tional decisions, such as where to partition the DNN, how
to allocate wireless resources, and how much intermediate
feature data to transmit, create a complex interplay between
inference accuracy, end-to-end latency, and device energy
consumption. An attempt to improve one metric, such as
increasing accuracy by sending more data, can negatively im-
pact others by increasing energy use and potentially violating
the task deadline. Compounding this difficulty is the non-
analytical nature of DNNs, where the specific mapping of
inference accuracy and energy consumption lacks a closed-
form expression. Such inherent conflicts of tight coupling and
model intractability render the optimization of the energy-
accuracy trade-off exceptionally difficult. This challenge is
further magnified in a multi-user EI system, where multiple
devices must simultaneously compete for limited wireless
bandwidth and shared edge-server computational resources.
Recent studies have explored joint optimization frameworks
to balance latency, energy consumption, and QoS in edge in-
ference. These frameworks build upon pioneering research that
established task offloading through online resource optimiza-
tion [8]–[10] and strategies for model pruning or partitioning
to reduce computation and communication costs [11], [12].
arXiv:2601.08135v1  [cs.NI]  13 Jan 2026


--- Page 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
2
Motivated by the pressing demand for energy efficiency in
green edge AI [13], representative efforts include dynamic par-
titioning schemes with early exits via reinforcement learning
[14] and Lyapunov-based methods for long-term energy and
latency management through joint partitioning and resource
allocation [15]. Further advancements investigate semantic
communication-based offloading [16] and layer-level DNN
partitioning with distributed game-theoretic optimization [17].
In multi-user scenarios, batching strategies are widely em-
ployed to enhance server throughput [18], and similar batch-
processing techniques are utilized to reduce collective energy
overheads [19]. To address the challenge of performance
estimation, [20] focuses on performance modeling for energy-
aware scheduling. In video analytics and AR, researchers
model the relationship between transmitted data and inference
accuracy using curve fitting or configuration adaptation to
address the non-analytical nature of DNNs [21]–[23].
Recent advancements further broaden the optimization
scope to environmental dynamics and new system paradigms.
Considering the dynamic nature of edge environments, robust
optimization is introduced to address uncertainties in comput-
ing capacity [24] and inference delay [25]. In the emerging
paradigm of ISEA, research targets ultra-low-latency edge
inference for distributed sensing [26]. Complementary studies
also investigate event-triggered offloading strategies to effec-
tively mitigate communication overheads [27]. Furthermore,
hierarchical scheduling is employed to separates long-term
energy provisioning from short-term task offloading decisions
in multi-tier edge systems [28]. Similarly, a two-timescale
approach in satellite edge computing [29] decouples slow
service deployment optimization from fast task scheduling.
Despite notable progress in comprehensive resource and
quality management, contemporary split-inference frameworks
still face two critical challenges:
(1) Mismatch in scheduling granularity. The high dimen-
sionality of intermediate representations in modern DNNs,
such as the 256 × 56 × 56 feature maps in ResNet, totaling over
8 million real-valued parameters [30], creates a severe commu-
nication bottleneck over bandwidth-limited wireless links. To
alleviate this issue, a viable approach is to employ a progres-
sive feature transmission strategy [31], wherein intermediate
feature representations are transmitted in multiple adaptive
stages. At each stage, the edge server selectively receives
the most informative feature dimensions and decides whether
to continue transmission based on feedback about accuracy
improvement or channel quality. This fine-grained, feedback-
driven mechanism significantly improves transmission effi-
ciency and enables slot-wise adaptation to dynamic wireless
conditions, which naturally aligns with the fine-grained, slot-
by-slot nature of wireless resource allocation. While the mech-
anism improves communication efficiency, integrating such
packet-level transmission mechanism with macro task-level
resource allocation remains difficult, revealing a fundamental
mismatch in scheduling granularity.
(2) Lack of task-aware adaptation. Most existing split-
inference frameworks adopt uniform offloading strategies that
treat all inference tasks equally, neglecting the inherent hetero-
geneity among them. In practice, tasks often differ in impor-
tance, input complexity, and feature representation sparsity, re-
sulting in distinct transmission priorities and accuracy-energy
trade-offs. For instance, tasks with higher semantic importance
or denser features may require deeper layer transmission or
finer feature granularity, whereas simpler tasks can be satisfied
with partial feature subsets [32], [33]. Without explicitly
modeling such task-specific variations, current frameworks
fail to allocate communication and computation resources
efficiently across heterogeneous workloads. This lack of task-
aware adaptation ultimately restricts both inference accuracy
and energy efficiency in dynamic edge environments.
In this paper, we consider a multi-user EI system, where
users collaborate with the edge server for split inference tasks.
To cope with the constraint of scarce wireless and local com-
putation resources, we propose a novel ENergy-ACcuracy Hi-
erarchical optimization for split-Inference framework, named
ENACHI. We formulate an optimization problem to maximize
the aggregate inference accuracy of all users, while ensuring
energy consumption stability and satisfying hard deadline con-
straints. The framework uses a two-tier Lyapunov architecture,
integrating progressive transmission to handle varying task
complexity within the stochastic optimization. At the task
level, an outer loop decides DNN partitioning and bandwidth
allocation while setting a reference power budget. At the
packet level, a nested inner loop employs an uncertainty-
aware progressive transmission mechanism, integrated with a
reference-tracking policy to adjust per-slot transmit power for
coping with channel fluctuations.
The main contributions are summarized as follows:
• We propose a novel hierarchical optimization frame-
work ENACHI to address the joint optimization of
long-term energy consumption and inference accuracy
in deadline-critical split inference, which coordinates
task-level scheduling and packet-level transmission by
employing a Lyapunov-based nested drift-plus-penalty
method and integrates adaptive progressive transmission
with batched inference.
• To address the intracable nature of long-term stochastic
problem and the non-analytical and non-convex inference
accuracy objective, we develop a tractable surrogate
model, which enables efficient online task-level schedul-
ing while providing theoretical performance guarantees
for long-term system stability and performance.
• We design a nested inner control loop to solve the
scheduling granularity mismatch between task-level de-
cisions and packet-level channel dynamics, which imple-
ments a task-aware, reference-tracking policy that tracks
the power budget from task-level to ensure consistency,
while adapting to channel conditions and task complexity.
• We conduct extensive simulations on the ImageNet
dataset [34] to validate our framework. The results
demonstrate that ENACHI outperforms benchmarks by
achieving a favorable energy-accuracy trade-off and ro-
bust scalability, especially under stringent deadlines and
in congested multi-user scenarios.


--- Page 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
3
...
...
...
 
 
 
DOG
BIRD
FENCE
...
...
User devices
Progressive packet 
transmission
Batched edge inference
Results
Local inference
Frame m with task deadline T
...
BS and edge server
Sequential task arrivals
Fig. 1: Illustration of the multi-user split inference system.
II. SYSTEM MODEL
We consider a device-edge collaborative edge AI system,
as illustrated in Fig. 1, where n computation-limited devices
collected in a set of N = {1, ..., N}, cooperate with an edge
server to execute DNN-based inference tasks. For flexibility,
pre-trained DNN models of different types are cached in both
the devices and the edge server.
The system time is structured into discrete frames. Each
task arrives at the beginning of frame m ∈M, where
M = {1, 2, . . . , M}, with a deadline T. Typical use cases in-
clude AR, where sequential image or video frames necessitate
periodic processing to overlay digital information, or industrial
IoT, where automated inspections must be performed on items
on a production line at regular intervals. Consistent with these
dedicated application scenarios, we assume all users execute
tasks of a homogeneous type, i.e., leveraging an identical DNN
architecture, throughout the entire operational period. Within
each frame, a device may perform inference locally or engage
in collaborative processing with the edge server.
A. Split Inference Model
The DNN model of task in frame m consists of km
sequential layers. For device n, when collaborative infer-
ence is performed, the model is partitioned at a layer sn,m
selected from a predefined set of feasible partition points,
S = {0, 1, 2, . . . , km}, such that layers 0 to sn,m are executed
locally on the device, while layers sn,m + 1 through km
are offloaded and executed at the edge server. Specifically,
sn,m = 0 indicates that the entire task is fully offloaded to the
edge server for execution, whereas sn,m = km corresponds to
fully local execution without offloading.
The computational complexity of AI inference tasks is
quantified in terms of the number of multiply-accumulate
operations (MACs) [23]. For a given task in frame m with
partition point sn,m, the total task workload is denoted by
Rm (in MACs), and the number of intermediate feature maps
is represented as btotal(sn,m). Typically, the size of the final
output is far smaller than the input or intermediate features
and can thus be neglected in transmission considerations. [35].
We denote fn,m as the local computing ability of the n-
th user device during frame m. The correspondingly local
computing delay on frame m can be written as:
tlocal
n,m =
Rlocal
sn,m
fn,m
,
(1)
where Rlocal
sn,m represents the local computing workload of de-
vice n when choosing partition point sn,m. The corresponding
energy consumption is
Elocal
n,m = αnf 3
n,mtlocal
n,m = αnf 2
n,mRlocal
sn,m,
(2)
where αn is a coefficient determined by the corresponding
device chip architecture [36].
After completing the local computation portion of the
inference task, each element of feature maps is quantized with
a sufficiently high resolution of D bits, rendering quantization
errors negligible. We divide task transmission time into time
slots of duration tslot seconds (typically 1ms), which represent
the typical scheduling interval in contemporary wireless com-
munication systems [37], and the wireless channel is assumed
to remain constant within each time slot. For each frame m,
device n is allocated a dedicated narrowband channel with an
allocated bandwidth of ωn,m that remains constant across all
slots. Correspondingly, its achievable transmission rate in slot
k is given by the Shannon capacity formula:
rn,m,k = ωn,m log2

1 + hn,m,kpn,m,k
N0ωn,m

,
(3)
where hn,m,k is the channel gain and pn,m,k is the transmit
power of user n in slot k. Note that the rate rn,m,k still varies
across slots because the channel gain and transmit power can
change. N0 is the Gaussian noise power spectral density.
We define the transmission unit as a packet and one such
packet is transmitted per time slot. This packet serves as
a container for one or multiple feature maps, with bn,m,k
indicating their exact number. This number is constrained by
the channel capacity and the size of an individual feature
map, which depends on the partition point sn,m. Specifically,
a feature map is represented as an Lh
s × Lw
s matrix, therefore
the value of bn,m,k is then calculated as the total transmissible
bits in a slot divided by the size of a single feature map:
bn,m,k =
rn,m,ktslot
DLhsLws

.
(4)


--- Page 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
4
Untransmitted 
feature maps
Untransmitted 
feature maps
Slot k
Slot k+1
Accuracy 
evaluation
Slot k+2
Untransmitted 
feature maps
Accuracy 
evaluation
Server
Inference
Accuracy 
evaluation
Reshape
FC
Reshape
FC
Concatenate
FC
FC
FC
Index(            )
Predictor  hs(·|As)
Reshape
FC
Reshape
FC
Concatenate
FC
FC
FC
Index(            )
Predictor  hs(·|As)
Total received 
packets
Total received 
packets
Total received 
packets
Total received 
packets
Server sub-model

Fig. 2: Illustration of the progressive packet transmission mechanism.
B. Task-aware Progressive Packet Transmission Model
After local computation, the device uploads the intermediate
feature maps generated by the local sub-model to the edge
server. To avoid inter-device interference, we adopt frequency
division multiple access (FDMA) for offloading. Furthermore,
both small-scale Rayleigh fading and large-scale path loss are
considered in the device–server uplink channel model.
We emphasize that in delay-critical AI inference tasks, the
total latency budget is typically insufficient to allow trans-
mission of all intermediate features. Moreover, for a given
task type, the inference accuracy varies significantly across
different input samples based on their intrinsic complexity.
However, from the perspective of the device, it only has statis-
tical priors and cannot deterministically model this per-sample
complexity, and may waste resources on simple samples or fail
to achieve sufficient accuracy on complex ones.
To address the challenges and adapt to per-sample complex-
ity, we adopt a progressive packet transmission mechanism
with server-side stopping control, as illustrated in Fig. 2. For
each task, the device sequentially transmits the most informa-
tive feature map packets. At the end of each slot, the server
evaluates the inference confidence based on the cumulative
packets received thus far. The transmission process terminates
once this confidence exceeds a predefined threshold.
Following [31], we measure confidence using the predictive
uncertainty of the inference. To this end, a lightweight MLP
predictor hs(Xn,m,k | Λs) is trained to estimate the uncertainty
based on the feature map packets Xn,m,k received up to slot
k, where Λs denotes the model parameters. Its runtime is
negligible compared to that of the main task inference model.
The uncertainty is quantified by the predictive entropy:
H(Xn,m,k) ≜−
L
X
l=1
Pr(l|Xn,m,k) log Pr(l|Xn,m,k),
(5)
where L denotes the number of classes and Pr(l|Xn,m,k) is
the posterior probability of class l. The predictor hs is trained
to approximate H(Xn,m,k) by minimizing the loss between
its output and the true entropy. The detailed implementation
of this predictor will be discussed in Section III-C.
We define kn,m as the total number of slots spent on the
actual transmission, which corresponds to the frame duration
excluding the local and edge computation times. Accordingly,
the communication energy consumption can be expressed as:
Etr
n,m =
kn,m
X
k=1
pn,m,ktslot,
(6)
where pn,m,k stands for the transmit power for device n in
frame m, slot k.
Since the edge server is generally powered by a stable
power grid, the energy consumption of edge computation
is not considered in this work. Therefore, the total energy
consumption on frame m for device n is
En,m = Elocal
n,m+Etr
n,m = αnf 2
n,mRlocal
sn,m+
kn,m
X
k=1
pn,m,ktslot. (7)
C. Batched Task Execution Model at the Edge
Upon reception of feature map packets, the edge server,
equipped with cached DNN models, executes the remaining
layers of the inference task and may sends the inference results
back to the user device.
The edge server is assumed to have a fixed computing capa-
bility f edge
m
per frame, and the corresponding edge computing
delay given the edge computing workload Redge
sn,m similar to
that of the local computing part can be written as:
tedge
n,m = Redge
sn,m
f edge
m
.
(8)
To further enhance system efficiency and align with the
periodic nature of task arrivals, the edge server adopts a
lightweight batching mechanism. This mechanism synchro-
nizes the execution of DNN inference across all N devices
within each frame m. The key principle is to ensure that edge-
side inference for the batch starts only after all devices have
completed their feature packets transmission, thereby avoiding
server-side idle time and improving computational throughput.
This design is motivated by the common hard deadline T,
which naturally imposes a shared time constraint for all tasks
within the frame. The batch starting time, tbatch
m
, is determined
by the device with the largest edge inference delay, as this
device dictates the latest possible moment the server can begin
computation while still meeting all deadlines. Formally, tbatch
m
is expressed as:
tbatch
m
= tframe
m
+ T −tedge
max,
(9)
where tframe
m
denotes the starting time of the current frame,
and tedge
max = maxn∈N tedge
n,m represents the maximum edge
computation delay, as defined in (8), among all devices,
ensuring that the edge inference part of each task can be
completed within the delay bound.
This start time tbatch
m
also defines the hard transmission
deadline for each device. A task for device n is considered


--- Page 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
5
successful if its feature transmission is completed before tbatch
m
.
Conversely, if additional time beyond tbatch
m
would be required
for transmission, i.e., tlocal
n,m + ttr
n,m > tbatch
m
−tframe
m
, the process
is forcibly terminated. Consequently, the edge server performs
inference using only the received features, inevitably resulting
in degraded accuracy or wasted energy resources.
D. Inference Task Accuracy Model
For a given task n in frame m and its corresponding DNN
partition point sn,m, the inference accuracy depends mainly
on the number of received feature maps Xn,m,k. According to
[21], [22], more Xn,m,k generally leads to better inference
accuracy, and the incremental gain diminishes as Xn,m,k
increases. Therefore, by defining An,m(sn,m, Xn,m,k) as a
monotonically non-decreasing function, the average accuracy
for all tasks in frame m can be expressed as:
Am = 1
N
X
n∈N
An,m(sn,m, Xn,m,k).
(10)
E. Problem Formulation
We aim to maximize the long-term average inference accu-
racy of all devices, while ensuring that long-term average local
energy consumption of each device remains below a prescribed
threshold ¯En:
P1 :
max
sm,ωm,pm
1
M
M
X
m=1
Am
(11a)
s.t.
1
M
M
X
m=1
En,m ≤¯En, ∀n ∈N,
(11b)
N
X
n=1
ωn,m ≤ω, ∀m ∈M,
(11c)
sn,m ∈{0, 1, 2, · · · , km},
(11d)
pn,m ∈(0, pmax],
(11e)
where the optimization variables are the DNN partition point
sm = [s1,m, ..., sN,m], the bandwidth allocated to each de-
vice ωm = [ω1,m, ..., ωN,m] and the transmit power pm =
[p1,m, ..., pN,m]. In the above problem, (11a) is the opti-
mization goal of maximizing the long-term task inference
accuracy, (11b) is the long-term energy constraint ensuring that
the computing and transmission energy of each task remains
within a stable range, and constraints (11c)-(11e) are used
to constrain the range of the optimization variables, where
ω is the total bandwidth allocated to the system in a frame.
Given the complexity of this mixed-integer nonlinear problem,
we omit adaptive CPU frequency scaling and concentrate on
optimizing the communication resources. Recall that in system
model we have assumed that feature transmission time is
predefined to be the remaining part of task deadline excluding
local and edge computing delays, so here we no longer list
the delay limit requirements.
Problem P1 is essentially a stochastic optimization problem.
Its intractability arises from several coupled challenges: (1)
The problem is inherently stochastic: the system aims to
optimize long-term inference accuracy and energy efficiency,
but at the beginning of each frame only the instantaneous
channel state is observable, while future channel conditions
are unknown and difficult to predict, making direct offline op-
timization extremely challenging. (2) The inference accuracy
function is non-analytical: although accuracy is known to be
non-decreasing with respect to the number of received feature
maps, its exact deterministic expression is unavailable, pre-
venting direct use in analytical optimization. (3) Some decision
variables are integer-constrained: for example, the selection
of partition points sn,m is inherently discrete. When combined
with the discrete local computational costs determined by the
DNN architecture, this results in a non-convex feasible region
and combinatorial complexity. Overall, problem P1 is a non-
convex mixed-integer stochastic optimization problem, which
is generally complex to solve.
III. ENACHI: ENERGY-ACCURACY HIERARCHICAL
OPTIMIZATION ALGORITHM FOR SPLIT INFERENCE
In this section, we propose ENACHI to solve Problem
P1. The framework is built upon a nested drift-plus-penalty
architecture that operates at two granularities. At the task-level,
an outer loop transforms the long-term stochastic problem into
a per-frame optimization. To address the non-analytical accu-
racy objective, this loop employs a tractable surrogate model
derived from statistical fitting. At the packet-level, a nested
inner loop is designed to mitigate sample fluctuations, which
works in conjunction with progressive packet transmission
to dynamically adjust per-slot transmission policies, ensuring
adherence to the reference budget set at the task-level.
A. Problem Conversion
To make the stochastic optimization problem tracable, this
section first decomposes the original problem at the task level
by investigating the long-term stochastic dynamics using Lya-
punov optimization. This approach establishes virtual queues
that capture long-term constraints and decomposes them into
per-frame decisions, thereby decoupling the problem from a
temporal perspective. Under the drift-plus-penalty framework
[38] [39], the system stabilizes the virtual queues while
minimizing the penalty, converting the long-term stochastic
optimization into an online decision process that ensures
system stability and sustained performance improvement.
We first create a virtual energy queue Qn,m ≥0 for each
device n to track the long-term energy constraint (11b). The
actual energy consumption En,m is treated as the per-frame
arrival to the queue, while the energy budget
¯En is the
constant service rate. The queue backlog Qn,m thus represents
the cumulative energy deficit relative to the budget. The queue
dynamics are expressed as:
Qn,m+1 =

Qn,m + (En,m −¯En)
+ ,
(12)
where [x]+ is defined as max{x, 0}, and the queue is ini-
tialized with Qn,1 = 0. The queue accumulates the excess
energy consumption whenever the per-frame energy usage
En,m exceeds the threshold ¯En, and decreases otherwise.
We denote the queue vector at frame m as of all users by
Qm = [Q1,m, . . . , QN,m].


--- Page 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
6
Lemma 1. Using Lyapunov drift-plus-penalty framework, the
long-term stochastic optimization problem P1 can be trans-
formed into the online, per-task optimization problem P1.1:
P1.1 :
max
sm,ωm,pm
V × Am −
N
X
n=1
Qn,mEn,m
s.t.
N
X
n=1
ωn,m ≤ω, ∀n ∈N,
sn,m ∈{0, 1, 2, · · · , km},
pn,m ∈(0, pmax].
(13)
Proof. See Appendix A.
The objective of problem P1.1 is twofold: the first term
represents the optimization goal (accuracy), while the second
term, the drift, serves to reduce queue backlog and maintain
energy stability. The Lyapunov control parameter V
is a
weighting coefficient that balances this trade-off. A larger V
prioritizes inference accuracy, potentially at the cost of higher
transient energy consumption, whereas a smaller V enforces
stricter energy savings, possibly at the expense of accuracy.
However, problem P1.1 still remains intractable. This stems
from its mixed-integer non-linear programming (MINLP) na-
ture, the non-analytical accuracy objective, and the inherent
scheduling granularity mismatch between the task-level prob-
lem and packet-level variables. To efficiently solve P1.1, we
employ the hierarchical approach previously introduced, which
is decomposed into two stages as follows:
1) Stage I — Task-Level Resource Scheduling via Surrogate
Model: First, to address the non-analytical accuracy objective,
we fit a tractable surrogate model bAn,m to empirical data
obtained from a ResNet-50 model [40] trained on ImageNet
dataset. Substituting bAn,m into the task-level drift-plus-penalty
problem P1.1 yields a tractable per-frame optimization. Solv-
ing this determines the task-level decisions: the DNN partition
point sm, bandwidth allocation ωm, and estimated reference
transmit power ˜pm. This process also implicitly governs the
batching deadline tbatch
m
as defined in Section II-C, enabling
integrated control of computation and transmission.
2) Stage II — Packet-Level Reference-Tracking and Adap-
tation: Then, the packet-level inner control loop addresses the
granularity mismatch. It implements a reference-tracking pol-
icy that dynamically adjusts the per-slot transmit power pm to
follow the reference ˜pm from Stage I, adapting to fine-grained
channel dynamics. This is integrated with an uncertainty-
aware progressive feature packet transmission mechanism. The
system computes predictive entropy to determine a stopping
criterion, ensuring per-sample accuracy and mitigating the
statistical gap of the surrogate model.
B. Task-Level Resource Scheduling via Surrogate Model
We note that task-level optimization operates on a per-frame
basis, which cannot precisely control per-slot power dynamics.
Therefore, we first solve for split point sm, bandwidth ωm and
a task-level reference transmit power, denoted by ˜pm, which
serves as an estimated power budget for the entire frame,
Stage I scheduling decision point
…
…
frame m 
frame m + 1 
Comp (local) 
…
…
Comp (edge) 
Comp (local) 
Comm
Comp (edge) Comp (local) 
Comm
Comp (edge) 
Comm
Stage II scheduling decision points
…
…
Accuracy and energy consumption 
are estimated at stage I decision point
Fig. 3: Illustration of the two-stage scheduling workflow.
providing a baseline that guides the fine-grained, slot-wise
adjustments in Stage II.
Using empirical accuracy curves obtained from ImageNet,
we fit a tractable surrogate bAn,m(sn,m, βn,m) to represent
the original accuracy function An,m(sn,m, Xn,m,k), where
βn,m ∈[0, 1] denotes the proportion of input feature maps
relative to the total feature maps at the corresponding split
layer. Considering bAn,m(sn,m, βn,m) is monotonically non-
decreasing in βn,m and shows diminishing returns as βn,m
increases, this surrogate allows the originally intractable P1.1
to be reformulated into a per-task drift-plus-penalty problem.
Surrogate Accuracy Model: Based on our experimental
results in Fig. 4, we model bAn,m as:
bAn,m(sn,m, βn,m) = −
1
a0βn,m −a1
+ a2,
(14)
where a0 ≥0, a1 ≥0 and a2 ≥0 are constants depending
on the DNN model architecture and partition layer position.
Then, according to (3), the transmission proportion βn,m can
be expressed as:
βn,m = bn,m(sn,m)
btotal(sn,m) =
ωn,mT tr
n,m log2

1 + hn,m ˜pn,m
σ2

btotal(sn,m)DLhsLws
, (15)
where hn,m is the estimated average frame channel gain
constant and σ2 is the noisepower spectrum density, which
in practical system implementations is commonly used to
represent the overall noise power N0ωn,m, and here we
adopt the equivalent representation throughout the subsequent
analysis and experiments for clarity and consistency. T tr
n,m
indicates the maximum allowable communication delay. As
discussed in Section II-B, the total latency budget is typically
insufficient to allow transmission of all intermediate features.
Thus, the communication delay in each frame is:
T tr
n,m = T −(tlocal
n,m + tedge
n,m).
(16)
Once the mathematical form of the accuracy function has
been obtained, we can further reformulate problem P1.1 as:
P1.2 :
max
sm,ωm,˜pm
V
N
X
n=1
bAn,m(sn,m, βn,m) −
N
X
n=1
Qn,m ˜En,m
s.t.
(11c), (11d) and (11e),
(17)
where ˜En,m = Elocal
n,m+˜pn,mT tr
n,m denotes the estimated energy
consumption of (6) associated with the estimated power ˜pn,m.
For notational simplicity, starting from P1.2, we omit the
frame index m in this subsection since the scheduling and
optimization are performed within the current frame.
To efficiently solve this MINLP problem, we seek to decou-
ple the optimization of the resource variables (ω, ˜p) and the


--- Page 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
7
Algorithm 1 Task-Level Iterative Bandwidth and Power Al-
location Algorithm
1: Input: sm, ω, pmax, ϵconv, Imax, Qm;
2: Output: Optimal ω∗
m, ˜p∗
m, and Utility Usn,m(ω∗, ˜p∗);
3: Initialize: i = 0, ω(0)
n,m = ω/N, ˜p(0)
n,m = pmax for all n;
4: Compute initial utility U (0)
sn,m and reward Φ(0)
n,m.
5: repeat
6: i = i + 1;
7: Compute Φtotal = PN
k=1 Φ(i−1)
k,m (˜p(i−1)
k,m );
8: Update ω(i)
n,m based on (21) for all n;
9: Update ˜p(i)
n,m based on Lemma 2 for all n;
10: Compute U (i)
sn,m using (19);
11: Update Φ(i)
n,m according to (20);
12: until |U (i)
sn,m −U (i−1)
sn,m | < ϵconv or i ≥Imax;
13: Usn,m(ω∗
m, ˜p∗
m)) = PN
n=1 U (i)
sn,m;
14: return ω∗
m = ω(i)
m , ˜p∗
m = ˜p(i)
m , Usn,m(ω∗
m, ˜p∗
m).
DNN partition point s. We then have the following lemmas
for problem P1.2.
Lemma 2. For a fixed DNN partition point s, problem
P1.2 is concave with respect to ˜p when ω is fixed. The
Karush–Kuhn–Tucker (KKT) conditions then yield the condi-
tional optimal transmit power:
˜p∗
n = σ2
hn
 
2γ exp
 
2W
"
1
2
√
2
γ
s
ln 2γhnV
a1σ2T tr
n Qn
#!
−1
!
,
(18)
where W(·) denotes the Lambert W function, and γ =
a1btotal(sn)DLL
sLw
s
a0ωnT tr
n
for notational convenience.
Proof. See Appendix B.
Lemma 3. Problem P1.2 is a monotonically increasing func-
tion of ω when ˜p and s are fixed.
Proof. This can be easily proved by deriving the derivative of
P1.2 with each ω, and thus it is omitted here.
Based on Lemma 2 and 3, we develop an iterative algorithm,
detailed in Algorithm 1, to solve the problem efficiently, which
jointly optimizes ω and ˜p. First, we define a utility function
Usn based on the optimization objective:
Usn(ωn, ˜pn) = V bAn(sn, βn) −Qn ˜En.
(19)
Then, a unit-bandwidth reward function aiming to quantify
the achievable benefit per unit bandwidth is defined as:
Φn(˜pn) = Usn(˜pn, ω0),
(20)
where ω0 represents the unit bandwidth. Using this reward,
the total bandwidth ω is allocated among devices at iteration
i ≥0 proportional to their respective contributions:
ω(i)
n =
Φ(i)
n (˜pn)
PN
j=1 Φ(i)
j (˜pj)
ω.
(21)
Given this allocation, the estimated transmit power ˜p can be
updated by Lemma 2, ensuring that the utility is maximized
under the given bandwidth constraints.
Algorithm 2 ENACHI Algorithm
1: Input: Qm, V, S, ω, pmax, ¯En.
2: Output: Decisions [ω∗
m, p∗
m, s∗
m]
3: Initialize partition vector s∗
m = [s0, . . . , s0].
4: for each user n = 1 to N do
5:
for each candidate partition point i ∈S do
6:
Calculate Ui(s∗
n,m, ˜p∗
n,m) using Algorithm 1;
7: s⋆
m = arg maxi∈S U i;
8: Solve [ω⋆
m, ˜p⋆
m] using Algorithm 1(s⋆
m, Qm);
9: Initialize inner power queues qn,m,1 = 0 for all n.
10: Initialize received packets set Xn,m,0 = ∅for all n.
11: for each time slot k = 1, 2, . . . , K do
12:
for each user n = 1 to N do
13:
if user n is not terminated then
14:
Calculate p∗
n,m,k using (25);
15:
for j =1, 2, · · ·, min{|Xn,m|−|Xn,m,k|, bn,m,k} do
16:
Select feature packet according to (26);
17:
Transmit feature packet Ψn,m,k with p∗
n,m,k;
18:
Updates Xn,m,k = Xn,m,k−1 ∪Ψn,m,k;
19:
Evaluates uncertainty hs(Xn,m,k | Λs);
20:
if hs(Xn,m,k|Λs)<Hth or deadline reached then
21:
Send TERMINATION signal to user n;
22:
Update qn,m,k.
23: Observe actual energy En,m for all n;
24: Update Qn,m+1 = [Qn,m + En,m −¯En]+.
The algorithm alternates between updating the bandwidth
allocation based on (21) and transmit power via Lemma 2,
iterating until the utility function converges within a threshold
ϵconv or total number of iterations exceeds Imax.
For initialization, the total bandwidth ω is uniformly allo-
cated across all devices, which is expressed as ω(0)
n
=
ω
N ,
and the initial transmit power of each device ˜p(0)
n
is set to
its maximum admissible value pmax. Based on these initial
allocations, the utility function U (0)
sn and the reward function
Φ(0)
n
are then computed to serve as the starting point for the
subsequent iterative procedure.
After obtaining the optimal bandwidth ω∗and transmit
power ˜p∗for fixed partition points, we next determine the
optimal partition points. We adopt a greedy approach by eval-
uating the utility for each candidate in the finite partition set S
and select the s that yields the maximum utility. Specifically,
for each device n, we evaluate the best utility Usn(ω∗
n, ˜p∗
n) for
each candidate sn ∈S using ω∗
n and ˜p∗
n. The split point that
maximizes this utility, s⋆
n = arg maxs∈S Usn(ω∗
n, ˜p∗
n), is then
selected, and its associated (ω⋆
n, ˜p⋆
n) are retained as the global
optimal task-level resource allocation.
Furthermore, to enhance scalability and reduce computa-
tional complexity in dense-user scenarios, a regional aggrega-
tion strategy can be employed. This approach groups users
with similar channel conditions in geographical locations,
computing a representative resource allocation for the cluster
rather than iterating for each user individually.


--- Page 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
8
C. Packet-Level Reference-Tracking and Adaptation
While Stage I provides a frame-wise resource reference
based on a tractable surrogate model, this solution is derived
from statistical fitting, failing to capture the variability of
individual inputs.
To address the limitations of the task-level solution, we seek
to enable the per-slot adaptation. Since the partition point sm
and bandwidth ωm have already been determined in the first
stage, the online adaptation focuses on adjusting the transmit
power pn,m,k and regulating packet delivery via task-aware
progressive transmission to balance per-slot efficiency with
long-term energy sustainability.
Consequently, the task-aware packet-level inner control loop
is introduced. Its objective is twofold: opportunistically max-
imize the per-slot transmission of feature packets based on
instantaneous channel conditions, and ensure the long-term
average transmit power robustly tracks the reference ˜p⋆
n,m from
the task-level. This forms the per-slot optimization problem:
P2.1 : max
pn,m,k
1
kn,m
kn,m
X
k=1
bn,m,k
(22a)
s.t.
1
kn,m
kn,m
X
k=1
pn,m,k ≤˜p⋆
n,m,
(22b)
pn,m,k ∈(0, pmax],
(22c)
where bn,m,k represents the maximal packet size defined in (4),
pn,m,k denotes the per-slot transmit power, and kn,m =
T tr
n,m
tslot
is the actual transmit slot number.
To enforce the long-term power constraint (22b) in the
per-slot domain, we adopt a finite-horizon Lyapunov-based
optimization for the inner control loop, as it operates over the
limited number of slots within a single frame. A virtual power
queue qn,m,k ≥0 is defined for each device n within frame m,
which tracks the deviation between the accumulated transmit
power pn,m,k and the reference power ˜pn,m obtained from
the task-level stage. This approach decomposes the problem
into tractable, per-slot subproblems, analogous to the task-level
optimization. We define the virtual power queue dynamics as:
qn,m,k+1 = [qn,m,k + pn,m,k −˜pn,m]+ ,
(23)
where qn,m,1 = 0.
Then, we transform problem P2.1 into a multiple determin-
istic, per-slot optimization problem, which aims to opportunis-
tically maximize the drift-plus-penalty term. This transformed
drift-plus-penalty problem can be stated as:
P2.2 : max
pn,m,k
v × bn,m,k −qn,m,kpn,m,k
(24a)
s.t.
(22b), (22c),
(24b)
where v is the inner-loop Lyapunov control parameter. Anal-
ogous to V in the task-level, v governs the trade-off in the
packet-level optimization, balancing the objective of maxi-
mizing per-slot transmitted packets bn,m,k against the need
to stabilize the power queue qn,m,k.
Problem P2.2 is a strictly concave optimization problem
with respect to the variable pn,m,k. As detailed in the Ap-
pendix C, this holds because the Hessian matrix of the objec-
tive function is symmetric and positive definite. Therefore, the
optimal solution for the per-slot transmit power p⋆
n,m,k can be
derived analytically using the KKT conditions:
p⋆
n,m,k =
v ωn,m
qn,m,kDLhsLws
ln 2 −
σ2
hn,m,k
,
(25)
where hn,m,k is the deterministic channel gain of time slot k.
This solution yields a dynamic per-slot power policy that
adapts to instantaneous queue states and channel dynamics.
This per-slot policy thus translating the static, task-level budget
into a fine-grained, adaptive control action.
Once the optimal per-slot power p⋆
n,m,k is determined,
device starts the progressive transmission of feature packets,
which generally consists of one or multiple feature maps.
Instead of transmitting arbitrary feature maps, the server-
side controller performs importance-aware feature selection,
choosing the most informative and yet untransmitted features
at the beginning of each slot. Following the methodology in
[41], the importance of the jth parameter from the split layer
of DNN model is quantitatively estimated through a first-order
Taylor expansion of the learning loss L, which can be obtained
from the last round of model training as computed using the
back-propagation algorithm: ˜I(wj) =

∂L
∂wj · wj
2
.
Let Xn,m denote the set of total feature maps generated
from the split point sn,m, then the total importance of the i-
th feature map Xi ∈Xn,m is the sum of these values for all
parameters wj within the corresponding filter that generates
it, represented by gc(Xi) = P
wj∈Xi ˜I(wj), ∀Xi ∈Xn,m.
Recall that Xn,m,k is the cumulative feature maps trans-
mitted by slot k, and let Ψn,m,k denote the feature packet
transmitted at slot k. During each slot, as long as the packet
size satisfies |Ψn,m,k| < min{|Xn,m| −|Xn,m,k|, bn,m,k},
new feature maps are iteratively added according to their
importance scores as:
Ψn,m,k =Ψn,m,k∪
(
arg max
Xi∈(Xn,m\Xn,m,k−1\Ψn,m,k)
gc(Xi)
)
. (26)
Upon receiving the packets at the end of each slot, the
server performs an interim inference using the cumulative set
of all packets received thus far. Then, the server evaluates
the inference reliability using the trained uncertainty predictor
hs (Xn,m,k | Λs) mentioned in Section II.
The transmission process for device n is terminated if the
uncertainty condition Hth is met, hs(Xn,m,k|Λs)≤Hth, or the
hard transmission deadline, tbatch
m
, is reached. Otherwise, the
progressive transmission continues to the next slot.
D. Performance Analysis
We now characterize the theoretical performance of the
proposed ENACHI algorithm by establishing a comparison
with an ideal offline optimum of solving problem P1. Let
{s∗
m, ω∗
m, p∗
m} represent the optimal offline decision se-
quence, and let A∗
m denote the corresponding maximum
accuracy of frame m.
In our setting, decisions rely on the surrogate accuracy
model bAm and the estimated energy consumption
˜En,m


--- Page 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
9
derived from the task-level scheduler. Define PM
m=1 bA‡
m as
the cumulative average inference accuracy of the proposed
algorithm, which is obtained by solving P1.2 in each frame.
We consider the wireless channel states to be temporally
independent, without imposing any specific distributional as-
sumption. The performance bound of the proposed algorithm
is presented in the following theorem.
Theorem 1. Relative to the offline optimal solution, the
cumulative inference accuracy of ENACHI is bounded by
M
X
m=1
A‡
m ≥
M
X
m=1
A∗
m −θ0M 2 + M(M −1)δ0
PN
n=1 θn
V
−2ξ0,
(27)
and the cumulative energy consumption violation satisfies
M
X
m=1
En,m ≤M ¯En+
v
u
u
t2θ0M 2 + 2M(M −1)δ0
N
X
n=1
θn + 4ξ0V ,
(28)
where θ0 ≜PN
n=1
1
2θ2
n, θn ≜maxm{|En,m −¯En|}, δ0 ≜
max{n,m}
n ˜En,m−En,m

o
and ξ0 ≜maxm
nbAm−Am

o
.
Proof. We employ a two-step bounding technique to establish
the proof; see Appendix D for details.
Theorem 1 indicates that the inference performance of the
proposed algorithm can be bounded relative to its offline
optimal counterpart. Meanwhile, the discrepancy between cu-
mulative energy consumption of each device and its allocated
budget is also limited. The worst-case performance can be
enhanced by decreasing the upper bound of the energy usage
deviation θn, the maximum energy estimation error δ0 and
the approximation error ξ0. Additionally, the weight param-
eter V allows balancing the trade-off between task inference
performance and energy consumption of devices. In practical
implementations, energy should be managed carefully to keep
θn, ξ0 small, and V should be chosen judiciously to achieve
the best inference performance within the energy constraints.
IV. EXPERIMENTS
In this section, we conduct extensive simulations to evaluate
the performance of our proposed ENACHI framework. We
validate its effectiveness in maximizing long-term inference
accuracy while ensuring device energy stability under strict
deadlines. We validate our approach by comparing it against
several state-of-the-art benchmark schemes.
A. Simulation Setup
We simulate a split inference system with N users and
one edge server over a channel modeling both large-scale
path loss and small-scale Rayleigh fading. The inference
task employs a trained ResNet-50 model with upperbound
accuracy of 80.38%, and we use the ImageNet dataset. A
set of feasible partition points S is predetermined. The ac-
curacy–transmission relationship in (14) is modeled by fitting
50
60
70
80
90
100
Transmitted Feature Maps Ratio (%)
0.70
0.72
0.74
0.76
0.78
0.80
Accuracy (%)
Upperbound
Fitted curve, L1
Experiment, L1
Fitted curve, L2
Experiment, L2
Fitted curve, L3
Experiment, L3
Fitted curve, L4
Experiment, L4
Fig. 4: The experimental and fitted curves of image classification task. L1 to
L4 are the selected representative partition layers from shallow to deep.
10
1
10
2
10
3
V
70
72
74
76
78
80
Accuracy (%)
Accuracy
Energy Consumption
Accuracy Upperbound
0.25
0.26
0.27
0.28
0.29
0.30
Energy Consumption (J)
Fig. 5: Inference accuracy and energy consumption under different V .
to empirical data from the validation set of the dataset. All
reported results are averaged over 1000 simulation rounds.
Unless otherwise specified, the main simulation parameters
are set according to the configurations in [12], [23], [42] and
summarized in Table I.
TABLE I: Simulation Parameters
Parameter
Value
Noise Power Spectral Density (σ2)
1 ×10−13 W
Device CPU Clock Speed (fn,m)
2.0 GHz
Edge Server GPU frequency (fedge
m )
20 GHz
Device Chip Power Constant (αn)
10−28
Maximum Transmit Power (pmax)
2 W
Long-term Energy Budget ( ¯En)
0.25 J
Outer Lyapunov Control Parameter (V )
50
Inner Lyapunov Control Parameter (v)
5
Frame length
300ms
Slot length
1ms
For comparison, the following benchmarks are considered:
(1) EFFECT-DNN [14]: A Lyapunov-based framework that
minimizes long-term device energy consumption. It targets
average latency bound rather than hard deadlines and lacks
hierarchical task-aware scheduling.
(2) Semantic Communication based Computation-Aware
Offloading (SC-CAO) [15]: This framework uses semantic
communication to jointly optimize the compression ratio,
computation, and transmission resources. It employs a myopic
optimization strategy that focuses on the current frame rather
than long-term system stability.
(3) Progressive Feature Transmission (ProgressiveFTX)
[31]: This is an ablation study of our proposed algorithm where


--- Page 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
10
100
150
200
250
300
350
400
450
500
Hard Latency Deadline (ms)
60.0
62.5
65.0
67.5
70.0
72.5
75.0
77.5
80.0
Accuracy (%)
ENACHI(Proposed)
EFFECT-DNN
SC-CAO
Edge-only
Device-only
ProgressiveFTX-fix-L1
ProgressiveFTX-fix-L2
ProgressiveFTX-fix-L3
ProgressiveFTX-fix-L4
Upperbound
(a) Accuracy under different deadlines.
1
2
3
4
5
6
7
8
9
Bandwidth (MHz)
72
74
76
78
80
Accuracy (%)
ENACHI(Proposed)
EFFECT-DNN
SC-CAO
Edge-only
ProgressiveFTX-fix-L1
ProgressiveFTX-fix-L2
ProgressiveFTX-fix-L3
ProgressiveFTX-fix-L4
Upperbound
(c) Accuracy under different bandwidth.
5
10
15
20
25
Number of Users
55
60
65
70
75
80
Accuracy (%)
ENACHI(Proposed)
EFFECT-DNN
SC-CAO
Edge-only
Upperbound
ProgressiveFTX-fix-L1
ProgressiveFTX-fix-L2
ProgressiveFTX-fix-L3
ProgressiveFTX-fix-L4
(e) Accuracy under different user numbers.
100
150
200
250
300
350
400
450
500
Hard Latency Deadline (ms)
0.3
0.4
0.5
0.6
0.7
Energy Consumption (J)
ENACHI(Proposed)
EFFECT-DNN
SC-CAO
Edge-only
Device-only
ProgressiveFTX-fix-L1
ProgressiveFTX-fix-L2
ProgressiveFTX-fix-L3
ProgressiveFTX-fix-L4
(b) Energy consumption under different deadlines.
1
2
3
4
5
6
7
8
9
Bandwidth (MHz)
0.25
0.30
0.35
0.40
0.45
0.50
0.55
Energy Consumption (J)
ENACHI(Proposed)
EFFECT-DNN
SC-CAO
Edge-only
Device-only
ProgressiveFTX-fix-L1
ProgressiveFTX-fix-L2
ProgressiveFTX-fix-L3
ProgressiveFTX-fix-L4
(d) Energy consumption under different band-
width.
5
10
15
20
25
Number of Users
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Energy Consumption (J)
ENACHI(Proposed)
EFFECT-DNN
SC-CAO
Edge-only
ProgressiveFTX-fix-L1
ProgressiveFTX-fix-L2
ProgressiveFTX-fix-L3
ProgressiveFTX-fix-L4
(f) Average energy consumption under different
user numbers.
Fig. 6: Performance of the proposed algorithm and benchmarks on ImageNet.
DNN split point is fixed. We test 4 representative partition
points, from shallow to deep.
(4) Edge-Only: Offloads the entire task to the edge server.
(5) Device-Only: Executes the entire task locally.
B. Simulation Results
Our experimental evaluations begin by validating the surro-
gate model and control parameters in single-user scenarios to
establish a performance baseline and subsequently extend to
multi-user scenarios to evaluate system scalability.
1) Surrogate Model Validation: We validated our hyper-
bolic surrogate model by selecting representative partition
points L1 to L4 from the shallow to deep stages of ResNet-50.
These points correspond to the 1st, 4th, 8th, and 14th convo-
lutional layers respectively. As illustrated in Fig. 4, our fitted
model exhibits a strong agreement with the empirical accuracy
curves. This close match confirms that the surrogate function
effectively captures the diminishing returns of accuracy and
transmitted feature ratio, justifying its use in our framework.
The experiments are conducted using the ImageNet dataset.
2) Impact of V: The parameter V in the drift-plus-penalty
formulation (17) is a crucial control knob that governs the
trade-off between maximizing inference accuracy and main-
taining energy queue stability. A higher V prioritizes the ac-
curacy objective, while a lower V emphasizes strict adherence
to the long-term energy budget. Fig. 5 illustrates the impact
of varying V over several orders of magnitude on the long-
term average accuracy and energy consumption of the system
in a single-user scenario. As depicted in Fig. 5, the behavior
of the system changes significantly with the magnitude of V .
For small values (V ∈[100, 101]), the framework operates
in an energy-conservative mode, maintaining low energy con-
sumption at the cost of reduced accuracy. As V increases
into the range of (101, 102], the system enters a balanced
trade-off regime. Here, inference accuracy rises sharply with
only a marginal increase in energy cost, indicating that our
framework efficiently co-optimizes resources to achieve per-
formance gains. When V becomes very large (V > 102), the
accuracy curve begins to saturate as it approaches its upper
bound. In this regime, pursuing marginal accuracy gains incurs
a large energy cost, signifying diminishing returns.
3) Impact of frame deadline T: We first evaluate per-
formance under varying hard latency deadlines with a fixed
3 MHz bandwidth. Inference accuracy and device energy
consumption are used as the key performance metrics. As
shown in Fig. 6(a), our framework achieves a remarkable
43.12% accuracy gain over benchmarks at the stringent 100 ms
deadline, reaching 72.5% accuracy and smoothly rising to
80.1% at 300 ms. Concurrently, it reduces energy consumption
by up to 62.13% as depicted in Fig. 6(b), maintaining stable
usage below 0.28 J. This performance is attributed to the
dynamic algorithm with energy reference, resulting in stable
energy consumption while achieving robust accuracy against
varying deadlines. In comparison, SC-CAO also considers hard
deadline and show robustness to changes in deadline. Progres-
siveFTX and Device-only schemes are inflexible, becoming
entirely infeasible for deadlines below 275 ms. The EFFECT-
DNN framework, which targets average latency rather than
hard deadlines, incurs an accuracy gap even under relaxed
deadlines, and exhibits a significant energy performance degra-
dation in scenarios with stringent deadline constraints.
4) Impact of user bandwidth ω: We evaluate the per-
formance under varying channel bandwidth, with the hard


--- Page 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
11
deadline fixed at 300 ms. As shown in Fig. 6(c), our pro-
posed framework achieves the best accuracy-bandwidth trade-
off across all conditions. The advantage is most pronounced
in the communication-constrained region of 1 to 3 MHz.
Specifically, at 1 MHz, our method realizes a 9.39% accuracy
gain over benchmarks to reach 76% accuracy, eventually
saturating near 6 MHz. Regarding energy stability in Fig. 6(d),
our approach reduces energy consumption by 42.74% at
1 MHz compared to baselines and maintains the lowest usage
across all bandwidths. This robustness under low bandwidth
is attributed to the progressive transmission mechanism. In
communication-constrained scenarios, this mechanism selects
high-value features based on their importance, which signif-
icantly saves communication overhead. In comparison, the
SC-CAO framework, which also considers data compression
and dynamic bandwidth allocation, shows some robustness
in communication-constrained scenarios. The EFFECT-DNN
framework suffers from significant accuracy degradation and
high energy consumption at low bandwidths, mainly due to its
lack of a packet-level dynamic transmission mechanism. The
Edge-Only scheme is entirely infeasible below 2.5 MHz, and
ProgressiveFTX schemes perform poorly at some points due
to the inflexible strategy.
5) Scalability Analysis: Finally, we evaluate the scalability
of the system in a multi-user scenario. In this experiment,
we fix the total system bandwidth to 20 MHz and increase
the number of users from 5 to 25, simulating an environment
with increasing resource contention. As shown in Fig. 6(e),
as the per-user available bandwidth decreases, the accuracy
of all schemes inevitably degrades. While our framework
drops to nearly 70% accuracy in bandwidth-scarce scenarios,
it degrades gracefully and achieves a 14.19% accuracy gain at
25 users compared to benchmarks. Most critically, Fig. 6(f)
demonstrates strong energy scalability. As the user count
rises, the energy cost remains remarkably flat below 0.28 J,
realizing a 37.65% energy saving at 25 users. This stability
is attributed to our two-tier Lyapunov framework which sta-
bilizes individual energy consumption and mitigates inter-user
competition through dynamic allocation. In comparison, the
SC-CAO framework, despite also using dynamic allocation,
suffers from linearly increasing energy consumption due to its
myopic optimization. The accuracy and energy performance
of the EFFECT-DNN framework degrade once the user count
exceeds 10, as the bandwidth available per user drops. The
Edge-Only and static-partition schemes demonstrate a lack of
adaptability in congested networks.
V. CONCLUSION
In this paper, we aim to realize energy-aware, deadline-
critical DNN offloading in multi-user split inference systems.
The main challenges include the scheduling granularity mis-
match between task-level and packet-level operations, and the
lack of task-aware adaptation to the inherent heterogeneity
of inference tasks. We proposed ENACHI, a novel ENergy-
ACcuracy Hierarchical optimization for split-Inference frame-
work built on a nested drift-plus-penalty architecture. ENACHI
operates at two scales: a task-level outer loop uses a surro-
gate model to manage the long-term energy-accuracy trade-
off and set a power reference, while a packet-level inner
loop implements a reference-tracking policy to dynamically
adjust per-slot packet transmission. This is integrated with an
importance-aware progressive transmission and an adaptive
uncertainty-based stopping criterion. Comprehensive simula-
tions on ImageNet demonstrated that ENACHI significantly
outperforms state-of-the-art benchmarks, achieving a better
energy-accuracy trade-off and higher scalability, particularly
under stringent deadlines.
REFERENCES
[1] G. K. Walia, M. Kumar, and S. S. Gill, “AI-empowered fog/edge
resource management for IoT applications: A comprehensive review,
research challenges, and future perspectives,” IEEE Commun. Surveys
Tuts., vol. 26, no. 1, pp. 619–669, 2023.
[2] Z. Liu, X. Chen, H. Wu, Z. Wang, X. Chen, D. Niyato, and K. Huang,
“Integrated sensing and edge AI: Realizing intelligent perception in 6G,”
IEEE Commun. Surveys Tuts., 2025.
[3] C. W. Chen, “Internet of video things: Next-generation IoT with visual
sensors,” IEEE Internet Things J., vol. 7, no. 8, pp. 6676–6685, 2020.
[4] R. P. et al. (Dec. 2019), The Artificial Intelligence Index Re-
port 2019, [Online]. Available: https://hai.stanford.edu/sites/default/
files/aiindex2019report.pdf.
[5] R. Desislavov, F. Mart´ınez-Plumed, and J. Hern´andez-Orallo, “Trends
in AI inference energy consumption: Beyond the performance-vs-
parameter laws of deep learning,” Sustainable Computing: Informatics
and Systems, vol. 38, p. 100857, 2023.
[6] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge
intelligence: Paving the last mile of artificial intelligence with edge
computing,” Proc. IEEE, vol. 107, no. 8, pp. 1738–1762, 2019.
[7] Framework and Overall Objectives of the Future Development of IMT
for 2030 and Beyond, document ITU-R, DRAFT NEW RECOMMEN-
DATION, 2023.
[8] W. Shi, Y. Hou, S. Zhou, Z. Niu, Y. Zhang, and L. Geng, “Improving
device-edge cooperative inference of deep learning via 2-step pruning,”
in IEEE INFOCOM 2019-IEEE Conference on Computer Communica-
tions Workshops (INFOCOM WKSHPS).
IEEE, 2019, pp. 1–6.
[9] Y. Deng, Z. Chen, X. Yao, S. Hassan, and A. M. Ibrahim, “Parallel
offloading in green and sustainable mobile edge computing for delay-
constrained IoT system,” IEEE Trans. Veh. Technol., vol. 68, no. 12, pp.
12 202–12 214, 2019.
[10] C. Dong, S. Hu, X. Chen, and W. Wen, “Joint optimization with dnn
partitioning and resource allocation in mobile edge computing,” IEEE
Transactions on Network and Service Management, vol. 18, no. 4, pp.
3973–3986, 2021.
[11] Y. Sun, S. Zhou, and J. Xu, “EMM: Energy-aware mobility management
for mobile edge computing in ultra dense networks,” IEEE J. Sel. Areas
Commun., vol. 35, no. 11, pp. 2637–2646, 2017.
[12] Z. Tong, J. Cai, J. Mei, K. Li, and K. Li, “Dynamic energy-saving
offloading strategy guided by Lyapunov optimization for IoT devices,”
IEEE Internet Things J., vol. 9, no. 20, pp. 19 903–19 915, 2022.
[13] Y. Mao, X. Yu, K. Huang, Y.-J. A. Zhang, and J. Zhang, “Green edge
AI: A contemporary survey,” Proc. IEEE, 2024.
[14] X. Xu, K. Yan, S. Han, B. Wang, X. Tao, and P. Zhang, “Learning-
based edge-device collaborative DNN inference in IoVT networks,”
IEEE Internet Things J., vol. 11, no. 5, pp. 7989–8004, 2023.
[15] X. Zhang, M. Mounesan, and S. Debroy, “Effect-dnn: Energy-efficient
edge framework for real-time dnn inference,” in 2023 IEEE 24th
International Symposium on a World of Wireless, Mobile and Multimedia
Networks (WoWMoM).
IEEE, 2023, pp. 10–20.
[16] G. Zheng, M. Wen, Z. Ning, and Z. Ding, “Computation-aware offload-
ing for dnn inference tasks in semantic communication assisted mec
systems,” IEEE Trans. Wireless Commun., 2025.
[17] M. Gao, R. Shen, L. Shi, W. Qi, J. Li, and Y. Li, “Task partitioning
and offloading in DNN-task enabled mobile edge computing networks,”
IEEE Trans. Mobile Comput., vol. 22, no. 4, pp. 2435–2445, 2021.
[18] Z. Liu, Q. Lan, and K. Huang, “Resource allocation for multiuser edge
inference with batching and early exiting,” IEEE J. Sel. Areas Commun.,
vol. 41, no. 4, pp. 1186–1200, 2023.
[19] W. Shi, S. Zhou, Z. Niu, M. Jiang, and L. Geng, “Multiuser co-
inference with batch processing capable edge server,” IEEE Trans.
Wireless Commun., vol. 22, no. 1, pp. 286–300, 2022.


--- Page 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
12
[20] Y. Han, Z. Nan, S. Zhou, and Z. Niu, “DVFS-aware DNN inference
on GPUs: Latency modeling and performance analysis,” in ICC 2025
- IEEE International Conference on Communications, 2025, pp. 1274–
1279.
[21] C. Wang, S. Zhang, Y. Chen, Z. Qian, J. Wu, and M. Xiao, “Joint
configuration adaptation and bandwidth allocation for edge-based real-
time video analytics,” in IEEE INFOCOM 2020-IEEE Conference on
Computer Communications.
IEEE, 2020, pp. 257–266.
[22] Q. Liu, S. Huang, J. Opadere, and T. Han, “An edge network orches-
trator for mobile augmented reality,” in IEEE INFOCOM 2018-IEEE
conference on computer communications.
IEEE, 2018, pp. 756–764.
[23] G. Pan, H. Zhang, S. Xu, S. Zhang, and X. Chen, “Joint optimization
of video-based AI inference tasks in MEC-assisted augmented reality
systems,” IEEE Trans. Cogn. Commun. Netw., vol. 9, no. 2, pp. 479–
493, 2023.
[24] Z. Nan, Y. Han, J. Yan, S. Zhou, and Z. Niu, “Robust task offloading
and resource allocation under imperfect computing capacity information
in edge intelligence systems,” IEEE Trans. Mobile Comput., 2025.
[25] Z. Nan, Y. Han, S. Zhou, and Z. Niu, “Robust DNN Partitioning and
Resource Allocation Under Uncertain Inference Time,” IEEE Trans.
Mobile Comput., pp. 1–17, 2025.
[26] Z. Wang, A. E. Kalør, Y. Zhou, P. Popovski, and K. Huang, “Ultra-low-
latency edge inference for distributed sensing,” IEEE Trans. Wireless
Commun., 2025.
[27] Y. Zhou, C. You, and K. Huang, “Communication efficient cooperative
edge AI via event-triggered computation offloading,” arXiv preprint
arXiv:2501.02001, 2025.
[28] H. Ma, P. Huang, Z. Zhou, X. Zhang, and X. Chen, “Greenedge:
Joint green energy scheduling and dynamic task offloading in multi-tier
edge computing systems,” IEEE Transactions on Vehicular Technology,
vol. 71, no. 4, pp. 4322–4335, 2022.
[29] Q. Tang, R. Xie, Z. Fang, T. Huang, T. Chen, R. Zhang, and F. R.
Yu, “Joint service deployment and task scheduling for satellite edge
computing: A two-timescale hierarchical approach,” IEEE Journal on
Selected Areas in Communications, vol. 42, no. 5, pp. 1063–1079, 2024.
[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[31] Q. Lan, Q. Zeng, P. Popovski, D. G¨und¨uz, and K. Huang, “Progressive
feature transmission for split classification at the wireless edge,” IEEE
Trans. Wireless Commun., vol. 22, no. 6, pp. 3837–3852, 2022.
[32] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and
L. Tang, “Neurosurgeon: Collaborative intelligence between the cloud
and mobile edge,” ACM SIGARCH Computer Architecture News, vol. 45,
no. 1, pp. 615–629, 2017.
[33] L. Guo, W. Chen, Y. Sun, and B. Ai, “Digital-SC: Digital Semantic
Communication With Adaptive Network Split and Learned Non-Linear
Quantization,” IEEE Transactions on Cognitive Communications and
Networking, vol. 11, no. 4, pp. 2499–2511, 2025.
[34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition.
Ieee, 2009, pp. 248–255.
[35] T. Taleb, K. Samdanis, B. Mada, H. Flinck, S. Dutta, and D. Sabella,
“On multi-access edge computing: A survey of the emerging 5G network
edge cloud architecture and orchestration,” IEEE Commun. Surveys
Tuts., vol. 19, no. 3, pp. 1657–1681, 2017.
[36] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey
on mobile edge computing: The communication perspective,” IEEE
Commun. Surveys Tuts., vol. 19, no. 4, pp. 2322–2358, 2017.
[37] 3GPP, “Study on traffic characteristics and performance requirements
for AI/ML model transfer in 5Gs,” 3GPP document TR 22.874, 2021.
[38] M. Neely, Stochastic network optimization with application to commu-
nication and queueing systems.
Morgan & Claypool Publishers, 2010.
[39] M. J. Neely, “Stochastic network optimization with non-convex utilities
and costs,” in 2010 Information Theory and Applications Workshop
(ITA).
IEEE, 2010, pp. 1–10.
[40] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[41] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Importance
estimation for neural network pruning,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2019, pp.
11 264–11 272.
[42] Z. Ji and Z. Qin, “Computational offloading in semantic-aware cloud-
edge-end collaborative networks,” IEEE J. Sel. Topics Signal Process.,
2024.


--- Page 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
13
APPENDIX A
PROOF OF LEMMA 1
To characterize the congestion level of the virtual queues, we define a queue backlog vector Θm = (Q1,m, Q2,m, · · · , Qn,m).
For Θm, we introduce the quadratic Lyapunov function, which is formally defined as:
L(Θm) = 1
2
N
X
n=1
(Qn,m)2,
(29)
the size of which can provide a direct measure of the relative severity of queues accumulation. Clearly, the Lyapunov function
is non-negative and we define L(Θ0) = 0.
Then, we define the Lyapunov drift function as
∆(Θm) = E

L(Θm+1) −L(Θm)
 Θm

,
(30)
which characterizes the expected change of the Lyapunov function between consecutive frames, i.e., m and m + 1, and it is
crucial to the system stability.
According to (12):
1) if Qn,m+En,m−¯En ≥0, then Qn,m+1 = Qn,m + En,m −¯En, so (Qn,m+1)2 = (Qn,m + En,m −¯En)2, ∀n ∈N;
2) if Qn,m+En,m−¯En ≤0, then Qn,m+1 =0>Qn,m+En,m−¯En, so (Qn,m+1)2 =0<(Qn,m+En,m−¯En)2, ∀n∈N.
In conclusion, we have
(Qn,m+1)2 ≤(Qn,m + En,m −¯En)2.
(31)
By expanding the above equation, we can obtain that
1
2
N
X
n=1
(Qn,m+1)2 ≤1
2
N
X
n=1
(Qn,m)2 + 1
2
N
X
n=1
(En,m −¯En)2 +
N
X
n=1
Qn,m × (En,m −¯En).
Rearranging the terms in the above inequality and taking the expectation on both sides, we obtain
∆(Θm) = 1
2
N
X
n=1
E[(Qn,m+1)2  Θm] −1
2
N
X
n=1
E[(Qn,m)2  Θm]
≤1
2
N
X
n=1
E[(En,m −¯En)2  Θm] +
N
X
n=1
E[Qn,m(En,m −¯En)
 Θm]
≤θ0 +
N
X
n=1
E[Qn,m(En,m −¯En)
 Θm]
(32)
where θ0 ≜PN
n=1
1
2θ2
n and θn ≜maxm{|En,m −¯En|}. Therefore, for a given Lyapunov function L(Θm), when E[L(Θm)] ≤
∞, the upper bound of the subsequent Lyapunov drift can be expressed as
∆(Θm) = E

L(Θm+1) −L(Θm)
 Θm

≤θ0 +
N
X
n=1
Qn,m × (En,m −¯En),
(33)
The established upper bound of the Lyapunov drift quantifies the impact of current decisions on the long-term constraint,
providing a basis for per-frame optimization.
To incorporate the original system objective, we introduce the penalty term in the Lyapunov optimization framework, which
represents the system objective that we aim to minimize. Therefore, problem P1 can be equivalently transformed into
max
sm,ωm,pm
V × Am −∆(Θm)
s.t.
(11c), (11d) and (11e),
(34)
where V is the Lyapunov control parameter.
By decomposing the optimization into slot-wise problems, (34) is transformed into problem P1.1, thus completing the proof.


--- Page 14 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
14
APPENDIX B
POOF OF LEMMA 2
Once sn,m and ωn,m is fixed, problem P1.2 is reduced into the following resource allocation problem:
max
ωm,˜pm
V
N
X
n=1
bAn,m(βn,m) −
N
X
n=1
Qn,m ˜En,m
s.t.
(11c), (11e).
(35)
Latter term −Qn,m ˜En,m is linear in ˜pn,m, and is therefore concave. Term bAn,m(βn,m) is a concave and non-decreasing
function of the transmission ratio βn,m. For a fixed ωn,m, βn,m is:
βn,m(˜pn,m) = C1 · log(1 + C2˜pn,m),
where C1 =
ωn,mT
btotal(sn,m)DLhsLws > 0 and C2 = hn,m
σ2
> 0 are constants. The second derivative is
∂2βn,m
∂˜p2n,m
= C1 ·

−C2
2
ln(2)(1 + C2˜pn,m)2

.
Since C1, C2 > 0, we have ∂2βn,m
∂˜p2n,m < 0 for all ˜pn,m > 0. So βn,m(˜pn,m) is a concave function.
The term bAn,m(βn,m) is the composition of a concave, non-decreasing function and a concave function βn,m. By composition
rules, bAn,m(βn,m) is concave. Since V > 0, V bAn,m is also concave. Therefore, (35) is the sum of two concave terms and is
therefore concave. The total sum objective is also concave, thus complete the proof.
APPENDIX C
PROOF OF CONCAVITY OF PROBLEM P2.2
Let p = pn,m,k be the optimization variable. The objective function f(p) can be written as:
f(p) = K1 · log2(1 + K2p) −qn,m,k · p,
where K1 =v

ωn,mT
DLh
s Lw
s

and K2 = hn,m,k
σ2
. Since v, ωn,m, T, D, Lh
s, Lw
s , hn,m,k, σ2 are all positive quantities, K1 >0 and K2 >0.
To check for concavity, we compute the second derivative of f(p) with respect to p. The second derivative f ′′(p) is:
f ′′(p) = −
K1K2
2
ln(2)(1 + K2p)2 .
Since K1 > 0, K2 > 0, ln(2) > 0, and (1 + K2p)2 > 0 for all p > 0, the numerator is strictly positive and the denominator
is strictly positive. Therefore, f ′′(p) < 0 for the entire domain p > 0, therefore the objective function f(pn,m,k) is strictly
concave, thus complete the proof.
APPENDIX D
PROOF OF THEOREM 1
We define the Lyapunov function as L(m) ≜PN
n=1
1
2Q2
n,m, and the Lyapunov drft as ∆1(m) ≜L(m + 1) −L(m). The
upper bound on the single-round drift-plus-penalty function is given by
∆1(m) = L(m + 1) −L(m) =
N
X
n=1
1
2Q2
n,m+1 −1
2Q2
n,m

≤θ0 +
N
X
n=1
Qn,m
 En,m −¯E

,
(36)
where θ0 ≜PN
n=1
1
2θ2
n and θn ≜maxm{|En,m −¯En|}. By adding −V Am on both sides of (36) and define ξm ≜bAm −Am,
an upperbound on the single-frame drift-plus-penalty function is given by:
∆1(m) −V Am ≤θ0 +
N
X
n=1
Qn,m
 En,m −¯E

−V Am
(37)
= θ0 +
N
X
n=1
Qn,m

˜En,m −δn,m −¯E

−V

bAm −ξm

.
(38)
The classical Lyapunov drift-plus-penalty algorithm is designed to minimize the upper bound of ∆1(m)−V Am, as expressed
in (37). Since the exact values of En, m and Am are unavailable, we instead focus on minimizing the estimated drift-plus-
penalty, as presented in (38).
Define the M-frame drift as ∆M ≜L(M + 1) −L(1). Then the M-frame drift-plus-penalty function can be bounded by:


--- Page 15 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
15
∆M −V
M
X
m=1
Am ≤
M
X
m=1
 
θ0 +
N
X
n=1
Qn,m( ˜En,m −δn,m −¯En)
!
−V
M
X
m=1
Am
= θ0M +
M
X
m=1
 N
X
n=1
Qn,m

˜En,m −¯En

−V Am −
N
X
n=1
Qn,mδn,m
!
= θ0M +
M
X
m=1
 N
X
n=1
Qn,m

˜En,m −¯En

−V

bAm −ξm

−
N
X
n=1
Qn,mδn,m
!
.
(39)
We denote the optimal offline solution of P1 with superscript ∗, the solution obtained by classical drift-plus-penalty method
for P1.1 with superscript †, and the result produced by our estimated drift-plus-penalty approach for P1.2 with superscript ‡.
The drift-plus-penalty over M frames satisfies the following bound:
∆†
M −V
M
X
m=1
A†
m ≤θ0M
M
X
m=1
 N
X
n=1
Qn,m

˜En,m −¯En
‡
−V A‡
m −
N
X
n=1
Qn,mδ‡
n,m
!
= θ0M +
M
X
m=1
 N
X
n=1
Qn,m

˜En,m −¯En
‡
−V (bA‡
m −ξ‡
n,m) −
N
X
n=1
Qn,mδ‡
n,m
!
(a)
≤θ0M +
M
X
m=1
 N
X
n=1
Qn,m

˜En,m −¯En
†
−V (bA†
m −ξ‡
n,m) −
N
X
n=1
Qn,mδ‡
n,m
!
= θ0M +
M
X
m=1
 N
X
n=1
Qn,m
 En,m −¯En
† + δ†
n,m

−V
 A†
m + ξ†
n,m −ξ‡
n,m

−
N
X
n=1
Qn,mδ‡
n,m
!
= θ0M +
M
X
m=1
 N
X
n=1
Qn,m
 En,m −¯En
† −V
 A†
m + ξ†
n,m −ξ‡
n,m

+
N
X
n=1
Qn,m
 δ†
n,m −δ‡
n,m

!
(b)
≤θ0M +
M
X
m=1
 N
X
n=1
Qn,m
 En,m −¯En
∗−V A∗
m + 2δ0
N
X
n=1
Qn,m + 2ξ0V
!
,
(40)
where δ0 ≜max{n,m}
n ˜En,m −En,m

o
and ξ0 ≜max{m}
nbAm −Am

o
.
Inequality (a) follows from the fact that the optimal solution to P1 attains the minimal objective value PN
n=1 Qn,m ˜E‡
n,m −
V A‡
m for each frame m. Inequality (b) holds because the drift-plus-penalty procedure minimizes PN
n=1 Qn,mEn,m −V Am,
therefore, substituting the offline optimal policy on the right-hand side can only result in a larger (or equal) value.
Now we bound the right-hand-side of (40). Note that Qn,m+1 −Qn,m ≤θn, ∀m, n, and thus
Qn,m = Qn,m −Qn,1 =
m−1
X
τ=1
(Qn,τ+1 −Qn,τ) ≤(m −1)θn,
(41)
Qn,m
 En,m −¯En
∗= (Qn,m −Qn,1)
 En,m −¯En
∗≤(m −1)θ2
n.
(42)
Substituting (41) and (42) into (40) yields:
∆‡
M −V
M
X
m=1
A‡
m ≤θ0M −V
M
X
m=1
A∗
m +
M
X
m=1
N
X
n=1
(m + 1)θ2
n + 2δ0
M
X
m=1
N
X
n=1
(m −1)θn + 2ξ0V
= θ0M −V
M
X
m=1
A∗
m + θ0M(M −1) + M(M −1)δ0
N
X
n=1
θn + 2ξ0V
= −V
M
X
m=1
A∗
m + θ0M 2 + M(M −1)δ0
N
X
n=1
θn + 2ξ0V.
(43)
Observing that ∆†
M ≥0, the result in (27) of Theorem 1 follows directly from (43) after dividing both sides by V .
As Am ≥0, and for ∀n, 1
2Q2
n,M+1 ≤∆M, we get
M
X
m=1
 En,m −¯En

≤
M
X
m=1
(Qn,m+1 −Qn,m) = Qn,M+1 ≤
p
2∆M ≤
v
u
u
t2θ0M 2 + 2M(M −1)δ0
N
X
n=1
θn + 4ξ0V .
(44)
Thus eq. (28) in Theorem 1 is proved.
