--- Page 1 ---
SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing
System Simulation
Jun-Qi Chen*1, Kun Zhang†2, Rui Zheng‡1, and Ying Zhong§3
1Institute of Statistics and Big Data
Renmin University of China
Beijing, China
2School of Information
Renmin University of China
Haidian District, Beijing, China
3School of Management and Economics
University of Electronic Science and Technology of China
Chengdu, China
January 13, 2026
Abstract
The Python package SimPy is widely used for modeling queueing systems due to its ﬂexibility,
simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent ad-
vances in large language models (LLMs) have shown strong ability in generating clear and executable
code, making them powerful and suitable tools for writing SimPy queueing simulation code. However,
directly employing closed-source models like GPT-4o to generate such code may lead to high compu-
tational costs and raise data privacy concerns. To address this, we ﬁne-tune two open-source LLMs,
*jqchen@ruc.edu.cn
†kunzhang@ruc.edu.cn
‡zr20230000789@ruc.edu.cn
§yzhong4@uestc.edu.cn
1
arXiv:2601.06543v1  [cs.CL]  10 Jan 2026


--- Page 2 ---
Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their
code-generating performance in executability, output-format compliance, and instruction-code consis-
tency. Particularly, we proposed a multi-stage ﬁne-tuning framework comprising two stages of super-
vised ﬁne-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing
the models ability in SimPy-based queueing simulation code generation. Extensive evaluations demon-
strate that both ﬁne-tuned models achieve substantial improvements in executability, output-format com-
pliance, and instruct consistency. These results conﬁrm that domain-speciﬁc ﬁne-tuning can effectively
transform compact open-source code models into reliable SimPy simulation generators which provide
a practical alternative to closed-source LLMs for education, research, and operational decision support.
Keywords: queueing simulation, SimPy, large language models, supervised ﬁne-tuning, direct prefer-
ence optimization
1
Introduction
Discrete-event simulation (DES) is a foundational analytical tool in operations research and management
science, which enables researchers and practitioners to study stochastic systems through explicit modeling
of events, resources, and state transitions. Among various DES scenarios, queueing-system simulation plays
a central role in analyzing customer waiting behavior, resource utilization, congestion patterns, and overall
service efﬁciency. The practical importance of queueing-system simulation has been demonstrated across
multiple domains. In healthcare operations, Rema and Sikdar (2021) developed a queue-based Monte Carlo
simulation model to optimize patient ﬂow in an Indian multi-specialty hospital, which reduces average
waiting time by 44–78%. In transportation management, Dorton and Liu (2012) applied discrete-event
simulation to passenger screening and lane conﬁguration at major international airports, which achieves
noticeable reductions in queue length and service delays. In manufacturing systems, Hsu et al. (1993)
employed queueing-network models to analyze ﬂexible production lines, identify bottlenecks, and improve
overall throughput. In service operations, particularly multi-skill call centers, Cezik and L’Ecuyer (2008)
utilized simulation optimization to design efﬁcient stafﬁng and routing strategies, which substantially lowers
customer abandonment rates and improves service quality. These studies collectively highlight the essential
role of queueing-system simulation in supporting decision-making across diverse industries.
To implement queueing-system simulation in practice, researchers and practitioners typically rely on
commercial platforms such as Arena and AnyLogic, as well as code-based tools such as SimPy, across man-
2


--- Page 3 ---
ufacturing, healthcare, transportation, and service operations. To improve transparency and reproducibility,
implementing simulation models in code has become increasingly popular. In this context, selecting an
appropriate coding tool is crucial for developing queueing simulations reliably and efﬁciently. Schruben
and Yucesan (1993) compare major paradigms for discrete-event simulation, namely (i) event scheduling,
(ii) activity scanning, and (iii) process-interaction, and discuss their implications for structuring simulation
logic and representing system dynamics. For queueing applications, the process-interaction paradigm is
often a natural choice because it aligns with the ﬂow of entities through service stages and facilitates mod-
ular and extensible model construction. Accordingly, we investigate SimPy, a Python-based discrete-event
simulation package that aligns closely with the process-interaction paradigm by representing system dynam-
ics through interacting entity processes. Beyond this alignment, SimPy is open-source and enables direct
code-based modeling of queueing systems, making it ﬂexible and accessible for research and large-scale
experimentation.
Despite SimPy’s advantages, the development of reliable queueing-system simulations still places strong
demands on both domain expertise and coding skills. In practice, conducting a queueing-system simulation
requires analysts to ﬁrst construct an accurate mathematical model of the underlying queueing process and
then translate this model into executable simulation code, which is time-consuming (Tako, 2011). Further-
more, both of them demand a high level of precision, since even a small modeling or programming error can
propagate through the simulation and lead to substantial distortions in performance. As a result, conduct-
ing a queueing-system simulation requires domain experts to simultaneously possess strong proﬁciency in
coding.
Fortunately, recent advances in large language models (LLMs) have offered new opportunities to ad-
dress the limitation, which is able to automate the generation of reliable simulation code. Domain experts
can rely on their comprehensive knowledge to clearly specify the desired queueing system in precise nat-
ural language descriptions, and then pass them to an LLM. The LLM then generates the corresponding
simulation code that correctly realizes the intended operational mechanisms. Modern models such as GPT-
4o, Qwen2.5, and DeepSeek exhibit strong code-generation ability and structured reasoning. This makes
them capable of interpreting domain-speciﬁc instructions into executable Python programs. For instance,
Zhang et al. (2025) conducted a large-scale empirical study, which shows that GPT-4 and DeepSeek-Coder
achieve superior performance in Python-based code generation across diverse programming tasks. Ridnik
et al. (2024) proposed the AlphaCodium framework, which applies ﬂow-engineering principles to iteratively
3


--- Page 4 ---
reﬁne GPT-4 generated programs, boosting its pass@5 accuracy from 19% to 44% on competitive coding
benchmarks. A recent survey by Bistarelli et al. (2025) also highlights the growing reliability of LLMs in
generating and reﬁning Python scripts for analytical and algorithmic applications.
However, three foundational challenges arise when directly applying existing general-purpose LLMs
(such as GPT-4o) to queueing-system simulation code generation. Firstly, the understanding of queueing
mechanisms exhibited by general-purpose models is often under developed, which limits their ability to
faithfully describe the intrinsic logic of queueing systems. Secondly, the generated simulation code script
can be unreliable in practice, and may exhibit various errors, even when the code scripts are syntactically
valid. Thirdly, general-purpose models are expensive to use and difﬁcult to deploy in settings that require
reproducibility, data privacy, or local on-premise computation. These challenges suggest that the general
models like GPT-4o cannot be used directly for queueing-system simulation code generation. To overcome
these limitations, a more practical and reliable solution is to ﬁne-tune a compact open model and deploy it
locally.
When applying supervised ﬁne-tuning (SFT) to SimPy-based queueing-system simulation, the primary
challenge lies in the inadequacy of high-quality SimPy queueing-system simulation code for training . This
is reﬂected in three aspects. Firstly, only a limited amount of simulation code is publicly available, even
when considering the examples provided in ofﬁcial SimPy tutorials and online repositories. Secondly, the
existing simulation codes exhibit narrow scenario coverage and fail to represent the wide range of queueing
scenarios that may arise in practical applications. Thirdly, only a small portion of the available code follows
a uniﬁed and standardized output format. As demonstrated in Huang et al. (2025), one possible solution
is to leverage LLMs to automatically generate simulation code. However, the generated code may contain
logical errors in queueing mechanism or lead to unstable execution results which requires further validation
and revising. In this paper, we propose a Multi-Stage Data Construction and Fine-Tuning Framework
that organizes the training pipeline into three stages: two stages of supervised ﬁne-tuning (SFT) to address
data scarcity and uneven data quality, followed by a stage of Direct Preference Optimization (DPO) (Rafailov
et al., 2023) for alignment. In the following, we detail the proposed framework.
In Stage I, we construct a large-scale, high-quality training dataset with broad coverage of queueing-
system scenarios using a Category-Template Framework (CTF). CTF provides the dataset with diverse
natural-language descriptions and mutiple SimPy programming styles. In Stage II, we further strengthen
the models understanding of queueing mechanisms through supervised ﬁne-tuning with masked code com-
4


--- Page 5 ---
pletion. Speciﬁcally, the model is trained to complete functional code segments via masked-completion data,
which enhances its capability to reason about and integrate different components of queueing systems. In
Stage III, we perform preference alignment using Direct Preference Optimization (DPO). By contrasting
preferred code scripts that faithfully reﬂect the intended queueing mechanism with rejected ones containing
logical deviations, this stage guides the model toward correct queueing logic.
Through this multi stage ﬁne-tuning process, we show that compact open-source models such as Qwen2.5-
Coder-7B-Instruct and DeepSeek-Coder-6.7B-Instruct can be transformed into reliable generators of
SimPy-based queueing-system simulation code. Evaluations on a 600-task benchmark demonstrate that
our ﬁne-tuning framework markedly enhances model reliability in executability rate, output-format com-
pliance, and instruction-code consistency. Both Qwen2.5-Coder-7B and DeepSeek-Coder-6.7B show sub-
stantial improvements after ﬁne-tuning compared with their pre-trained counterparts. For Qwen, the average
executability and format compliance increase from 80.4% to 86%, while instruction consistency rises from
0.0% to 76.8%. DeepSeek exhibits even larger relative gains, with executability improving from 26.2% to
75% and consistency reaching 62.3%. These results conﬁrm that our Multi-Stage Data Construction and
Fine-Tuning Framework enables compact open-source models to function as reliable SimPy simulation
generators.
Our contributions are as follows:
1. We propose the Category-Template Framework (CTF), a systematic data-construction methodol-
ogy for queueing-system simulation. CTF offers broad mechanism coverage across multiple queue-
ing categories with graded complexity, and it generates richly varied instruction-code pairs through
diverse natural-language templates and multiple SimPy coding styles. This framework provides a
reliable foundation for downstream model training.
2. We introduce a multi-stage ﬁne-tuning framework. The framework consists of (i) foundational su-
pervised learning on validated instruction-code pairs, (ii) understanding-oriented masked-completion
training that strengthens mechanism reconstruction and understanding, and (iii) preference-based
alignment using DPO to guide the model toward mechanism-consistent program generation. These
stages progressively enhance the models ability to produce correct and reliable simulation code.
3. Through extensive experiments, we demonstrate that compact open-source models, including Qwen2.5-
Coder-7B-Instruct and DeepSeek-Coder-6.7B-Instruct, can be transformed into reliable generators of
5


--- Page 6 ---
queueing-system simulation code. The ﬁne-tuned models achieve substantial improvements in exe-
cutability, output-format compliance, and instruction consistency across the benchmark.
2
Literature Review
Our work is related to several streams of literature, including queueing simulation, discrete-event simula-
tion methodology, and recent advances in large language models for automated modeling and code gener-
ation. We review the most relevant studies below and explain how the present work differs from existing
approaches.
2.1
Foundations of Queueing Simulation and Discrete-Event Modeling
Queueing systems are among the most classical and extensively studied application domains of discrete-
event simulation (DES). The theoretical and methodological foundations of queueing simulation are well
established and systematically documented in classic textbooks, spanning model speciﬁcation, experiment
design, and statistical output analysis. Textbooks such as Law and Kelton (1999), Banks et al. (2004), and
Nelson (2013) present the canonical modeling workﬂow, including the speciﬁcation of arrival processes, ser-
vice mechanisms, queue disciplines, capacity constraints, and performance measures, together with standard
practices for conducting simulation experiments and interpreting stochastic outputs (e.g., replication-based
estimation, variance assessment, and issues related to initialization and steady-state behavior). These refer-
ences frame DES as a rigorous modeling methodology for complex stochastic service systems, and they lay
the conceptual foundations for constructing faithful and reproducible queueing-system simulation models.
Building on these theoretical and methodological foundations, queueing simulation has been widely
adopted across a broad range of real-world service and production settings, where entity ﬂows and resource
constraints naturally arise. We next review representative application studies in healthcare, service opera-
tions, manufacturing, and transportation.
2.2
Applications
In what follows, we review representative application studies in healthcare, service operations, manufactur-
ing, and transportation. We use these examples to illustrate the practical impact of queueing simulation.
In healthcare domain, Jun et al. (1999) survey the application of discrete-event simulation in health
6


--- Page 7 ---
care clinics and hospital systems, with an emphasis on resource allocation, patient ﬂow, and operational
decision making in complex service environments. Rema and Sikdar (2021) develop a data-driven Monte
Carlo simulation model to analyze patient ﬂow and waiting times in hospital outpatient departments, while
? employ discrete-event simulation in SimPy to evaluate server utilization and patient waiting times in a
cloud-assisted remote healthcare setting under high service demand.
In service operations, call centers provide a canonical example of large-scale queueing systems. Gans
et al. (2003) present a comprehensive tutorial and review of call-center modeling, incorporating features
such as time-varying arrivals, customer abandonment, skill-based routing, and heterogeneous service times.
Building on this foundation, Cezik and L’Ecuyer (2008) combine optimization and simulation to study
stafﬁng decisions in multiskill call centers, which uses simulation to evaluate operational policies that are
analytically intractable.
Similar applications also arise in manufacturing and transportation systems. Hsu et al. (1993) review
queueing-network models for ﬂexible manufacturing systems, while Dorton and Liu (2012) study airport
security screening checkpoints using a combination of queueing-network analysis and discrete-event simu-
lation, and evaluate performance measures such as throughput and passenger cycle time under varying alarm
rates and baggage volumes.
2.3
SimPy and LLM-Assisted Modeling
To improve transparency and reproducibility, implementing simulation models in code has become increas-
ingly popular. Selecting an appropriate coding tool for simulation is crucial. In this work, we investi-
gate SimPy, a discrete-event simulation package in Python. To facilitate the use of SimPy, ? provide a
tutorial-style introduction that explains the basic concepts of discrete-event simulation and demonstrates
how arrivals, services, and resource constraints can be modeled in SimPy using Python processes and event
scheduling. Zinoviev (2024) provide detailed guidance on implementing discrete-event simulation models
in SimPy, while ? apply discrete-event simulation implemented in SimPy to a cloud-assisted remote health-
care system, using patient-level models to analyze server utilization and waiting times under high service
demand. These studies document the growing use of code tools for queueing simulation and highlight their
advantages for reproducible simulation workﬂows.
Nevertheless, the adoption of script-based frameworks requires substantial manual effort. Experts must
translate descriptions of simulation system into executable code and ensuring its correctness require sub-
7


--- Page 8 ---
stantial additional effort. Based on an empirical study of six expert DES modelers, Tako (2011) report that
experts spend a signiﬁcant amount of time on model coding, which reﬂects the effort required to turn a
conceptual description into an executable simulation program.
To address this limitation, recent advances in LLMs have generated growing interest in further automat-
ing modeling and code generation tasks. In the broader context of program synthesis, Li et al. (2022) in-
troduce AlphaCode and demonstrate that large-scale models can generate executable programs that achieve
strong performance on standardized coding benchmarks. Similarly, Rozière et al. (2024) present Code
Llama, an open foundation model designed for code generation across a wide range of programming tasks.
A parallel line of work emphasizes that executability and runtime behavior are critical for evaluating gener-
ated code. ? introduce InterCode as a lightweight framework and benchmark for interactive code generation,
where execution feedback is integrated into the generation loop.
Building on these works in code generation, recent studies have begun to explore the use of LLMs for
modeling tasks in operations research. Huang et al. (2025) propose ORLM, which ﬁne-tunes open-source
LLMs on domain-speciﬁc synthetic data to generate optimization formulations and solver code from natural
language descriptions. Gao et al. (2024) survey LLM-empowered agent-based modeling and simulation,
organize recent studies across multiple application domains, and discuss challenges and open problems
related to perception, alignment, action generation, and evaluation.
To conclude, this stream of work suggests a natural progression from manual simulation modeling to
script-based implementations, and more recently to LLM-assisted automation. However, existing studies pri-
marily focus on optimization models or general-purpose code generation, leaving the automation of faithful,
executable queueing-system simulation models largely unexplored.
3
Multi-Stage Data Construction and Fine-Tuning Framework
In this section, we present the Multi-Stage Data Construction and Fine-Tuning Framework and explain
high-quality datasets’ construction for each stage of the training pipeline. The section consists of three parts
corresponding to three training stages. For each part, we ﬁrst describe the objective and motivation of the
aligned training step and then introduce the data construction.
8


--- Page 9 ---
3.1
Stage I: Instruction-Code Data Construction and Initial Supervised Fine-Tuning
In this subsection, we ﬁrst discuss the motivation behind Stage I training and then introduce the proposed
Category-Template Framework (CTF) for dataset construction. Finally, the resulting dataset is used for
supervised ﬁne-tuning of the model.
3.1.1
Motivation of Stage I
In Stage I, our goal is to train the model to generate executable SimPy programs that correctly reﬂect
the structural organization and queueing mechanisms of queueing-system simulations, while being able to
compute and report task-required performance measures in the speciﬁed standardized output format.
To achieve these training objectives, a suitable training dataset is needed. As noted by ?, the quality of
initial data plays a critical role in large-model training by providing a strong foundation in both diversity
and difﬁculty. In our queueing code generating settings, the dataset is supposed to achieve broad coverage
of queueing scenarios and to exhibit diversity in both instruction formulations and code structures. To
meet these requirements, we propose the Category-Template Framework (CTF), under which we can
systematically generate diverse instruction-code pairs with comprehensive coverage of queueing scenarios.
3.1.2
Category-Template Framework
In this section, we provide a detailed introduction to the CTF. The framework consists of the following three
components:
1. Queueing category: speciﬁes the queueing scenario that represents a type of queueing task;
2. Instruction template: provides alternative linguistic formulations describing the same queueing task
for constructing the instruction;
3. Code template: offers different code styles for generating the corresponding SimPy simulation code.
In our design, each queueing category represents a family of related queueing tasks. For every such cate-
gory, we construct a corresponding set of instruction templates and code templates designed for its speciﬁc
mechanisms. During the construction of an instruction-code pair, we ﬁrst select a simulation task within
the chosen category. Then we sample one instruction template and one code template from that category.
This procedure produces a natural-language description and its corresponding simulation code script. The
9


--- Page 10 ---
multi-category and multi-template design of the framework is what yields diversity in both instruction for-
mulations and code styles. We now introduce these three components in detail.
Category
Deﬁning Mechanism
Represented Mechanisms
Complexity
(A) Basic Systems
ﬁnite capacity
Limited queue length K
Blocking and system loss constraints
Simple
general distributions
Non-exponential service or
interarrival
Arbitrary stochastic variability
Simple
multi server sched rules
Multiple servers with sim-
ple scheduling
Load
balancing
across
identical
servers
Simple
(B) Intermediate Behavioral Extensions
balking reneging
Customer
balking
and
reneging
Queue avoidance and impatience be-
havior
Intermediate
batch arrivals
Grouped Poisson arrivals
Burst-type demand or batch process-
ing
Intermediate
multi class customers
Distinct customer classes
Heterogeneous demand and service
differentiation
Intermediate
piecewise arrival
Time-varying or piecewise
arrival rate
Stepwise nonstationary input process
Intermediate
production kanban
Work-in-process
control
via tokens
Inventory-limited manufacturing sys-
tems
Intermediate
(C) Complex Networked or Multi-Mechanism Systems
breakdown maintenance
Random
server
failures
and repairs
Reliability and maintenance effects
Complex
parallel two resources
Joint resource coordination Parallel processing or shared-resource
synchronization
Complex
open network
Exogenous arrivals to mul-
tiple nodes
Distributed and interconnected queue-
ing systems
Complex
feedback network
Internal routing between
nodes
Reentrant service ﬂow and job feed-
back
Complex
Table 1: Overview of queueing categories by increasing system complexity.
Queueing Category. To ensure the broad coverage of queueing mechanisms, we select twelve represen-
tative queueing scenarios as the basis for deﬁning our queueing categories which are listed in Table 1. The
reasons for selecting these twelve categories are as follows: (i) they cover the core queueing mechanisms
that frequently arise in queueing-system simulation, including arrival and service dynamics, server conﬁgu-
10


--- Page 11 ---
rations, capacity constraints and customer behaviors (e.g., blocking, balking, reneging), scheduling/priority
rules, and networked routing/feedback structures; and (ii) they cover most common queueing problems en-
countered in both the queueing literature and practical service systems. These twelve queueing categories
are organized into three groups of increasing complexity.
The basic group captures foundational queueing scenarios such as ﬁnite-capacity queues, general arrival
and service distributions, and multi-server scheduling rules. A wide range of classical queueing problems
fall under this group, such as M/M/1, M/M/1/K, M/M/c, M/G/1, and M/G/c, as well as renewal-arrival
single-server queues, ﬁnite-buffer loss systems, multi-server FCFS/LCFS priority queues, and settings with
general service-time distribution.
The intermediate group incorporates behavioral extensions including balking and reneging, batch ar-
rivals, multi-class customers, piecewise arrival processes, and production-kanban control. This group cov-
ers a broad set of queueing problems involving many extensions, such as M/M/1 with balking, M/M/1 with
reneging, the Erlang–A (M/M/c with abandonment), batch-arrival systems such as MX/M/1 and MX/G/1,
priority-based multi-class queues, piecewise-constant nonstationary arrival processes, and kanban-controlled
production queues. These categories incorporate a rich set of non-stationary and behavior-dependent mech-
anisms that substantially extend the modeling scope beyond basic stationary queues.
The complex group includes multi-mechanism and networked systems such as breakdown-maintenance
interactions, parallel two-resource synchronization, open networks, and feedback routing. It includes queue-
ing systems with multiple interacting mechanisms, including breakdown-repair dynamics in which servers
may fail and later return to service, two-resource synchronized service systems where each job must seize
two resources simultaneously, and open queueing networks with probabilistic routing between service sta-
tions. These scenarios create interdependencies between arrival, service, and routing behaviors across dif-
ferent resources or stations. Evidently, these categories represent the most intricate settings in our dataset,
which captures the multi-layer interactions that arise in manufacturing, service operations, and networked
stochastic systems.
Instruction Templates.
The instruction templates are designed to ensure linguistic diversity through
different phrasings and contextual emphases of the same task. These variations help the model learn that
semantically equivalent descriptions can differ substantially in wording, structure, or level of detail, thereby
improving the model’s robustness to language description in the provided instructions and reducing prompt
11


--- Page 12 ---
sensitivity.
Instruction Templates for the Batch Arrivals Category
Prompt 1 (T0) Concise and instructional
Batch arrival scenario: external arrival rate λ=0.641, batch size 15, and service rate µ=0.982. Run the simulation
up to time t=1100. Please implement a runnable Python+SimPy script that executes until the speciﬁed end time
and prints exactly two lines (six decimal places): Average waiting time and Utilization. Return only the raw Python
source codeno explanations or markdown formatting.
Prompt 2 (T1) Explanatory and context-rich
Consider a single-server batch-arrival model where inter-batch arrivals follow an exponential distribution with rate
λ=0.641, and each batch contains 15 customers arriving simultaneously. The service rate is µ=0.982, and the
simulation runs until t=1100. Provide a fully executable SimPy script that prints two summary statistics at the end,
Average waiting time and Server utilization, each formatted to six decimal places, without any additional text or
explanations.
Prompt 3 (T2) Another variant
Simulate a single-server batch arrivals system where the external arrival rate is λ=0.641, each batch contains 15
customers, and the service rate is µ=0.982. Run the simulation until time t=1100, and print exactly two lines
(six decimals): Average waiting time and Utilization. Only output the raw Python code without explanations or
markdown.
Figure 1: Three instruction templates for the batch arrivals category.
Figure 1 provides an example from the batch arrivals category, which shows three instruction templates
that convey the same mechanism using different phrasings, levels of context, and stylistic choices.
Code Templates. The code templates, which are Python code skeletons, provide diverse code structures
and multiple programming styles for the same queueing task. For each category, we present three code
templates, each characterized by a distinct code structure and programming style. Speciﬁcally, we construct
the following templates:
1. Procedural code template: organizes the simulation as a collection of top-level functions;
2. Object-oriented code template: encapsulates state and event logic within classes;
3. Functional code template: decomposes the simulation into smaller functions that explicitly pass state.
12


--- Page 13 ---
Implementation Templates for Batch Arrivals
T0 (Procedural) ﬂat script using global functions.
1
def customer(env, server, waits, busy, t_arr):
2
with server.request() as req:
3
yield req
4
st = env.now
5
yield env.timeout(random.expovariate(SERVICE_RATE))
6
waits.append(st - t_arr)
T1 (Object-oriented) encapsulated as a simulation class.
1
class BatchSim:
2
def customer(self, t_arr):
3
with self.res.request() as req:
4
yield req
5
st = self.env.now
6
yield self.env.timeout(random.expovariate(SERVICE_RATE))
7
self.waits.append(st - t_arr)
T2 (Functional) modular structure with helper functions.
1
def serve_one(env, res, t_arr, waits):
2
with res.request() as req:
3
yield req
4
st = env.now
5
yield env.timeout(random.expovariate(SERVICE_RATE))
6
waits.append(st - t_arr)
Figure 2: Representative code fragments from the batch arrivals category illustrating Three Implementation
Templates: procedural (T0), object-oriented (T1), and functional (T2).
We select these three templates because they cover the most common and mainstream code styles for
discrete-event simulation in Python and align well with practitioners’ programming habits. Figure 2 illus-
trates representative code fragments for the three code templates in the batch arrivals category.
By organizing the data into queueing categories, we ensure broad coverage of different queueing mecha-
nisms. The use of diverse templates introduces diversity in both instruction formulations and code structures.
The CTF design enables the construction of a high-quality dataset suitable for initial model training.
13


--- Page 14 ---
CTF Stage I Entry
1. Select Queueing Category (12)
2. Sample System Parameters
3. Select Instruction Template
(generated by LLM)
4. Select Code Template
(generated by LLM)
5. Insert Parameters into Templates
Raw Instruction–Code Pair
Validator:
Executability &
Output Format
Stage-I SFT Dataset
Pass
Figure 3: Stage I data construction pipeline of the Category-Template Framework (CTF).
3.1.3
Python-Based Generation of Instruction-Code Pairs under the CTF
In this section, we introduce the construction of the initial-stage dataset. The data construction process
proceeds as follows. For a task that belongs to a given queueing category, a Python program ﬁrst samples
the task-speciﬁc parameters associated with that category, such as arrival and service rates, queue capac-
ities, behavioral thresholds (e.g., balking or reneging probabilities), breakdown and repair rates, or other
mechanism-dependent quantities. The sampling ranges for these parameters are predetermined for each cat-
egory and are chosen to ensure that the instantiated queueing system remains stable. After the task-speciﬁc
parameters are obtained, the Python program then automatically selects one instruction template and one
code template from the pre-designed template pool of the corresponding category and inserts the sampled
parameters into both templates, which produces a concrete instruction-code pair. Figure 3 describes the data
construction process shown above.
The construction process above under CTF offers a practical advantage. Because each code template
is a parameter-free code skeleton, if the template itself satisﬁes the following three criteria, then all code
generated by the template will also satisfy these criteria. (1) executability, the code skeleton is syntactically
valid; (2) output-format compliance, the code skeleton produces the task-speciﬁed performance report; and
(3) instruction-code consistency, the template correctly encodes the queueing logic described in the instruc-
tion. This ensures the correctness of the generated data and substantially reduces the manual effort required
14


--- Page 15 ---
to validate whether each sample is suitable for inclusion in the training set.
Although the generated code does not require manual validation under this template-based guarantee,
we further enhance the reliability of the dataset by implementing a Python-based automatic checker for the
ﬁrst two criteria. This additional step ensures the dataset’s usability while keeping the validation cost low.
3.1.4
Supervised Fine-Tuning Using the CTF-Constructed Dataset
Using the dataset constructed under CTF, we perform the ﬁrst stage SFT to train the model on queueing-
system simulation tasks. Each training sample consists of an instruction describing a simulation task and
a corresponding SimPy code script that satisﬁes executability, output-format compliance, and instruction-
code consistency.
During SFT, the model is optimized to map natural-language instructions to complete simulation pro-
grams, which enables it to learn the structural patterns, queueing mechanisms, and reporting requirements
shared across diverse queueing scenarios. This supervised training stage serves as the ﬁrst step in our multi-
stage ﬁne-tuning pipeline and prepares the model for subsequent training stages.
3.2
Stage II: Masked-Completion Data Construction and Understanding-Oriented Fine-
Tuning
In the second training stage, we focus on strengthening the models understanding of queueing mechanisms.
We ﬁrst discuss the motivation behind Stage II training and then describe the construction of masked-
completion data, which trains the model to reconstruct missing segments of a simulation script and thereby
deepen its understanding of queueing logic. The resulting masked-completion data is subsequently used for
training in Stage II.
3.2.1
Motivation of Stage II
Training in Stage I already enhances the models capability to generate queueing executable SimPy pro-
grams, produce task-required performance reports in the correct standardized format, and implement the
basic queueing logic speciﬁed in natural-language instructions. However, training in a single stage is often
insufﬁcient: although the model can produce complete simulation scripts, its understanding of queueing
mechanisms remains relatively shallow and largely relies on surface pattern matching.
15


--- Page 16 ---
To further deepen the models understanding of queueing mechanisms, we introduce a second training
stage based on masked code completion. In this stage, functional segments of a simulation program are
deliberately masked, and the model is trained to reconstruct the missing segments based on the surrounding
code context and the instruction. This training process equips the model with the following capabilities:
1. the capability to infer missing functional segments from surrounding context, enabling accurate re-
construction of local logic blocks;
2. the capability to understand queueing mechanisms across diverse template realizations, natural-language
phrasing, and parameter conﬁgurations;
3. the capability to integrate reconstructed code segments seamlessly with the surrounding program,
ensuring structural and logical coherence.
By enhancing these abilities, the training process substantially deepens the models understanding of
queueing mechanisms. With masked-completion data, the model learns to associate natural-language de-
scriptions with the corresponding functional roles of different code segments, thereby deepening its under-
standing of queueing mechanisms. This process moves the model beyond memorization of complete scripts
and enables it to acquire an enhanced understanding of queueing-system simulation.
The training in this stage focuses on teaching the model to complete missing code segments, rather than
directly optimizing for end-to-end code generation. Although this objective deviates from that of the initial
supervised ﬁne-tuning, it substantially improves the models understanding of queueing mechanisms and is
therefore a necessary component of the overall training pipeline.
3.2.2
Mask Construction for Stage-II Masked-Completion Training
In this subsection, we describe the construction of masked-completion data. A straightforward way to
construct masked-completion data is to manually write a code script with a particular functional segment
removed and then provide its completed version as the training target. However, this approach suffers
from low generation efﬁciency and substantial manual effort, since each incomplete code script must be
completed and validated individually. To address this limitation, we propose a more efﬁcient procedure: we
begin with a complete code script and decompose it according to its functional segments. This allows a
single complete code script to produce multiple functional-masked code on different functional segments.
16


--- Page 17 ---
Masked-Completion Examples for the Batch Arrivals Category (Stage II)
A. Busy-time accumulation Complete the update of busy-time over [0, SIM_TIME].
1
# ... inside service block ...
2
s = random.expovariate(SERVICE_RATE); ed = st + s
3
# TODO: add busy-time accumulation (overlap with [0, SIM_TIME])
4
yield env.timeout(s)
B. Output format (two-line KPI) Restore printing of “Average waiting time” and “Utilization”.
1
# ... end of main script ...
2
aw = sum(waits)/len(waits) if waits else 0.0
3
util = busy_time / SIM_TIME if SIM_TIME > 0 else 0.0
4
# TODO: print two lines: Average waiting time / Utilization
C. Batch loop (multi-customer arrival) At each batch time, create BATCH_SIZE customers simultaneously.
1
# ... inside arrival source ...
2
yield env.timeout(random.expovariate(ARRIVAL_RATE)); t0 = env.now
3
# TODO: generate BATCH_SIZE customers at same time
D. Header parameters Fill in the top-level simulation parameters.
1
# TODO: add simulation parameters here
2
# RANDOM_SEED = ...; ARRIVAL_RATE = ...; BATCH_SIZE = ...
3
# SERVICE_RATE = ...; SIM_TIME = ...
Figure 4: Representative masked-completion tasks in Stage II for the batch arrivals category.
By adding an instruction that asks the model to reconstruct the missing segment and by using the original
complete code script as the target output, we obtain a masked-completion data.
Despite this improvement, a challenge remains: the functional segments within each code script would
still need to be identiﬁed and masked individually, which is also inefﬁcient. Under the CTF framework,
however, we only need to design a masking strategy once for each code template, rather than for every com-
plete code script. As a result, the decomposition can be applied automatically to all instruction-code pairs
generated from that template in Stage I. This makes efﬁcient large-scale generation of masked-completion
data.
The masking strategies we design cover the essential mechanisms of queueing-system simulation and
the critical components required for producing valid program outputs. Key examples include:
• arrival and batch-generation logic (e.g., single-customer arrivals, batch releases);
17


--- Page 18 ---
• service and resource operations (resource requests, releases, and service-time handling);
• state and busy-time updates (tracking busy intervals and computing utilization);
• routing and transition behavior (customer handoffs in networked systems);
• behavioral extensions (balking, reneging, and breakdown/repair routines);
• reporting and KPI formatting (the standardized performance report).
Figure 4 illustrates representative masked regions drawn from the batch arrivals category, which shows
how different masking strategies target distinct functional roles within the same template.
After obtaining the masked-completion dataset, we mix it with the Stage I instruction–code dataset for
Stage II training. This mixture brings two beneﬁts. First, the inclusion of instruction-code pairs in Stage
I prevents the model from overﬁtting to completion tasks and preserves the training effects established
in Stage I. Second, the mixture reduces the risk of catastrophic forgetting1, as the model is exposed to
both complete instruction-code pairs and masked-completion data that require reconstruction. Overall, this
mixed training dataset enables the model to acquire a deeper understanding of queueing mechanisms while
maintaining the improvment gained in Stage I. Figure 5 describes the data construction process shown above.
Stage II (Masked Completion)
Start from a code template in CTF
Identify functional segments
Mask one target segment (remove it and insert a placeholder)
Write an instruction to complete the missing segment
Masked-Completion Dataset
Figure 5: Stage II masked-completion data construction pipeline.
3.3
Stage III: Preference Data Construction and DPO-Based Alignment
In this section, we introduce the third stage of model training. At this stage, we employ Direct Preference
Optimization (DPO) to further align the models generation behavior with desired simulation-quality criteria.
1Catastrophic forgetting refers to the phenomenon in which a model loses previously acquired knowledge when trained sequen-
tially on new data or objectives; see Kirkpatrick et al. (2017).
18


--- Page 19 ---
We begin by explaining the motivation for adopting DPO and then describe the construction process of
corresponding preference data for Stage III training.
3.3.1
Motivation of Stage III
In Stage II, training based on masked-completion data enhances the models understanding of the underlying
mechanisms. However, two limitations still remain. First, the model may still generate recurring errors, as
supervised ﬁne-tuning does not explicitly penalize such error patterns. Second, when multiple codes are
plausible under the same instruction, supervised learning lacks a mechanism to express relative preferences
between them, which often reinforces bias in code generation.
To address these limitations, we introduce a third training stage based on DPO. By constructing pref-
erence pairs from the models own erroneous outputs, DPO directly optimizes the relative generation like-
lihood between preferred and rejected code scripts under the same instruction. In practice, this helps the
model avoid repeatedly making the same mistakes, learn preferences that supervised training alone cannot
express, and generate more stable and reliable code.
3.3.2
Preference Data Construction for Stage III
In this section, we introduce the process of constructing the DPO dataset. The goal is to obtain pairs of
chosen and rejected code scripts, where the rejected codes reﬂect the types of errors that the model actually
produces in realistic settings.
Stage III (DPO)
Run the Stage II model on a query set
Collect outputs that fail the evaluation metrics
Generate corrected solutions using GPT-4o (teacher model)
Manual validation to ensure correctness of the corrected code
Form preference pairs: chosen y+ vs. rejected y−
DPO Dataset
Figure 6: Stage III DPO data construction pipeline.
19


--- Page 20 ---
Compact Preference Pair for Batch Arrivals (Stage III DPO)
Instruction
Simulate a batch-arrival single-server queue with λ=0.510, batch size 14, µ=0.674, horizon t=1491. Print exactly two
lines (six decimals): “Average waiting time:” and “Utilization:”. Return raw Python code only.
Chosen (y+) key excerpts
1
def batch_arrivals(env, server, waits, busy):
2
while True:
3
yield env.timeout(random.expovariate(ARRIVAL_RATE))
4
t0 = env.now
5
for _ in range(BATCH_SIZE):
#
same-time batch
creation
6
env.process(customer(env, server, waits, busy, t0))
7
# ...
8
aw = sum(waits)/len(waits) if waits else 0.0
9
util = busy[0] / SIM_TIME if SIM_TIME > 0 else 0.0
#
overlap
-clipped utilization
10
print(f"Average waiting time: {aw:.6f}")
#
exact
two-line format
11
print(f"Utilization: {util:.6f}")
Rejected (y−) key excerpts
1
def batch_arrivals(env, server, waits, busy, up_down):
2
yield env.timeout(random.expovariate(ARRIVAL_RATE))
3
env.process(customer(env, server, waits, busy, up_down))
#
single arrivals only
4
# ...
5
env.process(up_down_proc(env, server, up_down))
#
irrelevant failure logic
6
# ...
7
# (format inconsistencies / extra outputs omitted)
Notes
The Chosen code restores true batch-at-once arrivals and preserves the two-line KPI contract, whereas the rejected one introduces
unrelated failure toggling and does not synchronize customers within a batch.
Figure 7: DPO training example with abbreviated code; only key differences are shown for brevity.
The construction process begins by generating model outputs using the Stage II model on a curated held-
out query set. This set is balanced across queueing categories and complexity levels. Its instructions are
strictly held out from all Stage I/II training data to prevent leakage. The models inference output for each
question in this query set is evaluated using three criteria: (i) executability, (ii) output-format compliance,
and (iii) instruction consistency. Outputs that fail any of these criteria are retained as rejected candidates
20


--- Page 21 ---
(y−). To obtain the corresponding Chosen candidates (y+), corrected versions are ﬁrst generated by a
stronger reference model, GPT-4o, and manually validated to ensure correctness. Then, each pair (y+, y−)
presents a clear contrast and can be used for DPO training. Figure 6 describes the data construction process
shown above.
Figure 7 illustrates one such preference pair from the batch arrivals category. The chosen code restores
same-time batch creation and the output format KPI, whereas the rejected one incorrectly generates single
arrivals, inserts irrelevant failure logic, and violates the output format requirement. This example shows a
rejected generated code in supervised training stage and motivates the need for preference-based alignment.
4
Experiments
In this section, we present empirical evaluations conducted on models which are trained on the datasets
generated under the proposed multi-stage data construction and ﬁne-tuning framework. The experiments
aim to assess the improvements in executability rate, output-format compliance, and instruction-code con-
sistency across diverse queueing-system categories. The following subsections detail the experimental setup,
evaluation metrics, and results across ﬁne-tuning stages, categories, and representative examples.
4.1
Experimental Setup
In this section, we describe the experimental setup. We details the models, training data, training conﬁgura-
tion, and evaluation metric adopted in experiments.
Models and Initialization.
We ﬁne-tune two open-source code language models, Qwen2.5-Coder-7B and
DeepSeek-Coder-6.7B, as representative compact code LLMs. Both models are initialized from their pub-
licly released instruction-tuned checkpoints and use identical tokenizer conﬁgurations for a fair comparision.
Training Data.
All training data are derived from the multi-stage data construction and ﬁne-tuning frame-
work. Stage I constructs 7,200 validated instruction-code pairs (600 per category-template combination)
for supervised ﬁne-tuning. In Stage II the model is trained on a dataset of 20,000 samples, which includes
both the newly created masked-completion data and the original Stage 1 data. Stage III constructs 380
human-validated preference pairs for Qwen2.5-Coder-7B and 420 for DeepSeek-Coder-6.7B.
21


--- Page 22 ---
Training Conﬁguration.
All stages are trained using the AdamW optimizer under cosine decay schedul-
ing with a weight decay of 0.1, and a maximum sequence length of 2048 tokens. Stage I is trained for three
epochs with a learning rate of 2 × 10−5, Stage II continues for two epochs with a reduced rate of 1 × 10−5,
and Stage III applies Direct Preference Optimization (DPO) for one to two epochs, with the preference
strength parameter set to β = 0.2. Training is performed on a single NVIDIA RTX 4090 GPU (24 GB
memory) with a batch size of 2 and a maximum sequence length of 2048 tokens.
Test Set and Evaluation Metric.
Evaluation is conducted on a held-out test set of 600 tasks (50 per
category) balanced across difﬁculty levels and instruction variants. Each model is evaluated by executing its
generated code scripts and measuring three criteria:
1. executability rate: which determines whether the generated simulation code runs to completion with-
out runtime errors;
2. output format compliance: which validates that the program produces the required standardized
reporting lines (e.g., average waiting time and utilization) in the correct numerical format;
3. instruction-code consistency: which assesses whether the operational logic implemented by the gen-
erated code faithfully satisﬁes the simulation speciﬁcation described in the instruction.
The ﬁrst two criteria are straightforward to evaluate, as both can be automatically evaluated through code
execution and output validation in Python environment. A code script passes the executability check if it
runs to completion within the time limit without raising runtime errors, and passes the output format check
if it prints the required output lines, such as Average waiting time and Utilization.
However, validating instruction-code consistency is challenging as it requires manually reviewing every
generated code script, which is highly time-consuming. To address this challenge, we adopt an objective
evaluation method: the assessment of instruction-code consistency is conducted by a large-model-based
evaluator, motivated by recent evidence that LLMs can function as reliable judges for code-generation tasks
(Tong and Zhang, 2024). In our setting, DeepSeek acts as an independent evaluator: for each generated
code script, it examines the script together with the corresponding task description and determines whether
the code script satisﬁes the required logic and semantic.
22


--- Page 23 ---
Evaluation Prompt for Instruction Consistency
SYSTEM_PROMPT
You are an expert evaluator for queueing theory and discrete-event simulation code. Task: Based on the task
description + provided Python/SimPy code, determine whether the implementation follows the intended speciﬁcation.
Evaluation Principles:
1. Executable: The generated script must run from start to ﬁnish without raising syntax errors, runtime exceptions, or unresolved
references.
2. Structural integrity: The code must clearly include core SimPy elements such as Environment, Resource, Process,
request, and timeout.
3. Precise mechanism matching: The mechanisms required in the prompt must be correctly implemented in the code logic, not
merely mentioned or partially simulated.
4. Logical correctness: The generated code must be logically sound and fully consistent with the task description, implementing
all required processes and mechanisms without missing or irrelevant components.
Expected Output (strict JSON format):
1
{"label": 1 or 0, "reasons": ["brief reason", "..."]}
Explanation:
• "label":
1 all structures and mechanisms are complete; logic is correct.
• "label":
0 missing key mechanisms or incorrect logic.
• No extra natural-language commentary beyond the JSON is allowed.
Figure 8: Strict evaluation prompt used by the large-model-assisted evaluator for automatic instruction-
consistency assessment.
As shown in Figure 8, the evaluation process is driven by a structured system prompt. The prompt en-
forces strict rules to ensure that each submission uses the appropriate SimPy primitives, such as Environment,
Resource, Process, and request. It further requires that the code implements all mechanisms explic-
itly speciﬁed by the task description, including capacity limits, balking and reneging behaviors, and routing
or feedback in networked systems. Code scripts that merely mention relevant keywords without implement-
ing their functional logic are rejected. This design ensures that the consistency metric accurately reﬂects
whether the generated code follows the intended queueing logic.
23


--- Page 24 ---
4.2
Performance Comparison
This subsection introduces the performance of the ﬁne-tuned models on the held-out test set. The results
demonstrate that the proposed dataset construction and training framework enhance the executablility, output
format compliance and instruction-code consistency of queueing-simulation code generation.
Metric
Qwen2.5-Coder-7B
DeepSeek-Coder-6.7B
Pre-trained
Fine-tuned
Pre-trained
Fine-tuned
Executability Rate (%)
80.4
86.0
26.1
75.0
Output Format Compliance (%)
80.4
86.0
0.09
74.8
Instruction Consistency (%)
0
76.8
0
62.3
Table 2: Performance comparison on the held-out test set .
As shown in Table 4.2, ﬁne-tuning on the proposed dataset leads to substantial improvement across all
three evaluation metrics. For Qwen2.5-Coder-7B, executability and output format compliance both rise
from 80.4% to over 86.0%, while instruction-code consistency improves dramatically from 0% to 76.8%.
This result indicates that the baseline Qwen model already possesses strong capabilities in generating ex-
ecutable code with correct standardized outputs, but lacks an understanding of the domain-speciﬁc logic
required in queueing-system simulation tasks. DeepSeek-Coder-6.7B exhibits a sharp improvement, with
executability increasing from 26.1% to 75.0%, output compliance from 0% to 74.8%, and instruction-code
consistency rising from 0% to 62.3%, which implies that ﬁne-tuning effectively improves the models capa-
bility in generating executable code with correct standardized outputs and correct queueing logic. This result
suggests that the baseline DeepSeek model lacks prior knowledge of queueing-speciﬁc code generation and
therefore beneﬁts substantially from domain-targeted ﬁne-tuning.
This result also shows that our ﬁne-tuning process enhances more than surface syntactic or formatting
correctness. More importantly, it enables the model to generate logically valid queueing simulation code
which reﬂect that our model has obtained a substantially deeper understanding of queueing mechanisms.
Such improvement demonstrates that domain-speciﬁed ﬁne-tuning enables compact open models to acquire
strong capabilities in the queueing-system simulation code generation.
24


--- Page 25 ---
4.3
Category-wise Results
In this section, we present a detailed analysis of the performance within each queueing category which
provides a clearer view of the model behavior across different types of queueing systems. The detailed
per-category results for both ﬁne-tuned models are shown in Tables 3 and 4. Each table reports the pre-
and post-ﬁne-tuning performance across twelve representative queueing categories in terms of executability,
output-format compliance, and instruction-code consistency.
Template
Pre-trained Qwen
Qwen2.5-Coder-7B (DPO)
Exec
Format Consistency
Exec
Format Consistency
(A) Basic and Stationary Systems
ﬁnite_capacity
100.0
100.0
0.0
100.0
100.0
97.5
general_distributions
100.0
100.0
0.0
100.0
100.0
88.5
multi_server_sched_rules 100.0
100.0
0.0
100.0
100.0
87.7
(B) Intermediate Behavioral Extensions
balking_reneging
5.0
5.0
0.0
67.5
67.5
57.5
batch_arrivals
90.0
90.0
0.0
87.5
87.5
72.5
multi_class_customers
82.5
82.5
0.0
95.0
95.0
82.5
piecewise_arrival
90.3
90.3
0.0
93.5
93.5
62.9
production_kanban
100.0
100.0
0.0
100.0
100.0
82.8
(C) Complex Networked or Multi-Mechanism Systems
breakdown_maintenance
52.5
52.5
0.0
70.0
70.0
32.5
parallel_two_resources
100.0
100.0
0.0
96.9
96.9
95.4
open_network
93.0
93.0
0.0
93.0
93.0
33.3
feedback_network
55.0
55.0
0.0
60.0
60.0
52.5
Average
80.4
80.4
0.0
86.0
86.0
76.8
Table 3: Performance of Qwen2.5-Coder-7B before and after ﬁne-tuning (DPO stage) across 12 simulation
templates. Values are pass rates (%).
Table 3 summarizes the performance of Qwen2.5-Coder-7B before and after the full three-stage ﬁne-
tuning process across the twelve queueing-system templates. Across these categories, the majority shows
substantial performance improvements in instruction-code consistency.
1. For the basic categories, instruction-code consistency improves dramatically, rising from essentially
25


--- Page 26 ---
Template
Pre-trained DeepSeek
DeepSeek-Coder-6.7B (DPO)
Exec Format Consistency
Exec
Format
Consistency
(A) Basic and Stationary Systems
ﬁnite_capacity
32.5
12.5
0.0
67.5
67.5
35.0
general_distributions
14.8
4.9
0.0
91.8
91.8
80.3
multi_server_sched_rules
40.4
21.1
0.0
70.0
70.0
47.4
(B) Intermediate Behavioral Extensions
balking_reneging
12.5
0.0
0.0
95.0
95.0
82.5
batch_arrivals
17.5
5.0
0.0
95.0
95.0
62.5
multi_class_customers
15.0
10.0
0.0
100.0
100.0
97.5
piecewise_arrival
35.5
8.1
0.0
72.6
72.6
40.3
production_kanban
8.6
1.7
0.0
70.7
70.7
63.8
(C) Complex Networked or Multi-Mechanism Systems
breakdown_maintenance
42.5
17.5
0.0
87.5
87.5
82.5
parallel_two_resources
27.7
3.1
0.0
70.0
70.0
55.4
open_network
35.1
14.0
0.0
55.0
55.0
26.3
feedback_network
30.0
20.0
0.0
97.5
97.5
57.5
Average
26.2
9.5
0.0
75.0
74.8
62.3
Table 4: Performance of DeepSeek-Coder-6.7B before and after ﬁne-tuning (DPO stage) across 12 simula-
tion templates. Values are pass rates (%).
zero to levels exceeding 85% after the full three-stage training. Categories such as ﬁnite_capacity
and multi_server_sched_rules show particularly strong gains, with consistency increasing from 0 to
nearly 98% and 88%, respectively.
2. The intermediate categories also exhibit substantial improvements. For instance, multi_class_customers
increases from 0 to more than 82%, and batch_arrivals rises from 0 to over 72%. These results in-
dicate that the model successfully internalizes behavioral extensions beyond the stationary setting
and can produce acquired logic for tasks involving balking, reneging, batching, or multiple customer
classes.
3. Among the complex categories, several templates also attain impressive gains despite their struc-
tural difﬁculty. The parallel_two_resources template increases from to more than 95%, and even
26


--- Page 27 ---
feedback_network, whose mechanism is very complex, improves from to above 52% relative to the
pre-trained baseline. However, two of the complex categories show only limited improvement. break-
down_maintenance and open_network both increase from 0 to roughly 33 %. This is because both
categories involve multi-layer event interactions and stochastic dependencies that are inherently difﬁ-
cult for the model to learn from the available examples. Even so, the observed progress still illustrates
that the three-stage training procedure strengthens the models ability to capture complex queueing
logic, though further targeted data would be needed to achieve higher consistency in these cases.
It is also worth noting that a small number of templates like parallel_two_resources and batch_arrivals
display mild decreases in executability or output-format compliance after training. This is because the
second-stage masked-completion training introduces trade-offs that shift the model’s focus toward recon-
structing the correct operational logic. The three-stage framework places greater emphasis on the instruction-
code consistency of queueing mechanisms, so the model occasionally sacriﬁces surface-level formatting or
full execution stability in order to generate code that more accurately reﬂects the intended behavior.
Table 4 summarizes the performance of DeepSeek-Coder-6.7B. Similar to the results observed for
Qwen2.5-Coder-7B, most categories exhibit substantial improvements in instruction-code consistency. The
gains cover basic, intermediate, and complex queueing categories, which indicates that the three-stage frame-
work helps the model acquire a better understanding of queueing logic.
Unlike Qwen, however, DeepSeek exhibits several irregular patterns that suggest its pre-training char-
acteristics are not as well aligned with queueing-style simulation tasks. In particular, some of the simple
stationary categories show only moderate post-training performance, even though these tasks do not involve
complex mechanisms. This difference from Qwen2.5-Coder-7B suggests that ﬁne-tuning with a similar
dataset and conﬁguration is less effective when the pre-trained model is less stable in generating basic cate-
gories.
In all, the results for both Qwen2.5-Coder-7B and DeepSeek-Coder-6.7B demonstrate that the three-
stage ﬁne-tuning framework consistently improves model reliability across the twelve queueing-system cate-
gories. Executability, output-format compliance, and instruction-code consistency show clear improvements.
These patterns indicate that the training pipeline enables the models to better understand the mechanisms
and logic of queueing systems, thereby allowing them to generate more stable compliant and more reliable
simulation code across a diverse range of queueing settings.
27


--- Page 28 ---
4.4
Stage-wise Ablation Study
To examine the improvement each training stage contributes to model performance, an ablation study is
conducted based on the overall metrics for Qwen and DeepSeek. We track the executability, output-format
compliance, and instruction-code consistency across training stages. Table 5 summarizes these stage-wise
effects, which reports the aggregated performance changes across the three training stages for both models.
Model & Stage
Executability
Format
Consistency
Qwen2.5-Coder-7B
Stage-1 (SFT-1)
80.7
80.7
46.2
Stage-2 (SFT-2)
85.0
85.0
70.3
Stage-3 (DPO)
86.0
86.0
76.8
DeepSeek-Coder-6.7B
Stage-1 (SFT-1)
74.7
74.2
44.3
Stage-2 (SFT-2)
67.2
66.8
62.7
Stage-3 (DPO)
75.0
74.8
62.3
Table 5: Overall ablation results for Qwen and DeepSeek across three training stages.
As shown in Table 5, the two models exhibit distinct behavior across training stages. For Qwen2.5-
Coder-7B, performance steadily improves at every stage: executability and output-format compliance rise
from 80.7% to 86.0%, and instruction consistency grows from 46.2% to 76.8%.
In contrast, for DeepSeek-Coder-6.7B, the three-stage pipeline produces a distinct learning trajectory.
During Stage 2, the masked-completion objective signiﬁcantly boosts instruction-code consistency, raising
it from 44.3% to 62.7%. However, this gain comes with a reduction in executability and output-format
compliance. This is because the masked-completion task optimizes a different learning objective than our
target code-generation task. As a result, improvements on masked completion do not necessarily lead to
improvements on the evaluation metrics (i.e., executability, output-format compliance, and instruction-code
consistency), which are generation-oriented, and the mismatch can even shift the model away from behaviors
that those metrics reward. DeepSeek appears more sensitive to this objective mismatch, which leads to
performance declines under our evaluation.
In Stage 3, the DPO-based preference optimization helps correct these effects. Executability and for-
mat accuracy recover to 75.0% and 74.8% respectively, which indicates that preference signals successfully
28


--- Page 29 ---
guide the model toward more stable and well-formed code. This adjustment, however, slightly lowers consis-
tency, as the optimization shifts part of the models focus back toward structural correctness and formatting.
To conclude, these patterns show that DeepSeek is more sensitive to the masked-completion objective
than Qwen, but it ultimately converges to a balanced generator of queueing-system simulation code.
4.5
Error Analysis
To better understand the limitations that remain after the full three-stage training process, in this section we
further examine the generated scripts and analyze the speciﬁc queueing mechanisms that the models still
struggle with. This analysis identiﬁes the queueing scenarios in which the model still exhibits weaknesses
and highlights where inconsistencies are most likely to arise. Examining these failure modes provides
a clearer understanding of the remaining gaps and informs future directions for data augmentation and
targeted reﬁnement. Two common issues that appear both in Qwen2.5-Coder-7B and Deepseek-Coder-6.7B
are introduced as follows.
Network Routing and Custom Movement
One issue often arises when the model needs to generate
code for systems that involve multiple service nodes. Although many of the generated scripts can execute
successfully, they often fail to maintain the intended custom movement of customers across nodes in an
open network. Instead of transferring a customer from one node to the next after service completion, the
model may generate a new and unrelated arrival at the downstream node. This breaks the required routing
structure and leads to network behavior that does not match the task description.
Figure 9 shows that the model does not preserve the intended custom movement of customers between
service nodes in an open network. In this code segment, each external arrival at Node1 immediately and
independently triggers a possible arrival at Node2 through a second call to serve with the current simula-
tion time env.now. As a result, customers at Node2 do not originate from completed service at Node1,
but are instead created as new and unrelated arrivals. This breaks the intended routing structure, because the
ﬂow from Node1 to Node2 should depend on service completion events and the routing probability P12,
rather than on the initial external arrival stream.
Interrupt-Driven Service and Remaining-Time Management
A second type of issue appears in sce-
narios that require interruption-based service logic, including systems with breakdown and repair events.
Although the model is generally able to construct the server state machine, initiate interruptions, and resume
29


--- Page 30 ---
Instruction and Erroneous Code segment (Open Network Routing)
Instruction
Simulate a two-node open queueing network where jobs arrive at Node 1, receive service, then move to Node 2 with probability
p12. Run to time t and print exactly two lines: “Average waiting time:” and “Utilization:” (six decimals). Return Python code only.
Error Code
1
def arrivals(env, n1, n2):
2
while True:
3
yield env.timeout(random.expovariate(ARRIVAL_RATE))
4
env.process(serve(env, n1, env.now))
5
#
incorrectly spawns new arrivals into Node2
6
if random.random() < P12:
7
env.process(serve(env, n2, env.now))
Figure 9: Representative failure case for network routing semantics.
service after the failure is resolved, it does not always maintain a correct record of the remaining service
time at the moment of interruption. This mistake does not prevent the script from running, but it alters the
simulated service trajectory by allowing a customer to redo portions of service that were already completed
before the breakdown.
As shown in Figure 10, The error lies in the handling of the remaining service time after a breakdown
interrupt. At the start of service, the code samples a total service requirement and stores it in remaining.
Inside the loop, a breakdown correctly triggers an interrupt, and the code records the elapsed busy time
during the current attempt. However, after the interruption, the statement remaining = max(0.0,
remaining) does not subtract the elapsed time from remaining. The job therefore resumes service
with the same total requirement as before the breakdown, as if no work had been completed. This mistake
causes each interruption to restart part of the service instead of resuming from the true residual time. As a
result, the simulated service durations are inﬂated and the estimates of utilization and waiting time become
biased.
Overall, these two recurrent error patterns reveal that the model is still weak in several queueing-system
simulation tasks. The ﬁrst issue shows that the model struggles to preserve custom movement across nodes in
networked systems, and the second issue indicates unstable handling of state-dependent updates in interrupt-
driven service. These observations suggest that routing semantics and interruption-resumption mechanics
remain challenging areas, and they point to important directions for future data augmentation and targeted
training.
30


--- Page 31 ---
Prompt and Erroneous Code Snippet (Breakdown with Interrupt-Driven Service)
Prompt
Simulate a single-server queue with random breakdowns and repairs. Jobs arrive according to a Poisson process, and the server
alternates between “up” and “down” states with exponential mean time between failures (MTBF) and mean time to repair (MTTR). When
a breakdown occurs during service, the job should resume with its remaining service time once the server is repaired. Run the simulation
to time t and print exactly two lines: “Average waiting time:” and “Utilization:” (six decimals).
Error Code (erroneous excerpt)
1
t_start = self.env.now
2
remaining = random.expovariate(SERVICE_RATE)
3
while remaining > 0:
4
self._on = True
5
self._t0 = self.env.now
6
try:
7
yield self.env.timeout(remaining)
8
self.busy_time += self.env.now - self._t0
9
self._on = False; self._t0 = None
10
remaining = 0.0
11
except simpy.Interrupt:
12
# breakdown interrupt
13
elapsed = self.env.now - self._t0
14
if elapsed > 0:
15
self.busy_time += elapsed
16
self._on = False; self._t0 = None
17
yield self.up_event
18
remaining = max(0.0, remaining)
Figure 10: Representative failure case for interrupt-driven service with breakdowns.
References
Banks, J., J. Carson, B. L. Nelson, and D. Nicol (2004). Discrete-Event System Simulation (4th ed.). Prentice
Hall.
Bistarelli, S., F. Fioravanti, and F. Santini (2025). Using large language models for code generation tasks: A
systematic literature review. SN Computer Science 6(1), 58.
Cezik, M. T. and P. L’Ecuyer (2008). Stafﬁng multiskill call centers via linear programming and simulation.
Management Science 54(2), 310–323.
Dorton, S. and D. Liu (2012). Analysis of airport security screening checkpoints using queuing networks
and Discrete-Event simulation. 62nd IIE Annual Conference and Expo 2012, 63–72.
31


--- Page 32 ---
Gans, N., G. Koole, and A. Mandelbaum (2003). Telephone call centers: Tutorial, review, and research
prospects. Manufacturing & Service Operations Management 5(2), 79–141.
Gao, C., X. Lan, N. Li, Y. Yuan, J. Ding, Z. Zhou, F. Xu, and Y. Li (2024).
Large language models
empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social
Sciences Communications 11.
Hsu, L.-F., C. S. Tapiero, and C. Lin (1993). Network of queues modeling in ﬂexible manufacturing systems:
A survey. RAIRO–Operations Research 27(2), 201–248.
Huang, C., Z. Tang, S. Hu, R. Jiang, X. Zheng, D. Ge, B. Wang, and Z. Wang (2025). ORLM: A cus-
tomizable framework in training large models for automated optimization modeling.
Operations Re-
search 73(6), 2986–3009.
Jun, J. B., S. H. Jacobson, and J. R. Swisher (1999). Application of Discrete-Event simulation in health care
clinics: A survey. Journal of the Operational Research Society 50(2), 109–123.
Kirkpatrick, J., R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ra-
malho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell (2017). Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences 114(13),
3521–3526.
Law, A. M. and D. M. Kelton (1999). Simulation Modeling and Analysis (3rd ed.). McGraw-Hill Higher
Education.
Li, Y., D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,
A. Dal Lago, T. Hubert, P. Choy, C. de Masson d’Autume, I. Babuschkin, X. Chen, P.-S. Huang,
J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Fre-
itas, K. Kavukcuoglu, and O. Vinyals (2022). Competition-level code generation with AlphaCode. Sci-
ence 378(6624), 1092–1097.
Nelson, B. (2013). Foundations and Methods of Stochastic Simulation: A First Course. Springer.
Rafailov, R., A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn (2023). Direct preference op-
timization: Your language model is secretly a reward model. Advances in neural information processing
systems 36, 53728–53741.
32


--- Page 33 ---
Rema, V. and K. Sikdar (2021). Optimizing patient waiting time in the outpatient department of a multi-
specialty indian hospital: The role of technology adoption and queue-based monte carlo simulation. SN
Computational Science 2(3), 200.
Ridnik, T., A. Ben-Cohen, O. Basha, and I. Friedman (2024). AlphaCodium: From prompt engineering to
ﬂow engineering in large language model code generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, pp. 5604–5613.
Rozière, B., J. Gehring, et al. (2024). Code Llama: Open foundation models for code.
Schruben, L. and E. Yucesan (1993). Modeling paradigms for discrete-event simulation. Operations Re-
search Letters 13(5), 265–275.
Tako, A. A. (2011). Model development in discrete-event simulation: Insights from six expert modelers. In
Proceedings of the 2011 Winter Simulation Conference (WSC), pp. 3923–3934.
Tong, W. and T. Zhang (2024). CodeJudge: Evaluating code generation with large language models.
Zhang, W., Y. Chen, and H. Liu (2025). Large language models for code generation: An empirical study
across programming tasks. Journal of Systems and Software 212, 112046.
Zinoviev, D. (2024). Discrete event simulation: It’s easy with SimPy!
33
