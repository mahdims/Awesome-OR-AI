--- Page 1 ---
Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis
Richard Roberson1, Gowtham Kaki1, Ashutosh Trivedi1
1University of Colorado Boulder, Boulder, Colorado, USA
richard.roberson@colorado.edu, gowtham.kaki@colorado.edu, ashutosh.trivedi@colorado.edu
Abstract
This study investigates various approaches to using Large
Language Models (LLMs) for Text-to-SQL program synthe-
sis, focusing on the outcomes and insights derived. Employ-
ing the popular Text-to-SQL dataset, spider, the goal was to
input a natural language question along with the database
schema and output the correct SQL SELECT query. The ini-
tial approach was to fine-tune a local and open-source model
to generate the SELECT query. After QLoRa fine-tuning
WizardLM’s WizardCoder-15B model on the spider dataset,
the execution accuracy for generated queries rose to a high
of 61%. With the second approach, using the fine-tuned gpt-
3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error cor-
rection), the execution accuracy reached a high of 82.1%. Of
all the incorrect queries, most can be categorized into a seven
different categories of what went wrong: selecting the wrong
columns or wrong order of columns, grouping by the wrong
column, predicting the wrong values in conditionals, using
different aggregates than the ground truth, extra or too few
JOIN clauses, inconsistencies in the Spider dataset, and lastly
completely incorrect query structure. Most if not all of the
queries fall into these categories and it is insightful to under-
standing where the faults still lie with LLM program synthe-
sis and where they can be improved.
1
Introduction
Text-to-SQL program synthesis involves converting natural
language (NL) questions into SQL queries for database in-
teraction. Historically, this task has been approached using
smaller sequence-to-sequence (seq2seq) pre-trained models,
which, until recently, represented the forefront of Text-to-
SQL methodologies (Li et al. 2023). Recent advancements
in large parameter pre-trained language models, such as gpt-
3.5-turbo and gpt-4, have shifted the paradigm in Text-to-
SQL research (Dong et al. 2023; Gao et al. 2023). The in-
troduction of the spider1 dataset by Yale researchers in 2019
marked a significant milestone in Text-to-SQL research (Yu
et al. 2019). These models have not only outperformed tra-
ditional seq2seq approaches but have also established new
benchmarks on the spider leaderboard.
Open-source LLMs are characterized by their publicly
available model weights and the ability to undergo fine-
tuning. The resulting model, named Spider Skeleton Wizard
1https://yale-lily.github.io/spider
Coder, demonstrated competitive zero-shot Text-to-SQL ca-
pabilities, rivaling those of ChatGPT (gpt-3.5-turbo). Then,
in response to challenges encountered, our research piv-
oted towards closed-source models. This shift was also
when OpenAI introduced fine-tuning capabilities for gpt-
3.5-turbo in their API. The ability to train state-of-the-art
LLMs opened new avenues for Text-to-SQL program syn-
thesis. This paper aims to detail the strategies employed us-
ing both open and closed-source models, evaluate the results
from each, and extract key insights crucial for advancing re-
search in this field.
2
Open Source Models
For our first set of experiments, we chose to fine-tune Wiz-
ardLM’s WizardCoder, a 15B parameter model renowned
for its proficiency in programming tasks (Luo et al. 2023).
This decision was driven by the need for a coding model, es-
pecially pertinent to the spider dataset in Text-to-SQL pro-
gram synthesis. The fine-tuned model, named Spider Skele-
ton Wizard Coder, attained an execution accuracy of 61% On
the spider development set comprising 1,034 entries. The
model is publicly available on the HuggingFace Transform-
ers Library2.
Fine-tuning
With the goal of minimizing costs, LoRA
fine-tuning, denoting low-rank adapters, modifies only a
fraction of the total model weights. This results in a reduced
memory footprint during training. The compact LoRA is
then integrated with the complete model weights, tailor-
ing the model closer to the training data. QLoRA further
minimizes the memory footprint by employing quantized
weights for LoRA training, a method nearly as effective as
full model fine-tuning (Dettmers et al. 2023).
Data
The enhanced spider dataset for fine-tuning includes
the database schema for each entry, along with the skeleton
format in the responses. Figure 1 illustrates an example from
this dataset. The training data response encompasses both
the skeleton and the actual SQL query, prompting the model
to generate responses in a similar format.
Challenges
While Spider Skeleton Wizard Coder demon-
strates commendable zero-shot execution accuracy, it still
2https://huggingface.co/richardr1126/spider-skeleton-wizard-
coder-merged
arXiv:2401.12379v1  [cs.AI]  22 Jan 2024


--- Page 2 ---
Figure 1: Shows the open-source fine-tuning dataset format
for a single entry. The bold sections show the spider NL
question, the database schema format, and the skeleton for-
mat in the response (Li et al. 2023).
falls short. The model, fine-tuned on zero-shot data, strug-
gles with few-shot in-context learning — the capacity to pro-
cess follow-up instructions for, in this case, SQL query cor-
rection and repair (Brown et al. 2020). This limitation con-
trasts with the capabilities of OpenAI’s closed-source mod-
els like gpt-3.5-turbo, gpt-3.5-turbo-16k, and gpt-4-turbo,
which excel in few-shot learning due to extensive training
on such data.
3
Closed Source Models
For our second set of experiments, we start with the applica-
tion of gpt-3.5-turbo-16k, an extended context length variant
of gpt-3.5-turbo, in a zero-shot configuration. The objective
was to assess the baseline capabilities of these models prior
to implementing multi-shot techniques and fine-tuning. Ex-
isting research, particularly the state-of-the-art zero-shot ap-
proach for gpt-3.5-turbo, utilizes the C3 method (Dong et al.
2023). We follow some their methods by using a prompt
alignment strategy and a database schema format. Refer to
Figure 2.
In preliminary trials, conducted without a clear database
context and prompt alignment, and conducted mainly for
comparison with Spider Skeleton Wizard Coder, the exe-
cution accuracy stood at 57.6%. The transition from open-
source to closed-source methodologies marked the adoption
of more robust strategies for OpenAI’s models akin to those
in C3 (Dong et al. 2023). This approach, within a zero-shot
framework, achieved an execution accuracy of 68.2%.
Fine-tuning
The subsequent phase involved fine-tuning
OpenAI’s gpt-3.5-turbo-16k on the spider dataset, incorpo-
rating database context. The fine-tuning process was facili-
tated using OpenAI’s API. Even in a zero-shot environment,
the fine-tuned model demonstrated a substantial improve-
ment, achieving a 73.4% execution accuracy in generating
SQL queries, highlighting the effectiveness of fine-tuning
models.
Figure 2: Shows the clear context database schema format
that OpenAI suggests using for Text-to-SQL program syn-
thesis with their models. The C3 method, zero-shot state-of-
the-art, also uses this db format (Dong et al. 2023).
Example
Driven
Correction
The
incorporation
of
example-driven correction, utilizing output examples to
represent the expected result table after executing the
queries on a real database, significantly enhanced the
execution accuracy. To streamline this approach, we utilized
the ground truth (gold) result table in markdown format.
Once the fine-tuned gpt-3.5-turbo-16k generates a SQL
query, its output is compared with the provided example. If
there is a discrepancy, the model is informed that the output
of the generated query does not align with expectations.
Subsequently, the model is presented with the correct result
table output example and prompted to revise the query
based on this new information.
Error Driven Correction
Next, a phase known as Er-
ror Driven Correction, is activated if the SQL query, post-
Example Driven Correction, still fails to align with the out-
put example. This phase eliminates the context history from
the preceding shots and establishes a fresh zero-shot envi-
ronment for error correction, employing gpt-4-turbo instead
of the fine-tuned gpt-3.5-turbo-16k. The strategy involves
executing the previously generated SQL query on the ac-
tual database from the spider dataset to detect any execution
errors. Should an execution error be identified, it is incorpo-
rated into the input provided to gpt-4-turbo. This zero-shot
configuration worked better than a few-shot approach did for
error correction. By integrating these correction techniques,
the model’s execution accuracy notably increased, reaching
a peak of 82.1%.
4
Insights
The following discussion centers on the highest execution
accuracy achieved: the spider dataset fine-tuned version of
gpt-3.5-turbo-16k, supplemented by gpt-4-turbo for error
correction. This section will delve into the key insights
gleaned from the SQL queries generated by this approach,
particularly focusing on their comparison with the ground
truth (gold) queries.
The spider dataset can be a very challenging dataset for
the extra hard queries. The 82.1% peak execution accu-
racy comes from the official spider evaluation script, which
groups queries based on difficulty, assigns a score for each
SQL hardness category, then does a weighted average to get
the final execution accuracy number. We will focus on the


--- Page 3 ---
Easy
Medium
Hard
Extra
All
Count
248
446
174
166
1034
Exec Accuracy
0.940
0.857
0.805
0.566
0.821
Table 1: Execution Accuracy by Difficulty
extra hard queries in this discussion due to this being the
main area of failure in our approach.
4.1
SELECT
Many of the incorrect queries face the issue of selecting the
wrong columns in the first line of the SELECT statement.
Either they use the wrong column from the selected table
or it gets the column names correct but the order they are
placed in doesn’t match the ground truth. While the few-
shot Example Driven Correction does correct a lot of these
instances, they still pop up within the final predicted results.
4.2
GROUP BY
The spider dataset can be challenging at times, especially in
the extra hard queries. Let’s take a closer look a query trying
to answer the NL question: ”What is the name of the course
with the most students enrolled?”.
Listing 1: Generated SQL Query
1
SELECT courses.course_name
2
FROM student_enrolment_courses
3
JOIN courses ON
student_enrolment_courses.course_id =
courses.course_id
4
GROUP BY student_enrolment_courses.
course_id
5
ORDER BY COUNT(student_enrolment_courses
.student_course_id) DESC
6
LIMIT 1
Listing 2: Ground Truth (Gold) SQL Query
1
SELECT T1.course_name
2
FROM Courses AS T1
3
JOIN Student_Enrolment_Courses AS T2 ON
T1.course_id = T2.course_id
4
GROUP BY T1.course_name
5
ORDER BY count(*) DESC
6
LIMIT 1
These queries are almost exactly the same, with the only
difference being in the GROUP BY and ORDER BY clause.
The evaluation script marks it as incorrect because both the
queries return a different course with ”the most students en-
rolled”, but in reality it is just as correct as the ground truth
query. The problem lies more with the spider dataset than a
problem with generation. When LIMIT 1 is removed from
both of these queries the result table shows five different
courses that all have the same number of students taking
them. The differing GROUP BY clause is simply ordering
the same results differently, so when LIMIT 1 is called, they
both return different results that could both be considered
correct.
4.3
Predicting Values
Another
major
problem
with
LLM
generated
SQL
queries is when the LLM tries to use a conditional
on a column when it doesn’t know what is actually
stored in that column. It is given the database schema
in the format shown in Figure 2, but not the values
stored inside those all of the table columns. For exam-
ple, when trying to find the number of ships that sank
in the spider battle_death db, the LLM might
add
WHERE disposition_of_ship = ’lost’
when
it
really
should
be
WHERE disposition_of_ship = ’sank’.
There
are no ships ’lost’, they are either ’captured’, ’wrecked’,
’sank, or ’scuttled’. We try to correct things kinds of errors
with few-shot learning and Example Driven Correction, but,
in this case, the column disposition_of_ship is not
in the output result table so the Example Driven Correction
shot gives no indication to the LLM as to what kinds of
values are really stored in this column.
4.4
Aggregate Columns
The problem with using LLMs for Text-to-SQL is partially
how the model interprets what the NL question is asking
for. Given the NL question: ”Which owner has paid for the
most treatments on his or her dogs? List the owner id and
last name.”, our LLM approach identifies that the question
is asking for which owner paid the most for their dog treat-
ments. In the case of the ground truth query, it interprets the
question as which owner brought there dog to treatment the
most, and has nothing to do with the cost of treatments.
In the actual queries, the generated SQL uses a SUM ag-
gregate in the ORDER BY clause to order the results by
which owner paid the most for all pet treatments, causing
the whole query to be incorrect. The ground truth SQL just
uses a COUNT aggregate. And since the question only asks
for id and last name to be selected, the aggregate column
SUM or COUNT never gets shown in the output result ta-
ble, which is why it never gets corrected in Example Driven
Correction.
4.5
JOIN
A substantial problem arises in the generated SQL when the
LLM predicts that it needs to JOIN on an extra table in order
to get what the NL question is asking for. Let’s take a look at
this SQL answering the NL question: ”What is the model for
the car with a weight smaller than the average?”. See Listing
3 and 4.
Listing 3: Generated SQL Query
1
SELECT DISTINCT model_list.model
2
FROM model_list
3
JOIN car_names ON model_list.modelid =
car_names.model
4
JOIN cars_data ON car_names.makeid =
cars_data.id
5
WHERE cars_data.weight <
6
(SELECT AVG(weight)
7
FROM cars_data)


--- Page 4 ---
Listing 4: Ground Truth (Gold) SQL Query
1
SELECT T1.model
2
FROM CAR_NAMES AS T1
3
JOIN CARS_DATA AS T2 ON T1.MakeId = T2.
Id
4
WHERE T2.Weight <
5
(SELECT avg(Weight)
6
FROM CARS_DATA)
The generated SQL joins on the model_list table,
however the ground truth SQL (correctly) doesn’t because
the column model is already in the car_names table.
Even when the database schema is given (Format shown
in Figure 2) the LLM still geenrates the extra JOIN. This
causes the generated SQL to return nothing as a output re-
sult table, while the ground truth returns 230 rows of data in
its result table.
4.6
Spider Inconsistencies
A key finding of our study is the demonstration of inconsis-
tencies within the spider dataset. During our experiments,
we observed that a significant number of SQL queries gen-
erated through our LLM-based approach, which are seman-
tically correct and yield result tables identical to those of
the ground truth, are still being classified as incorrect. This
includes some other instances where the LLM-generated
queries are an exact match, yet are flagged as incorrect by
the dataset’s evaluation script. This discovery shows a crit-
ical flaw in the Spider’s evaluation methodology and high-
lights the need for a more nuanced approach to assessing the
accuracy of LLM-generated SQL queries.
When presented with the natural language question,
”Show the stadium name and capacity with the most num-
ber of concerts in 2014 or later”, both the generated SQL
and the ground truth SQL produce identical results. The
only distinction lies in the usage of table aliases; the ground
truth employs aliases T1 and T2 for table names, whereas
the generated SQL directly utilizes the actual names of the
tables. Many of the incorrect queries are exactly like this,
making it extremely hard to tell when a query is actually
incorrect or not.
Incorrect Ground Truth
In the spider dataset, in a few
instances that we could see, the generated SQL is more
accurate than the dataset’s provided ground truth. A no-
table example is found in entry 944 of the development
set. The corresponding natural language question is: ”What
are the first name and last name of the professionals who
have done treatment with cost below average?”. The issue
with the ground truth SQL lies in its use of a JOIN oper-
ation without specifying the joining condition. This over-
sight leads to an excessively broad result set. It is essential
to join the Professionals and Treatments tables on
the professional_id field to accurately reflect the re-
lationships in the data. Failing to specify this join condition
results in the inclusion of professionals who have never re-
ceived treatment, assigning them a default treatment cost of
zero. Consequently, when comparing treatment costs against
the average, these professionals erroneously appear in the re-
sults. This misalignment with the natural language query’s
intent indicates a critical flaw in the ground truth SQL pro-
vided in the dataset.
4.7
Query Structure
A significant portion of the SQL queries incorrectly gen-
erated by the LLMs falls into a category characterized by
their unnecessary complexity and deviation from the sim-
plicity of the ground truth solutions. Notably, in numerous
instances, the LLMs resorted to using nested sub-queries to
address problems that the ground truth solutions efficiently
solved through straightforward table joins, and vice-versa.
This complexity presents a substantial challenge in repair-
ing these SQL queries. Due to their fundamentally incorrect
structure, these queries are particularly resistant to correc-
tion through both LLM few-shot learning and algorithmic
repair methods. In an analysis of forty randomly selected
incorrect queries, nine exhibited this type of over complica-
tion, indicating it as one of the more prevalent and challeng-
ing issues in SQL generation via LLMs.
5
Conclusions
Testing and using these different approaches has provided
clear insights to the process of using LLMs for Text-to-SQL
program synthesis. It is now very clear that the closed-source
models are still the leaders in high performing LLMs, with
their ability to perform few-shot in-context learning (Brown
et al. 2020). This could be due to lower parameter counts
in WizardLM’s WizardCoder model or just the underlying
data it is created with. It should be noted that gpt-3.5-turbo
was just revealed to be only a 20B parameter model, not too
much larger than WizardCoder.
This research has yielded critical insights into the cat-
egorization of inaccuracies in SQL queries generated by
LLMs. Despite their proficiency, LLMs encounter consid-
erable challenges, particularly with the more complex en-
tries in the spider dataset. The errors predominantly fall
into seven categories: incorrect selection or ordering of
columns, erroneous grouping, inaccurate conditional value
predictions, divergence in aggregate functions compared to
the ground truth, inappropriate or insufficient JOIN clauses,
discrepancies within the spider dataset itself, and funda-
mentally incorrect query structures. Considering that a sig-
nificant number of incorrect queries have a fundamentally
flawed query structure, any post-processing step that aims to
“fix” generated queries has to go beyond superficial features
and consider the semantics of the generated query.
A notable observation is that many SQL queries, incor-
rectly flagged as erroneous, actually produce result tables
identical to those generated by the ground truth SQL. In
some cases, the ground truth SQL itself contains inaccura-
cies. These findings highlight the need for nuance in evalu-
ating the accuracy of LLM-generated SQL queries.
References
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,


--- Page 5 ---
A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Models
are Few-Shot Learners. arXiv:2005.14165.
Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,
L. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.
arXiv:2305.14314.
Dong, X.; Zhang, C.; Ge, Y.; Mao, Y.; Gao, Y.; lu Chen;
Lin, J.; and Lou, D. 2023. C3: Zero-shot Text-to-SQL with
ChatGPT. arXiv:2307.07306.
Gao, D.; Wang, H.; Li, Y.; Sun, X.; Qian, Y.; Ding, B.; and
Zhou, J. 2023. Text-to-SQL Empowered by Large Language
Models: A Benchmark Evaluation. arXiv:2308.15363.
Li, H.; Zhang, J.; Li, C.; and Chen, H. 2023. RESDSQL:
Decoupling Schema Linking and Skeleton Parsing for Text-
to-SQL. arXiv:2302.05965.
Luo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao,
C.; Ma, J.; Lin, Q.; and Jiang, D. 2023. WizardCoder: Em-
powering Code Large Language Models with Evol-Instruct.
arXiv:2306.08568.
Yu, T.; Zhang, R.; Yang, K.; Yasunaga, M.; Wang, D.; Li,
Z.; Ma, J.; Li, I.; Yao, Q.; Roman, S.; Zhang, Z.; and Radev,
D. 2019. Spider: A Large-Scale Human-Labeled Dataset for
Complex and Cross-Domain Semantic Parsing and Text-to-
SQL Task. arXiv:1809.08887.
