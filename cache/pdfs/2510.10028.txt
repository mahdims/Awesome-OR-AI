--- Page 1 ---
1
Efficient Onboard Vision-Language Inference in
UAV-Enabled Low-Altitude Economy Networks via
LLM-Enhanced Optimization
Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Fellow, IEEE,
Abbas Jamalipour, Fellow, IEEE, Xianbin Wang, Fellow, IEEE, and Dong In Kim, Fellow, IEEE,
Abstractâ€”The rapid advancement of Low-Altitude Economy
Networks (LAENets) has enabled a variety of applications, includ-
ing aerial surveillance, environmental sensing, and semantic data
collection. To support these scenarios, unmanned aerial vehicles
(UAVs) equipped with onboard vision-language models (VLMs)
offer a promising solution for real-time multimodal inference.
However, ensuring both inference accuracy and communication
efficiency remains a significant challenge due to limited onboard
resources and dynamic network conditions. In this paper, we first
propose a UAV-enabled LAENet system model that jointly cap-
tures UAV mobility, user-UAV communication, and the onboard
visual question answering (VQA) pipeline. Based on this model,
we formulate a mixed-integer non-convex optimization problem
to minimize task latency and power consumption under user-
specific accuracy constraints. To solve the problem, we design
a hierarchical optimization framework composed of two parts:
(i) an Alternating Resolution and Power Optimization (ARPO)
algorithm for resource allocation under accuracy constraints, and
(ii) a Large Language Model-augmented Reinforcement Learning
Approach (LLaRA) for adaptive UAV trajectory optimization. The
large language model (LLM) serves as an expert in refining reward
design of reinforcement learning in an offline fashion, introducing
no additional latency in real-time decision-making. Numerical
results demonstrate the efficacy of our proposed framework in
improving inference performance and communication efficiency
under dynamic LAENet conditions.
Index Termsâ€”Low-altitude economy networks, vision-language
model, unmanned aerial vehicles, trajectory optimization, large
language model, and deep reinforcement learning.
I. INTRODUCTION
Low-Altitude Economy Networks (LAENets) have recently
garnered growing attention as a novel paradigm that leverages
the low-altitude airspace (typically below 1000 meters) to
deliver digital services [1]. Their primary goal is to unlock
Y. Li and G. Liu are with the College of Computing and Data Science,
the Energy Research Institute @ NTU, Interdisciplinary Graduate Program,
Nanyang Technological University, Singapore (e-mail: yang048@e.ntu.edu.sg;
liug0022@e.ntu.edu.sg).
R. Zhang, Y. Liu and D. Niyato are with the College of Computing
and Data Science, Nanyang Technological University, Singapore (e-mails:
ruichen.zhang@ntu.edu.sg; yinqiu001@e.ntu.edu.sg; dniyato@ntu.edu.sg)
A. Jamalipour is with The University of Sydney, Sydney, Australia (e-mail:
a.jamalipour@ieee.org).
X. Wang is with the Department of Electrical and Computer Engineering,
Western University, London, Canada (e-mail: xianbin.wang@uwo.ca).
D. I. Kim is with the Department of Electrical and Computer Engineering,
Sungkyunkwan University, South Korea (email: dongin@skku.edu).
commercial and societal benefits through flexible aerial oper-
ations. Specifically, a typical LAENet refers to an integrated
network composed of low-altitude aerial platforms, such as
general aviation aircraft, Unmanned Aerial Vehicles (UAVs),
and electric Vertical Take-Off and Landing (eVTOL) aircraft.
Compared to ground-based systems, these intelligent platforms
offer unique advantages of high mobility and adaptive deploy-
ment, making LAENets suitable for diverse services, includ-
ing wireless communication, environmental sensing, and edge
computing [2]â€“[4]. Thereafter, LAENets are expected to play a
significant role in supporting the pervasive services envisioned
for future 6G networks. This potential is also reflected in the
rapid growth of the LAE economy. For instance, the Civil
Aviation Administration of China claims that the countryâ€™s low-
altitude market is expected to grow from $70 billion in 2023
to $200 billion by 2025, and reach $480 billion by 2035 [5].
UAV-enabled LAENets. Among various LAENet platforms,
UAV-based networks stand out due to their ability to per-
form intelligent tasks in complex low-altitude environments.
Equipped with onboard sensors and processors, UAVs could
function as flying agents that are capable of executing entire
mission lifecycles autonomously without additional ground
support. This autonomy also reduces reliance on the communi-
cation between UAV and base station, enabling service delivery
in resource-constrained or infrastructure-sparse environments.
As a result, recent works have highlighted the potential of
UAV-enabled LAENets for different applications, including
aerial surveillance [6], disaster response [7], and autonomous
delivery [4]. In addition, with advances in battery technology
and AI-powered analytics, UAVs are promising to provide
intelligent services with longer flight times, wider coverage,
and enhanced analytical capabilities. These features align well
with the service requirements of LAENets.
Integrating VLMs into UAV-enabled LAENets. Recent
advances in vision-language models (VLMs), e.g., LLaVA [8],
can strengthen UAV perception and reasoning, enabling appli-
cations such as object detection [9] and geo-localization [10].
Hence, embedding these models onboard can offer a promising
avenue to provide real-time, high-quality inference-as-a-service
to ground users within LAENets. Moreover, large VLMs have
exhibited robust zero-shot generalization, eliminating the need
for task-specific fine-tuning [11]. This makes them especially
suitable for practical LAE scenarios, where ground users may
arXiv:2510.10028v1  [cs.LG]  11 Oct 2025


--- Page 2 ---
2
request different types of inference on demand. By deploy-
ing a single VLM, UAVs can handle diverse tasks without
switching models, reducing memory usage and improving
overall efficiency. Notably, Zhao et al. [12] demonstrated the
onboard deployment of a 14B-parameter DeepSeek-R1 [13] on
a medium-size UAV and achieved an inference speed of 5â€“6
tokens/sec for task planning. This provides a practical paradigm
for integrating VLMs of comparable scale, such as LLaVA,
into UAVs for aerial inference-as-a-service. Similar directions
have also been explored in vehicular networks, where embodied
AI frameworks integrate VLMs with reinforcement learning to
enhance semantic communication and decision-making [14].
VLM Task-Driven Optimization in LAENets. Deploying
VLMs on UAVs enables rich inference but strains communi-
cation and computing resources. Effective resource allocation
is thereafter essential to the utilization of limited resources in
LAENets to improve system performance. Unlike generic on-
board computations, VLM inference couples perception quality
with system performance: increasing input image resolution
typically improves task accuracy but with diminishing returns
that saturate beyond a task-dependent threshold, and it also
incurs transmission overhead and lengthens model runtime [15].
This interplay of communication and computation performance
as well as inference accuracy makes the optimization problem
different from conventional UAV resource allocation, where ob-
jectives are typically confined to throughput or energy. Hence,
a practical framework should explicitly model and optimize the
accuracyâ€“efficiency trade-off unique to VLM inference.
Challenges. To summarize, while deploying VLMs on UAVs
for agentic AI services offers great potential, it also introduces
several major challenges for effective resource allocation:
1) Unlike traditional UAV tasks focusing on throughput or
sensing coverage, VLM services require a holistic model
that jointly captures image resolution selection, transmis-
sion delay, and model processing time, all of which jointly
determine overall service performance. Such modeling is
challenging as it must integrate communication, computa-
tion, and AI inference into a unified system model.
2) UAVs often operate under limits of flight regulations,
communication resources, and onboard computing power.
These constraints are critical when serving multiple users
with diverse demands, such as different accuracy and
latency requirements. How to formulate an optimization
problem effectively managing the resources while ensuring
service efficiency remains a key challenge.
3) Based on Challenges 1) and 2), the resulting optimization
problem would inevitably involve heterogeneous vari-
ables, including discrete image resolutions, continuous
transmit power, and dynamic UAV trajectories. Such a
problem is typically mixed-integer, non-convex, and high-
dimensional, which makes conventional optimization in-
feasible and necessitates new solution paradigms.
Our solution. To address those challenges, we formulate a
joint optimization problem for LAENets that accounts for the
accuracyâ€“efficiency trade-off of VLM tasks. Then, we develop
a hierarchical framework to alternatively optimize the variables.
Specifically, we first optimize image resolution and transmit
power with standard solver-based methods. This is due to their
efficiency in yielding the optimal solution for a finite mixed-
integer subproblem whose continuous part (power allocation)
is convex. We then optimize the UAV trajectory with deep
reinforcement learning (DRL), since trajectory planning is a
long-horizon problem that requires continuous optimization
under dynamic channels and mobility constraints. Notably,
we introduce the LLM as an offline reward designer in our
DRL setting. Traditionally, manually crafted reward functions
in DRL are often heuristic, relying on ad hoc rules that struggle
to adapt to dynamic system conditions. In contrast, the LLM
leverages its reasoning and domain knowledge to automatically
generate and iteratively refine reward functions that integrate
multiple objectives in a principled manner. This automated de-
sign not only mitigates human bias but also has the potential to
uncover latent optimization objectives, improving convergence
speed and the overall effectiveness of DRL policies.
Contribution. The major contribution of this paper is listed
as follows, each directly targeting one challenge:
â€¢ System Modeling in LAENet: For Challenge 1), we
propose a UAV-enabled LAENet that leverages VLMs for
onboard inference. We then formulate a unified system
model that captures the coupling between UAV trajectory,
communication channels, and the onboard VLM inference
pipeline, providing the foundations for holistic perfor-
mance assessment.
â€¢ Optimization Problem Formulation: For Challenge 2),
we formulate a mixed-integer problem that jointly opti-
mizes UAV trajectory, user transmit power, and discrete
image resolution to minimize the worst-case task latency
while keeping power usage efficiency. In addition, we
empirically observe how image resolution affect: (i) task
accuracy, (ii) model inference speed, and (iii) payload size,
and convert observations into lookup tables. In the opti-
mizer, these lookup tables encode the accuracyâ€“efficiency
trade-off and guide resource-allocation decisions, enabling
the system to meet diverse user needs (e.g., different
accuracy requirements) under limited system resources.
â€¢ Hierarchical Optimization Framework: For Challenge
3), we design a hierarchical framework that decouples
the proposed problem into two subproblems for the so-
lutions. The first subproblem on image resolution and
power allocation is solved by an Alternating Resolution
and Power Optimization (ARPO) algorithm. The second
subproblem on UAV trajectory is tackled by an LLM-
augmented Reinforcement learning Approach (LLaRA).
Specifically, the LLM serves as an offline reward design
expert to improve DRL for better convergence and more
stable trajectory policies.
The remainder of the paper is structured as follows: Sec-
tion II reviews the related works; Section III introduces the
system model and problem formulation; Sections IV and V de-
tail the proposed ARPO and LLaRA methods. The complexity
analysis is in Section VI. Simulation results are in Section VII.
Finally, Section VIII concludes the whole paper.


--- Page 3 ---
3
II. RELATED WORK
This section surveys recent work: Section II-A on LAENets,
Section II-B on VLMs for edge inference, and Section II-C on
LLM-enhanced DRL methods.
A. LAE Networks
In recent years, both the conception and core technologies of
LAENets have been widely discussed in a series of studies [2],
[4], [16]. For instance, He et al. [2] investigated the utilization
of satellite technology for providing ubiquitous connectivity
and enhancing communication, control, and computation in
LAENets through advanced architectures and optimization
schemes. Wang et al. [4] highlighted the synergy of communi-
cation, sensing, computing, and control technologies as a key
driver for advancing LAE networks. Jiang et al. [16] reviewed
the use of sensing-communication integration and discussed
prerequisite technologies, e.g., network coverage and aircraft
detection, enhancing awareness in LAENets.
In addition to conceptual discussions, recent studies also
develop system-level solutions to improve the efficiency of
LAENets. These solutions focus on diverse technical perspec-
tives. Some works enhance communication efficiency through
advanced wireless technologies and optimizations [17], [18].
For instance, Ahmed et al. [17] studied reconfigurable in-
telligent surface (RIS)-assisted UAV networks and detailed
strategies such as trajectory optimization and power control to
enhance energy efficiency in low-altitude operations. Likewise,
Salim et al. [18] proposed a DRL-based method for energy op-
timization in irregular RIS-aided UAV-assisted networks. Some
other works manage to integrate multi-domain intelligence with
LAENets to enhance sensing and control efficiency: Yang et
al. [19] proposed an embodied artificial intelligence (EAI)
framework that unifies sensing, communication, computation,
and control to enhance LAE efficiency. Besides, some studies
address resource-aware lifecycle management for sustainable
LAE growth, e.g., Zhou et al. [20] evaluated the long-term op-
erational and environmental impacts of UAV-enabled services.
Despite these advances, LAENets still remain constrained in
resources, particularly as they are expected to support more
intelligent tasks.capabilities [21]. These observations motivate
our design of a more efficient LAENet.
B. VLMs for Edge Inference
Advanced VLMs have excelled in multimodal reasoning
tasks. Hence, embedding VLMs into UAV has motivated a
number of vision-language applications for LAENets, e.g., geo-
localization [10], urban patrols [22] and navigation [23]. Recent
vision-language-action architectures further extend these abili-
ties to embodied and robotic control, enabling real-time infer-
ence on autonomous platforms. We next examine optimization
strategies for VLM-based edge inference.
Some works focus on lightweight architectures and inference
optimizations to improve the efficiency of VLM deployment
on edge platforms [15], [24], [25]. Sharshar et al. [24] com-
prehensively surveyed popular strategies for making VLMs
edge-compatible, including pruning, quantization, knowledge
distillation, and hardware acceleration. LiteVLM [25] presents
an efficient VLM pipeline that leverages patch selection, token
filtering, and speculative decoding techniques. It can achieve a
2.5Ã— reduction in latency without compromising task accuracy.
Luo et al. [15] proposed a mixture-of-resolution adaptation
strategy with a dual-path design that preserves high-resolution
benefits while improving efficiency, achieving nearly 3Ã— faster
inference than LLaVA-1.5. Such efficiency-oriented frame-
works offer valuable benchmarks for edge inference.
Moreover, some other studies investigate VLM-based edge
inference from a system-level perspective, focusing on adaptive
orchestration and QoE optimization [26]â€“[28]. For instance,
Sun et al. [26] proposed DiSCo, a deviceâ€“server cooperative
scheduler that dynamically routes LLM inference between local
devices and servers to jointly optimize latency and energy;
and such an idea can be readily extended to VLM scenarios.
PerLLM [27] presents a personalized scheduling framework
that employs edgeâ€“cloud collaboration and an upper confidence
bound algorithm to balance latency, energy, and service quality.
Its adaptive mechanism is readily transferable to QoE-aware
VLM scheduling. Li et al. [28] proposed a distributed archi-
tecture for VLMs that addresses high computational demands
by partitioning model components between edge devices and
central servers. Specifically, vision components run on edge
devices, while language generation runs on servers, resulting
in up to a 33% improvement in throughput.
Although our work does not focus on optimizing VLMs for
edge inference, our LLaRA approach is significantly different
to the above model-level and system-level solutions and can
integrate them to further enhance aerial service efficiency.
C. LLM-Enhanced DRL Methods
Despite its remarkable successes in various fields such as
robotics, gaming, and autonomous control [14], [29], DRL still
suffers from key issues including sample inefficiency, reward
design difficulty, and limited language understanding [30]. The
recent emergence of advanced LLMs offers a promising way to
address these issues with pre-trained knowledge and high-level
general abilities. Next, we provide further details on how to
leverage LLMs to enhance DRL.
To integrate LLMs into DRL, Cao et al. [30] proposed
a structured taxonomy categorizing LLMs into four com-
plementary roles: information processors, reward designers,
decision-makers, and generators. Firstly, as information proces-
sors, LLMs help extract meaningful features for downstream
networks [31] or translate natural information into formal
task languages [32]. Pang et al. [31] proposed an inside-out
approach that trains an LLM to translate natural language
instructions into task-specific representations. Spiegel et al. [32]
developed RLang to convert natural language into Markov de-
cision process (MDP) specifications for using prior knowledge.
As reward designers, LLMs leverage pre-trained knowledge
and code generation capabilities to provide implicit or explicit
reward functions [33]â€“[35]. Kwon et al. [33] streamlined re-
ward design by prompting an LLM to act as a proxy reward


--- Page 4 ---
4
function using examples and descriptions of desired behaviors.
Eureka [34] introduces a self-reflective algorithm to iteratively
generate and refine reward functions via a coding LLM, which
can achieve human-level reward design and enable dexterous
manipulation tasks. Another work, Text2Reward [35], enables
the generation of shaped, dense reward functions as executable
programs grounded in compact environment representations.
As decision-makers, LLMs help action decisions, or provide
action candidates and reference policies that guide explo-
ration [36], [37]. Li et al. [36] used LLMs to combine goals and
observations into sequential inputs, which improves both com-
binatorial generalization and out-of-distribution performance.
In contrast, Shek et al. [37] introduced a hierarchical DRL
framework that leverages LLMs to generate subgoals from task
descriptions, select reusable options, and execute action-level
policies, thereby improving decision-making.
Finally, as generators, LLMs simulate environment dynamics
for RL [38] and provide interpretable policy explanations [39].
Robine et al. [38] used a Transformer-based world model to
address long-term dependencies and achieve good performance
on the Atari 100k benchmark. Silva et al. [39] developed an
adaptive explainability framework that personalizes explanation
modalities to balance user preferences and task performance,
enhancing decision-making in human-AI collaboration.
Prior studies on LLMs as reward designers [33]â€“[35] mostly
target single-task robot manipulation or game control and focus
less on multi-user service scenarios. In contrast, we consider
an LAENet where the UAV serves multiple users concurrently.
The employed LLM needs to synthesize a compact reward that
aggregates multi-user information, which is more challenging.
III. SYSTEM MODEL
This section presents an overall system model for a LAENet.
We first present the task formulation model in Section III-A,
and then model the UAV trajectory, user-UAV communication,
and VLM inference in Sections III-B, III-C, and III-D, respec-
tively. The optimization problem is formulated in Section III-E.
A. Task Formulation
We consider a UAV-assisted LAE network comprising a
single UAV that serves as an intelligent aerial agent and a
set of ground users denoted by N := {1, . . . , N}. Our system
model be readily extended to multi-UAV scenarios by assigning
different UAVs to serve diverse user sets, as in [40]. We assume
that ground user n generates a visual-language task, and the
UAV equipped with onboard computation capabilities executes
those inference tasks in real time. To analyze user-UAV interac-
tions over time, we model the system as a time-slotted network
operating over a finite horizon T := {1, . . . , T}.
Fig. 1 illustrates our system model and its working pipelines.
Ground users need to upload their requests to the UAV for
TextVQA inference services [41]. In detail, user n generates
one or more queries, where each query includes a visual input
(i.e., an image In with a resolution of rn) and a corresponding
textual prompt Qn. Specifically, the resolution rn is defined as
UAV
Onboard VLM Inference-Driven LAENet
UAV trajectory
UAV-user link
VLM task
VLM model
Ground users
â€œI serves as a flying 
agent to provide VLM 
inference service.â€
Future movement path
UAV-Assisted VLM Inference Service
UAV
User
Uplink
Downlink
Image
Text query
Answer
Visual encoder
Modality 
projection layer
Tokenizer
Prompt constructor
LLM decoder
VLM Inference Pipeline
â€œGround users upload multimodal queries 
to UAV for real-time inference.â€
Fig. 1: An overview of the onboard VLM inference-driven
LAENet. The upper part depicts a UAV serving as a flying
agent that providing VLM inference services to ground users;
the lower part details the onboard VLM pipeline from user
queries to answer generation.
the total number of pixels, i.e., rn = Hn Ã—Wn, where Hn and
Wn denote the vertical and horizontal dimensions of In, respec-
tively. The UAV must first collect data via the corresponding
wireless communication, then perform the inference using its
deployed VLM, and finally return the response.
B. UAV Trajectory
We consider a 3D Cartesian coordinate system and denote the
UAVâ€™s horizontal position at time slot t as (x[t], y[t], z[t]) [42].
We further define q[t] := (x[t], y[t]) to represent the horizontal
coordinate. Let Î± denote the elemental time slot length, which
is deemed to be sufficiently short such that the distances
between the UAV and users remain approximately constant
within each slot t. The UAVâ€™s flight trajectory is discretized
and represented in the set {(q[t], z[t]) | t âˆˆ{1, . . . , T}}, with
the continuous path approximated by connecting these discrete
waypoints via line segments. To comply with aerial regulations,
the UAV is required to operate within a specified altitude range.
Hence, the UAVâ€™s altitude must satisfy the following constraint:
hmin â‰¤z[t] â‰¤hmax,
âˆ€t.
(1)
Besides, we assume that the UAV can independently control
its horizontal and vertical flight speeds. Let V max
xy
and V max
z
denote the maximum allowable horizontal and vertical speeds,


--- Page 5 ---
5
respectively. Accordingly, the UAVâ€™s mobility is subject to the
following constraints:
(
âˆ¥q[t + 1] âˆ’q[t]âˆ¥â‰¤Î± Â· V max
xy
,
âˆ€t,
(2a)
|z[t + 1] âˆ’z[t]| â‰¤Î± Â· V max
z
,
âˆ€t.
(2b)
C. User-UAV Communication Model
We focus on modeling uplink communication from ground
users to the UAV, as it accounts for the majority of transmission
costs due to the large image payload. Similarly, we denote
the position of each ground user n âˆˆN as (wn, hn), where
wn := (xn, yn) represents the corresponding horizontal coordi-
nate. Next, we present channel modeling between ground users
and the UAV, and then we analyze the transmission latency.
Channel Modeling. We consider a quasi-static air-to-ground
(A2G) channel, where the small-scale fading remains constant
within each slot and may vary across slots. Firstly, the UAV-user
n distance and elevation angle at slot t is defined as follows:
ï£±
ï£´
ï£²
ï£´
ï£³
dn[t] =
p
âˆ¥q[t] âˆ’wnâˆ¥2 + (z[t] âˆ’hn)2,
(3a)
Î¸n[t] = arctan
 z[t] âˆ’hn
âˆ¥q[t] âˆ’wnâˆ¥

.
(3b)
Specifically, A2G has an elevation-dependent Line-of-Sight
(LoS) probability. We adopt a probabilistic LoS/NLoS model:
ï£±
ï£´
ï£²
ï£´
ï£³
PLoS(Î¸n[t]) =
1
1 + a exp
 âˆ’b(Î¸n[t] âˆ’a)
,
(4a)
PNLoS(Î¸n[t]) = 1 âˆ’PLoS(Î¸n[t]),
(4b)
where a, b > 0 are LoS-probability parameters. The large-
scale gains under LoS and NLoS are Î²LoS
n
[t] =
Î²0
dn[t]Î³LoS and
Î²NLoS
n
[t] =
Î²0
dn[t]Î³NLoS , with Î²0 the reference gain at d0 = 1
m and path-loss exponents Î³LoS, Î³NLoS. Then, we define the
elevation-aware gain by averaging over the LoS state:
Â¯Î²n[t]=PLoS(Î¸n[t]) Î²LoS
n
[t]+
 1 âˆ’PLoS(Î¸n[t])

Î²NLoS
n
[t]. (5)
The baseband-equivalent channel coefficient is modeled as:
hn[t] =
q
Â¯Î²n[t] Ë†hn[t],
(6)
where Ë†hn[t] denotes small-scale fading, modeled as a complex-
valued random variable with zero mean and unit variance.
Uplink Transmission Time. Recall that each query from
ground user n includes an image In with resolution rn and a
text query Qn. The total data size for transmission is denoted
by Dn(rn), where the image dominates the payload size and
the query size is negligible [41]. Hence, we can hold that:
Dn(rn) =
 Dimg
n (rn) + Dtxt
n

Â· Mn â‰ˆDimg
n (rn) Â· Mn.
(7)
where Dimg
n (rn) and Dtxt
n represent the transmitted data size for
the image and query, respectively, and Mn denotes the number
of queries from ground user n.
Given channel gain hn[t] in (6) and let Pn denote the transmit
power of user n. The signal-to-noise ratio (SNR) at time step
t is expressed as:
SNRn[t] = Pn|hn[t]|2
Ïƒ2n
,
âˆ€n, t,
(8)
where Ïƒ2
n represents the noise power. Hence, the achievable
uplink transmission rate is given by:
Rn[t] = Bn log2(1 + SNRn[t]),
âˆ€n, t,
(9)
where Bn denotes the available bandwidth to each user n.
Notably, rn and Pn in our uplink communication model re-
main constant over t. We adopt the following assumption for
simplicity and practicality:
Assumption 1. For each user n, the image resolution rn and
transmit power Pn remain fixed during its upload window,
as resolution is set before transmission and power control
typically operates on coarser timescales [43]. In our setting,
both the uplink window and the slot length are short, making
slot-varying power control costly. Changing rn or Pn mid-
stream also incurs re-encoding/control overhead and breaks
a stable accuracyâ€“latency mapping, offering limited gain [44].
We therefore optimize (rn, Pn) at the session level and let the
trajectory policy handle slot-level fluctuations [45].
We now analyze the uplink transmission time. Since the
uplink rate Rn[t] in (9) can vary across slots, the simple form
Dn(rn)/Rn does not generally apply. With slot length Î±, user
n can transmit at most Î±Â·Rn[t] bits of data in slot t. Given the
payload size Dn(rn), we define the uplink completion index
(the smallest slot index by which user n finishes uploading):
T cmp
n
= min
n
t âˆˆT |
t
X
Ï„=1
Î± Rn[Ï„] â‰¥Dn(rn)
o
.
(10)
The integer-slot latency is Î± Â· T cmp
n
. If the final slot is only
partially used, the overall uplink time T up
n is computed as:
T up
n = Î±
 T cmp
n
âˆ’1

+ Dn(rn) âˆ’PT cmp
n
âˆ’1
Ï„=1
Î± Rn[Ï„]
Rn[T cmp
n
]
,
(11)
where T up
n is a continuous-time and does not need to align with
slot boundaries.
D. VLM Inference Model
To support inference services for ground users, the VLM
deployed on the UAV processes user inputs. First, we present an
overview of the unified inference pipeline commonly adopted
by modern VLMs [8], [11].
1) Visual Encoding: Given an image In(rn)âˆˆRHnÃ—WnÃ—C1,
a pretrained visual encoder extracts visual embeddings:
Zn(rn) = g
 ËœIn(rn)

,
(12)
where Zn(rn) âˆˆRL(rn)Ã—d is the obtained visual tokens;
L(rn) is the number of tokens and d is the embedding
size. In common image encoders, L(rn) grows with rn
for preserving richer image information [8], [15].
1Here C is the channel count (e.g., C=3 for RGB), and resolution rn =
Hn Â· Wn. Most VLMs use a fixed input aspect ratio (e.g., Hn:Wn = 1:1 in
LLaVA); thus, given rn, the pair (Hn, Wn) is uniquely determined.


--- Page 6 ---
6
2) Modality Projection: The visual tokens are then mapped
into the LLM input space via a learnable projector:
En(rn) = W Â· Zn(rn),
(13)
where En(rn) âˆˆRLÃ—dâ€² and dâ€² matches the LLMâ€™s input
embedding size.
3) Prompt Construction: Next, the projected tokens are con-
catenated with the question tokens Qn = {w1, . . . , wT }
to form a multimodal prompt:
Xn(rn) = [En(rn); Qn; <eos>],
(14)
where <eos> indicates the end of the question prompt.
4) Answer Generation: The prompt is finally passed into
a decoder-only language model parameterized by Î¸ to
generate the predicted answer:
Apred
n (rn) = LLM(Xn(rn); Î¸),
(15)
where Apred
n (rn) denotes the generated answer sequence
comprising S tokens from the model vocabulary.
Inference Accuracy. We consider a standard top-1 accuracy
metric from the TextVQA benchmark [41] to evaluate the VLM
inference performance:
An(rn) = E

min
1
3
K
X
k=1
I

Apred
n (rn) = agt(k)
n

, 1
	
,
(16)
where {agt(k)
n
}K
k=1 are the human-annotated ground truths and
Apred
n (rn) is the model output under rn. Larger rn often tends
to preserve more fine-grained details of the image. Empirically,
An(rn) increases with resolution rn, but exhibits diminishing
returns, with smaller gains at high rn. Such observations are
also reported in [15], [46].
Inference Latency. The selection of image resolutions also
affects inference speed [47]. Since the generated answer length
is roughly resolution-invariant (e.g., short phrases in TextVQA),
we define the processing time T proc
n
(rn) as:
T proc
n
(rn) = E[|Apred
n |]
v(rn)
,
(17)
where E[|Apred
n |] denotes the expected number of output tokens
generated by the VLM, and v(rn) is the resolution-dependent
inference speed (in tokens/s) measured empirically. The specific
form of v(rn) is given in Section VII-B.
Downlink Time. After inference, the generated answer
Apred
n (rn) is sent back to user n. Since Apred
n (rn) is typically a
short text (e.g., less than 20 tokens in TextVQA [41]), we treat
downlink latency as a fixed constant T down
n
.
E. Problem Formulation
Our first objective is to minimize the maximum latency for
all ground users completing their tasks. Hence, we first express
the total time consumption for each user n as:
T total
n
= T up
n + T proc
n
(rn) + T down
n
,
âˆ€n,
(18)
and the first part of the objective function is expressed as:
min
n
max
nâˆˆN T total
n
o
.
(19)
To balance latency and power consumption, we add a reward
term that encourages lower transmit power:
max
n
âˆ’
X
nâˆˆN
Pn
o
.
(20)
To ensure inference performance, we further impose that the
expected accuracy An(rn) meets each userâ€™s minimum require-
ment Amin
n . We then formulate a joint optimization problem over
the UAV trajectory (q[t], z[t]), user transmit power Pn, and
image resolution rn, subject to constraints on UAV mobility,
power, and resolution selection:
P0 :
min
{q[t],z[t],P,r}
ï£±
ï£²
ï£³max
nâˆˆN T total
n
+ Î¶
X
nâˆˆN
Pn
ï£¼
ï£½
ï£¾
(21)
s.t. C1 : An(rn) â‰¥Amin
n
,
âˆ€n,
(21a)
C2 : hmin â‰¤z[t] â‰¤hmax,
âˆ€t,
(21b)
C3 : âˆ¥q[t + 1] âˆ’q[t]âˆ¥â‰¤Î± Â· V max
xy
,
âˆ€t,
(21c)
C4 : |z[t + 1] âˆ’z[t]| â‰¤Î± Â· V max
z
,
âˆ€t,
(21d)
C5 : Pn â‰¤P max
n
,
âˆ€n,
(21e)
C6 : rn âˆˆRres = {r(1), . . . , r(J)},
âˆ€n.
(21f)
Here, Î¶ â‰¥0 is a tunable coefficient that controls the relative
importance of power consumption versus latency minimiza-
tion. The constraint (21a)) guarantees that the aerial infer-
ence accuracy remains above a required threshold Amin
n . Con-
straints (21c) and (21d) limit the UAVâ€™s displacement between
consecutive time slots. Constraint (21e) restricts each userâ€™s
transmit power within feasible bounds. Finally, constraint (21f)
restricts the resolution selection to a finite candidate set Rres =
{r(1), . . . , r(J)} containing J supported resolution choices.
Solving problem P0 is challenging for three reasons. First,
it is a mixed-integer non-linear program (MINLP) involving
coupled discrete (e.g., resolution) and continuous (e.g., trajec-
tory) variables, making it NP-hard. Second, it is non-convex
due to the dependence of SNR on (q[t], z[t]) and Pn, with
resolution rn further complicating uplink latency. Third, its
time-sequential nature enlarges the solution space with T,
causing high computational burden and making convex or
heuristic methods infeasible.
To tackle with the challenges of P0, we adopt a two-step
hierarchical framework. First, we optimize the transmit power
and image resolution via the ARPO algorithm. Second, we opti-
mize the UAV trajectory using the LLaRA. This decomposition
reduces overall complexity and allows each subproblem to be
solved with the most suitable method. Fig. 2 illustrates the
overall ARPO-LLaRA optimization framework.
IV. THE PROPOSED ARPO ALGORITHM FOR RESOLUTION
AND POWER CONTROL
This section introduces the ARPO algorithm, which jointly
optimizes image resolutions r := {rn}nâˆˆN and transmit


--- Page 7 ---
7
Original Optimization Problem â„™0: ğ, ğ«, ğªğ‘¡, ğ‘§ğ‘¡
H
Horizontal position
V
Vertical position
P Transmission power
R
Image resolution
Problem Decomposition
â„™3: ğªğ‘¡, ğ‘§ğ‘¡
UAV trajectory control
Time-sequential
â„™1: ğ, ğ«
Power and resolution selection 
MINLP
Optimized ğâˆ—, ğ«âˆ—
Branch and Bound 
algorithm
KKT conditions
ARPO algorithm
â„™2: ğ
Power allocation
Convex
Optimized ğ«âˆ—
Optimization 
of resolution
Integer Programming
Solve
Solve
Solve
LLaRA
Deep reinforcement learning method
LLM expert
LLM-augmented
reward design
GAE-PPO
Enhance
Backbone
Actor-Crtic network
Fig. 2: The framework of our proposed hierarchical ARPO-LLaRA optimization framework. At the start of uplink session, ARPO
determine image resolutions r and powers P for transmission using the B&B algorithm and KKT conditions, respectively. Then,
LLaRA uses an LLM-assisted DRL method for planning the slot-level UAV trajectory.
powers P := {Pn}nâˆˆN . Specifically, given the initial UAV
position {Â¯q, Â¯z} at the start of the uplink session, ARPO chooses
slot-invariant (r, P). This aligns with our Assumption 1 that
fixes image resolution and power per query/user within an
uplink session, and cleanly separates the slower per-session
controls (r, P) from the slot-level trajectory evolution.
Based on the above statements, we now simplify the original
problem P0 to facilitate optimization. When choosing (r, P),
we evaluate the channel at the current pose {Â¯q, Â¯z} and treat it
as constant over the decision interval. Under this quasi-static
approximation, the per-slot rate becomes Rn[t] â‰¡Rn, and the
uplink time simplifies to the closed form:
T up
n (rn, Pn) =
Dn(rn)
Bn log2
 1 + Pn |hn|2
Ïƒ2n
,
(22)
where Dn(rn) is the payload at resolution rn and Bn is the
allocated bandwidth fixed by the system operator. After that,
the total latency for user n can be rewritten as:
T total
n
(rn, Pn) = T up
n (rn, Pn) + T proc
n
(rn) + T down
n
,
(23)
Following [48], we introduce an auxiliary variable Ï„ to replace
the term maxnâˆˆN T total
n
in (21) with a new constraint (24a).
Problem P0 is transformed into an equivalent form P1:
P1 :
min
r,p,Ï„
n
Ï„ + Î¶
X
nâˆˆN
Pn
o
(24)
s.t.
(21a), (21e), (21f),
T total
n
(rn, Pn) â‰¤Ï„,
âˆ€n.
(24a)
However, problem P1 is still an MINLP that cannot be solved
directly, and T total
n
(rn, Pn) in (24a) involves a non-convex term.
To facilitate the solution, we decouple variables {r, P} and
optimize them in an alternating manner. First, recalling that
the inference speed function v(rn) is non-decreasing with
resolution rn, we can derive that the objective function (24)
is also non-decreasing with rn. Hence, to reduce overall
latency, we choose the smallest resolutions that satisfy the
accuracy constraint (21a). In practice, VLMs usually support
only a small finite set of resolutions (e.g., 4-5 choices such as
{3362, 4482, 10242, 15362} for LLaVA-HR [15]). Hence, the
optimization on r is lightweight and can be implemented via a
Branch and Bound (B&B) algorithm [49] effectively.
Next, we can substitute the obtained râˆ—from the B&B
algorithm into problem P1 and reformulate it as follows:
P2 :
min
p,Ï„
n
Ï„ + Î¶
X
nâˆˆN
Pn
o
(25)
s.t.
(21e),
T total
n
(râˆ—
n, Pn) â‰¤Ï„,
âˆ€n.
(25a)
With both the objective and constraints of P2 being convex, it
now becomes a convex problem. Consequently, KKT conditions
can be applied directly to derive the optimal solution. With
Î¹ := [Î¹n|nâˆˆN ] and Ï‰ := [Ï‰n|nâˆˆN ] denoting the Lagrange
multipliers, the corresponding Lagrangian function is given by:
L1(P , Ï„, Î», Î¹, Ï‰) = Ï„ + Î¶
X
nâˆˆN
Pn +
X
nâˆˆN
Î¹n Â· (Pn âˆ’P max
n
)
+
X
nâˆˆN
Ï‰n Â· (T total
n
(râˆ—
n, Pn) âˆ’Ï„).
(26)


--- Page 8 ---
8
The KKT conditions are as follows:
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
âˆ‚L1
âˆ‚Pn
= Î¶ + Î¹n âˆ’Ï‰ngn(Pn, râˆ—
n) = 0,
âˆ€n,
(27a)
âˆ‚L1
âˆ‚Ï„
= 1 âˆ’
X
nâˆˆN
Ï‰n = 0,
(27b)
Î¹n Â· (Pn âˆ’P max
n
) = 0,
âˆ€n,
(27c)
Ï‰n Â· (T total
n
(râˆ—
n, Pn) âˆ’Ï„) = 0,
âˆ€n,
(27d)
where L1(P , Ï„, Î¹, Ï‰) is abbreviated as L1, and gn(Pn, râˆ—
n)
denotes the derivative of T total
n
(râˆ—
n, Pn) with respect to Pn:
gn(Pn, râˆ—
n) =
D(râˆ—
n)hn
BnÏƒ2(1 + hnPn
Ïƒ2 ) ln 2[log2(1 + hnPn
Ïƒ2 )]2 . (28)
With conditions (27a)-(27d), Proposition 1 is given to find the
optimal solution to problem P2.
Proposition 1. The optimal solution Pâˆ—= {P âˆ—
n}nâˆˆN and Ï„ âˆ—
to problem P2 are expressed as:
P âˆ—
n =
(
P max
n
,
if Î¶ = 0,
min{Pn(Ï„ âˆ—), P max
n
},
if Î¶ > 0,
(29)
Ï„ âˆ—is the solution to:
X
nâˆˆN
Î¶
gn(Pn(Ï„), râˆ—n) = 1,
(30)
where
ï£±
ï£´
ï£²
ï£´
ï£³
Pn(Ï„) = Ïƒ2
hn
 2
D(râˆ—
n)
Bn(Ï„âˆ’Î“n(râˆ—n)) âˆ’1

,
(31a)
Î“n(râˆ—
n) = T proc
n
(râˆ—
n) + T down
n
.
(31b)
Specifically, the solution Ï„ âˆ—can be efficiently obtained using
a 1-D bisection search, and the corresponding Ë†Pn(Ë†Ï„) can be
computed via (29). Thus, KKT removes the need for a multi-
dimensional search, i.e., we evaluate Pn(Ï„) in closed form and
only line-search over Ï„.
Proof. Recall that Î¶ in (27a) is a pre-determined coefficient
for latency-power trade-off. If Î¶ = 0, the objective ignores
the power cost. According to the condition (27a), the optimal
power is achieved at the upper bound P âˆ—
n = P max
n
. Otherwise,
from condition (27a), we obtain:
Î¶ + Î¹n âˆ’Ï‰ngn(Pn, râˆ—
n) = 0,
âˆ€n,
(32)
where only Ï‰n > 0 makes the equation hold. Substitute Ï‰n > 0
into (27d), we can obtain: T total
n
(Pn, râˆ—
n) = Ï„, which yields an
implicit relation between Pn and Ï„. Solving for Pn gives:
Pn(Ï„) = Ïƒ2
hn

2
D(râˆ—
n)
Bn(Ï„âˆ’Î“n(râˆ—n)) âˆ’1

,
(33)
where Î“n(râˆ—
n) = T proc
n
(râˆ—
n) + T down
n
. Substituting Pn(Ï„) into
the stationarity condition with Î¹n = 0 leads to:
Ï‰n(Ï„) =
Î¶
gn(Pn(Ï„), râˆ—n).
(34)
Then, from (27b), we obtain the condition:
X
nâˆˆN
Ï‰n(Ï„) = 1,
(35)
which can be efficiently solved for Ï„ âˆ—due to the monotonicity
of the left-hand side. The corresponding power allocation
is then given by P âˆ—
n = min{Pn(Ï„ âˆ—), P max
n
} to satisfy the
constraint Pn â‰¤P max
n
, and the corresponding dual variables
can be obtained from the KKT conditions.
Algorithm 1: ARPO algorithm for problem P1
Input: MINLP problem P1;
Output: Optimal solution {râˆ—, Pâˆ—, Ï„ âˆ—};
1 Search the minimum optimal râˆ—= {râˆ—
n}nâˆˆN for each
user n using the B&B algorithm, s.t., A(râˆ—
n) â‰¥Amin
n
;
2 Given the obtained râˆ—, transform P1 into a convex
problem P2;
3 Apply KKT conditions on P2 to obtain (27a)-(27d);
4 Derive the optimal solution {Pâˆ—, Ï„ âˆ—} to P2 according
to Proposition 1 when coefficient Î¶ is given;
5 return {râˆ—, Pâˆ—, Ï„ âˆ—}
V. THE PROPOSED LLARA METHOD FOR TRAJECTORY
OPTIMIZATION
This section details our proposed LLaRA method for opti-
mizing the UAV trajectory. With the obtained {râˆ—, pâˆ—} from
the ARPO algorithm, now we focus on the optimization of the
UAV trajectory {(x[t], y[t], z[t])}tâˆˆT . Accordingly, the original
problem P0 can be simplified as follows:
P3 :
min
{x[t],y[t],z[t]}

max
nâˆˆN T total
n
 râˆ—
n, P âˆ—
n

(36)
s.t.
(21b), (21c), (21d).
To solve problem P3, we propose LLaRA, which employs
the LLM as a reward designer for enhancing the traditional
RL ability. We present the design of LLaRA in the following
subsections.
A. An overview of LLaRA
Generally speaking, LLaRA integrates traditional DRL with
LLM-augmented reward design to optimize UAV trajectories
under dynamic LAENet conditions. Traditional DRL often
suffers from heuristic reward designs, which can hinder conver-
gence and limit generalization. LLaRA addresses this limitation
by combining a stable DRL backbone with the reasoning and
code generation capabilities of advanced LLMs. The overall
workflow is illustrated in Fig. 3.
Specifically, LLaRA proceeds in two separate stages. First,
before deployment, we apply an LLM in an offline reward
design loop, as shown in the upper part of Fig. 3. The
employed LLM receives structured prompts containing the
system information, and outputs candidate reward functions.
These candidate rewards are then evaluated in the training
environment, scored according to their effectiveness, and itera-
tively refined through an LLM self-improvement process [33],
[34]. By incorporating both performance feedback and optional


--- Page 9 ---
9
Environment
LLM input
Code
Prompt
LLM expert
Candidate rewards
â€¦
Generate candidate
reward functions
Scores of 
candidates
Feedback
Self-refinement loop
Human insight
LLM self-refinements
Input
Role definition: 
You are an expert in RL 
and UAV trajectoryâ€¦
Task description:
A single UAV serves N 
ground usersâ€¦
Requirements: Provide K 
candidates in JSON â€¦
CoT prompt
Critic-Network
Actor-Network
Replay
Buffer
ğ‘ğ‘¡, ğ‘ ğ‘¡, ğ‘ ğ‘¡+1
ğ‘ ğ‘¡
(ğ‘ğ‘¡)
Mini-batch
Sample
LLM-reward
ğ‘Ÿğ‘¡ 
Actor & Critic 
Optimizer
Update
Update
TD Error
MSE Loss
PPO clipping
GAE
LLM-augmented Reward Design
GAE-PPO with LLM-designed reward
Estimated
advantage value
â„’ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğœƒ
â„’ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘âˆ…
Compute
Flying  agent
Ground users
Interact
Fig. 3: The workflow of LLaRA. The LLM-augmented reward design employs an LLM expert to generate and iteratively refine
the candidate reward functions. The GAE-PPO strategy updates the Actor and Critic networks using the feedback provided by
the refined LLM-designed reward.
human guidance, the LLM progressively improves the reward
design toward better alignment with the optimization goals.
Second, once a refined reward function design is selected,
it is incorporated into the Proximal Policy Optimization (PPO)
backbone [50] for trajectory learning. PPO alternates between
data collection and policy update, with the clipped surrogate
objective ensuring training stability. The advantage estimates
used in PPO are computed via Generalized Advantage Esti-
mation (GAE), which further improves sample efficiency and
variance reduction. Importantly, because the LLM-assisted re-
ward generation occurs offline, no additional inference latency
is introduced during online UAV operation. During deployment,
trajectory decisions are obtained by a simple forward pass of
the trained actor network, ensuring that the method remains
efficient in real-time settings [14].
B. LLM-augmented Reward Design.
We present a novel LLM-augmented reward design scheme,
where the LLM acts as an automated reward design expert. Our
approach employs a Chain-of-Thought (CoT)-enhanced prompt
engineering strategy [1] to guide the LLM in understanding
our system model, identifying optimization factors, and gener-
ating executable codes for candidate reward functions. These
candidate functions are then iteratively evaluated, refined, and
integrated into our GAE-PPO method. In the following, we
provide a detailed description of the whole process, covering
prompt construction, reward generation, evaluation, and itera-
tive refinement. Fig. 4 presents an instance of LLM prompts
used for reward generation and subsequent refinement.
1) Prompt Engineering for Reward Design: To guide the
LLM in effectively designing reward functions, the first step is
to ensure that the LLM understands its role properly. Hence,
we adopt a CoT-enhanced prompt structure consisting of the
following elements:
Role definition. Firstly, the LLM is specified as a profes-
sional reward designer with expertise in DRL and LAENets.
Its tasks include: (i) understanding the underlying system
model, (ii) reasoning over observations and agentâ€“environment
interactions, and (iii) generating executable Python code for
reward functions. Normative constraints are also imposed, such
as prohibiting assumptions based on ungiven information and
prioritizing the most relevant factors in the reward to mitigate
potential LLM hallucinations [51]. To facilitate seamless in-
tegration with our PPO pipeline, the response format is also
required to be standardized (e.g., in JSON), enabling automated
validation and execution.
Task description. Task description is to help LLM grasp a
comprehensive understand on the studied problem, including
background, system model, and objective function, etc. To
bridge the gap between natural language and programmatic
understanding, we further embed code snippets (e.g., MDP
design) alongside textual descriptions as a unified input to
LLM. This hybrid expression motivates LLM to focus on task-
relevant contextual information, while reducing the likelihood
of producing overly generic reward functions.
2) Initial Reward Function Generation: With our crafted
prompts (i.e., role definition and task description), the LLM


--- Page 10 ---
10
generates a set of candidate reward functions RLLM:
RLLM = FGen(P, C; Î˜),
(37)
where FGen(Â·) is the function for generating reward functions
from the prompt, P denotes the textual part in role definition
and task description, C denotes the embedded code snippets,
and Î˜ is the LLM parameter set. Specifically, each candidate
reward Ri âˆˆRLLM should satisfy the predefined return type
and align with the optimization problems (e.g., incorporating
both objectives and penalties for constraint violations).
The obtained candidate reward functions RLLM can be
evaluated through interactions with the devised environment. In
detail, their effectiveness are quantified by a score set SLLM:
SLLM = FEval(RLLM; E, Î¦),
(38)
where FEval(Â·) is the function for evaluation, E denotes the
DRL environment, and Î¦ represents the evaluation configura-
tion (e.g., training episodes and performance metrics). Each
score Si âˆˆSLLM refers to the performance of candidate Ri âˆˆ
RLLM, and helps to guide refinements to reward functions.
3) Iterative Self-Refinement: To further improve the quality
of generated reward functions, we introduce a self-refinement
strategy that allows the LLM to iteratively make improvements
to reward functions RLLM. Specifically, the self-refinement
prompt fed to the LLM will include: (i) evaluation results of all
candidate functions SLLM, and (ii) human insights H (optional)
to guide the refinements. The refinement process is as follows:
Râ€²
LLM = FRef(SLLM, H; RLLM, Î˜),
(39)
where Râ€²
LLM denotes the set of refined candidates, and FRef(Â·)
refers to the function for refinements. In each iteration, the
LLM is re-prompted with the evaluation results SLLM, focusing
on refining the top-performing candidates. Human preferences
are optional but particularly valuable when multiple candidates
in RLLM exhibit comparable performance, as insights provide
additional guidance for steering the LLM toward more effective
reward designs.
After going through a certain number of refinement rounds
as shown in (39), we can perform a final evaluation and select
the best-performing reward function Râˆ—
LLM:
Râˆ—
LLM = arg
max
RiâˆˆRâ€²
LLM
Si,
(40)
and the selected Râˆ—
LLM is adopted as the final reward function
of LLaRA. We summarize the overall process in Algorithm 2.
C. MDP Formulation with LLM-designed Rewards
We formulate problem P3 as an MDP to facilitate the RL
solution. Specifically, the MDP is often defined by a tuple
âŸ¨S, A, R, M, ÏâŸ©, where S denotes for the state space, A the
action space, R the reward design, M the state transition
model, Ï âˆˆ(0, 1) the discount factor. Among these elements,
S, A, and R are pivotal in the agentâ€™s learning process, and
their definitions are detailed below.
Reward Function
Initialization Prompt
Reward Function
Self-refinement Prompt
Role definition:
You are a senior researcher  in reinforcement
learning and UAV trajectory optimization 
problem. You should understand the following 
system model, providing your suggestions and  
executable codes in reward function design.
Task description:
A single UAV serves N ground users over T 
slots. Users upload images + text to the UAV; 
the UAV runs VLM onboard. Total latency per 
user = uplink time + VLM processing + tiny 
downlink. ARPO has already fixed resolution r 
and power P; this task only optimizes the 
trajectory via RLâ€¦
Environment code snippets:
Class Env():â€¦
def __init__():â€¦
def _state():â€¦
def step():â€¦
Requirements:
You should design and implement K diverse, 
high-quality reward functions for a PPO agent. 
You must return strict JSON only (a list of 
length K). Each item must include an 
executable Python functionâ€¦
Evaluation results:
Candidate reward function 1
Iter 20: latency = 23.06 s, avg_reward = 44.01
Iter 40: latency = 21.85 s, avg_reward = 67.38
â€¦
Candidate reward function 2
Iter 20: latency = 22.19 s, avg_reward = 8.39
Iter 40: latency = 19.93 s, avg_reward = 10.54
â€¦
Your candidate reward function 1 generated a 
final latency of 15.07 s.
Your candidate reward function 2 generated a 
final latency of 12.36 s.
â€¦
Your candidate reward function K generated a 
final latency of 14.34 s.
Human insights:
Candidate reward function 2 performs the best 
in latency minimization, since it â€¦
Please further refine the M best-performing 
candidates and generate additional K-M new 
candidates for evaluation.
Fig. 4: Instance prompts used in the initialization and evolution
of our LLM-assisted reward design.
Algorithm 2: LLM-augmented Reward Design
Input: Role definition Prole, Task description Ptask,
Requirements Preq, Code snippets C,
DRL environment E, Evaluation policy Î¦,
Human insights H, LLM parameters Î˜;
Output: Final reward function Râˆ—
LLM;
1 Construct the CoT-enhanced prompt P:
P â†(Prole, Ptask, Preq);
2 Generate a set of candidate reward functions RLLM
via (37) by feeding P and C into the LLM;
3 Evaluate RLLM by interactions with environment E and
policy Î¦, obtain a score set SLLM via (38);
4 while termination criterion not met do
5
Given SLLM and human insights H, LLM refines
the candidates function into Râ€²
LLM via (39);
6
Evaluate refined Râ€²
LLM via (38) to obtain a new
score set SLLM;
7
Update candidates RLLM with new Râ€²
LLM;
8 end
9 Select the best-performing Râˆ—
LLM via (40);
10 return Râˆ—
LLM
1) State Space S: We design the state space S to contain
the key environmental factors for agent decision-making. To
better describe spatial relationships, we adopt relative positions
to represent all positional relations in our system. Accordingly,
the state st of the UAV at time slot t is defined as follows:
st =
h
{(x[t]âˆ’xn, y[t]âˆ’yn, z[t]âˆ’hn)}nâˆˆN
|
{z
}
UAV-user relative positions
; {rn}nâˆˆN ;
{Pn}nâˆˆN ; {hn[t]}nâˆˆN ; {dn[t]}nâˆˆN
i
,
âˆ€t.
(41)


--- Page 11 ---
11
{rn}nâˆˆN denotes the image resolutions; {Pn}nâˆˆN is the
transmit power; {hn[t]}nâˆˆN is the channel gain at slot t;
{dn[t]}nâˆˆN is the remaining data sizes waiting for transmis-
sion.
2) Action Space A: The action space A is directly related
to optimization variables. After obtaining the state information
S, the UAV needs to plan its next move based on the policy
distribution. The action at at time slot t consists of:
a[t] =

(âˆ†x[t], âˆ†y[t], âˆ†z[t])
	
,
âˆ€t,
(42)
where (âˆ†x[t], âˆ†y[t], âˆ†z[t]) denotes a movement vector for the
UAV mobility. It is bounded by predefined values to comply
with related constraints (21b)-(21d) as follows:
ï£±
ï£´
ï£²
ï£´
ï£³
hmin â‰¤z[t] â‰¤hmax,
âˆ€t,
(43a)
p
(âˆ†x[t])2 + (âˆ†y[t])2 â‰¤Î± Â· V max
xy
,
âˆ€t,
(43b)
|âˆ†z[t]| â‰¤Î± Â· V max
z
,
âˆ€t.
(43c)
3) LLM-designed Reward RLLM: Reward design is central
to an MDP, as it evaluates stateâ€“action pairs and guides the
agent toward the optimal policy. In this work, we utilize a â€œrisk-
awareâ€ reward function Rrisk
LLM refined by the LLM and detail
its expression as follows:
Rrisk
LLM = âˆ’VaRq
 d[t]

+ Âµ
X
nâˆˆN
min{dn[t], Î±Rn[t]}
+ Î³d Â· âˆ†dist[t],
(44)
where VaRq
 d[t]

= inf
n
Ï„ :
1
|N|
P
nâˆˆN 1{dn[t] â‰¤Ï„} â‰¥q
o
is the empirical q-quantile of user backlogs, penalizing the tail
and aligning with the worst-case latency in (36); the second
term P
nâˆˆN min{dn[t], Î±Rn[t]} counts only transmitted data
from unfinished users to preserve overall system efficiency; and
âˆ†dist[t] = âˆ¥(q[t], z[t]) âˆ’(wnt, hnt)âˆ¥2 âˆ’âˆ¥(q[t+1], z[t+1]) âˆ’
(wnt, hnt)âˆ¥2 with nt = arg maxn dn[t] rewards motion toward
the bottleneck user. Here q âˆˆ(0, 1) and Âµ, Î³d â‰¥0 are weights.
Notably, Rrisk
LLM designed by LLM includes not only a tail-
aware term VaRq
 d[t]

motivated by (36), but also incorporates
potentially influential components on overall effective through-
put and the distance change between the bottleneck user and
UAV. This formulation can help the agent to observe its action
values from a more comprehensive view, and hence increase
the convergence performance of the learned policy.
VI. COMPLEXITY ANALYSIS
Here we analyze the computational complexity of ARPO and
LLaRA in our optimization framework, and discuss the overall
complexity in the real-time decision-making process.
A. Complexity of ARPO
The ARPO algorithm optimizes resolution selection r and
power allocation P in an alternating fashion. Firstly, the optimal
râˆ—is determined using a B&B method over the discrete set
Rres. Let N denote the number of users and J the number
of candidate resolutions. In the worst case, the computational
complexity of this search is O
 JN
[52], but the B&B pruning
mechanism often reduces the effective search space. If L de-
notes the estimated number of explored branches, the practical
time complexity is approximately O
 L Â· N

. Given râˆ—, the
power allocation subproblem is convex and solved via a one-
dimensional bisection search over Ï„, yielding a complexity of
O
 N Â·log(1/Ïµ)

, where Ïµ is the convergence tolerance. Finally,
an additional O
 N

complexity accounts for the final back-
substitution of Ï„ âˆ—to compute Pâˆ—. Hence, the overall complexity
of ARPO is O
 (log(1/Ïµ) + L + 1) Â· N

.
B. Complexity of LLaRA
The complexity of the LLaRA algorithm consists of two
main components: the complexity of the GAE-PPO backbone,
and the complexity of LLM-augmented reward design.
1) GAE-PPO Complexity: The complexity of GAE-PPO is
primarily determined by the architecture of the deep neural
networks (DNNs) used in the actor and critic networks. The
complexity of DNNs is expressed as O
  PP
p=1 npâˆ’1np

, where
np denotes the number of neurons in the p-th layer and P
is the total number of layers in the actorâ€“critic networks. In
addition, GAE introduces an additional complexity of O
 M

per iteration, where M denotes the number of time steps used
in advantage computation [14]. Hence, the overall complexity
of GAE-PPO is O
 M Â· PP
p=1 npâˆ’1np

.
2) LLM-Enhanced Reward Design Complexity: The reward
design involves prompting the LLM and evaluating candidate
reward functions. Let K denote the number of reward candi-
dates, I denote the number of refinement rounds, and CLLM
denote the average cost of one LLM call. Then, the overall
complexity is O
 IÂ·KÂ·(CLLM+Ceval)

, where Ceval is the cost
of evaluating one candidate in the DRL environment. Since both
LLM calls and evaluations are performed before deployment,
its impact on real-time deployment is negligible.
Overall Complexity in Real-time Decision-Making. The
major computational overhead of the proposed ARPO-LLaRA
framework arises from the training of the PPO backbone and
the LLM-assisted reward refinement. However, these phases are
performed offline and prior to deployment. During real-time
operation, the decision-making process only involves ARPO
and a single forward pass of LLaRA, bringing an overall time
complexity of O
 (log(1/Ïµ)+L+1)Â·N +M Â·PP
p=1 npâˆ’1np

.
VII. EXPERIMENTS
We conduct extensive experiments to validate the proposed
framework. Section VII-A details experimental parameter set-
tings; Section VII-B quantifies the impact of input resolution on
VLM accuracy and efficiency; Sections VII-C to VII-F present
different simulation results.
A. Experimental Settings
1) VLM Settings: To investigate how input image resolution
affects the performance of VLMs, we conduct an empirical
study on two representative architectures: LLaVA-1.5 [47] and
its high-resolution variant LLaVA-HR [15]. Both models are
evaluated on the TextVQA benchmark [41], which is a widely


--- Page 12 ---
12
used dataset designed to evaluate multimodal inference and
aligns well with our considered LAENet usersâ€™ tasks. Interested
readers can refer to https://github.com/luogen1996/LLaVA-HR
for more technical details of LLaVA models.
2) Reward-Design Settings: Reward design is performed of-
fline with the help of an LLM: We prompt GPT-4o [53] to syn-
thesize and refine executable reward candidates (code/JSON).
Specifically, we use GPT-4o because prior work has verified
that it reliably produces executable reward code [54] and can
serve as an automatic monitor in RL reward pipelines [55].
3) Parameter Settings: Here we give the default parameter
settings, primarily adapted from [42]. Specifically, we consider
a UAV-enabled LAENet with N = 4 ground users distributed
within an 1000 Ã— 1000 m2 square area, facilitating clear visu-
alization in square-shaped figures. The UAVâ€™s initial location
is set as (âˆ’500, âˆ’500, 150). The minimum and maximum
allowable altitudes for the UAV, i.e., hmin and hmax, are
set as 50 m and 300 m, respectively. Ground users are at
altitudes hn = 0 m. The elemental time slot length Î± is
1 s and the number of slots T is 50. The UAVâ€™s maximum
allowable horizontal and vertical speeds, i.e., V max
xy
and V max
z
,
are 100 m/s and 20 m/s, respectively. For the communica-
tion model parameters, we adopt a standard large-scale path-
loss model with exponents Î³LoS and Î³NLoS as 2, parameters
(a, b) = (4.88, 0.43), reference channel gain Î²0 = âˆ’50 dB,
and noise power Ïƒ2 = âˆ’100 dBm. Each user is allocated a
bandwidth of Bn = 1 MHz and a maximum transmit power of
P max
n
= 0.1 W. The downlink time T down
n
is set as 0.1 s.
B. Empirical Study on Resolution-Aware Model Performance
To better illustrate the impact of visual input resolution on
VLM performance, we conduct both qualitative and quantitative
analyses. Fig. 5 summarizes our empirical study.
First, we conduct a TextVQA case study to examine how
input resolution influences the quality of answers produced
by diverse VLMs, including LLaVA-1.5-13B, LLaVA-HR-7B,
Qwen-3-225B [56] and ChatGPT-4o. As shown in the left
and middle panels of Fig. 5, the 1024p image preserves fine-
grained details such as the tiniest carâ€™s true color, whereas
downsampling to 336p blurs small objects and reduces color
information. This resolution gap directly affects the reasoning
performance of VLMs: for Question 1 (â€œWhat type the biggest
car is?â€), all models consistently answer â€œbus,â€ since the large
object remains identifiable at both resolutions. However, for
Question 2 (â€œWhat color the tiniest car is?â€), only models that
can process higher-resolution inputs are able to give the correct
answer. LLaVA-1.5-13B defaulting to 336p fails on this task,
while LLaVA-HR-7B successfully outputs the correct answer.
Notably, the comparison shows that LLaVA-HR-7B achieves
answer quality comparable to larger models such as Qwen3-
225B and GPT-4o, despite its significantly smaller size. This
suggests that enabling high-resolution visual input offers a more
efficient path to improving answer accuracy than simply scaling
up model parameters. Given that our target application lies in
UAV-enabled LAENet, where onboard storage and computa-
tional resources are inherently limited, we adopt LLaVA-HR
as the reference VLM in our subsequent analysis.
To complement the quantitative analysis, we further include
the profiling results of LLaVA-HR [15], as shown in the right
panel of Fig. 5. These results provide a system-level view of the
resolution-accuracyâ€“efficiency trade-off. Specifically, accuracy
improves from 59.63% at 384p to 67.96% at 1536p, while
inference speed decreases from 23.8 to 12.6 tokens/s and input
size grows from 0.42 MB to 6.74 MB. To support resolution-
aware optimization in our system design, we define empirical
lookup functions for both accuracy and inference speed. The
resolution-dependent accuracy function An(rn) is modeled as:
An(rn) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
59.63%,
if rn = 384p,
64.36%,
if rn = 768p,
67.11%,
if rn = 1024p,
67.96%,
if rn = 1536p.
(45)
Similarly, we have the model inference speed function v(rn)
reported in [15] as follows:
v(rn) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
23.8 tokens/s,
if rn = 384p,
19.9 tokens/s,
if rn = 768p,
19.7 tokens/s,
if rn = 1024p,
12.6 tokens/s,
if rn = 1536p,
(46)
These empirical mappings serve as key inputs to our designed
optimization framework, enabling resolution selection based on
the desired trade-off between accuracy and efficiency.
C. Comparative Analysis with Different Baselines
To comparatively analyze our ARPO-LLaRA framework,
we introduce three baselines. The first is a Random Policy
(RP), where the agent selects resolutions, velocities, and pow-
ers arbitrarily, serving as a benchmark without optimization.
The second is ARPO-Geometric Heuristic (ARPO-GH), which
applies ARPO for resolution and power allocation and then
directs the UAV toward the geometric center of users [57].
The third is ARPO-PPO, which integrates ARPO with a PPO
backbone [50] using manually crafted reward functions aim to
minimize the backlog of the bottleneck user. This comparison
highlights the advantages of incorporating LLM-assisted reward
design in ARPO-LLaRA.
Convergence Performance. Fig. 6 compares the conver-
gence performance of our ARPO-LLaRA method against the
baselines. The shaded area denotes the variance, and the solid
line denotes the mean in terms of the latency performance.
It can be observed that ARPO-LLaRA constantly outper-
forms all other baselines over the entire training episodes,
which validates the superiority of our hierarchical optimization
framework. Compared with RP and ARPO-GH, ARPO-LLaRA
achieves significant improvements of around 45% and 25%
in latency performance, respectively. Even against the robust
ARPO-PPO baseline with a manually designed reward function,
ARPO-LLaRA delivers a 13.7% improvement in latency reduc-
tion, owing to the effectiveness of proposed LLM-augmented


--- Page 13 ---
13
Application Example
Symbol Description
â€œWhat type the biggest car is?â€
â€œWhat color the tiniest car is?â€
The tiniest car visible in the image 
is red.
The tiniest car is white.
The tiniest car is red.
The tiniest car in the image is red.
User
Question 1
The biggest vehicle visible in the image is a 
bus (or large coach).
The biggest car in the image is a bus.
The biggest car is a bus.
The biggest car is a bus.
Agent
Answer 1
User
Question 2
Agent
Answer 2
Effect of Different Resolutions on Key information
High (1024p)
Low (336p)
Low resolution significantly blurs the shape of the objects and 
reduces the color information.
VS
Effect of Resolution on VLM Responses
Profiling Results of LLaVA-HR
Resolution vs Model Accuracy
Resolution vs Inference Speed
Resolution vs Data Size
Type of VLM
Qwen 3-225B
GPT-4o 
LLaVA-V1.5-13B
LLaVA-HR-7B (ours)
Correct answer
Wrong answer
ï‚§â€œWhat type the 
biggest car is?â€
ï‚§â€œWhat color the 
tiniest car is?â€
Text query:
UAV
Onboard camera
Text query
Image
VLM
Environment
User
Fig. 5: Impact of input resolution on TextVQA. Left and middle: our reproduced experiments using LLaVA-HR show that
high-resolution inputs (e.g., 1024p) preserve fine details such as the tiny â€œredâ€ car, enabling correct answers that are lost when
downsampled to 336p. Notably, LLaVA-HR-7B achieves accuracy comparable to much larger models (Qwen3-225B, GPT-4o).
Right: published profiling results of LLaVA-HR [15], illustrating the accuracyâ€“efficiencyâ€“size trade-off across resolutions.
Fig. 6: Convergence performance comparison between ARPO-
LLaRA and different baselines.
reward design. By comparing RP and ARPO-GH, we can also
demonstrate the effectiveness of the proposed ARPO algorithm.
Moreover, ARPO-LLaRA demonstrates faster convergence and
greater training stability, thanks to the LLM-designed reward
function that incorporates multiple informative terms. Specifi-
cally, the LLM-designed reward function penalizes tail backlog
(aligning with worst-case latency), emphasizes overall through-
put from unfinished users, and prioritizes users who currently
dominate the overall completion time, thereby guiding toward
a stable optimal policy more efficiently.
Trajectory and Latency Performance. To provide a clearer
illustration of the methodâ€™s performance in deployment, Figs. 7
and 8 compare UAV trajectories and latency under two user
distributions comprising both high-demand and low-demand
users. Specifically, high-demand users are defined as those
issuing multiple queries (Mn = 2) with stricter accuracy re-
quirements (Amin
n
= 0.67), whereas low-demand users submit
a single query (Mn = 1) with a standard accuracy requirement
(Amin
n
= 0.60). Next, we detail our analysis on Figs. 7 and 8.
Fig. 7 examines performance under user distribution I, with
two high-demand users located farther away in the northwest
corner and two low-demand users positioned closer to the UAV
start point. As shown in Fig. 7(a), both ARPO-PPO and ARPO-
LLaRA guide the UAV toward the high-demand users, since
they dominate the system latency. Compared with ARPO-PPO,
ARPO-LLaRA produces a smoother and more stable trajectory,
avoiding unnecessary detours while still prioritizing bottleneck
users. This suggests that the LLM-augmented reward design not
only balances the UAVâ€™s service priorities more effectively but
also helps the agent converge toward a more efficient trajectory
that minimizes latency with improved stability. The quantitative
results in Fig. 7(b) further confirm these observations. While
RP suffers from the highest latency, ARPO-GH reduces latency
by 27.4%. ARPO-PPO achieves an additional 10.8% reduction,
and ARPO-LLaRA yields the best performance with a further
11.7% improvement over ARPO-PPO. These results highlight
that the proposed ARPO method is effective in reducing latency,
and the LLM-augmented reward design in ARPO-LLaRA pro-
vides a significant performance gain beyond manually designed
rewards.


--- Page 14 ---
14
-600
-400
-200
0
200
400
X (m)
-600
-400
-200
0
200
400
600
Y (m)
Low-Demand
High-Demand
RP
ARPO-GH
ARPO-PPO
ARPO-LLaRA
Start Point
(a) UAV horizontal trajectory.
RP
GH
PPO
LLaRA
User Distribution 1
0
5
10
15
20
25
30
Latency (s)
22.65
15.92
14.21
12.48
29.7%
10.7%
12.2%
RP
ARPO-GH
ARPO-PPO
ARPO-LLaRA
(b) Achieved minimum latency.
Fig. 7: Optimized UAV trajectory and minimum latency com-
parisons under user distribution I, where high-demand users
located farther and low-demand users closer.
-600
-400
-200
0
200
400
X (m)
-600
-400
-200
0
200
400
600
Y (m)
Low-Demand
High-Demand
RP
ARPO-GH
ARPO-PPO
ARPO-LLaRA
Start Point
(a) UAV horizontal trajectory.
RP
GH
PPO
LLaRA
User Distribution 2
0
5
10
15
20
25
Latency (s)
19.73
14.32
12.78
11.29
27.4%
10.8%
11.7%
RP
ARPO-GH
ARPO-PPO
ARPO-LLaRA
(b) Achieved minimum latency.
Fig. 8: Optimized UAV trajectory and minimum latency com-
parisons under user distribution II, where high-demand users
located closer and low-demand users farther.
Fig. 8(a) shows the UAV trajectory under user distribution II,
where the high-demand users are located closer to the UAV start
point and the low-demand users are farther away. Compared
with distribution I, the UAV trajectories become shorter, as the
bottleneck users can be reached more quickly. Both ARPO-
PPO and ARPO-LLaRA still prioritize high-demand users, but
ARPO-LLaRA maintains a smoother and more efficient path.
The latency results in Fig. 8(b) follow a similar trend as before.
ARPO-GH achieves a notable latency reduction compared with
RP, while ARPO-PPO brings further improvements. ARPO-
LLaRA again achieves the lowest latency, with an additional
11.7% gain over ARPO-PPO, demonstrating the consistent
advantage of the LLM-augmented reward design.
D. Impact of Weight Coefficients on Latency and Power
We now examine how the weight coefficient influences the
optimization result. Recall that in the objective function (21) of
P0, the parameter Î¶ determines the relative importance assigned
200
400
600
800
1000
Coeï¬ƒcient 
8.00
8.50
9.00
9.50
10.00
10.50
Latency (s)
0.05
0.10
0.15
0.20
0.25
Total Power (W)
Latency
Total Power
Fig. 9: Overall latency and sum of user transmit powers under
weight coefficient Î¶ with different values from 100 to 1000.
to power versus latency. When communication bandwidth is
relatively abundant, or when the system is more sensitive to
power consumption, increasing Î¶ encourages users to adopt
lower transmit power.
We set the coefficient Î¶ within the range of 100 to 1000,
while allocated bandwidth is set as 2 MHz. The experimental
result is reported in Fig. 9. As illustrated, increasing Î¶ leads
to a significant reduction in the total transmit power, which
decreases from around 0.20 W at Î¶ = 100 to below 0.07 W
at Î¶ = 1000. This trend evidently verifies that a larger weight
on power effectively motivates users to transmit with smaller
power levels. In contrast, latency exhibits the opposite trend: it
grows from roughly 8.5 s at Î¶ = 100 to nearly 10 s at Î¶ = 700,
after which it continues to increase more gradually. This occurs
because reducing transmit power lowers the transmission rate,
thereby increasing the overall latency.
E. Multi-round Optimization Performance for Multiple Batches
To further evaluate the adaptability of our proposed frame-
work, we simulate a multi-round service scenario, where the
UAV sequentially serves different batches of users. As shown in
Fig. 10(a), the UAV first departs from the initial location (green
dot) and completes service for Batch 1 users (orange stars).
Afterwards, instead of returning to the initial point, it continues
from the last position to serve Batch 2 users (yellow stars),
and then proceeds to Batch 3 users (purple stars), eventually
reaching the final destination (red dot).
The 3D trajectory in Fig. 10(b) illustrates the UAVâ€™s altitude
variations and time progression, where the color bar indicates
the time step. The results demonstrate that our proposed opti-
mization framework can effectively plan the UAV trajectories
across multiple service rounds, ensuring smooth transitions
between spatially distributed users.
F. Impact of Maximum Power and Bandwidth on Latency
To investigate how system resources affect the optimization
performance, Fig. 11 shows the achieved latency under differ-
ent maximum transmit power P max
n
and allocated bandwidth
Bn. Intuitively, we can see that latency decreases as either


--- Page 15 ---
15
-500
0
500
1000
1500
X (m)
-500
0
500
1000
1500
Y (m)
Trajectory
Start
End
Batch 1 Users
Batch 2 Users
Batch 3 Users
(a) Top view of the UAV horizontal trajectory.
(b) Visualization of the UAV 3D trajectory.
Fig. 10: UAV trajectory performance for a multi-round service scenario. The UAV starts from the initial location, sequentially
serves Batch 1, Batch 2, and Batch 3 users, and terminates at the final position.
Fig. 11: The overall latency versus different maximum transmit
power and communication bandwidth.
bandwidth or transmit power increases, since both parameters
directly enhance the achievable transmission rate. Specifically,
bandwidth has a more dominant impact on the latency per-
formance. For instance, with a bandwidth of 3 MHz and a
maximum transmit power of 0.1 W, the latency is reduced to
6.48 s. In contrast, when bandwidth is constrained to 1 MHz
and transmit power is 0.5 W, latency is reduced to 10.29 s.
VIII. CONCLUSION
In this paper, we have proposed a UAV-enabled LAENet
that leverages VLMs for onboard inference services. Then,
we have formulated a joint optimization problem for latency
and power efficiency and introduced a hierarchical framework
combining ARPO and LLaRA to optimize resolutions, transmit
powers, and UAV trajectories. Simulations have shown that our
approach reduces latency while meeting accuracy requirements
and scales well under diverse resource and service settings,
highlighting its potential for practical inference-as-a-service in
LAENets.
REFERENCES
[1] L. Cai, R. Zhang, C. Zhao, Y. Zhang, J. Kang, D. Niyato, T. Jiang,
and X. Shen, â€œLarge language model-enhanced reinforcement learning
for low-altitude economy networking,â€ arXiv preprint arXiv:2505.21045,
2025.
[2] S. He, J. Wang, Y.-C. Liang, G. Sun, and D. Niyato, â€œSatellite-assisted
low-altitude economy networking: Concepts, applications, and opportu-
nities,â€ arXiv preprint arXiv:2505.04098, 2025.
[3] L. Cai, J. Wang, R. Zhang, Y. Zhang, T. Jiang, D. Niyato, X. Wang,
A. Jamalipour, and X. Shen, â€œSecure physical layer communica-
tions for low-altitude economy networking: A survey,â€ arXiv preprint
arXiv:2504.09153, 2025.
[4] Y. Wang, G. Sun, Z. Sun, J. Wang, J. Li, C. Zhao, J. Wu, S. Liang, M. Yin,
P. Wang et al., â€œToward realization of low-altitude economy networks:
Core architecture, integrated technologies, and future directions,â€ arXiv
preprint arXiv:2504.21583, 2025.
[5] C. Daily, â€œChinaâ€™s low-altitude economy soars at high speed,â€ https:
//www.chinadaily.com.cn/a/202412/19/WS6763b8b7a310f1265a1d3d24.
html, 2024.
[6] D. He, W. Yuan, J. Wu, and R. Liu, â€œUbiquitous uav communication
enabled low-altitude economy: Applications, techniques, and 3gppâ€™s
efforts,â€ IEEE Network, 2025.
[7] A. Khan, S. Gupta, and S. K. Gupta, â€œEmerging uav technology for
disaster detection, mitigation, response, and preparedness,â€ Journal of
Field Robotics, vol. 39, no. 6, pp. 905â€“955, 2022.
[8] H. Liu, C. Li, Q. Wu, and Y. J. Lee, â€œVisual instruction tuning,â€ Advances
in neural information processing systems, vol. 36, 2023.
[9] J. Yao, J. Li, Y. Li, M. Zhang, C. Zuo, S. Dong, and Z. Dai, â€œA visionâ€“
language model-based traffic sign detection method for high-resolution
drone images: A case study in guyuan, china,â€ Sensors, 2024.
[10] J. Wu and G. Feng, â€œClip-ug: Clip-driven vision-language model for
uav-view geo-localization,â€ IEEE Transactions on Consumer Electronics,
2025.
[11] J. Li, D. Li, S. Savarese, and S. Hoi, â€œBlip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language
models,â€ in International conference on machine learning. PMLR, 2023.
[12] J. Zhao and X. Lin, â€œGeneral-purpose aerial intelligent agents empowered
by large language models,â€ arXiv preprint arXiv:2503.08302, 2025.
[13] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma,
P. Wang, X. Bi et al., â€œDeepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning,â€ arXiv preprint arXiv:2501.12948, 2025.


--- Page 16 ---
16
[14] R. Zhang, C. Zhao, H. Du, D. Niyato, J. Wang, S. Sawadsitang,
X. Shen, and D. I. Kim, â€œEmbodied ai-enhanced vehicular networks: An
integrated vision language models and reinforcement learning method,â€
IEEE Transactions on Mobile Computing, pp. 1â€“16, 2025.
[15] G. Luo, Y. Zhou, Y. Zhang, X. Zheng, X. Sun, and R. Ji, â€œFeast your eyes:
Mixture-of-resolution adaptation for multimodal large language models,â€
arXiv preprint arXiv:2403.03003, 2024.
[16] Y. Jiang, X. Li, G. Zhu, H. Li, J. Deng, K. Han, C. Shen, Q. Shi, and
R. Zhang, â€œ6g non-terrestrial networks enabled low-altitude economy:
Opportunities and challenges,â€ arXiv preprint arXiv:2311.09047, 2023.
[17] M. Ahmed, A. A. Soofi, F. Khan, S. Raza, W. U. Khan, L. Su, F. Xu, and
Z. Han, â€œToward a sustainable low-altitude economy: A survey of energy-
efficient ris-uav networks,â€ arXiv preprint arXiv:2504.02162, 2025.
[18] M. M. Salim, K. M. Rabie, and A. H. Muqaibel, â€œEnergy-efficient irreg-
ular ris-aided uav-assisted optimization: A deep reinforcement learning
approach,â€ arXiv preprint arXiv:2504.15031, 2025.
[19] Y. Yang, Y. Chen, J. Wang, G. Sun, and D. Niyato, â€œEmbodied ai-
empowered low altitude economy: Integrated sensing, communications,
computation, and control (isc3),â€ arXiv preprint arXiv:2412.19996, 2024.
[20] Y. Zhou, â€œUnmanned aerial vehicles based low-altitude economy with
lifecycle techno-economic-environmental analysis for sustainable and
smart cities,â€ Journal of Cleaner Production, p. 145050, 2025.
[21] R. Zhang, H. Du, Y. Liu, D. Niyato, J. Kang, Z. Xiong, A. Jamalipour,
and D. In Kim, â€œGenerative AI agents with large language model for
satellite networks via a mixture of experts transmission,â€ IEEE Journal
on Selected Areas in Communications, vol. 42, no. 12, 2024.
[22] Z. Yuan, F. Xie, and T. Ji, â€œPatrol agent: An autonomous uav framework
for urban patrol using on board vision language model and on cloud large
language model,â€ in 2024 6th International Conference on Robotics and
Computer Vision (ICRCV).
IEEE, 2024, pp. 237â€“242.
[23] M. KrupÂ´aË‡s, L. UrblÂ´Ä±k, and I. ZolotovÂ´a, â€œMultimodal ai for uav: Visionâ€“
language models in humanâ€“machine collaboration,â€ Electronics, 2025.
[24] A. Sharshar, L. U. Khan, W. Ullah, and M. Guizani, â€œVision-language
models for edge networks: A comprehensive survey,â€ IEEE Internet of
Things Journal, 2025.
[25] J. Huang, Y. Jin, L. An, and J. Park, â€œLitevlm: A low-latency vision-
language model inference pipeline for resource-constrained environ-
ments,â€ arXiv preprint arXiv:2506.07416, 2025.
[26] T. Sun, P. Wang, and F. Lai, â€œDisco: Device-server collaborative llm-
based text streaming services,â€ arXiv preprint arXiv:2502.11417, 2025.
[27] Z. Yang, Y. Yang, C. Zhao, Q. Guo, W. He, and W. Ji, â€œPerllm: Person-
alized inference scheduling with edge-cloud collaboration for diverse llm
services,â€ arXiv preprint arXiv:2405.14636, 2024.
[28] Y. Li, D. Gumaste, M. K. Turkcan, J. Ghaderi, G. Zussman, and Z. Kostic,
â€œDistributed vlms: Efficient vision-language processing through cloud-
edge collaboration,â€ in 2025 IEEE International Conference on Pervasive
Computing and Communications Workshops and other Affiliated Events
(PerCom Workshops).
IEEE Computer Society, 2025, pp. 280â€“286.
[29] R. Zhang, K. Xiong, Y. Lu, P. Fan, D. W. K. Ng, and K. B. Letaief,
â€œEnergy efficiency maximization in ris-assisted swipt networks with rsma:
A ppo-based approach,â€ IEEE Journal on Selected Areas in Communi-
cations, vol. 41, no. 5, pp. 1413â€“1430, 2023.
[30] Y. Cao, H. Zhao, Y. Cheng, T. Shu, Y. Chen, G. Liu, G. Liang, J. Zhao,
J. Yan, and Y. Li, â€œSurvey on large language model-enhanced reinforce-
ment learning: Concept, taxonomy, and methods,â€ IEEE Transactions on
Neural Networks and Learning Systems, 2024.
[31] J.-C. Pang, X.-Y. Yang, S.-H. Yang, and Y. Yu, â€œNatural language-
conditioned reinforcement learning with inside-out task language devel-
opment and translation,â€ arXiv preprint arXiv:2302.09368, 2023.
[32] B. A. Spiegel, Z. Yang, W. Jurayj, B. Bachmann, S. Tellex, and
G. Konidaris, â€œInforming reinforcement learning agents by grounding
language to markov decision processes,â€ 2024.
[33] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, â€œReward design with
language models,â€ arXiv preprint arXiv:2303.00001, 2023.
[34] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman,
Y. Zhu, L. Fan, and A. Anandkumar, â€œEureka: Human-level reward design
via coding large language models,â€ arXiv preprint arXiv:2310.12931,
2023.
[35] T. Xie, S. Zhao, C. H. Wu, Y. Liu, Q. Luo, V. Zhong, Y. Yang, and T. Yu,
â€œText2reward: Reward shaping with language models for reinforcement
learning,â€ arXiv preprint arXiv:2309.11489, 2023.
[36] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A. Huang,
E. AkyÂ¨urek, A. Anandkumar et al., â€œPre-trained language models for
interactive decision-making,â€ Advances in Neural Information Processing
Systems, vol. 35, pp. 31 199â€“31 212, 2022.
[37] C. L. Shek and P. Tokekar, â€œOption discovery using llm-guided semantic
hierarchical reinforcement learning,â€ arXiv preprint arXiv:2503.19007,
2025.
[38] J. Robine, M. HÂ¨oftmann, T. Uelwer, and S. Harmeling, â€œTransformer-
based world models are happy with 100k interactions,â€ arXiv preprint
arXiv:2303.07109, 2023.
[39] A. Silva, P. Tambwekar, M. Schrum, and M. Gombolay, â€œTowards
balancing preference and performance through adaptive personalized
explainability,â€ in Proceedings of the 2024 ACM/IEEE international
conference on human-robot interaction, 2024, pp. 658â€“668.
[40] C. Zhan and Y. Zeng, â€œCompletion time minimization for multi-uav-
enabled data collection,â€ IEEE Transactions on Wireless Communications,
vol. 18, no. 10, pp. 4859â€“4872, 2019.
[41] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh,
and M. Rohrbach, â€œTowards vqa models that can read,â€ in Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition,
2019, pp. 8317â€“8326.
[42] C. Zhan, H. Hu, X. Sui, Z. Liu, J. Wang, and H. Wang, â€œJoint resource
allocation and 3d aerial trajectory design for video streaming in uav
communication systems,â€ IEEE Transactions on Circuits and Systems for
Video Technology, vol. 31, no. 8, pp. 3227â€“3241, 2020.
[43] G. Fodor, M. Johansson, and P. Soldati, â€œNear optimum power control
and precoding under fairness constraints in network mimo systems,â€
International Journal of Digital Multimedia Broadcasting, vol. 2010,
no. 1, p. 251719, 2010.
[44] W. Yang, G. Caire, G. Durisi, and Y. Polyanskiy, â€œOptimum power control
at finite blocklength,â€ IEEE Transactions on Information Theory, vol. 61,
no. 9, pp. 4598â€“4615, 2015.
[45] T. Ao, K. Zhang, H. Shi, Z. Jin, Y. Zhou, and F. Liu, â€œEnergy-efficient
multi-uavs cooperative trajectory optimization for communication cover-
age: An madrl approach,â€ Remote Sensing, vol. 15, no. 2, p. 429, 2023.
[46] Z. Guo, R. Xu, Y. Yao, J. Cui, Z. Ni, C. Ge, T.-S. Chua, Z. Liu,
and G. Huang, â€œLlava-uhd: an lmm perceiving any aspect ratio and
high-resolution images,â€ in European Conference on Computer Vision.
Springer, 2024, pp. 390â€“406.
[47] H. Liu, C. Li, Y. Li, and Y. J. Lee, â€œImproved baselines with visual
instruction tuning,â€ in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2024.
[48] Y. Li, X. Zhou, and J. Zhao, â€œResource allocation for the training
of image semantic communication networks,â€ IEEE Transactions on
Wireless Communications, 2025.
[49] R. T. Zoppei, M. A. Delgado, L. H. Macedo, M. J. Rider, and R. Romero,
â€œA branch and bound algorithm for transmission network expansion plan-
ning using nonconvex mixed-integer nonlinear programming models,â€
IEEE Access, vol. 10, pp. 39 875â€“39 888, 2022.
[50] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProx-
imal policy optimization algorithms,â€ arXiv preprint arXiv:1707.06347,
2017.
[51] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
A. Madotto, and P. Fung, â€œSurvey of hallucination in natural language
generation,â€ ACM computing surveys, vol. 55, no. 12, pp. 1â€“38, 2023.
[52] D. R. Morrison, S. H. Jacobson, J. J. Sauppe, and E. C. Sewell,
â€œBranch-and-bound algorithms: A survey of recent advances in searching,
branching, and pruning,â€ Discrete Optimization, vol. 19, 2016.
[53] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark,
A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., â€œGpt-4o system
card,â€ arXiv preprint arXiv:2410.21276, 2024.
[54] G. Xie, J. Xu, Y. Yang, Y. Ding, and S. Zhang, â€œErfsl: An efficient reward
function searcher via large language models for custom-environment
multi-objective reinforcement learning (student abstract),â€ in Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 39, no. 28, 2025.
[55] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry,
W. Zaremba, J. Pachocki, and D. Farhi, â€œMonitoring reasoning models
for misbehavior and the risks of promoting obfuscation,â€ arXiv preprint
arXiv:2503.11926, 2025.
[56] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao,
C. Huang, C. Lv et al., â€œQwen3 technical report,â€ arXiv preprint
arXiv:2505.09388, 2025.
[57] A. DaË˜gasÂ¸an and E. KarasÂ¸an, â€œResilient mobile multi-target surveillance
using multi-hop autonomous uav networks for extended lifetime,â€ arXiv
preprint arXiv:2311.03030, 2023.
