--- Page 1 ---
Quantifying the Carbon Reduction of DAG Workloads:
A Job Shop Scheduling Perspective
Roozbeh Bostandoost, Adam Lechowicz, Walid A. Hanafy, Prashant Shenoy, Mohammad Hajiesmaili
University of Massachusetts Amherst
Abstract
Carbon-aware schedulers aim to reduce the operational carbon foot-
print of data centers by running flexible workloads during periods
of low carbon intensity. Most schedulers treat workloads as single
monolithic tasks, ignoring that many jobs, like video encoding or
offline inference, consist of smaller tasks with specific dependencies
and resource needs; however, knowledge of this structure enables
opportunities for greater carbon efficiency.
We quantify the maximum benefit of a dependency-aware ap-
proach for batch workloads. We model the problem as a flexible
job-shop scheduling variant and use an offline solver to compute up-
per bounds on carbon and energy savings. Results show up to 25%
lower carbon emissions on average without increasing the optimal
makespan (total job completion time) compared to a makespan-only
baseline. Although in heterogeneous server setup, these schedules
may use more energy than energy-optimal ones. Our results also
show that allowing twice the optimal makespan nearly doubles the
carbon savings, underscoring the tension between carbon, energy,
and makespan. We also highlight key factors such as job structure
and server count influence the achievable carbon reductions.
1
Introduction
In recent years, rising compute demand and data center buildings
have raised concerns about electricity use and the resulting opera-
tional carbon emissions of ICT [16, 40]. To address these concerns,
data center operators improved energy efficiency for more than a
decade [3], but these improvements are approaching fundamental
limits with the advent of hyperscale data centers [4]. Thus, future
growth in demand [40] is expected to increase energy usage signif-
icantly [40] â€“ since the favored choices for new power generation
to support data centers continue to be fossil-based resources such
as natural gas turbines [12, 41], this projected growth is expected
to correlate with increases in carbon emissions.
To address concerns about carbon footprint, operators turned
to supply-side optimizations, e.g., by purchasing renewable power
or co-locating data centers with wind and solar farms [1, 6, 17].
Researchers have also proposed demand-side techniques to reduce
the operational carbon of batch workloads using temporal or spa-
tial shifting [19, 21, 28, 44, 48, 51]. These works rely on visibility
into the gridâ€™s carbon intensity (i.e., amount of carbon emitted per
unit of energy used) and flexible applications that can modulate
their resource consumption in response to variations on the grid.
Existing work ranges from theoretical models of single jobs with
deadlines [25] to experimental systems that provision resources for
many jobs in response to carbon intensity [1, 8, 19, 34, 37, 42].
Existing carbon-aware schedulers typically treat each workload
as a single, monolithic task. This model overlooks a key reality:
Many workloads decompose into dependent tasks with different
resource and energy profiles, often across heterogeneous machines
(e.g., CPU vs. GPU, hardware generations). Examples of these work-
loads include video encoding/transcoding, serverless pipelines [24],
storage management [38], and offline inference [46, 47]. Recogniz-
ing this structure opens new opportunities for carbon efficiency.
As Fig. 1 shows, a dependency-unaware scheduler may force tasks
into high-carbon periods, negating potential savings.
Figure 1: A dependency-unaware scheduler attempts to run
all tasks in the low-carbon period, but precedence constraints
force most tasks to run during a high-carbon period. A
dependency-aware scheduler makes a better trade-off for
a lower total carbon footprint.
While recent workflow systems (e.g., Caribou [19]) begin to
model multi-stage dependencies as directed acyclic graphs (DAGs)
in the serverless setting, data center operators need to know the
maximum possible (carbon or energy) savings before investing in
complex online solutions. This paper quantifies the upper bound of
the (carbon or energy) savings by assuming perfect offline knowl-
edge of job arrivals, task dependencies (in the format of DAGs), and
their energy profiles. Our analysis focuses on batch workloads as
they represent a significant portion of data center computation.
To quantify the potential savings of temporal shifting for DAG
jobs, we model the problem as a variant of the flexible job-shop
scheduling problem (FJSP) [10]. Our formulation optimizes for en-
ergy or operational carbon under a makespan constraint and com-
pares against the standard makespan objective. We generate batch-
job DAGs inspired by Alibaba traces [2], pair them with Electricity
Maps carbon traces [30], and use a constraint solver [36] to explore
feasible schedules. Our contributions are as follows:
(1) Formulate a carbon-aware flexible job shop problem with inter-
task dependencies and compute upper bounds on achievable
carbon savings. Our results highlight that carbon-aware sched-
uling can lower carbon emissions by up to 25% on average
without increasing the optimal makespan.
(2) Quantify trade-offs between carbon, energy, and makespan.
In heterogeneous server setup, we show that carbon-optimal
schedules can use up to 7% more energy than energy-optimal
arXiv:2512.07799v1  [cs.DC]  8 Dec 2025


--- Page 2 ---
Bostandoost et al.
schedules with the same makespan, and that doubling the
makespan flexibility nearly doubles carbon savings.
(3) Identify parameters that drive carbon savings: more tasks per
job raises server utilization and cuts savings by up to 35%, while
adding servers lowers server utilization and yields up to 30Ã—
higher savings.
2
Problem Statement
In this section, we present the flexible job shop scheduling problem
(FJSP) in the computing context of tasks with dependencies before
formalizing the main objectives (makespan, energy, carbon) that we
consider in the rest of the paper. We give brief background on the
history of FJSP, including some recent works that have expanded
the study of the problem beyond the singular objective of makespan.
We consider a set of jobs denoted by J, where each job ğ‘—âˆˆJ
consists of ğ‘›ğ‘—tasks {ğ‘¡ğ‘—,1, Â· Â· Â· ,ğ‘¡ğ‘—,ğ‘›ğ‘—}. Each job also has a dependency
graph ğº(as a DAG) with directed edges E between tasks; for in-
stance, if the edge (ğ‘˜âˆ’1,ğ‘˜) exists in ğ¸, this indicates that task ğ‘¡ğ‘—,ğ‘˜
cannot run until task ğ‘¡ğ‘—,ğ‘˜âˆ’1 completes.
Given a finite set of machines M that may be heterogeneous,
any task ğ‘¡can run on a subset of machines denoted by Mğ‘¡, with
instantaneous energy usage ğ¸ğ‘¡,ğ‘šand processing time ğ‘ğ‘¡,ğ‘šon ma-
chine ğ‘šâˆˆMğ‘¡. Finally, to model realistic scenarios in a data center,
each job has an arrival time ğ‘ğ‘—â‰¥0. If ğ‘ğ‘—> 0, job ğ‘—â€™s first task
cannot begin until after time ğ‘ğ‘—.
A feasible schedule S assigns each task to a machine and a start
time, respecting task dependencies and machine capacity. For the
ğ‘–th task in job ğ‘—, we denote the start time as ğ‘ ğ‘—,ğ‘–and ğ‘¥ğ‘—,ğ‘–,ğ‘šâˆˆ{0, 1}
equals 1 if task ğ‘¡ğ‘—,ğ‘–runs on machine ğ‘š. (see Appendix A for the
problem formulation). We evaluate three objectives:
Definition 2.1 (Makespan). The makespan of a schedule is
defined as maxğ‘—âˆˆJ ğ¶ğ‘—,ğ‘›ğ‘—, where ğ¶ğ‘—,ğ‘›ğ‘—is the completion time of job ğ‘—
(i.e., the time at which task ğ‘¡ğ‘—,ğ‘›ğ‘—completes).
Definition 2.2 (Energy Usage). The energy usage of a schedule
is given by Ã
ğ‘—âˆˆğ½
Ãğ‘›ğ‘—
ğ‘–=1
âˆ«ğ¶ğ‘—,ğ‘–
ğ‘ ğ‘—,ğ‘–
ğ¸ğ‘¡ğ‘—,ğ‘–,ğ‘šğ‘‘ğœ, where ğ¸ğ‘¡ğ‘—,ğ‘–,ğ‘šis the energy usage
of task ğ‘–belonging to job ğ‘—on the assigned machine ğ‘š.
Definition 2.3 (Carbon Emissions). The carbon emissions of
a schedule are quantified by Ã
ğ‘—âˆˆğ½
Ãğ‘›ğ‘—
ğ‘–=1
âˆ«ğ¶ğ‘—,ğ‘–
ğ‘ ğ‘—,ğ‘–
ğ¸ğ‘¡ğ‘—,ğ‘–,ğ‘šÂ· ğ¼(ğœ) ğ‘‘ğœ, where
ğ¼(ğœ) denotes the grid carbon intensity at time ğœ.
In most of the literature on JSP and related problems, the studied
metric is makespan: the total time required to complete all jobs
according to a given schedule. This is distinct from e.g., average
job completion time, which measures the time elapsed between a
single jobâ€™s submission and its completion. Since we are motivated
by background batch jobs such as video encoding that are often
centrally orchestrated by or â€œbelong toâ€ the data center operator,
we focus on makespan in the interest of ensuring that all daily back-
ground jobs are completed as a group in a timely fashion. However,
we note that other performance metrics such as job completion
time may be more appropriate in e.g., multi-tenant scenarios.
To solve the FJSP under these objectives, we first solve the prob-
lem for the classic case of unconstrained makespan minimization
(i.e., not minimizing energy or carbon), obtaining the optimal fea-
sible makespan OPT. From this â€œbaselineâ€, we then solve the prob-
lem with either energy or carbon as an objective, formalizing the
makespan as a constraint (e.g., for a given stretch factor ğ‘†â‰¥1, the
second-level scheduleâ€™s makespan must be at most ğ‘†Ã— OPT). We
give a toy example of this bi-level optimization in Fig. 2, and detail
our solver implementation in Section 3.1.
Figure 2: An example FJSP with homogeneous machines. The
top schedule is optimized for the fastest completion time
(makespan). The middle schedule is optimized for the lowest
carbon footprint, shifting tasks to align with low-carbon
periods (bottom) while respecting the makespan constraint.
Example Job: Offline Inference. To motivate how FJSP models
important problems in the data center, we consider the example of
offline inference, where a collection of new data is processed by a
trained ML model to generate new predictions at regular intervals
(e.g., daily) [31]. In an offline inference job, there are three distinct
tasks to be executed in sequence, each using different resources
and with different energy footprints. First, one task loads the new
inference data to be processed in batch using e.g., Hadoop or a
Spark DataFrame [50]. Then, the next task initializes and uses the
ML model to predict outputs for each row in the batched data.
Finally, the last task stores and visualizes the resulting predictions
for future use. Note that each of these tasks involves different
operations, and may even require different hardware (e.g., if the ML
model is designed for deployment on a GPU cluster). Depending on
the complexity of the model and the size of the data, these jobs can
require hours or even days of runtime [43]. For instance, using the
smallest No Language Left Behind model for a language translation
task can require at least 480 hours of runtime, with 440 of those
counting as GPU-hours [23, Table 15]. Other examples, include
batch processing and HPC applications such as DNA sequencing [7],
drug discovery [26], web crawling [22].
Background. The job shop scheduling problem (JSP) models each
job as a sequence of tasks on specific machines, aiming to minimize
the overall makespan. JSP is known to be NP-hard [15], so studies
have focused on simple heuristics (e.g., list scheduling [18]) and
approximation algorithms (see Xiong et al. [49] for a survey). The
flexible job shop problem (FJSP) [5] generalizes JSP by allowing
each operation to be assigned to any machine in a candidate set.
Although the JSP and FJSP have roots in operations research and
manufacturing contexts, they are also relevant to computer systems


--- Page 3 ---
Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective
scheduling, where jobs arrive online and must be dispatched in real-
time (e.g., using heuristics like earliest-task-first [9]).
In recent years, some studies have considered competing pri-
orities, such as time-varying costs of production and/or energy
usage [32, 45]. For instance, Park and Ham [33] considers the FJSP
under time-of-use electricity pricing, aiming to minimize both the
makespan and total energy cost. They solve the problem optimally
and show it is possible to significantly cut energy costs without
compromising productivity. However, most of these works ignore
carbon emissions and are grounded in manufacturing or industrial
contexts. Thus, in the rest of the paper, we explore the solution
space of schedules for the FJSP in a setting tailored for computing
workloads, using tools developed in operations research.
3
Evaluation
In this section, we describe our experimental setting and solution
approach before presenting our main results. Our code and data is
available at GitHub.
3.1
Experimental Setup
Using the formulation from Section 2, we construct flexible job
shop scheduling instances and solve them as follows.
Server setup. We schedule each FJSP instance on ğ‘€servers, with
ğ‘€= 5 unless noted. We consider two server setups that model
homogeneity and heterogeneity, respectively. In the homogeneous
setup, all the servers draw 1kW and run at the same speed. In the
heterogeneous setup, we use 5 server classes with power draws
of 0.25, 0.5, 1, 1.5, and 2kW. The 1kW server is the baseline with
the same speed as in the homogeneous case. The servers run at
{1/3, 1/2, 1, 4/3, 2}Ã— this baseline speed.
Carbon intensity data. We use hourly carbon intensity data
from Electricity Maps [30], mainly from the South Australia re-
gion (AU-SA) in 2024. This region shows high daily variation and
strong penetration of renewables, making it a good case for carbon-
aware scheduling [44]. Results for other regions appear in Fig. 6.
Each instance starts at a random point in the trace (ğ‘¡= 0). Each of
the ğ‘›jobs has a uniform random arrival time in the next 24 hours.
We discretize time into 15-minute epochs, rounding each taskâ€™s
processing time up to reduce solver complexity.
Jobs and task dependency structures. Each instance has ğ‘›jobs,
each with ğ‘˜tasks. Unless noted, we set ğ‘›= 10 and ğ‘˜= 4. Jobs follow
simple dependency graphs: a chain, two branches from a root, or
one root feeding all other tasks (Fig. 3). In the homogeneous setup,
Figure 3: For the case of ğ‘˜= 4 tasks, the dependency struc-
tures that we sample from in our experiments.
all servers run tasks at the same speed with the same energy use.
We sample task durations from an exponential distribution with
mean ğœ†= 7 epochs, which matches the average length of tasks that
are longer than one minute in the Alibaba trace [2]. This models
the long batch jobs we seek to address.
In the heterogeneous setup, task times scale with server class.
For example, a task that takes 10 epochs on the 1kW baseline server
takes {30, 20, 10, 7.5, 5} epochs on the five server types, in line with
their relative speeds.
Solver implementation.
We model FJSP with constraint pro-
gramming in Google OR-Tools [35, 36]. For each instance, we first
solve for the optimal makespan using the standard FJSP model, ig-
noring carbon or energy optimization. Fig. 4 shows the distribution
of this optimal makespan in the homogeneous and heterogeneous
setups. We then re-solve each instance with objectives of minimiz-
ing energy or operational carbon consumption. In these runs, we
add a makespan constraint with a stretch factor ğ‘†â‰¥1. If the optimal
makespan is 10 epochs, ğ‘†= 2 allows any schedule that finishes
within 20 epochs. Larger ğ‘†values give the solver more room to
shift tasks into low-carbon periods, but also make the search space
and problem complexity grow. To keep runtimes manageable, we
set solver timeouts of 1, 3, and 5 minutes for ğ‘†= 1, 1.5, and 2,
respectively.
50
100
150
200
Makespan(epoch)
Homogeneous
Heterogeneous
Figure 4: Optimal makespan (in
epochs, 1 epoch = 15 min.) for
homogeneous and heterogeneous
server settings in our experi-
ments. ğ‘›= 10 jobs, ğ‘€= 5 servers,
and each job has ğ‘˜= 4 operations.
3.2
Experimental Results
Carbon emissions vs. makespan trade-off.
We study how
much carbon-aware scheduling can reduce emissions compared to
a baseline that only minimizes makespan (carbon-agnostic). Carbon
savings are reported relative to this baseline schedule. There is a
trade-off between carbon savings and job completion time: more
slack gives the solver more freedom to move tasks into low-carbon
periods [44]. We evaluate 1000 instances from the AU-SA grid region.
Fig. 5 shows the results for homogeneous and heterogeneous setups.
With no slack (ğ‘†= 1), the carbon-aware scheduler cuts emis-
sions by an average of 25% in the homogeneous case and 18% in
the heterogeneous case. Allowing more slack boosts these gains:
At ğ‘†= 2, average savings rise to 54% and 52% respectively. In the
heterogeneous setup (Fig. 5b), the solver occasionally finds worse
solutions than the carbon-agnostic one, resulting in negative sav-
ings. This happens because a larger ğ‘†increases the complexity of
the problem, and the added complexity makes it harder for the
solver to find an optimal solution within the time limit.
Fig. 5 also shows that the heterogeneous setup achieves lower car-
bon savings than the homogeneous one. The scheduler has less flex-
ibility to shift tasks due to two factors: a shorter overall makespan
(Fig. 4) and higher average server utilization. For example, when
ğ‘†= 1, the average utilization is 55.02% in the heterogeneous case
versus 47.15% in the homogeneous case.
Effect of carbon trace. Carbon savings depend heavily on the grid
region [44]. Fig. 6 shows achieved carbon savings for ğ‘†= 1 using
2024 data from California (CAL), Ontario (CA-ON), and Texas (TEX),
in addition to South Australia (AU-SA). We find large differences.
Texas shows smaller savings because its carbon intensity varies
less during the day and is higher on average than in California
or South Australia. Ontario shows the opposite: while variation
is high, its average carbon intensity is already very low (about
90% low-carbon), so the room for improvement is limited. These


--- Page 4 ---
Bostandoost et al.
1.0
1.5
2.0
Stretch Factor S
25
0
25
50
75
100
Carbon Savings%
(a) Homogeneous
1.0
1.5
2.0
Stretch Factor S
25
0
25
50
75
100
Carbon Savings%
(b) Heterogeneous
Figure 5: Carbon savings in the AU-SA
grid at different stretch factors ğ‘†. (a)
Homogeneous servers. (b) Heteroge-
neous servers.
TEX
CAL
AU-SA
CA-ON
Location
25
0
25
50
75
100
Carbon Savings%
(a) Homogeneous
TEX
CAL
AU-SA
CA-ON
Location
25
0
25
50
75
100
Carbon Savings%
(b) Heterogeneous
Figure 6: Carbon savings achieved for ğ‘†= 1 in
4 different grid regions. (a) shows results for
homogeneous case, while (b) shows results for
the heterogeneous case.
1.0
1.5
2.0
Stretch Factor S
0
10
20
30
40
50
60
Avg. Carbon Savings%
Carbon-Aware
Energy-Aware
(a) Carbon savings
1.0
1.5
2.0
Stretch Factor S
0
5
10
15
20
Avg. Energy Savings%
Carbon-Aware
Energy-Aware
(b) Energy savings
Figure 7: Comparing the carbon sav-
ings and energy savings achieved in
AU-SA grid by solvers that optimize
for carbon and energy, respectively.
Table 1: Average carbon savings, makespan, and utilization
for varying instance parameters in AU-SA grid, with ğ‘†= 1 and
homogeneous servers.
(a) No. of servers ğ‘€
ğ‘€
2
5
10
Carbon
Savings
1.13%
24.64%
33.98%
Makespan
(epochs)
153.74
117.73
117.63
Utilization
89.39%
47.15%
23.6%
(b) No. of tasks per job ğ‘˜
ğ‘˜
3
4
5
Carbon
Savings
30.43%
24.64%
19.69%
Makespan
(epochs)
110.42
117.73
121.77
Utilization
36.39%
47.15%
56.61%
contrasts highlight how both the variability and the baseline level
of carbon intensity shape the benefits of carbon-aware scheduling.
Effect of number of servers. We evaluate the effect of server
count ğ‘€= {2, 5, 10} in the homogeneous setting with ğ‘›= 10 jobs
and ğ‘˜= 4 operations per job. Table 1a reports results for ğ‘†= 1 in
the AU-SA grid, averaged over 1000 instances.
More servers give the scheduler more room to place tasks in
low-carbon periods. Moving from 2 to 10 servers increases savings
by up to 30Ã—. A larger server pool also lowers server average uti-
lization, since the same workload spreads across more machines.
This increases parallelism and shortens the makespan. But with
only 10 jobs, the gain from 5 to 10 servers is negligible, and the
makespan hardly improves.
Effect of number of tasks. We vary the number of tasks per job
with ğ‘˜= {3, 4, 5} while keeping other parameters fixed. Table 1b
reports results for ğ‘†= 1 in the AU-SA grid, averaged over 1000
instances. More tasks reduce the achievable carbon savings. With
three tasks per job, average savings are 30.4%, but with five tasks,
the savings drop to 19.7%. The reason is higher server utilization: as
ğ‘˜increases from 3 to 5, average server utilization rises from 36.4%
to 56.6%. Higher server utilization leaves the solver less flexibility
to shift workloads into low-carbon periods.
Optimizing for energy usage vs. carbon emissions. Operators
face a fundamental choice between energy efficiency and carbon ef-
ficiency as the primary objective when scheduling batch workloads.
Energy efficiency pushes workloads onto the most power-efficient
servers, maximizing throughput per watt. Carbon efficiency shifts
workloads into time windows when the grid is cleanest. These goals
can conflict: the most efficient server may be active during a high-
carbon period, while a less efficient server running later could yield
lower emissions overall. To quantify this trade-off, we configure
our makespan-constrained solver to prioritize energy reduction
(Def. 2.2). Also, to implement tie-breaking rules, we assign a small
weight to carbon emissions, so when two schedules use the same
energy, the solver prefers the one with lower carbon emissions.
We evaluate heterogeneous servers and average results over
1000 instances in the AU-SA grid region. We first use a solver as the
baseline to compute the optimal makespan, then solve a constrained
FJSP with carbon or energy as the primary objective. Savings are
reported relative to the baselineâ€™s energy or carbon consumption.
Fig. 7 shows the results. The energy-focused solver reduces carbon
emission by âˆ¼30% when ğ‘†= 2, but the carbon-focused solver
achieves âˆ¼50% savings. For energy efficiency, the pattern reverses:
the energy-focused solver achieves about 10% savings at ğ‘†= 2,
while the carbon-focused solver achieves only about 3%.
These results highlight a tension between energy usage and
carbon emissions. These two objectives align when saving energy
occurs during a low-carbon period. However, they conflict if an
energy-saving strategy extends a task into a high-carbon period,
which can increase overall carbon emissions.
4
Discussion
We quantified upper bounds on the achievable carbon reduction for
workloads with inter-task dependencies in offline settings, illustrat-
ing a clear trade-off between carbon, energy, and makespan. These
findings may inform the design of online heuristics to solve the
problem in practice, and can give intuition about the preferrable
â€œtrade-off pointâ€ for relevant applications.
Carbon savings without makespan loss. Compared to a carbon-
agnostic, makespan-minimizing schedule, a carbon-aware schedule
can reorder tasks to exploit low-carbon periods. This works be-
cause servers in the makespan-optimal schedule may sit idle in
low-carbon periods: using those idle gaps, a scheduler can increase
server utilization in low-carbon periods, cutting emissions without
extending the makespan (Fig. 2).
Carbon vs. makespan. Scheduling purely for carbon can increase
makespan, so both metrics must be considered together. Our results
show diminishing returns: allowing makespan of 1.5Ã— the optimal
achieves most of the carbon savings, while further relaxation yields
little additional savings (Fig. 5).
Carbon vs. energy. Prior work [20] showed the tension between
carbon, energy, and performance in homogeneous clusters. We find
that heterogeneity intensifies this conflict. Running a workload on
a slower, more energy-efficient server during a high-carbon period
might save energy but can ultimately increase carbon emissions,
while using a faster, less efficient server on low-carbon periods may
lower emissions but waste energy. This matters for heterogeneous
environments, such as GPU clusters [13, 29], if carbon efficiency
is also a goal alongside energy efficiency. Operators can tune how
much they value carbon vs. energy by assigning weights to each.
Limitations. A key challenge is the complexity of the FJSP. Solvers
work for small instances, but complexity grows exponentially with


--- Page 5 ---
Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective
scale. Data centerâ€“sized problem settings will need approxima-
tion methods or online heuristics with guarantees. Machine learn-
ingâ€“based techniques for sustainable systems [11, 39], which have
accelerated large-scale optimization and stochastic scheduling, may
offer a path by learning heuristics that approximate optimal solu-
tions at scale. We note that the model we consider in this paper
simplifies some real-world conditions. We do not account for sev-
eral factors that influence carbon reduction, including: Server idle
power draw and energy proportionality [3, 27], limits on task par-
allelization (i.e., whether a task is truly parallelizable [14]), and
embodied carbon from manufacturing and deployment of servers.


--- Page 6 ---
Bostandoost et al.
References
[1] Bilge Acun, Benjamin Lee, Fiodar Kazhamiaka, Kiwan Maeng, Udit Gupta, Manoj
Chakkaravarthy, David Brooks, and Carole-Jean Wu. 2023. Carbon Explorer: A
Holistic Framework for Designing Carbon Aware Datacenters. In Proceedings of
the 28th ACM International Conference on Architectural Support for Programming
Languages and Operating Systems, Volume 2 (Vancouver, BC, Canada) (ASPLOS
2023). Association for Computing Machinery, New York, NY, USA, 118â€“132.
https://doi.org/10.1145/3575693.3575754
[2] Alibaba. 2018. Cluster data collected from production clusters in Alibaba for
cluster management research.
https://github.com/alibaba/clusterdata/tree/
master/cluster-trace-v2018
[3] Luiz AndrÃ© Barroso and Urs HÃ¶lzle. 2009. The Datacenter as a Computer: An
Introduction to the Design of Warehouse-Scale Machines. Morgan & Claypool
Publishers. http://dx.doi.org/10.2200/S00193ED1V01Y200905CAC006
[4] Noman Bashir, David Irwin, Prashant Shenoy, and Abel Souza. 2023. Sustainable
Computing - Without the Hot Air. SIGENERGY Energy Inform. Rev. 3, 3 (Oct.
2023), 47â€“52. https://doi.org/10.1145/3630614.3630623
[5] P. Brucker and R. Schlie. 1990. Job-shop scheduling with multi-purpose machines.
Computing 45, 4 (Dec. 1990), 369â€“375. https://doi.org/10.1007/bf02238804
[6] Justine Calma. 2024. Google inks major new offshore wind deal. (1 February
2024).
https://www.theverge.com/2024/2/1/24056819/google-offshore-wind-
farms-data-centers-europe Accessed: 2025-05-09.
[7] Jason L. Causey, Cody Ashby, Karl Walker, Zhiping Paul Wang, Mary Yang,
Yuanfang Guan, Jason H. Moore, and Xiuzhen Huang. 2018. DNAp: A Pipeline
for DNA-seq Data Analysis. Scientific Reports 8, 1 (2018), 6793.
[8] Mohak Chadha, Thandayuthapani Subramanian, Eishi Arima, Michael Gerndt,
Martin Schulz, and Osama Abboud. 2023. GreenCourier: Carbon-Aware Sched-
uling for Serverless Functions. In Proceedings of the 9th International Workshop
on Serverless Computing (Bologna, Italy) (WoSC â€™23). Association for Computing
Machinery, New York, NY, USA, 18â€“23. https://doi.org/10.1145/3631295.3631396
[9] Jared Coleman and Bhaskar Krishnamachari. 2025.
PISA: An Ad-
versarial Approach To Comparing Task Graph Scheduling Algorithms.
arXiv:2403.07120 [cs.DC] https://arxiv.org/abs/2403.07120
[10] StÃ©phane DauzÃ¨re-PÃ©rÃ¨s, Junwen Ding, Liji Shen, and Karim Tamssaouet. 2024.
The flexible job shop scheduling problem: A review. European Journal of Opera-
tional Research 314, 2 (2024), 409â€“432.
[11] Priya L Donti and J Zico Kolter. 2021. Machine learning for sustainable energy
systems. Annual Review of Environment and Resources 46, 1 (2021), 719â€“747.
[12] Rebecca F. Elliott. 2025. Why a Plane-Size Machine Could Foil a Race to Build Gas
Power Plants. (8 April 2025). https://www.nytimes.com/2025/04/08/business/
energy-environment/gas-turbines-power-plants.html Accessed: 2025-05-09.
[13] Jeremy Enos, Craig Steffen, Joshi Fullop, Michael Showerman, Guochun Shi,
Kenneth Esler, Volodymyr Kindratenko, John E Stone, and James C Phillips. 2010.
Quantifying the impact of GPUs on performance and energy efficiency in HPC
clusters. In International Conference on Green Computing. IEEE, 317â€“324.
[14] Jared Fernandez, Luca Wehrstedt, Leonid Shamis, Mostafa Elhoushi, Kalyan
Saladi, Yonatan Bisk, Emma Strubell, and Jacob Kahn. 2025. Hardware Scal-
ing Trends and Diminishing Returns in Large-Scale Distributed Training.
arXiv:2411.13055 [cs.LG] https://arxiv.org/abs/2411.13055
[15] M. R. Garey, D. S. Johnson, and Ravi Sethi. 1976. The Complexity of Flowshop
and Jobshop Scheduling. Mathematics of Operations Research 1, 2 (May 1976),
117â€“129. https://doi.org/10.1287/moor.1.2.117
[16] Goldman Sachs Research. 2025. AI to drive 165% increase in data center power
demand by 2030. https://www.goldmansachs.com/insights/articles/ai-to-drive-
165-increase-in-data-center-power-demand-by-2030 Accessed: 2025-05-06.
[17] Google. 2018.
Meeting our match: Buying 100 percent renewable en-
ergy. https://blog.google/outreach-initiatives/environment/meeting-our-match-
buying-100-percent-renewable-energy/ Accessed: 2025-05-06.
[18] R. L. Graham. 1966. Bounds for certain multiprocessing anomalies. The Bell
System Technical Journal 45, 9 (1966), 1563â€“1581. https://doi.org/10.1002/j.1538-
7305.1966.tb01709.x
[19] Viktor Urban Gsteiger, Pin Hong (Daniel) Long, Yiran (Jerry) Sun, Parshan
Javanrood, and Mohammad Shahrad. 2024. Caribou: Fine-Grained Geospatial
Shifting of Serverless Applications for Sustainability. In Proceedings of the ACM
SIGOPS 30th Symposium on Operating Systems Principles (Austin, TX, USA) (SOSP
â€™24). Association for Computing Machinery, New York, NY, USA, 403â€“420. https:
//doi.org/10.1145/3694715.3695954
[20] Walid A. Hanafy, Roozbeh Bostandoost, Noman Bashir, David Irwin, Moham-
mad Hajiesmaili, and Prashant Shenoy. 2023.
The War of the Efficiencies:
Understanding the Tension between Carbon and Energy Optimization. In
Proceedings of the 2nd Workshop on Sustainable Computer Systems. 7 pages.
https://doi.org/10.1145/3604930.3605709
[21] Walid A. Hanafy, Qianlin Liang, Noman Bashir, David Irwin, and Prashant
Shenoy. 2023. CarbonScaler: Leveraging Cloud Workload Elasticity for Op-
timizing Carbon-Efficiency.
Proceedings of the ACM on Measurement and
Analysis of Computing Systems 7, 3, Article 57 (December 2023), 28 pages.
https://doi.org/10.1145/3626788
[22] Andreas Harth, JÃ¼rgen Umbrich, and Stefan Decker. 2006. MultiCrawler: A
Pipelined Architecture for Crawling and Indexing Semantic Web Data. In The
Semantic Web - ISWC 2006. Springer Berlin Heidelberg, Berlin, Heidelberg, 258â€“
271.
[23] Yeskendir Koishekenov, Alexandre Berard, and Vassilina Nikoulina. 2023.
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively
Multilingual Machine Translation Model. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-
pers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Asso-
ciation for Computational Linguistics, Toronto, Canada, 3567â€“3585.
https:
//doi.org/10.18653/v1/2023.acl-long.198
[24] Samuel Kounev, Nikolas Herbst, Cristina L. Abad, Alexandru Iosup, Ian Foster,
Prashant Shenoy, Omer Rana, and Andrew A. Chien. 2023. Serverless Computing:
What It Is, and What It Is Not? Commun. ACM 66, 9 (Aug. 2023), 80â€“92. https:
//doi.org/10.1145/3587249
[25] Adam Lechowicz, Nicolas Christianson, Jinhang Zuo, Noman Bashir, Mohammad
Hajiesmaili, Adam Wierman, and Prashant Shenoy. 2023. The Online Pause and
Resume Problem: Optimal Algorithms and An Application to Carbon-Aware
Load Shifting. Proceedings of the ACM on Measurement and Analysis of Computing
Systems 7, 3, Article 53 (Dec 2023), 36 pages. https://doi.org/10.1145/3626776
arXiv:2303.17551 [cs.DS]
[26] Sumudu P. Leelananda and Steffen Lindert. 2016. Computational methods in
drug discovery. Beilstein Journal of Organic Chemistry 12 (2016), 2694â€“2718.
https://doi.org/10.3762/bjoc.12.267
[27] Charles Lefurgy, Xiaorui Wang, and Malcolm Ware. 2007. Server-Level Power
Control. In Fourth International Conference on Autonomic Computing (ICACâ€™07).
4â€“4. https://doi.org/10.1109/ICAC.2007.35
[28] Liuzixuan Lin and Andrew A Chien. 2023. Adapting datacenter capacity for
greener datacenters and grid. In Proceedings of the 14th ACM International Con-
ference on Future Energy Systems. 200â€“213.
[29] Kai Ma, Xue Li, Wei Chen, Chi Zhang, and Xiaorui Wang. 2012. Greengpu: A
holistic approach to energy efficiency in gpu-cpu heterogeneous architectures.
In 2012 41st international conference on parallel processing. IEEE, 48â€“57.
[30] Electricity Maps. 2023. Electricity Map. https://www.electricitymap.org/map.
[31] MLRun Documentation. 2025. Batch inference - Using MLRun.
https://docs.
mlrun.org/en/stable/deployment/batch_inference.html Accessed: 2025-05-06.
[32] Jesus Para, Javier Del Ser, and Antonio J. Nebro. 2022. Energy-Aware Multi-
Objective Job Shop Scheduling Optimization with Metaheuristics in Manufactur-
ing Industries: A Critical Survey, Results, and Perspectives. Applied Sciences 12,
3 (Jan. 2022), 1491. https://doi.org/10.3390/app12031491
[33] Myoung-Ju Park and Andy Ham. 2022. Energy-aware flexible job shop scheduling
under time-of-use pricing. International Journal of Production Economics 248
(June 2022), 108507. https://doi.org/10.1016/j.ijpe.2022.108507
[34] Lucas Perotin, Chaojie Zhang, Rajini Wijayawardana, Anne Benoit, Yves Robert,
and Andrew Chien. 2023. Risk-aware scheduling algorithms for variable capacity
resources. In Proceedings of the SCâ€™23 Workshops of The International Conference
on High Performance Computing, Network, Storage, and Analysis. 1306â€“1315.
[35] Laurent Perron and FrÃ©dÃ©ric Didier. 2024. CP-SAT. https://developers.google.
com/optimization/cp/cp_solver/
[36] Laurent Perron and Vincent Furnon. 2024. OR-Tools. https://developers.google.
com/optimization/
[37] Ana Radovanovic, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre
Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, et al.
2022. Carbon-Aware Computing for Datacenters. IEEE Transactions on Power
Systems (2022).
[38] Varsha Rao and Andrew A. Chien. 2025. Understanding the Operational Carbon
Footprint of Storage Reliability and Management. SIGENERGY Energy Inform.
Rev. 4, 5 (April 2025), 180â€“187. https://doi.org/10.1145/3727200.3727227
[39] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste,
Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
Anna Waldman-Brown, et al. 2022. Tackling climate change with machine
learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1â€“96.
[40] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei,
Md Abu Bakar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, and
Dale Sartor. 2024. 2024 United States Data Center Energy Usage Report. Technical
Report LBNL-2001637. Lawrence Berkeley National Lab (LBL) - Energy Analysis
& Environmental Impacts Division. https://eta-publications.lbl.gov/sites/default/
files/2024-12/lbnl-2024-united-states-data-center-energy-usage-report.pdf
[41] Zachary Skidmore. 2025. Natural gas plant planned for Stargate AI data cen-
ter campus. https://www.datacenterdynamics.com/en/news/natural-gas-plant-
planned-for-stargate-ai-data-center-campus-report/ Accessed: 2025-05-05.
[42] Abel Souza, Noman Bashir, Jorge Murillo, Walid Hanafy, Qianlin Liang, David
Irwin, and Prashant Shenoy. 2023. Ecovisor: A Virtual Energy System for Carbon-
Efficient Applications. In ACM International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS). 252â€“265.
[43] Varadarajan Srinivasan and Jim Parker. 2021.
How Acxiom reduced
their model inference time from days to hours with Spark on Amazon
EMR.
https://aws.amazon.com/blogs/industries/how-acxiom-reduced-their-


--- Page 7 ---
Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective
model-inference-time-from-days-to-hours-with-spark-on-amazon-emr/ Ac-
cessed: 2025-05-08.
[44] Thanathorn Sukprasert, Abel Souza, Noman Bashir, David Irwin, and Prashant
Shenoy. 2024. On the Limitations of Carbon-Aware Temporal and Spatial Work-
load Shifting in the Cloud. In Nineteenth European Conference on Computer
Systems (EuroSys). Athens, Greece.
[45] Hajo Terbrack and Thorsten Claus. 2025. The generalized energy-aware flexible
job shop scheduling model: A constraint programming approach. Computers &
Industrial Engineering 204 (June 2025), 111065. https://doi.org/10.1016/j.cie.2025.
111065
[46] Huangshi Tian, Yunchuan Zheng, and Wei Wang. 2019. Characterizing and
Synthesizing Task Dependencies of Data-Parallel Jobs in Alibaba Cloud. In
Proceedings of the ACM Symposium on Cloud Computing (Santa Cruz, CA, USA)
(SoCC â€™19). Association for Computing Machinery, New York, NY, USA, 139â€“151.
https://doi.org/10.1145/3357223.3362710
[47] Jia Wei, Mo Chen, Longxiang Wang, Pei Ren, Yujia Lei, Yuqi Qu, Qiyu Jiang,
Xiaoshe Dong, Weiguo Wu, Qiang Wang, et al. 2022. Status, challenges and
trends of data-intensive supercomputing. CCF Transactions on High Performance
Computing 4, 2 (2022), 211â€“230.
[48] Philipp Wiesner, Ilja Behnke, Dominik Scheinert, Kordian Gontarska, and Lauritz
Thamsen. 2021. Letâ€™s Wait Awhile: How Temporal Workload Shifting Can
Reduce Carbon Emissions in the Cloud. In Proceedings of the 22nd International
Middleware Conference (Middleware).
[49] Hegen Xiong, Shuangyuan Shi, Danni Ren, and Jinjin Hu. 2022. A survey of
job shop scheduling problem: The types and models. Computers & Operations
Research 142 (June 2022), 105731. https://doi.org/10.1016/j.cor.2022.105731
[50] Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J.
Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016.
Apache Spark: a unified engine for big data processing. Commun. ACM 59,
11 (Oct. 2016), 56â€“65. https://doi.org/10.1145/2934664
[51] Jiajia Zheng, Andrew A. Chien, and Sangwon Suh. 2020. Mitigating Curtailment
and Carbon Emissions through Load Migration between Data Centers. Joule 4,
10 (Oct. 2020), 2208â€“2222. https://doi.org/10.1016/j.joule.2020.08.001


--- Page 8 ---
Bostandoost et al.
APPENDIX
A
Problem Formulation
We model our problem as a variant of the Flexible Job Shop Sched-
uling Problem (FJSP) with job arrivals, DAG precedence, heteroge-
neous servers, and time-varying carbon intensity. Our formulation
follows the constraint programming style used in prior work [33].
The inputs to our problem are as follows:
â€¢ J: set of jobs; ğ‘ğ‘—: arrival time of job ğ‘—.
â€¢ ğ‘‰ğ‘—: set of tasks in job ğ‘—({ğ‘¡ğ‘—1,ğ‘¡ğ‘—2, ...,ğ‘¡ğ‘—ğ‘›ğ‘—, } ), with ğ‘›ğ‘—= |ğ‘‰ğ‘—|.
â€¢ ğºğ‘—= (ğ‘‰ğ‘—, Eğ‘—): dependency DAG of job ğ‘—with edges Eğ‘—.
â€¢ ğ‘ğ‘—ğ‘–ğ‘š: processing time of task ğ‘–of job ğ‘—(ğ‘¡ğ‘—ğ‘–) on machine ğ‘š.
â€¢ ğ¶ğ‘—ğ‘–: completion time of task ğ‘¡ğ‘—ğ‘–.
â€¢ M: set of machines and Mğ‘—ğ‘–: set of machines that can run
task ğ‘¡ğ‘—ğ‘–.
â€¢ ğ‘ƒğ‘š: power draw of machine ğ‘š(kW).
â€¢ ğ¼(ğœ): average carbon intensity at time ğœ(gCO2/kWh).
A feasible schedule S assigns each task ğ‘¡ğ‘—ğ‘–to a machine and
start time, while respecting job arrivals, task dependencies, and
machine capacity. For the ğ‘–th task in job ğ‘—, the start time is ğ‘ ğ‘—,ğ‘–, and
ğ‘¥ğ‘—ğ‘–ğ‘šâˆˆ{0, 1} equals 1 if task ğ‘¡ğ‘—ğ‘–runs on machine ğ‘š.
The problem is formulated as follows:
min
âˆ‘ï¸
ğ‘¡ğ‘—ğ‘–
âˆ‘ï¸
ğ‘š
ğ‘¥ğ‘—ğ‘–ğ‘š
ğ¶ğ‘—ğ‘–
âˆ‘ï¸
ğœ=ğ‘ ğ‘—ğ‘–
ğ‘ƒğ‘šÂ· ğ¼(ğœ),
âˆ€ğ‘—âˆˆJ, âˆ€ğ‘¡ğ‘—ğ‘–âˆˆğ‘‰ğ‘—, âˆ€ğ‘šâˆˆğ‘€ğ‘—ğ‘–
(1)
min
âˆ‘ï¸
ğ‘¡ğ‘—ğ‘–
âˆ‘ï¸
ğ‘š
ğ‘¥ğ‘—ğ‘–ğ‘šğ‘ƒğ‘šğ‘ğ‘—ğ‘–ğ‘š,
âˆ€ğ‘—âˆˆJ, âˆ€ğ‘¡ğ‘—ğ‘–âˆˆğ‘‰ğ‘—, âˆ€ğ‘šâˆˆğ‘€ğ‘—ğ‘–
(2)
min
max
ğ‘—âˆˆJ ğ¶ğ‘—,ğ‘›ğ‘—
(3)
ğ‘ ğ‘—ğ‘–â‰¥ğ‘ğ‘—,
âˆ€ğ‘—âˆˆJ, âˆ€ğ‘¡ğ‘—ğ‘–âˆˆğ‘‰ğ‘—
(4)
ğ‘ ğ‘—ğ‘–â‰¥ğ¶ğ‘—ğ‘–â€²,
âˆ€(ğ‘¡ğ‘—ğ‘–,ğ‘¡ğ‘—ğ‘–â€²) âˆˆEğ‘—, âˆ€ğ‘—âˆˆJ
(5)
âˆ‘ï¸
ğ‘šâˆˆMğ‘—ğ‘–
ğ‘¥ğ‘—ğ‘–ğ‘š= 1,
âˆ€ğ‘—âˆˆJ, âˆ€ğ‘¡ğ‘—ğ‘–âˆˆğ‘‰ğ‘—
(6)
ğ¶ğ‘—ğ‘–= ğ‘ ğ‘—ğ‘–+
âˆ‘ï¸
ğ‘šâˆˆMğ‘—ğ‘–
ğ‘¥ğ‘—ğ‘–ğ‘šğ‘ğ‘—ğ‘–ğ‘š,
âˆ€(ğ‘¡ğ‘—ğ‘–,ğ‘¡ğ‘—ğ‘–â€²) âˆˆEğ‘—, âˆ€ğ‘—âˆˆJ
(7)
NoOverlap  {[ğ‘ ğ‘—ğ‘–, ğ‘ğ‘—ğ‘–ğ‘š] : ğ‘¥ğ‘—ğ‘–ğ‘š= 1} ,
(8)
âˆ€ğ‘—âˆˆJ, âˆ€ğ‘¡ğ‘—ğ‘–âˆˆğ‘‰ğ‘—, âˆ€ğ‘šâˆˆMğ‘—ğ‘–
The three objectives highlight different optimization goals that
the operator can choose from: (i) Equation 1 minimizes carbon
emissions by weighting energy consumption by time-varying car-
bon intensity, (ii) Equation 2 minimizes total energy consumption.
(iii) Equation 3 minimizes the makespan (total jobs completion
time). The baseline carbon-agnostic scheduler operates based on
this objective and only finds the optimal makespan.
When optimizing for carbon or energy objectives, we can im-
pose an upper bound on the makespan. For example, setting
maxğ‘—âˆˆJ ğ¶ğ‘—,ğ‘›ğ‘—â‰¤2 Ã— ğ¶OPT constrains the schedule to finish within
twice the optimal makespan ğ¶OPT, where ğ¶OPT is obtained from
Equation 3.
Equation 4 ensures tasks cannot start before their job arrives.
Equation 5 enforces the DAG dependencies between tasks of a job.
Equation 6 forces each task to be assigned to exactly one server.
Equation 7 defines completion times based on start times and server-
dependent processing times. Equation 8 prevents two tasks from
overlapping on the same server, capturing machinesâ€™ capacity lim-
its.
