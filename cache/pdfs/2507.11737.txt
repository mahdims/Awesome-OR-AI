--- Page 1 ---
Auto-Formulating Dynamic Programming Problems
with Large Language Models
Chenyu Zhou*
Antai College of Economics and Management, Shanghai Jiao Tong University, Shanghai, China, chenyuzhou@sjtu.edu.cn
Jingyuan Yang*
Booth School of Business, University of Chicago, Chicago, IL, jyang21@chicagobooth.edu
Linwei Xin†
School of Operations Research and Information Engineering, Cornell University, Ithaca, NY, lx267@cornell.edu
Yitian Chen
Cardinal Operations, Beijing, China, chenyitian@shanshu.ai
Ziyan He
Shanghai University of Finance and Economics, Shanghai, China, heziyan@stu.sufe.edu.cn
Dongdong Ge†
Antai College of Economics and Management, Shanghai Jiao Tong University, Shanghai, China, ddge@sjtu.edu.cn
Dynamic programming (DP) is a fundamental method in operations research, but formulating DP models
has traditionally required expert knowledge of both the problem context and DP techniques. Large Language
Models (LLMs) offer the potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of training data. These factors make
it difficult to directly apply existing LLM-based models or frameworks developed for other optimization
problems, such as linear or integer programming. We introduce DP-Bench, the first benchmark covering a wide
range of textbook-level DP problems to enable systematic evaluation. We present Dynamic Programming
Language Model (DPLM), a 7B-parameter specialized model that achieves performance comparable to state-
of-the-art LLMs like OpenAI’s o1 and DeepSeek-R1, and surpasses them on hard problems. Central to DPLM’s
effectiveness is DualReflect, our novel synthetic data generation pipeline, designed to scale up training
data from a limited set of initial examples. DualReflect combines forward generation for diversity and
backward generation for reliability. Our results reveal a new insight: backward generation is favored in low-
data regimes for its strong correctness guarantees, while forward generation, though lacking such guarantees,
becomes increasingly valuable at scale for introducing diverse formulations. This trade-off highlights the
complementary strengths of both approaches and the importance of combining them.
History : July 17, 2025
1
arXiv:2507.11737v1  [cs.AI]  15 Jul 2025


--- Page 2 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
2
1.
Introduction
Automating the formulation of decision-making problems represents a major step toward fully
autonomous decision-support systems. Traditionally, solving such problems involves two sequential
stages: first, translating real-world scenarios into well-defined mathematical models–an essential
skill emphasized in operations research education–and second, applying computational tools to
find optimal or near-optimal solutions. While substantial research in recent decades has primarily
focused on the second stage–enhancing algorithms and improving solver efficiency–advancements
span a wide range, from foundational developments such as reinforcement learning (RL) frame-
works (e.g., Sutton and Barto 2018) and approximate dynamic programming techniques (e.g.,
Powell 2011), to powerful solvers like COPT, CPLEX, and Gurobi. Such innovations coupled with
increasing computational power have led to high-impact real-world applications, exemplified by
AlphaGo, which leveraged deep learning and RL to solve complex, large-scale decision-making
problems (Silver et al. 2016). That said, while these advancements have shifted many computational
tasks to automated software, the initial problem formulation step has largely remained manual and
dependent on expert knowledge.
The recent rapid progress in large language models (LLMs) provides a promising opportunity to
automate this crucial first step. LLMs excel in natural language processing and have demonstrated
significant potential for effectively automating the formulation of mathematical models directly
from plain English descriptions. Leveraging LLMs can substantially reduce the human expertise
required, simplify the problem formulation process, and make advanced optimization methods
accessible to a broader audience.
Among various optimization problems, dynamic programming (DP) represents a particularly
important yet challenging category for formulation automation. Despite recent advances in lever-
aging LLMs to auto-formulate optimization problems such as LP and MIP, (e.g., Huang et al.
∗Co-first authors with equal contribution.
† Corresponding author.


--- Page 3 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
3
2025, Lu et al. 2025), directly applying existing techniques to DP problems proves insufficient for
several reasons. First, DP addresses multi-period decision-making with sequential transitions, often
requiring careful reasoning about the order of events, such as what occurs first, which costs are
incurred immediately, and how the system evolves over time. This complexity is further amplified
when transitions are stochastic, which is often the primary challenge even for human modelers.
In contrast, LP and MIP problems are deterministic and rarely involve such intricate transitions,
focusing instead on constraint formulation.
Second, beyond the technical complexity of modeling DPs, which even human experts may
struggle with, LLMs face an additional challenge that is especially salient in the OR/OM domain:
accurately interpreting natural-language problem descriptions. Unlike typical DP problems studied
in computer science, which are formally structured (see Appendix EC.1 for an example), OR/OM
problems are often embedded in text-rich, business-oriented scenarios that require understanding
implicit assumptions, domain-specific terminology, and contextual nuances. Unlike domain experts,
who naturally infer implicit constraints and contextual assumptions from subtle cues in wording
or context, LLMs often struggle to recognize these subtleties. This limitation makes DP tasks
particularly challenging to automate using LLMs. Let us illustrate this point through an example.
Example 1. A warehouse has an end-of-period capacity of 3 units. During a production period,
a $4 setup cost is incurred, and a holding cost of $1 per unit applies to the period’s ending inventory.
Variable production costs are $1 per unit. Demand each period is equally likely to be either 1 or 2
units, and all demand must be fulfilled immediately. The discount factor is β = 0.8. The objective
is to minimize the expected discounted costs over an infinite horizon, starting with zero initial
inventory.
Example 1 represents a toy inventory problem with random demand and a subtle distinction: the
term “end-of-period capacity of 3 units” differs from simply having a “capacity” of 3 units. Since
demand is at least one unit per period, the effective order-up-to level at the start of each period
can be 4 units. This nuance is often missed. Most state-of-the-art (SOTA) LLMs (e.g., o4-mini-
high, o3, DeepSeek-R1, and GPT-4o) fail to distinguish between the two notions, often leading to


--- Page 4 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
4
incorrect answers. While this may initially appear to be a simple word-understanding issue, which
could also arise in LP or other deterministic optimization problems, we find that the challenge
runs deeper. When we modify the same problem to use deterministic demand (always 2 units per
period), strong models like o4-mini-high, o3, and DeepSeek-R1 are able to correctly interpret the
meaning of “end-of-period” capacity. This suggests that the misunderstanding arises not purely
from language, but rather from the added complexity introduced by stochastic transitions. It is the
combination of subtle language cues and the need to reason over stochastic dynamics that makes
DP problems especially difficult for LLMs.
Beyond the already significant challenges LLMs face in automating DP formulations, an even
greater obstacle is the lack of a standardized test set for evaluating their performance on DP
problems, making it difficult to analyze or compare models in a consistent and rigorous manner.
To fill this gap, we introduce DP-Bench, the very first standardized benchmark1 dataset tailored
to DP problems.
DP-Bench consists of 132 carefully selected and augmented textbook-level DP problems, each
featuring a closed-ended, specific numerical answer, similar in spirit to benchmarks such as the cel-
ebrated Humanity’s Last Exam by Scale AI (Phan et al. 2025). Proof-based problems are explicitly
excluded. The goal is to assess LLMs’ ability to translate problem descriptions from plain English
into mathematical models. While we evaluate this end-to-end (i.e., having the LLM both formulate
and solve each problem via code), the final answer serves as a proxy for formulation accuracy.
This is because, for textbook-level problems, a correct solution typically follows directly from an
accurate formulation using standard textbook methods. Occasional coding errors may occur, but
these are less indicative of core understanding than errors in problem formulation.
Testing SOTA reasoning LLMs on DP-Bench reveals significant performance gaps. The best-
performing model, DeepSeek-R1, achieves only 59.1% average accuracy (see Table 2 in Section 6),
1 The term benchmark, in the context of datasets, refers to standardized and widely recognized collections used to
evaluate and compare the performance of machine learning models and algorithms.


--- Page 5 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
5
indicating substantial room for improvement. In contrast, LLMs perform considerably better on
benchmarks for other types of optimization problems. For instance, Lu et al. (2025) report that
DeepSeek-V3 achieves 95.9% accuracy on NL4OPT, a popular benchmark in operations research
featuring textbook-level LP problems, created by Ramamonjison et al. (2022). We also evaluate
ORLM (Huang et al. 2025), an LLM specifically designed for auto-formulating generic optimization
problems. On DP-Bench, ORLM achieves only 0.8% accuracy; even when allowing pass@10, i.e.,
measuring whether at least one of 10 sampled outputs yields a correct answer, its accuracy remains
as low as 8.3% (see Section EC.7 in Appendix for details). These results highlight the inherent
challenges LLMs face in auto-formulating DP problems compared to other optimization problems,
and the need for developing more capable models tailored specifically for DP problems.
In this paper, we aim to improve the ability of open-source LLMs to auto-formulate DP prob-
lems. There are three common approaches to leveraging existing commercial or open-source LLMs
to enhance task performance: (1) prompting, (2) commercial API-based fine-tuning, and (3) fine-
tuning open-source models. However, the first two approaches come with notable limitations. Both
require sending potentially sensitive data to third-party servers, raising privacy and security con-
cerns. Moreover, prompting does not change the base model’s weights, so performance remains
constrained by the model’s existing capabilities. For example, GPT-4o, with a trivial prompt,
achieves only 45.6% accuracy on the easy problems and 19.0% on the hard problems of DP-Bench.
To improve performance, we develop a carefully designed LLM-based agentic workflow that inte-
grates step-by-step reasoning and label-based retrieval-augmented generation (RAG). Even with
these enhancements, however, the agentic workflow achieves only 53.3% on the easy problems and
21.4% on the hard problems (see Table 1 for details). This modest improvement suggests that while
prompting helps, it remains fundamentally limited by the capabilities of the underlying model.
Commercial API-based fine-tuning faces similar constraints. It offers limited customization, and
the fine-tuned model is not owned by the user, resulting in vendor lock-in. This introduces opera-
tional risks, such as dependency on service availability, pricing changes, or platform discontinuation.
These limitations also reduce deployment flexibility, especially for low-latency or offline use cases.


--- Page 6 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
6
Given these limitations, we choose to fine-tune open-source models for the specialized task of
auto-formulating DP problems. To ensure feasibility under limited computational resources, we
focus on small-scale models. A key challenge is the lack of suitable training data. While many DP-
related problems exist on platforms like LeetCode, they are typically coding-style puzzles and do not
reflect the background-rich, natural-language problem descriptions common in operations research.
To address this, we build a complete training pipeline from scratch. We begin with a curated set of
high-quality seed problems, then introduce a dual synthetic data generation framework to ensure
both diversity and fidelity, and finally apply efficient fine-tuning recipes. As a result, our proposed
model, DPLM, achieves performance comparable to SOTA LLMs that are over 100 times larger.
1.1.
Our Contributions
This work is the first to systematically investigate the automatic formulation of DP problems using
LLMs. Our contributions are threefold.
First, to support the evaluation of LLMs on DP tasks, we introduce DP-Bench, a curated bench-
mark of 132 textbook-level DP problems. These problems span diverse domains and cover a variety
of problem types, including deterministic and stochastic settings, as well as finite- and infinite-
horizon formulations. Each problem is classified as either “easy” or “hard”, corresponding to the
expected difficulty level for undergraduate and graduate students, respectively. Designed to pro-
duce specific numerical answers, this benchmark offers a standardized and rigorous framework for
evaluating LLMs’ ability to auto-formulate DP problems from natural language descriptions.
Second, we present DPLM, a specialized model fine-tuned on open-source architectures and trained
entirely from scratch using synthetic data distilled from GPT-4o. Our results show that DPLM sig-
nificantly outperforms its teacher model, GPT-4o, on both the easy and hard subsets of DP-Bench.
Remarkably, despite being only 7B in size (which is nearly 100x smaller than DeepSeek-R1), DPLM
achieves the second-highest accuracy on easy problems and the highest accuracy on hard problems
among all evaluated models, including advanced reasoning models such as o1 and DeepSeek-R1.
These results highlight both the potential of LLMs for automating DP formulation and the promise
of compact, domain-specific models tailored to specialized problem classes like DP.


--- Page 7 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
7
Third, we introduce DualReflect, a scalable and principled data generation framework that
starts from a limited set of 91 seed examples, and produces a synthetic dataset of 113K problem
instances, strategically balancing accuracy and diversity. DualReflect combines forward genera-
tion, which creates problems first and then solves them, with backward generation, which begins
from a solution and constructs the corresponding problem. As part of backward generation, we
design a reflect-and-refine approach to recover 20.8% of examples that would otherwise be dis-
carded due to solution inconsistencies. In doing so, DualReflect mitigates a common limitation
in synthetic datasets, which tend to contain mostly accurate but overly simplistic problems. Our
empirical findings reveals a new insight: backward generation is preferred when only a small num-
ber of examples can be generated, due to its high reliability, while forward generation becomes
increasingly valuable at scale for its diversity, highlighting the necessity of combining both.
1.2.
Literature Review
Our work closely aligns with research that leverages LLMs to translate natural language descrip-
tions into optimization models. Early efforts by Ramamonjison et al. (2022) introduce the widely
used NL4OPT benchmark for LP problems. More recent work include LLM-agent prompting
approaches (Ahmaditeshnizi et al. 2024, Bertsimas and Margaritis 2024, Liang et al. 2025) and
fine-tuning methods (Yang et al. 2024, Huang et al. 2025, Lu et al. 2025), which primarily improve
LLM performance on deterministic LP, MILP, NLP, and robust optimization. Very recently, Li
et al. (2025b) propose a multi-agent architecture designed to handle different classes of problems in
operations research. However, DP, especially when involving stochastic transitions, remains unex-
plored.
Among existing works, the closest is Huang et al. (2025). Beyond the fact that auto-formulating
DP is fundamentally different from formulating LP, several key differences distinguish our approach.
First, ORLM relies on proprietary industrial datasets, whereas we propose a more generalizable
framework for emerging domains by systematically curating seed data and constructing bench-
marks from textbooks. Second, while ORLM uses only forward generation to expand its dataset,


--- Page 8 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
8
we combine both forward and backward generation to balance diversity and correctness. Finally,
ORLM trains its models using supervised fine-tuning (SFT) alone, whereas we further apply RL
to improve solution quality and robustness.
Preference-based RL alignment has recently emerged as a popular approach for enhancing
reasoning-oriented LLMs on complex tasks. Flagship models such as OpenAI’s o1 rely on variants
of reinforcement learning from human feedback (RLHF) to improve factuality and performance.
Representative techniques include on-policy RLHF (Ouyang et al. 2022), offline Direct Preference
Optimization (DPO) (Rafailov et al. 2023), and the lightweight Group-Relative Policy Optimiza-
tion (GRPO) (Shao et al. 2024). Although the training report of DeepSeek-R1-Zero (Guo et al.
2025) highlights the potential of RL-only approaches, relying exclusively on RL without prior SFT
often leads to training instability and prohibitive computational costs, especially for smaller mod-
els (Zheng et al. 2023). In this paper, we adopt a two-step approach that combines SFT with
RL. Our experiments further demonstrate the importance of SFT as a crucial preliminary step for
establishing a strong baseline prior to applying RL.
To address the scarcity of training data in DP auto-formulation, our research aligns with the
literature on generating synthetic data to alleviate the lack of human-labeled examples. A general
approach is called Self-Instruct, introduced by Wang et al. (2022). The core idea is to prompt an
LLM with a small set of seed examples, guiding it to generate its own instructions and correspond-
ing outputs with minimal human effort. This approach follows the forward generation paradigm,
where an instruction or problem is created first, followed by a corresponding solution. However, in
more reasoning-intensive domains such as mathematical problem solving, deriving solutions from
arbitrary problems can be highly nontrivial, limiting the accuracy of purely forward-generated
data. To address this, some studies adopt backward generation, where the process begins from a
known solution and constructs a compatible problem around it. A notable use of the combination
of forward and backward generation is by Lample and Charton (2019), who generate diverse math-
ematical functions along with their integrals or derivatives. Subsequently, Alfarano et al. (2024)


--- Page 9 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
9
apply this technique to discover Lyapunov functions that ensure the global stability of dynam-
ical systems, where algorithmic solvers exist for certain families of polynomial systems. In both
cases, the solutions derived from both forward and backward generation are verifiable. In sharp
contrast, in our context, there are no tools capable of deriving or verifying optimal solutions to
decision-making problems from the description alone via forward generation. As a result, existing
works (e.g., Yang et al. 2024, Lu et al. 2025) primarily focus on backward generation to guarantee
accuracy. In contrast, we highlight that while forward generation is inherently less accurate, it
significantly increases diversity and is especially valuable when generating larger training sets.
To enhance reasoning capabilities without extensive manual annotation, various techniques have
been proposed. Wei et al. (2023) demonstrate that providing step-by-step chain-of-thought (CoT)
reasoning examples substantially improves performance on math and logic problems. Building upon
this, Zelikman et al. (2022) propose the Self-Taught Reasoner (STaR), which iteratively prompts
an LLM to generate answer-conditioned rationales and fine-tunes it using only those verified as
correct. We adopt a similar idea through Reflected CoT but differ from STaR in two key ways. First,
while STaR focuses on problems where answers are informative for reasoning, DP answers (e.g.,
optimal values) provide little guidance for reconstructing the full model. Second, STaR assumes
access to a large-scale dataset with correct problems and answers but missing rationales, whereas
our backward-generated problems themselves may contain flaws. Providing the correct solution
and prompting the LLM to follow the STaR approach may lead it to fabricate a rationale that
aligns with the answer but is logically invalid. In some cases, the generated problems themselves
may also be flawed. In contrast, Reflected CoT allows the LLM to solve the problem independently,
compare its result to the ground truth, and iteratively refine its reasoning over multiple rounds.
LLMs have recently played an expanding role in OR/OM and related decision-making domains.
One stream of work leverages LLMs as intelligent agents to support or augment human decision-
making, drawing on their reasoning and linguistic capabilities. For example, Wang et al. (2024)
propose a novel statistical data augmentation method that efficiently integrates LLM-generated


--- Page 10 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
10
data with real data in conjoint analysis. Ye et al. (2025) introduce an LLM-assisted online learning
algorithm to predict the best-performing headline. Bray (2025) demonstrates that LLMs can serve
as personalized tutors or teaching assistants in data analytics education. A second line of research
investigates bias in LLM-generated decisions and data. For instance, Chen et al. (2025) show
that ChatGPT can replicate well-known human decision biases, such as loss aversion. Li et al.
(2025a) find that LLM-generated personas exhibit systematic demographic and ideological biases,
introducing distortion even before downstream tasks such as election forecasting or opinion polling.
The third direction applies operations research methods to optimize LLM inference efficiency.
For instance, Li et al. (2025c) develop the queuing fundamentals for LLM inference and prove
that a large class of work-conserving scheduling algorithms can achieve maximum throughput for
individual inference LLM engine. Ao et al. (2025) model LLM inference as a multi-stage online
scheduling problem with Key-Value (KV) cache constraints and establish asymptotic optimality
under heavy traffic. Jaillet et al. (2025) also incorporate KV cache constraints to minimize latency
and propose a batching and scheduling algorithm with performance guarantees.
2.
Problem Setup
Discrete-time stochastic DP provides a general framework for modeling sequential decision-making
under uncertainty over time. A decision-maker seeks to influence the evolution of a probabilistic
system over time by selecting actions that optimize a long-term performance criterion. Since each
choice impacts not only immediate outcomes but also future states, optimal decisions must antic-
ipate downstream consequences rather than focusing only on immediate rewards. Formally, a DP
problem can be mathematically represented as
MF = {T,S,A,pt(·|s,a),rt(s,a),γ},
(1)
which specifies the planning horizon T, state and action spaces S and A, transition probabilities
pt, reward functions rt, and discount factor γ. Full details and formulations for finite and infinite
horizons are deferred to Appendix Section EC.2.


--- Page 11 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
11
To formalize the modeling process of a multi-period decision-making problem, we introduce sev-
eral key concepts. Let P denote the problem natural language description, which describes the
decision-making context, objectives, constraints, and sources of uncertainty in plain language. It
also contains the central question posed by the decision-maker, such as: “What is the minimum
expected discounted cost over the next 12 months starting with a state 2 machine?” Given a P,
we expect an LLM to auto-formulate the mathematical model underlying the problem and then
develop an executable code. The formal mathematical representation of the problem is called a
model, denoted by M. This model explicitly defines state space, action space, system transitions,
reward functions, and boundary conditions. We denote by C the corresponding code implementa-
tion, typically written in Python, that translates the model M into a computational solution. The
code numerically solves for the optimal value function and policy using standard DP algorithms
such as backward induction, value iteration, or policy iteration. The numerical answer computed
by C is denoted by y, and directly addresses the decision question posed in P.
In addition to this core pipeline, we also prompt the LLM to generate a chain-of-thought reason-
ing process, denoted by CoT, as an intermediate step to enhance model formulation and code gen-
eration. Chain-of-thought, proposed by Wei et al. (2023), has been shown to significantly improve
large language models’ ability to perform complex reasoning tasks by encouraging step-by-step
intermediate reasoning. Inspired by this technique, we incorporate CoT generation before solving
the problem to help the LLM better structure its understanding and improve solution accuracy. In
summary, we define a complete solution to a problem P as the triplet r = (CoT,M,C). We use r∗
and y∗specifically to denote the solution and answer we consider correct.
It is important to distinguish M from MF, as defined in (1), which represents the abstract DP
structure independent of any specific problem data. In contrast, M instantiates MF with concrete
problem data PD extracted from the P. Thus, M can be viewed as the combination of the abstract
model structure MF and the specific data PD that together fully define the decision problem.


--- Page 12 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
12
Figure 1
Composition of DP-Bench problems by horizon type (left) and problem formulation: deterministic vs.
stochastic (right).
3.
Benchmark
To systematically evaluate the capability of LLMs to auto-formulate DP problems, we introduce
DP-Bench, the first benchmark dedicated to DP. It consists of 90 easy and 42 hard textbook-level
problems, all derived and augmented from standard textbooks. The easy problems are primarily
adapted from deterministic and probabilistic DP exercises in Winston (2004), while the hard prob-
lems are drawn from more advanced exercises in Puterman (2005). We aim to preserve the original
problems from the textbooks while making some necessary modifications to ensure clarity and con-
sistency. We defer these adjustment details to Appendix EC.3.1. Each problem in the benchmark
includes a “Question” and an “Answer” obtained by executing the solution. The solutions for the
easy benchmark problems are generally available online, while the solutions to the hard problems
are derived and verified by ourselves.
We annotate each problem with both its application domain and structural characteristics.
Although the domain distributions are broadly similar between the easy and hard sets (see
Appendix EC.3.2), Figure 1 reveals significant structural differences. The hard benchmark contains
20% fewer finite-horizon problems and a comparable increase in infinite-average-horizon problems
compared to the easy benchmark. Deterministic problems are rare in the hard problem set, only
4.8%, versus 45.6% in the easy problem set. Additional structural labels capturing unique model-
ing formulations or increased complexity, such as action-dependent transitions and time-dependent


--- Page 13 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
13
state spaces, are summarized in Appendix EC.3.2. These differences support the validity of our
easy-vs-hard benchmark classification.
The performance of SOTA LLMs (including o1 and DeepSeek-R1) on DP-Bench is reported in
Table 2 (see Section 6 for details). The best accuracies on the easy and hard benchmarks are 73.3%
and 31.0%, respectively. In contrast, small-scale models (with fewer than 10B parameters), which
we fine-tune in this study, achieve at most 10.0% and 2.4%, respectively. These results highlight
the difficulty of DP problems for LLMs, especially when compared to their strong performance on
textbook-level LP benchmarks.
4.
Framework of Synthetic Data Generation
Unlike LP and MIP, which benefit from mature benchmarks and broad access to problem datasets,
fine-tuning LLMs for DP auto-formulation faces a key challenge: the lack of high-quality, domain-
specific data. While public programming platforms like LeetCode host numerous DP exercises
(e.g., the longest valid parentheses problem provided in Example EC.1 in the appendix), these are
typically framed from a computer science perspective and differ significantly from the real-world,
operations-focused DP problems. To bridge this gap, we propose a two-direction data synthesis
framework, DualReflect, tailored for scenarios with limited high-quality seed data. DualReflect
strategically balances accuracy and diversity, enabling the creation of large-scale synthetic datasets
suitable for DP auto-formulation.
DualReflect begins with high-quality seeds sourced from multiple domains. The seed data
includes 91 problems sourced from textbooks, including both examples and exercises, and modified
in a manner consistent with the benchmark problems. Unlike benchmarks that include only the
problem description P and final answer y∗, we augment each seed example with a correct solution
r∗= (CoT∗,M∗,C∗). These curated seed data points serve as the foundation for creating synthetic
exercises, reasoning tasks, and corresponding code solutions tailored to DP problems.
From these seeds, our goal is to generate a large-scale synthetic data consisting of (P,r∗,y∗). The
key challenge lies in ensuring quality while maintaining diversity: even advanced LLMs struggle to


--- Page 14 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
14
Figure 2
Overview of the DualReflect synthetic data generation framework.
reliably solve DP problems. Simply prompting an LLM to solve new problems often yields a low
proportion of correct solutions, with no reliable way to verify their correctness. On the other hand,
reusing or slightly modifying seed examples guarantees accuracy but sacrifices diversity.
To balance correctness and diversity, we synthesize data along two complementary directions:
forward generation and backward generation. Both approaches expand seed problems through
scenario expansion, but they differ in the order of construction: whether the problem or the solution
is generated first, reflecting the fundamental trade-off between problem diversity and solution
accuracy. Forward generation begins with the problem, allowing greater variability but offering
limited guarantees of solution correctness. In contrast, backward generation starts from a known
solution to ensure correctness, but its variability is inherently limited by the original seed problems.
In what follows, we introduce how we construct a broad range of realistic application scenarios
in Section 4.1. Since both generation methods require solving DP problems, Section 4.2 introduces
our RAG-based prompting approach, which enhances LLM performance as a solution generator.


--- Page 15 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
15
Sections 4.3 and 4.4 describe the forward and backward generation procedures, respectively. Figure
2 illustrates the overall DualReflect data generation framework. All data are generated using
GPT-4o, and the resulting synthetic dataset is discussed in Section 5.
4.1.
Scenario Expansion
To ensure diversity and realism in our synthetic data, we begin by constructing a broad set of
application scenarios that reflect real-world contexts where DP is typically applied. These scenarios
do not specify complete problem instances on their own; rather, they serve as contextual templates
for reinterpreting and modifying our seed data. Specifically, each new problem is generated by
prompting the LLM to adapt a seed-derived component, either the original problem description
P in forward generation or the perturbed code ˜C (derived from seed C) in backward generation,
to fit a given DP scenario. We group our scenarios into six high-level application categories: (i)
Manufacturing and Inventory, (ii) Transportation and Logistics, (iii) Investment and Risk, (iv)
Game Strategy, (v) Resource Allocation, and (vi) Others. For each category, we prompt the LLM
to generate dozens of scenarios that involve clear trade-offs in decision-making, such as balancing
resource usage and profit or managing risk versus reward. Embedding these trade-offs directly
into the scenario setup helps guide downstream problem generation toward realistic operational
challenges. A concrete scenario example is provided in Example EC.2 in the appendix.
4.2.
RAG-Based Solution Generator
Even advanced models like GPT-4o achieve only 37.1% accuracy on DP-Bench, highlighting the dif-
ficulty of generating correct DP solutions directly. To improve the quality and efficiency of synthetic
solution generation, we implement a RAG-based few-shot prompting approach. Specifically, given
a newly generated problem P, we retrieve similar seed examples to guide the LLM in generating its
solution r∗. However, defining “similarity” for DP problems based on P is nontrivial. While seman-
tic similarity (e.g., matching an inventory problem with another inventory example) may seem
intuitive, even minor wording changes can imply fundamentally different solution approaches. For
example, finite-horizon and infinite-average-horizon inventory problems may have nearly identical


--- Page 16 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
16
Ps, yet require entirely different algorithms in C. In such cases, retrieving examples with similar
context but mismatched model structures can mislead the LLM and sometimes perform worse than
providing no example at all.
To address this challenge, we prioritize structural similarity over surface-level semantics. Each
problem is annotated with both its application domain and structural characteristics, such as
infinite-discounted and optimal stopping problem, to capture key modeling and solution character-
istics, consistent with the benchmark problems as discussed in Section 3.
We use GPT-4o to assign these labels to both seed and generated Ps. Label-based filtering is then
combined with semantic similarity search to retrieve the most relevant examples for each generated
P. The generation process is decomposed into sequential steps: first generating CoT, then using the
problem and CoT to generate M, and finally producing C. At each step, we tailor the prompt using
similar examples corresponding to the specific component to guide the LLM. Finally, we execute
the generated code C to obtain the final answer y.
Table 1 reports the performance of our RAG-based solution generator, which improves accuracy
45.6% to 53.3% on the easy benchmark, but yields only a limited gain of 2.4% on hard problems.
While the RAG-based solution generator can help extract better outputs from the base model,
it cannot create capabilities the model lacks out of the box. This highlights the limitations of
prompting-based approaches and the need for fine-tuning, especially on more challenging problems.
Table 1
Performance of our RAG-based solution generator using GPT-4o. “RAG” refers to the RAG framework
employed in the solution generator.
Method
Easy (%) Hard (%) Micro∗(%) Macro∗(%)
Zero-shot prompt
45.6
19.0
37.1
32.3
RAG
53.3
21.4
43.1
37.4
∗The micro-average and macro-average metrics are defined in Section 6.1.


--- Page 17 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
17
4.3.
Forward Generation
Forward generation consists of three main steps. First, we generate a new problem description ˜P
from a seed problem P. Next, we apply the RAG-based solution generator introduced in Section
4.2 to produce a corresponding solution r. Finally, we refine and filter the outputs to eliminate
potentially incorrect solutions. As this procedure is relatively standardized, we detail the steps in
Appendix Section EC.4.1, with the full procedure formalized in Algorithm 2.
Despite downstream augmentation, refinement, and filtering steps, many forward-generated sam-
ples still suffer from two major sources of error: (i) the generated solution may contain logical or
computational mistakes even when the problem description is valid, and (ii) in some cases, the
generated problem itself may be fundamentally ill-posed or incoherent. Both types of errors intro-
duce noise into the training set, and verifying correctness is both difficult and time-consuming for
humans. To address this, we turn to backward generation, introduced in the next section.
4.4.
Backward Generation
To complement forward generation with correctness guarantees, we adopt backward generation,
which reverses the synthesis process by starting with a correct solution and then constructing a
compatible problem description. This approach has been successfully applied in other mathematical
domains, such as Lyapunov function discovery, where correctness can be easily verified (Alfarano
et al. 2024). However, in the context of DP, generating valid and coherent problems from known
solutions poses challenges similar to those faced when verifying solutions in forward generation. To
overcome this, we design a tailored backward generation framework that leverages our confidence
in the correctness of the starting solution. Building on this idea, our backward generation approach
complements forward generation in three key ways: (i) it guarantees correctness in both the problem
and solution generation, (ii) it produces CoT reasoning-trajectory data that are especially valuable
for SFT, and (iii) it retains difficult problems in the synthetic dataset that would otherwise be
filtered out. The backward generation pipeline consists of the following three steps.


--- Page 18 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
18
Step 1: Generating the problem from the solution. We begin by constructing a new
problem description ˜P from a modified code ˜C. The modified code ˜C serves as the “ground-truth”
solution and is obtained by keeping the original mathematical formulation MF from a seed example
while perturbing problem data PD, such as the number of time periods, transition probabilities, or
cost coefficients, by modifying parameters, boundary conditions, or initial values in the seed code.
We start from a perturbed code implementation rather than directly modifying the model for two
practical reasons. First, M is written in natural language, and key components such as constraints
or transitions may not be stated explicitly. In contrast, C provides a precise and unambiguous
specification of the model. Second, since we ultimately need to execute the code to obtain the
final answer, working directly with code helps avoid potential inconsistencies or errors that could
arise from adding an extra step, such as regenerating code from a modified model description. To
ensure that the modified code corresponds to a meaningful and executable DP model, we apply
a code-level filter to discard any ˜C instances with runtime errors or semantically invalid outputs.
This process gives us high confidence in ˜C as a correct solution, which we treat as ground truth in
the backward generation process.
After passing this filter, ˜C is used to generate a new problem description ˜P, guided by a scenario
prompt. Interestingly, the textual similarity between the original P and the backward-generated
˜P is typically lower than in forward generation, since ˜P is written independently from the code
without referencing the original description.
Step 2: Solving and verifying the problem. Although we have already obtained the gener-
ated problem ˜P and its corresponding code ˜C, additional verification is necessary to ensure that ˜P
is well-posed and to reduce the risk of propagating flawed problems into the fine-tuning dataset. To
this end, we apply our RAG-based solution generator to produce an initial solution (CoT0,M0,C0)
and a final answer y0. We then verify whether the final answer y0 matches the output of ˜C. If the
outputs match, we accept (˜P,CoT0,M0,C0) as a valid training data sample.


--- Page 19 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
19
Figure 3
Illustration of the Reflected CoT process and outputs.
Step 3: Solution trajectory generation and problem recovery via Reflected CoT. To
more effectively leverage the ground-truth code ˜C, beyond its role as a filter for validating gen-
erated problems, we introduce a Reflected CoT approach that allows the LLM to learn from the
reference solution without merely copying it. This method is inspired by the rationalization tech-
nique proposed by Zelikman et al. (2022), but extends it to a substantially more complex setting.
Unlike the short math or commonsense problems considered in their work, our problem descriptions
may contain flaws, and the reasoning required to derive a solution is considerably more involved.
Specifically, we present the LLM with its own failed code C0 alongside the correct reference ˜C, and
prompt it to identify discrepancies and revise its reasoning. Rather than immediately regenerating
the full solution, which risks producing a superficially plausible response even when the problem
is ill-posed, we first prompt the model to generate a revised chain-of-thought CoT1 based on its
reflection. We then re-run Step 2 using CoT1 to produce an updated solution tuple (CoT1,M1,C1)
and check whether C1 yields the correct output. If not, the correction process is repeated. This loop
continues until the generated code matches the correct answer, with a maximum of six attempts.
We set this cap based on empirical observations that each additional round yields diminishing
returns. An illustration of the Reflected CoT process is shown in Figure 3.


--- Page 20 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
20
Reflected
CoT
naturally
produces
full
reasoning
trajectories
in
the
format
(˜P,CoT0,M0,C0,CoT1,M1,C1,...,y∗), providing valuable supervision signals that capture both the
final solution and the iterative reasoning process. More importantly, it enables us to recover 19.1%
of samples that would otherwise be discarded, including approximately 20.8% of new problems
that fail to yield a correct solution in any of the initial attempts across all roles. This enrichment
introduces more challenging problems into the dataset, which are especially valuable for RL.
Further details and the full algorithm are provided in Algorithm 3 in the appendix.
Although backward generation ensures solution correctness, it inherently limits problem diver-
sity. All backward-generated problems, regardless of how many are synthesized or how much the
industry context varies, ultimately share the same model formulations MF as the 91 seed problems.
Consequently, the variety of underlying problem structures remains constrained by the original seed
formulations. To introduce new dynamics, constraints, and decision structures, forward generation
is essential: it enables the creation of novel problem formulations by adapting the mathematical
structure to new application contexts beyond those captured by the original seeds. Together, for-
ward and backward generation offer a balanced trade-off: the former expands problem diversity,
while the latter ensures solution quality. This trade-off is also reflected in the experimental results
discussed in Section 6.3. To illustrate the semantic relationships among the synthetic data, seed
data, and benchmark, we provide a t-SNE visualization in Appendix Section EC.5.
5.
Training Method
Relying exclusively on RL to equip a model with complex task-solving capabilities often results
in training instability, unpredictable outputs, and substantial computational overhead (e.g., Zheng
et al. 2023). In contrast, “cold-starting” the model with SFT has proved to be a more efficient
way to learn the required output format (e.g., Ouyang et al. 2022). Recent comparative studies
highlight that both SFT and RL are essential and play complementary roles: SFT memorizes
demonstrations, whereas RL improves out-of-distribution generalization. Thus, we first fine-tune
our model on a dataset containing complex reasoning trajectories and self-reflective CoT, enabling


--- Page 21 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
21
Figure 4
Overview of our training pipeline.
the model to internalize the structure of sophisticated solutions. We then apply RL-based alignment
to further refine the model’s capability to consistently produce correct answers. For this RL stage,
we compare two representative algorithms: GRPO (Shao et al. 2024) for online training and DPO
(Rafailov et al. 2023) for offline training. Figure 4 provides an overview of our training pipeline.
5.1.
Stage 1: Supervised Fine-Tuning
The SFT stage “cold-starts” the base model by pulling the initial policy π0 toward the human
instruction distribution via maximum-likelihood training. This narrows the search space for subse-
quent RL, mitigates early-stage mode collapse, and approximates minimizing KL(π∗∥π0), yielding
a smoother reward landscape and faster convergence.
Dataset DSFT. To support SFT, our DualReflect data synthesis framework produces 113K
training samples generated by GPT-4o. This includes 70K forward-generated samples, 34K
backward-generated samples that were correct on the first attempt without using reflected
CoT. They share the standard format (P,CoT∗,M∗,C∗). The remaining 8K backward samples
are generated using reflected CoT and contain a full reflection–revision trajectory of the form
(P,CoT0,M0,C0,...,CoT∗,M∗,C∗,y∗).
5.2.
Stage 2: Reinforcement Learning Alignment
SFT on synthetic trajectories trains the model to produce coherent answers but merely approxi-
mates the empirical distribution of these demonstrations. RL addresses this limitation by directly
optimizing a reward aligned with the correctness of the final numeric answer. Consequently, RL
enables the model to (1) explore reasoning paths beyond those available in the demonstrations and


--- Page 22 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
22
(2) mitigate residual biases inherited from the teacher model. In analogy, SFT provides a struc-
tured lesson plan teaching essential skills, whereas RL offers personalized coaching that refines how
these skills are practically applied.
The RL alignment process is made possible by our verifiable synthetic dataset, which supplies
a reliable reward signal. Specifically, our RL training leverages a curated corpus, DRL, designed to
provide verifiable supervision on challenging instances: (1) approximately 8,000 “hard-recovered”
problems from the backward-generation pipeline–initially unsolved by the teacher LLM but later
passing the STAR self-consistency filter after iterative correction; (2) the 91 textbook seed prob-
lems, each with verified numeric and code solutions. In total, DRL contains roughly 8,100 verified
problem–solution pairs.
The standard approach to RL alignment is Proximal Policy Optimization (PPO), which involves
sampling rollouts2, estimating advantage functions using a value critic, and maximizing a clipped,
KL-regularised surrogate objective. However, on thousand-token reasoning traces, PPO’s value
critic significantly increases memory usage, and the extensive credit-assignment horizon often desta-
bilizes training. To address these issues, we adopt two recent, more lightweight alternatives: (1)
DPO, which eliminates the need for both the value critic and online sampling by training directly
on static preference pairs, requiring only a single forward pass through the policy; (2) GRPO, which
removes the value critic by computing baselines internally within groups of k rollouts generated
from the same prompt.
DPO: Direct preference optimization. For a given problem (or prompt) x in DRL, we sample
several completions using a base policy (either the SFT model or the base model), execute them,
and pair one correct trajectory yw with one incorrect trajectory yl. Let Dpref denote the resulting
dataset of paired preferences (x,yw,yl). DPO directly minimizes a KL-constrained objective without
an explicit reward model:
LDPO = −E(x,yw,yl)∼Dpref
h
log σ
 β
 log πθ(yw | x) −log πθ(yl | x)
i
,
2 A rollout is the sequence of states, actions, and rewards generated as an agent interacts with an environment while
following a specific policy. It captures the trajectory of the agent’s experience.


--- Page 23 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
23
where σ is the sigmoid function and β controls the reward–KL trade-off. This objective is equivalent
to maximizing the same KL-regularized policy improvement targeted by PPO-based RLHF, but
achieves it several times faster while requiring only preference pairs.
GRPO: Group relative policy optimization. For a given prompt x, we first sample k candidate
completions {yi}k
i=1 from the current policy πcurrent and evaluate them using the reward function,
yielding scores {ri}k
i=1. We then compute a group–normalized advantage function Ai = ri−mean(r)
std(r)
,
and optimize the following clipped surrogate objective:
max
θ
Ex∼D, yi∼πcurrent(·|x)

min(ρiAi, clip(ρi,1 ± ε)Ai)

−β DKL (πθ ∥πref),
where ρi =
πθ(yi|x)
πcurrent(yi|x) is the importance weight between the updated policy and current policy,
and the operator clip(ρ,1 ± ε) = max
 1 −ε, min(ρ, 1 + ε)

constrains the policy ratio to stabilize
updates. The reference policy πref denotes the checkpoint used for KL regularization prior to the
start of RL. Because the baseline is computed internally within each group, GRPO eliminates the
need for a learned value critic. Compared to PPO, this reduces GPU memory usage by approxi-
mately 40% and leads to faster convergence on long-horizon reasoning tasks (Shao et al. 2024).
To facilitate verifiable training, we define the total reward as a composition of two sub-rewards:
r(x, ˆa,a∗) = rformat(x, ˆa) + ranswer(ˆa,a∗), where rformat reflects the syntactic and executable validity
of the model output ˆa, and ranswer measures its semantic correctness against the ground-truth a∗:
rformat(x, ˆa) =









0.2
if ˆa is executable and well-typed;
0
if ˆa = null or fails execution,
ranswer(ˆa,a∗) =









0.8
if ˆa = a∗;
0
otherwise.
This hierarchical design encourages the policy to first generate structurally valid formulations before
converging toward exact numeric correctness. The reward of 0.2 for executable-but-incorrect com-
pletions provides an intermediate learning signal that helps mitigate the sparse-reward bottleneck
of binary schemes, especially in multi-stage tasks such as DP formulation. However, setting this
intermediate reward too high may lead to “reward hacking”, where the model converges to syntac-
tically plausible but semantically invalid outputs. Following prior work such as Shao et al. (2024),


--- Page 24 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
24
we separate execution feedback from solution accuracy, which has been shown to improve train-
ing stability. This shaping design is especially aligned with DP auto-formulation, where outputs
must satisfy both structural and computational constraints. For instance, an incorrect formulations
may still contain valid subcomponents (e.g., a correct recurrence structure but incorrect indexing),
making partial credit a valuable inductive bias.
In conclusion, GRPO is trained online, continually updating the policy using fresh rollouts and
their associated reward signals.
6.
Experiments
We conduct extensive numerical experiments to evaluate our proposed framework for DP auto-
formulation. Specifically, we aim to answer three primary research questions: (1) How effective is
our training pipeline, and can our fine-tuned model outperform its much larger teacher model?
(2) What are the distinct scaling behaviors of the forward and backward generation methods? (3)
Are both SFT and RL necessary components, and what are their individual contributions to the
model’s final performance?
We begin by presenting our main results against leading baselines in Section 6.2. We then analyze
the scaling properties of the forward and backward generation methods in Section 6.3 and conduct
a detailed ablation study in Section 6.4. Complementary studies on model-size and inference scaling
are deferred to Appendix Sections EC.9 and EC.10.
6.1.
Experimental Setup
Tasks and Data Splits. All experiments are conducted on DP-Bench, our curated benchmark of
textbook–level DP problems described in Section 3. The benchmark contains 132 problems, divided
into 90 easy and 42 hard instances.
Models and Training Recipes. We choose Qwen-2.5-7B-Instruct after a comparison with other
7–10B checkpoints; the full results are deferred to Appendix Section EC.6. We initialize DPLM and
employ a two-stage pipeline consisting of SFT followed by RL alignment. Detailed hyperparameter
settings are provided in Appendix Section EC.8.1.


--- Page 25 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
25
Baselines. We benchmark against a diverse suite of open-source and closed-source LLMs, span-
ning two orders of magnitude in parameter count. The complete catalog, along with citations, is
provided in Appendix Section EC.8.2.
Hardware and Decoding Configuration. Key hardware specifications, decoding parameters, and
API details are summarized in Appendix Section EC.8.3.
Evaluation Metrics. We report pass@1 accuracy, which measures the proportion of problems
solved correctly on the first attempt, reflecting the model’s ability to provide direct, accurate
answers without retries. We also compute micro-average and macro-average accuracies: the micro-
average aggregates all examples across difficulty levels into a single pool, ensuring each problem
contributes equally to the final score, while the macro-average takes the mean accuracy across
the easy and hard sets, offering a balanced view of performance across difficulty levels. Extended
metrics (e.g., pass@10 and self-consistency) are presented as part of the inference scaling analysis
in Appendix Section EC.10.
6.2.
Main Results
The results presented in Table 2 show that our final model, DPLM-7B-SFT-GRPO, delivers highly
competitive performance, achieving the highest scores on the hard problem set (38.1%) and the
macro-average (51.9%). It is also a top contender on the easy set (65.6%) and in the micro-average
(56.8%), ranking second only to DeepSeek-R1 while outperforming all other baselines, including
o1, GPT-4o, and DeepSeek-V3. Importantly, it significantly exceeds GPT-4o, the model initially
used for synthetic data generation. This outcome effectively addresses the key question of whether
our synthesis and training procedures can produce models that outperform the teacher model.
Notably, even without RL, the DPLM-7B-SFT variant achieves a pass@1 rate of 38.9% on
easy problems and 21.4% on hard problems, substantially outperforming the base model
Qwen2.5-7B-Instruct, which achieves only 10.0% and 2.4% respectively. This demonstrates the
effectiveness of the DualReflect data synthesis process and the high quality of the resulting syn-
thetic dataset. Incorporating the GRPO RL stage significantly boosts the performance, highlighting
the benefits of RL-based alignment.


--- Page 26 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
26
Table 2
Performance comparison across models on DP auto-formulation tasks. For each metric, the top value
is highlighted in bold, and the second-best is underlined. Note that we use the gpt-4o-2024-08-06 version of
GPT-4o to match the version used for generating our synthetic training data, while all other models use their
originally released versions.
Type
Model
Parameters
Easy(%)
Hard(%)
Micro(%)
Macro(%)
Baseline
Large-Scale
o1
∗300B
57.8
31.0
50.0
44.4
GPT-4o
∗200B
45.6
19.0
37.1
32.3
DeepSeek-R1
671B
73.3
28.6
59.1
51.0
DeepSeek-V3
671B
51.1
26.2
43.2
38.7
Qwen-2.5-72B-Instruct
72B
41.1
19.0
34.1
30.1
Qwen-2.5-32B-Instruct
32B
35.6
19.0
30.3
27.3
Baseline
Small-Scale
Gemma-2-9B-It
9B
4.4
2.4
3.8
3.4
LLama-3.1-8B-Instruct
8B
7.8
2.4
6.1
5.2
Qwen-2.5-7B-Instruct
7B
10.0
2.4
7.6
6.2
Ours
DPLM-7B-SFT
7B
38.9
21.4
33.3
30.2
DPLM-7B-SFT-GRPO
7B
65.6
38.1
56.8
51.9
∗These are estimations (e.g., Ben Abacha et al. 2025), given that OpenAI has not publicly disclosed the
information.
While our model notably outperforms DeepSeek-R1 on the hard problem set, demonstrating par-
ticular strength in complex reasoning scenarios, it lags behind on easy problems. Further inference
scaling experiments (Appendix Section EC.10) suggest this shortfall likely comes from limited cov-
erage of specialized domain knowledge within our current dataset. Expanding the initial seed data
and scaling up the synthetic data generation process seem promising to overcome this limitation
and enhance future performance.
Finally, the relatively poor performance of general-purpose large model baselines on DP-Bench
highlights the inherent challenges of applying such models to specialized domains such as DP. This
highlights the need for custom-tailored models explicitly trained for domain-specific tasks.


--- Page 27 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
27
0K
1K
2K
4K
8K
16K
32K
Training Set Size
5
10
15
20
25
30
Accuracy (%)
Balancing Diversity and Correctness: Scaling Properties of Generation Methods
Forward
Backward
Hybrid
Qwen2.5-7B-Instruct (Baseline)
Figure 5
Performance comparison of forward, backward, and hybrid generation methods as the scale of training
data increases.
6.3.
Forward vs. Backward Generation for Data Scaling
In this subsection, we evaluate how scaling the training dataset affects model performance by
comparing forward and backward generation techniques. We randomly sample subsets of varying
sizes (1K, 2K, 4K, 8K, 16K, 32K) from a validated dataset and use these subsets for SFT. We also
compare both methods to a hybrid approach in which each dataset consists of 50% forward- and
50% backward-generated data.
Figure 5 demonstrates that all three approaches improve model performance as training size
increases, though they do so differently. As discussed in Section 4, forward generation enriches the
dataset with diverse problem formulations. While it offers limited improvement initially, forward
generation significantly enhances the model’s ability to generalize as the dataset size increases.
Conversely, backward generation emphasizes high-quality, verified solutions, which initially pro-
vides rapid performance gains even at smaller dataset sizes. However, this approach encounters
diminishing returns as the dataset expands, primarily due to its inherent limitation in diversity.
Expanding the initial seed dataset can mitigate this constraint. Nonetheless, the backward gen-
eration method is valuable for its ability to produce verifiable problems and solutions, a crucial
advantage given data scarcity.


--- Page 28 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
28
The performance of the hybrid approach validates the trade-off between diversity and correctness.
At smaller data scales, where correctness is the dominant factor, it is outperformed by the backward
method. However, as the dataset grows, the hybrid approach consistently outperforms both pure
methods. Overall, our experiments highlight that both the diversity and quality of training data
are essential for maximizing model performance.
6.4.
Ablation Study of Two-Stage Training
To isolate and understand the individual contributions of SFT and RL alignment, we train six
variants of the base model Qwen-2.5-7B-Instruct and compare their performance. All variants
are trained using identical datasets and hyperparameters to ensure comparability.
Table 3
Pass@1 accuracy across different training recipes. “Hard” and “Easy” refer to the 42 difficult and 90
easier problems, respectively, in DP-Bench.
Model
Easy (%) Hard (%) Micro (%) Macro (%)
Base-Model-7B (no further training)
10.0
2.4
7.6
6.2
DPLM-7B (SFT only)
38.9
21.4
33.3
30.2
DPLM-7B (DPO only)
23.3
9.5
18.9
16.4
DPLM-7B (GRPO only)
27.8
14.3
23.5
21.1
DPLM-7B (SFT →DPO)
48.9
21.4
40.2
35.2
DPLM-7B (SFT →GRPO)
65.6
38.1
56.8
51.9
The results in Table 3 show that SFT is by far the dominant driver of performance improvement.
A single two-epoch SFT pass boosts micro-average accuracy from 7.6% to 33.3%, a gain of over 25
percentage points. In contrast, the RL-only variants achieve substantially lower performance: 23.5%
with GRPO and 18.9% with DPO, indicating that reward-driven training alone is not competitive
with high-quality supervision. One possible explanation is that a 7B-parameter base model lacks
the capacity to reliably discover high-quality reasoning trajectories from scratch; under constrained
training budgets, SFT is thus markedly more efficient than RL.


--- Page 29 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
29
Although RL alone underperforms SFT, it remains a valuable refinement step. When initialized
from the SFT checkpoint, GRPO raises accuracy to 56.8%, while DPO also improves, albeit to a
lower 40.2%. Across all configurations, GRPO consistently outperforms DPO in accuracy. However,
this advantage comes with a substantial computational cost: on identical hardware (eight H100-
80GB GPUs), GRPO requires approximately eight times more wall time due to the need for
on-policy rollout generation and group-normalized advantage computation.
7.
Conclusion
This paper explores the auto-formulation of DP problems using LLMs, a critical yet unexplored step
toward fully automating end-to-end sequential decision-making under uncertainty. We introduce
DP-Bench, the first benchmark for this task, featuring 132 textbook-level problems with standard-
ized metrics to evaluate LLM’s ability to translate natural language into formal DP models.
We present DPLM, a specialized 7B-parameter LLM trained from scratch on synthetic data dis-
tilled from GPT-4o. Despite its smaller size, DPLM significantly outperforms its teacher model and
rivals larger SOTA models including DeepSeek-R1 and OpenAI’s o1, particularly on hard problems.
This highlights the promise of domain-specific, small-scale LLMs for DP auto-formulation.
To support model training, we propose DualReflect, a data generation framework that balances
diversity and accuracy by combining forward and backward generation. A key component is the
Reflected CoT mechanism, which recovers difficult problems that would otherwise be discarded
and enables reflect-and-refine solution trajectories. Our empirical results reveal a new insight for
data synthesis: backward generation is preferred at small scales, while adding forward generation
improves performance as data scales.
Overall, our work demonstrates the potential of LLMs for DP automation and offers a practical
roadmap for fine-tuning domain-specific models in data-scarce settings with limited supervision.
References
Ahmaditeshnizi A, Gao W, Udell M (2024) OptiMUS: Scalable optimization modeling with (MI)LP solvers
and large language models. Proceedings of the 41st International Conference on Machine Learning,
volume 235 of Proceedings of Machine Learning Research, 577–596 (PMLR).


--- Page 30 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
30
Alfarano A, Charton F, Hayat A (2024) Global lyapunov functions: a long-standing open problem in math-
ematics, with symbolic transformers. Advances in Neural Information Processing Systems, volume 37,
93643–93670 (Curran Associates, Inc.).
Ao R, Luo G, Simchi-Levi D, Wang X (2025) Optimizing llm inference: Fluid-guided online scheduling with
memory constraints. URL https://arxiv.org/abs/2504.11320.
Ben Abacha A, Yim Ww, Fu Y, Sun Z, Yetisgen M, Xia F, Lin T (2025) Medec: A benchmark for medical
error detection and correction in clinical notes. arXiv preprint arXiv:2412.19260 .
Bertsimas D, Margaritis G (2024) Robust and adaptive optimization under a large language model lens.
arXiv preprint arXiv:2501.00568 .
Bray RL (2025) A tutorial on teaching data analytics with generative AI. INFORMS Journal on Applied
Analytics Forthcoming.
Chen Y, Kirshner SN, Ovchinnikov A, Andiappan M, Jenkin T (2025) A manager and an ai walk into a
bar: does chatgpt make biased decisions like we do? Manufacturing & Service Operations Management
27(2):354–368.
Guo D, Yang D, Zhang H, Song J, Zhang R, Xu R, Zhu Q, Ma S, Wang P (2025) Deepseek-r1: Incentivizing
reasoning capability in llms via reinforcement learning. arXiv:2501.12948 .
Huang C, Tang Z, Ge D, Hu S, Jiang R, Wang B, Wang Z, Zheng X (2025) Orlm: A customizable framework
in training large models for automated optimization modeling. Operations Research Fothcoming.
Jaillet P, Jiang J, Mellou K, Molinaro M, Podimata C, Zhou Z (2025) Online scheduling for llm inference
with kv cache constraints. URL https://arxiv.org/abs/2502.07115.
Lample G, Charton F (2019) Deep learning for symbolic mathematics, http://arxiv.org/abs/1912.01412.
Li A, Chen H, Namkoong H, Peng T (2025a) Llm generated persona is a promise with a catch. URL
https://arxiv.org/abs/2503.16527.
Li C, Xu X, Bergemann D, Fang EX, Wei Y, Yang Z (2025b) Coopa: A modular llm agent architecture for
operations research problems. Duke Fuqua Working Paper.
Li Y, Dai J, Peng T (2025c) Throughput-optimal scheduling algorithms for llm inference and ai agents, arXiv
preprint arXiv:2504.07347.


--- Page 31 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
31
Liang K, Lu Y, Mao J, Sun S, Yang C, Zeng C, Jin X, Qin H, Zhu R (2025) LLM for large-scale optimiza-
tion model auto-formulation: A lightweight few-shot learning approach. URL http://dx.doi.org/10.
2139/ssrn.5329027.
Lu H, Xie Z, Wu Y, Ren C, Chen Y, Wen Z (2025) Optmath: A scalable bidirectional data synthesis
framework for optimization modeling. arXiv preprint arXiv:2502.11102 .
Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, Zhang C, Agarwal S, Slama K (2022)
Training language models to follow instructions with human feedback. arXiv:2203.02155 .
Phan L, et al. (2025) Humanity’s last exam. arXiv preprint arXiv:2501.14249 .
Powell WB (2011) Approximate Dynamic Programming: Solving the Curses of Dimensionality (Hoboken,
NJ: John Wiley & Sons, Inc.), second edition.
Puterman ML (2005) Markov Decision Processes: Discrete Stochastic Dynamic Programming (Hoboken, NJ:
John Wiley & Sons, Inc.).
Rafailov R, Sharma A, Mitchell E, Ermon S, Manning CD, Finn C (2023) Direct preference optimization:
Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 .
Ramamonjison R, Yu T, Li R, Li H, Carenini G, Ghaddar B, He S, Mostajabdaveh M, Banitalebi-Dehkordi
A (2022) Nl4opt competition: Formulating optimization problems based on their natural language
descriptions. Ciccone M, Stolovitzky G, Albrecht J, eds., Proceedings of the NeurIPS 2022 Competitions
Track, volume 220 of Proceedings of Machine Learning Research, 189–203 (PMLR).
Shao Z, Wang P, Zhu Q, Xu R, Song J, Bi X, Zhang H, Zhang M, Li YK (2024) Deepseekmath: Pushing the
limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300.
Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche G, Schrittwieser J, Antonoglou I,
Panneershelvam V (2016) Mastering the game of go with deep neural networks and tree search. Nature
529(7587):484–489.
Sutton RS, Barto AG (2018) Reinforcement Learning: An Introduction (Cambridge, Massachusetts and
London, England: The MIT Press), second edition.
Wang SM, Zhang D, Zhang H (2024) Large language models for market research: A data-augmentation
approach. URL http://dx.doi.org/10.2139/ssrn.5057769.


--- Page 32 ---
Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
32
Wang Y, Kordi Y, Mishra S, Liu A, Smith NA, Khashabi D, Hajishirzi H (2022) Self-instruct: Aligning
language models with self-generated instructions. arXiv preprint arXiv:2212.10560 .
Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi E, Le Q, Zhou D (2023) Chain-of-thought
prompting elicits reasoning in large language models. URL https://arxiv.org/abs/2201.11903.
Winston WL (2004) Operations Research: Applications and Algorithm (Belmont, CA: Thomson Learning,
Inc.), fourth edition.
Yang Z, Wang Y, Huang Y, Guo Z, Shi W, Han X, Feng L, Song L, Liang X (2024) Optibench meets
resocratic: Measure and improve llms for optimization modeling, arXiv preprint arXiv:2407.09887.
Ye Z, Yoganarasimhan H, Zheng Y (2025) Lola: Llm-assisted online learning algorithm for content experi-
ments. Marketing Science Forthcoming.
Zelikman E, Wu Y, Mu J, Goodman N (2022) Star: Bootstrapping reasoning with reasoning. Advances in
Neural Information Processing Systems 35:15476–15488.
Zheng R, Dou S, Gao S, Hua Y, Shen W, Wang B, Liu Y, Jin S, Liu Q (2023) Secrets of rlhf in large language
models part i: Ppo. arXiv preprint arXiv:2307.04964 .


--- Page 33 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec1
Electronic Companion for “Auto-Formulating Dynamic
Programming Problems with Large Language Models”
EC.1.
Examples
Example: Computer Science–Style DP Problem
The following example is taken directly from a Leetcode problem3 categorized as “Hard” in their
Dynamic Programming section. It illustrates a typical computer science-style DP problem: concise
in structure, formally specified, and minimally ambiguous. In contrast, OR/OM problems often
lack such clarity and involve implicit assumptions.
Example EC.1. Given a string containing just the characters ‘(’ and ‘)’, return the length of
the longest valid (well-formed) parentheses substring.
Example: Category, Scenario and Characteristics
Example EC.2. Category: Thermal Plant Operational Scheduling. Scenario: Scheduling
operations for coal plants during peak demand in India. Characteristics: Focuses on minimizing
operational costs while meeting energy demands.
EC.2.
Mathematical Formulation of DP Problems
We describe each component in Equation (1) in more detail. The planning horizon is denoted by
T, which may be either finite (T < ∞) or infinite (T = ∞). S denotes the set of all possible states,
and A denotes the set of all possible actions across all decision periods. At each decision period
t = 1,...,T, the system occupies a state st and we denote by St ⊆S the subset of states that are
reachable at decision period t. Similarly, when the decision-maker observes the system in state
st ∈St, the decision-maker selects an action at from the set Ast,t. So A = S
t=1,...,T
S
st∈St Ast,t. After
a result of choosing action at ∈Ast,t, the decision-maker receives an immediate reward rt(st,at), and
the system state at the next decision period is described by the probability distribution pt(·|st,at).
Finally, γ represents the discount factor.
3 Link: https://leetcode.com/problems/longest-valid-parentheses/description/


--- Page 34 ---
ec2
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
A admissible policy π = {d1,d2,...,dN−1} is a sequence of decision rules, where each decision
rule dt specifies which action to take given the observed state st, i.e., at = dt(st) ∈Ast,t. In infinite-
horizon settings, stationary policies, where dt = d for all t, are often of interest. The set of all
admissible policies is denoted by Π
The sequence of events at each decision period t occurs as follows. First, the decision-maker
observes the current state st. Then, according to the chosen policy, the decision-maker selects an
action at = dt(st). After the action is selected, the random state transition is realized according to
pt(·|st,at), resulting in a new state st+1. Concurrently, an immediate reward rt(st,at) is incurred.
For finite-horizon problems, the performance of a policy can be evaluated via the Bellman opti-
mality equation. The value function vt(st), which represents the maximum expected cumulative
reward from time t onward, satisfies the optimality equation
vt(st) = sup
a∈Ast,t

rt(st,a) +
X
j∈St+1
pt(j|st,a)vt+1(j)


for t = 0,1,...,T −1,
with the boundary condition vT(sT) = rT(sT), and rT(sT) is the terminal value. The goal of the
DP formulation is to identify an optimal policy π∗such that maximizes the initial-stage reward
v1(s1) among the set of all admissible policies Π.
In infinite-horizon problems, we focus primarily on stationary policies. Accordingly, we slightly
abuse notation by omitting the time index and working with action sets As and state space S
instead of Ast,t and St. In the discounted infinite-horizon setting with discount factor γ ∈(0,1), we
seek a stationary value function v that maximizes the expected discounted total reward, i.e.,
v(s) = sup
a∈As
"
r(s,a) + γ
X
j∈S
p(j|s,a)v(j)
#
,
where r(s,a) is the one-stage reward function.
For the long-run average infinite-horizon cost problem, the performance of a policy π is evaluated
by the long-run average cost:
vπ(s1) = limsup
T →∞
1
T E
" T
X
t=1
r(sπ
t ,aπ
t )
#
.
The objective in this setting is to determine an optimal policy π∗that maximizes the average
reward for a given initial state s1 ∈S.


--- Page 35 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec3
EC.3.
Benchmark Details
EC.3.1.
Guidelines for Modifying Benchmark Textbook Questions
(a) In the textbooks, some problems are first introduced as examples to illustrate modeling and
then appear in exercises with specific parameter values or additional assumptions. We merged
these versions, incorporating modifications to focus on the main problem and eliminate unneces-
sary modeling details provided as examples.
(b) While optimal policy is sometimes more important then the optimal value in many DP prob-
lems, for benchmarking purposes we need clear evaluation criteria. Therefore, we modified problems
that originally required non-scalar answers to focus on computing the optimal value, a scalar met-
ric, rather than deriving the more complex optimal policy.
(c) It is common in exercises to present different variants of the same problem with only slight
parameter changes. In such cases, we retained versions where the modeling differed and removed
those with only minor parameter adjustments.
(d) There are some ambiguities in certain problems, particularly regarding the time sequence of
events and the timing of cost computations. To resolve these ambiguities, we clarify the required
time sequence.
(e) A few problems may include a hint, typically regarding how to model the space and actions.
For example, in Problem 4.33 in Puterman (2005), the hint is written as follows: “(Hint: Write a
system equation that describes the system state at each review period as a function of the system
state from the previous review period and the daily demand for each day between reviews. Note
further that the two-day demand can be found by computing the convolution of the demand dis-
tribution with itself. Solving this problem involves writing a computer program.)” We retain any
provided hints but will not add new hints to any problems.
EC.3.2.
Benchmark Annotation and Labeling Details
Figure EC.1 shows the distribution of problems across common application scenarios in our easy
and hard benchmarks. Overall, the two sets share similar top-level coverage. Both include over one-
fourth of problems in manufacturing and inventory management and another quarter in investment


--- Page 36 ---
ec4
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
Figure EC.1
Application scenario proportions: Easy vs. Hard.
Table EC.1
Proportion of DP-Bench problems with additional features.
Label
Easy (%) Hard (%)
action-dependent transition probability
22.22
66.67
optimal stopping problem
4.44
14.29
truncation-required state space
4.44
11.90
time-dependent state space
4.44
7.14
continuous or non-integer state space
2.22
7.14
and risk management, reflecting the typical emphasis in OR/OM curricula and textbooks. The
remaining categories show varying proportions, with small shifts in areas such as transportation
and logistics, and game theory.
Furthermore, as shown in Table EC.1, the hard benchmark contains a higher proportion of prob-
lems with structural labels indicating either unique modeling formulations or increased complexity,


--- Page 37 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec5
such as action-dependent transition probability, optimal stopping problem, truncation-required state
space, time-dependent state space, and continuous or non-integer state space.
EC.4.
Data Generation Details
EC.4.1.
Implementation Details of Forward Generation
Step 1: Generating the problem from the seed problem. The new problem description ˜P
is generated by prompting an LLM to adapt the original P from a seed problem to a new scenario,
as defined in Section 4.1. The prompt instructs the model to preserve core elements while making
context-specific adjustments. As a result, the model formulation
˜
MF in the adapted problem ˜P
often differs from that of the seed. This approach is effective when the seed problem’s domain
aligns with the target scenario (e.g., adapting an inventory problem to a manufacturing context). In
contrast, mapping to a conceptually distant domain, such as investment, is more likely to produce
incoherent or ill-posed problem descriptions. To ensure novelty and correctness, we apply filter
steps to remove Ps that are either too similar to their seed source or exhibit clear logical flaws.
Step 2: Solution generation. For each ˜P, we generate the solution r = (CoT,M,C) along with
the final answer y using the RAG-based method introduced in Section 4.2.
Step 3: Solution refinement and filtering. Since we cannot directly verify the correctness of
DP solutions, we apply two techniques to improve reliability: code refinement and majority voting.
Most DP implementations rely on simple loop-based logic rather than specialized solvers, but edge
cases, such as unbounded state spaces or indexing errors, can still trigger execution failures. When
such errors occur, we prompt the LLMs to reflect on common coding mistakes and regenerate its
output. This error-aware refinement step not only improves execution success but also expands
the candidate pool for majority voting. For each P, we run the RAG-based solution generator five
times, each with a distinct expert persona (e.g., a DP expert or an OR student; see details in
Section EC.4.2). A sample is retained only if the answer computed by its code matches the majority
of reasonable answers across the five generations. This ensemble approach not only improve the
reliability of the forward-generated training data samples but also provides a valuable signal for
downstream filtering: the distribution of answers across agents serves as a proxy for confidence
estimation and difficulty assessment of the data sample.


--- Page 38 ---
ec6
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
EC.4.2.
Algorithms
This section presents the pseudocode for the forward and backward generation algorithms in Algo-
rithms 2 and 3, respectively. Both rely on a shared subroutine, SolveDP, provided in Algorithm 1,
to generate solutions. We also introduce the hyperparameters associated with these procedures.
For self-consistency, we generate solutions in both the forward and backward processes using
K = 5 role-based agents, each simulating a different reasoning perspective: a DP expert (balanced
across problem types), an OR professor (produces more instructive chain-of-thought reasoning),
an OR student (writes more detailed models and code, mimicking homework-style solutions), a
researcher in decision-making under uncertainty (well-suited for stochastic DPs), and an MDP
specialist (effective on infinite-horizon problems), indexed by k = 1,...,5. These agents perform
majority voting to enhance solution accuracy and robustness, correcting errors and encouraging
diverse reasoning paths that converge on the correct answer.
We generate over 400 detailed scenarios, denoted by S. Each seed sample from Dseed is paired
with a scenario to create a new problem instance. To further expand the dataset, this pairing
process is repeated R times using a temperature of 0.7 to encourage diverse outputs.
In the backward generation process, when producing reflected CoT reasoning, we set an upper
bound P on the number of times the LLM compares its newly generated solution with the provided
reference solution. This is motivated by two observations. First, inconsistencies often arise not
from the LLM’s inability to solve the problem, but from flaws in the generated problem statement.
Second, our experiments show diminishing returns with repeated reflections, as the quality of
updated solutions tends to degrade over time. To balance effectiveness and cost, we set P = 6.


--- Page 39 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec7
Algorithm 1 SolveDP
Require:
Problem
description
P,
Chain-of-thought
CoT,
few-shot
examples
Efs =
{(Pfs,CoTfs,Mfs,Cfs,labelsfs)}, role index k
Ensure: (M,C,y)
1 M ←promptM(k)(P,CoT,Efs)
C ←promptC(k)(P,M,Efs)
y ←execute(C)


--- Page 40 ---
ec8
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
Algorithm 2 Forward Generation of Synthetic DP Data
Require: Seed set Dseed = {(Pi,MFi,Ci)}n
i=1, Seed examples E = {(Pe,CoTe,Me,Ce,labelse)},
scenarios S = {SCEj}m
j=1, repeats R, K roles
Ensure: Forward-generated dataset Dforward
1 foreach (Pi,MFi,Ci) ∈Dseed do
2
foreach SCEj ∈S do
3
for r = 1 to R do
4
˜Pijr ←promptForwardP(Pi,SCEj)
if ˜Pijr passes similarity and validity checks then
5
labels ←promptLabelAssign(˜Pijr)
Eijr ←ExamplesRetrieve(˜Pijr,E,labels)
for role k = 1 to K do
6
CoT(k)
ijr ←promptCoT(k)(˜Pijr,Eijr)
(M(k)
ijr,C(k)
ijr,y(k)
ijr) ←SolveDP(˜Pijr,CoT(k)
ijr,Eijr,role k)
if C(k)
ijr executable with error then
7
C(k)
ijr ←promptRefineCode(P(k)
ijr,C(k)
ijr,Code Error)
y(k)
ijr ←execute(C(k)
ijr)
8
y∗
ijr ←MajorityVote({y(k)
ijr}K
k=1)
if y∗
ijr is valid then
9
for k = 1 to K do
10
if y(k)
ijr = y∗
ijr then
11
add (˜Pijr,CoT(k)
ijr,M(k)
ijr,C(k)
ijr,y(k)
ijr) to Dforward


--- Page 41 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec9
Algorithm 3 Backward Generation of Synthetic DP Data
Require: Seed set Dseed = {(Pi,MFi,Ci)}n
i=1, Seed examples E = {(Pe,CoTe,Me,Ce,labelse)}, sce-
narios S = {SCEj}m
j=1, repeats R, K roles, P reflection attempts
Ensure: Backward-generated dataset Dbackward
1 foreach (Pi,MFi,Ci) ∈Dseed do
2
foreach SCEj ∈S do
3
for r = 1 to R do
4
˜Cijr ←Perturb(Ci); y∗
ijr ←execute(˜Cijr)
if y∗
ijr is valid then
5
˜Pijr ←promptBackwardP(˜Cijr,SCEj)
if ˜Pijr passes similarity and validity checks then
6
labels ←promptLabelAssign(˜Pijr)
Eijr ←ExamplesRetrieve(˜Pijr,E,labels)
for role k = 1 to K do
7
CoT0,(k)
ijr
←promptCoT(k)(˜Pijr,Eijr)
(M0,(k)
ijr ,C0,(k)
ijr ,y0,(k)
ijr ) ←SolveDP(˜Pijr,CoT0,(k)
ijr ,Eijr,role k)
8
if y0,(k)
ijr
= y∗
ijr then
9
add (˜Pijr,CoT0,(k)
ijr ,M0,(k)
ijr ,C0,(k)
ijr ,y∗
ijr) to Dbackward
10
else
11
for p = 1 to P do
12
CoTp,(k)
ijr
←promptCoTReflect(˜Pijr,Cp−1,(k)
ijr
, ˜Cijr,Eijr)
(Mp,(k)
ijr ,Cp,(k)
ijr ,yp,(k)
ijr ) ←SolveDP(˜Pijr,CoTp,(k)
ijr ,Eijr,role k)
if yp,(k)
ijr
= y∗
ijr then
13
add
(˜Pijr,CoT0,(k)
ijr ,M0,(k)
ijr ,C0,(k)
ijr ,...,CoTp,(k)
ijr ,Mp,(k)
ijr ,Cp,(k)
ijr ,y∗
ijr)
to
Dbackward; break


--- Page 42 ---
ec10
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
EC.5.
Visualizing Data Distributions with t-SNE
To illustrate the semantic relationships among the synthetic data generated by our DualReflect
framework, the seed data, and the benchmark, we conduct a t-SNE (t-distributed Stochastic Neigh-
bor Embedding) visualization, as shown in Figure EC.2. Each problem description is embedded
into a high-dimensional vector using a pre-trained language model and then projected into two
dimensions using t-SNE, which places semantically similar problems closer together.
The visualization confirms that the synthetic data cloud (forward and backward points) effec-
tively envelops the original seed and evaluation benchmark points. This demonstrates that our
generated data is highly relevant to the core tasks and successfully spans their semantic space.
A key insight from the plot is the differing distributions of the two synthetic generation methods.
The backward-generated data (orange) tends to form denser clusters as these problems, though var-
ied in parameters, are constrained by the fixed model structures of the original seeds. In contrast,
the forward-generated data (blue) is more dispersed, filling the space between clusters and extend-
ing toward the periphery. This pattern highlights the forward method’s strength in creating more
new problem formulations beyond the initial seeds, thereby enhancing the overall diversity of the
training set. This visual evidence reinforces the value of our dual-generation approach: backward
generation ensures fidelity, while forward generation drives diversity.
EC.6.
Base Model Selection
Before committing to a full training run, we conduct a small-scale study to identify a strong yet
computationally affordable starting checkpoint. Given our hardware budget and the need for rapid
iteration, we limit the search to open-source models in the 7–10B parameter range. Specifically, we
compare Gemma-2-9B-It, Llama-3.1-8B-Instruct, and Qwen-2.5-7B-Instruct. Each candidate
is evaluated (i) in its raw, pre-trained form, and (ii) after two epochs of SFT on our 113K synthetic
DP trajectories.4 Performance is measured by the micro-average pass@1 accuracy on DP-Bench.
As summarized in Table EC.2, Qwen-2.5-7B-Instruct emerges as the best performer in both
settings, and we adopt it as the backbone for all subsequent experiments.
4 The training hyperparameters mirrored those reported in Section 5.1.


--- Page 43 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec11
Figure EC.2
t-SNE visualization of the problem description distributions across datasets.
Table EC.2
Preliminary screening of candidate foundation models. Values report micro-average pass@1 (%) on
DP-Bench. For each metric, the highest value is highlighted in bold, and the second-highest is underlined.
Model
Parameters Base +SFT
∆
Gemma-2-9B-It
9B
3.8
27.2
23.4
Llama-3.1-8B-Instruct
8B
6.1
28.8
22.7
Qwen-2.5-7B-Instruct
7B
7.6
33.3 25.7
EC.7.
ORLM Baseline on DP-Bench
A natural question arises: Can a general optimization-focused LLM, such as ORLM (Huang et al.
2025), be directly repurposed to achieve strong performance on DP auto-formulation tasks? To
explore this, we evaluate the recently released ORLM (i.e., the ORLM-Llama-3-8B version), one of
the best-performing vertically trained models for optimization auto-formulation, on DP-Bench.
Despite its impressive performance on LP and MIP tasks as reported by Huang et al. (2025),
ORLM performs poorly on DP tasks. As shown in Table EC.3, it achieves only 0.8% pass@1 and
8.3% pass@10 micro-average accuracy. This underperformance likely comes from ORLM’s training


--- Page 44 ---
ec12
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
paradigm, which is heavily based on LP/MIP problems and encourages translating optimization
tasks (including DP) into LP formulations to be solved by external solvers. However, this rigid
pipeline results in performance that lags behind even general-purpose LLMs. The sharp drop
highlights a critical gap: techniques that excel in algebraic optimization do not seamlessly transfer
to sequential, stochastic decision-making problems, where success depends on navigating a larger
search space and correctly formulating Bellman recursions and state transitions.
Table EC.3
ORLM performance on DP-Bench.
Model
Parameters Easy(%) Hard(%) Micro(%) Macro(%)
ORLM-LLaMA-3-8B (pass@1)
8B
1.1
0.0
0.8
0.6
ORLM-LLaMA-3-8B (pass@10)
8B
11.1
2.4
8.3
6.8
DPLM-7B-SFT-GRPO (pass@1)
7B
65.6
38.1
56.8
51.9
EC.8.
Additional Experimental Details
EC.8.1.
Training Hyperparameters
(1) SFT. The model is fine-tuned for two epochs on 113K synthetic trajectories generated by
DualReflect using a batch size of 256, a learning rate of 1 × 10−5, a weight decay of 0.1, and a
total of two epochs.
(2) RL alignment. We explore two alignment methods: GRPO and DPO. For GRPO, we generate
k = 4 rollouts per prompt, set the KL coefficient β = 0.01, and conduct 5K training updates. For
DPO, we train for three epochs on 180K preference pairs with β = 0.1. Subsequent ablation studies
maintain these training parameters as consistently as possible across methods. For larger models
(14B and 32B parameters), we reduce the learning rate to 2×10−6 to achieve more stable training
dynamics. Additional details are provided in their respective sections.
EC.8.2.
Baseline Model Catalog
We compare DPLM against the following SOTA language models.
(1) Open-source baselines. (a) DeepSeek Series: DeepSeek-R1 671B and DeepSeek-V3 671B.


--- Page 45 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec13
(b) Qwen Series: models ranging from 0.5B to 72B parameters. (c) Other leading models:
Llama-3.1-8B-Instruct and Gemma-2-9B-It.
(2) Closed-source baselines. (a) ChatGPT (GPT-4o) and (b) o1, a proprietary 300B-parameter
reasoning model from OpenAI.
EC.8.3.
Hardware and Decoding
Both training and inference are conducted on a single node equipped with eight NVIDIA H100
GPUs (80GB each), interconnected via NVLink. During inference, we use nucleus sampling with
top-p = 0.95, temperature T = 0.7, and a maximum generation length of 8,192 tokens. Because
local deployment is not feasible for closed-source large models, we access these models remotely
via the provider’s cloud-based API. To ensure a fair comparison, we match the decoding parame-
ters for open-source models. However, since closed-source models run remotely, all other real-time
engineering and system-level configurations follow the provider’s default settings.
EC.9.
Model-Size Scaling
Table EC.4 reports the pass@1 accuracy of Qwen-2.5 models ranging from 0.5B to 32B parameters,
evaluated before and after two epochs of SFT on our 113K textbook–trajectory corpus. Overall,
performance improves sharply up to 7B parameters, with more gradual gains beyond that point.
According to established scaling laws, model quality improves predictably when model size,
dataset size, and compute are scaled in balance; if any one of these factors lags, it constrains the
benefits of the others. Our results suggest that model capacity is the primary bottleneck below
7B parameters. However, between 7B and 32B, the limiting factor shifts to the volume of training
data. While additional data can still improve performance in this regime, the model’s ability to
effectively leverage it becomes the limiting factor. Thus, a more effective path to further accuracy
gains is to co-scale model size and training data in tandem.
EC.10.
Inference Scaling Analysis
Sampling multiple candidate solutions at inference time and selecting the best-performing one has
proved effective for improving accuracy (e.g., Wei et al. 2023, Guo et al. 2025, Huang et al. 2025).


--- Page 46 ---
ec14
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
Table EC.4
“Base” refers to the raw pre-trained model; “+SFT” denotes two epochs of SFT (113K
trajectories). ∆denotes the improvement from SFT.
Parameters
Easy (%)
Hard (%)
Micro (%)
Macro (%)
Base +SFT
∆Base
+SFT
∆
Base
+SFT
∆
Base
+SFT
∆
0.5 B
0.0
8.9
+8.9
0.0
0.0
0.0
0.0
6.1
+6.1
0.0
4.5
+4.5
1.5 B
3.3
15.6
+12.3
0.0
2.4
+2.4
2.3
11.4
+9.1
1.7
9.0
+7.3
3 B
4.4
25.6
+21.2
0.0
4.8
+4.8
3.0
18.9
+15.9
2.2
15.2
+13.0
7 B
10.0
38.9 +28.9
2.4
21.4 +19.0
7.6
33.3 +25.7
6.2
30.2 +24.0
14 B
24.4
48.9
+24.5
9.5
23.8
+14.3
19.7
40.9
+21.2
17.0
36.4
+19.4
32 B
35.6
55.6
+20.0
19.0
28.6
+9.6
30.3
47.0
+16.7
27.3
42.1
+14.8
Understanding how accuracy scales with the number of generated samples helps practitioners
allocate computational resources more efficiently.
Experimental Setup. For each test instance, we generate k ∈{1,...,16} independent completions
from DPLM using nucleus sampling (p=0.95) and a moderate temperature (T =0.9). Each generated
completion is executed, and failures are recorded as null. Figure EC.3 reports two evaluation
metrics: pass@k, which measures whether at least one of the k completions is correct, and self-
consistency@k, which uses majority voting over the k generated answers.
Accuracy Scaling on the Easy Problem Set. For the 90 easy problems, accuracy increases rapidly
before plateauing. Sampling just four candidate solutions captures 93.4% of the total improvement
attainable at k = 16, and beyond eight samples, the marginal gain falls below one percentage point.
Self-consistency@k consistently lags behind pass@k by approximately three percentage points,
suggesting that most Easy instances have a single dominant or “canonical” solution. As a result,
generating additional diverse samples offers little benefit once this solution is identified. Another
reason for the relatively shallow improvement curve is that DPLM already achieves a high pass@1
accuracy (65.4%) on the easy problem set, leaving limited room for further improvement.


--- Page 47 ---
e-companion to Zhou et al.: Auto-Formulating Dynamic Programming Problems with Large Language Models
ec15
Figure EC.3
Test-time scaling for our DPLM model on DP-Bench Easy (left) and Hard (right): solid lines represent
DPLM pass@k, dashed lines represent DPLM self-consistency@k, and horizontal dotted lines indicate
the pass@1 accuracy of DeepSeek-R1 and GPT-4o for reference. The x-axis denotes the number of
sampled completions k, and the y-axis reports accuracy.
Accuracy Scaling on the Hard Problem Set. The 42 Hard problems exhibit a markedly different
scaling pattern. Accuracy increases approximately logarithmically, with pass@k more than dou-
bling from 22.1% to 47.6% as k increases from 1 to 16. Remarkably, just four samples are sufficient
to surpass DeepSeek-R1’s pass@1 performance, and twelve samples nearly match the pass@1 level
achieved on the Easy set. In contrast, self-consistency@k plateaus at around 26%, suggesting that
repeated sampling from the same model often yields similar reasoning paths and fails to cor-
rect early-stage, execution-level errors. The significant gap between pass@k and self-consistency@k
highlights that Hard problems benefit from exploring qualitatively diverse solution approaches,
making execution-based selection crucial for effectively leveraging this diversity.
