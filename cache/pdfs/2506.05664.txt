--- Page 1 ---
arXiv:2506.05664v1  [cs.LG]  6 Jun 2025
BAQ: Efficient Bit Allocation Quantization for Large
Language Models
Chao Zhang
Central South University, China
Khalifa University, UAE
Li Wang
Central South University, China
Samson Lasaulce
Khalifa University, UAE
CNRS, France
Merouane Debbah
Khalifa University, UAE
Abstract
Post-training model quantization is a widely adopted technique for reducing the
memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments, failing
to account for the nonuniform sensitivity of weights to quantization noise. In this
paper, we propose a novel framework for allocating quantization bitwidths based
on sensitivity metrics derived from a Hessian proxy. We make key assumptions,
which allow the layer/component-wise loss function to be expressed as an explicit
function of the bitwidths. This enables a neat formulation of the bit allocation
problem as a convex optimization task, whose closed-form solution adapts preci-
sion across weights to minimize the layer-wise quantization loss. Inspecting the
solution provides several insights (such as the equal-loss structure), which are then
exploited to design the proposed BAQ (Bit Allocation Quantization) algorithm.
The proposed algorithm achieves a good trade-off between loss minimization and
complexity and allows BAQ to be integrated into standard quantization pipelines
with minimal overhead. Experimental results show that BAQ consistently outper-
forms GPTQ, achieving up to 56× lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also provide a
theoretical explanation for the observed gains. All codes of this paper are available
at https://github.com/CSU-ModelCompression/BAQ.
1
Introduction
Large language models (LLMs) have achieved remarkable success across a wide range of natural
language processing tasks [1]. However, their immense scale poses significant challenges for
deployment in resource-constrained environments. Model quantization is one of the key techniques to
compress large neural networks (NNs) and thus for mitigating memory and compute costs. The current
literature shows how it is now possible to quantize large NN parameters with low-bit representations
(e.g., INT4) while maintaining good performance [2][3][4].
Despite promising progress, most post-training quantization methods rely on uniform or heuristic
bitwidth assignments, often treating all weights or all layers equally. This neglects the fact that
different model components exhibit vastly different sensitivities to quantization noise, particularly
in overparameterized models like LLMs. While recent methods such as GPTQ [3] and QuIP [5]
leverage second-order information (e.g., a proxy Hessian) to guide weight rounding, they typically
employ fixed-bit quantization, leaving the problem of how to optimally allocate a given bit budget
across weights largely unaddressed.
Preprint. Under review.


--- Page 2 ---
In this paper, we propose BAQ (Bit Allocation Quantization), a principled bit allocation formulation
for weight-only quantization that explicitly minimizes the expected quantization-induced loss under
a global bit budget. We leverage this formulation to derive a closed-form, layer-wise bit allocation
rule. Interestingly, we show that this optimal allocation satisfies an equal-loss principle, where each
component/block contributes equally to the overall quantization loss. This property not only provides
insight into the nature of optimal precision assignments, but also serves as a useful tool for designing
loss-controlled quantization algorithms.
Moreover, we demonstrate that our method can be integrated into existing quantization frameworks,
such as GPTQ and its variants, by replacing their bit allocation module with our efficient solution.
This upgrade leads to consistent performance improvements, particularly in the low-bit regime.
Contributions. First, we formulate a layer-wise quantization problem with an explicit, sensitivity-
aware distortion objective and derive a provably optimal bit allocation strategy via convex optimiza-
tion. We show that the resulting solution satisfies an equal-loss allocation principle, which ensures
uniform contribution to the total error and facilitates efficient implementation in practical compression
systems. We propose BAQ, a practical and lightweight algorithm that computes mixed-precision as-
signments in closed form with negligible computational overhead. BAQ can be seamlessly integrated
into GPTQ-type quantization methods, and we demonstrate consistent improvements over uniform
bit allocation schemes on LLMs ranging from 125M to 30B models. Finally, our bit allocation
framework is general and applicable to a broad class of compression methods where the loss can be
expressed in terms of compression error, enabling plug-in gains for quantization-based techniques.
2
Related Works
Post-Training Quantization (PTQ).
PTQ methods aim to convert pre-trained full-precision LLMs
into low-precision formats without requiring retraining, making them highly practical for deploy-
ment. These techniques are generally categorized into weight-only quantization, weight-activation
quantization [6][7], and KV (for Key and Value) cache quantization [8]. Our work focuses on the
weight-only PTQ setting. In this setup, notable methods such as GPTQ [3], QuIP [5], and AWQ [9]
achieve high accuracy by leveraging Hessian-informed loss approximations or outlier-aware scaling
strategies. These approaches often rely on estimating a proxy Hessian matrix from calibration data
to guide quantization. GPTQ, for instance, uses an Optimal Brain Compression framework with
inverse Hessian updates to minimize second-order loss. QuIP further enhances this by enforcing
incoherence between weights and the Hessian. Other PTQ works like OWQ [10], SqueezeLLM [11],
and SpQR [12] adopt sensitivity-based heuristics for identifying important weights and allocating bits
accordingly. However, these approaches typically use fixed or heuristic bitwidth. In contrast, BAQ
provides a theoretically grounded mechanism to derive optimal bit assignments based on a convex
formulation of Hessian-weighted quantization loss.
OBS-based Compression.
The Optimal Brain Surgeon (OBS) framework [13] and its precursor
OBD [14] laid the foundation for second-order model compression by quantifying the impact
of weight removal on the loss function and compensating for it through updates to remaining
parameters. This foundational principle has inspired a variety of pruning and quantization techniques
that leverage Hessian matrix information to guide compression decisions. Notably, GPTQ [3] extends
this paradigm to post-training quantization by minimizing a second-order Taylor expansion of the
loss. SparseGPT [15], OBC [16], and BiLLM [17] further generalize OBS methodology to structured
sparsity, joint quantization-pruning, and binary quantization, respectively. Our work draws from
this second-order perspective but shifts the focus from selecting or modifying weights to allocating
bitwidths under a global bit budget constraint. By minimizing a Hessian-weighted distortion objective,
BAQ introduces a low-overhead bit allocation strategy that enhances the efficiency of OBS-based
quantization pipelines.
3
Problem formulation
The main problem under consideration in this paper is to quantize the weights of a large NN model.
For the sake of clarity and following related papers such as [3], the layer or component index will
be removed from the notation but the considered operations can be performed for any layer or any
component of the NN. The focus will be primarily on feedforward architectures but the results may
2


--- Page 3 ---
potentially extend to models incorporating feedback loops. Denote by M ≥1 and N ≥1 the
respective sizes of the layer output and input. The weight matrix associated with the layer under
consideration is denoted by W ∈RM×N. Mainly for complexity reasons, it is assumed that each
entry of W is quantized with a scalar uniform quantizer and independently of the other entries.
Each entry wij, i{1, ..., M}, j{1, ..., N}, of the weight matrix is thus approximated by its quantized
version bwij = Qij(wij), Qij being a scalar uniform quantization function. The number of bits
(referred to as the bitwidth) assigned to the quantizer Qij is denoted by Rij. As well motivated by
previous works such as [13] and [3], a relevant loss function to be considered for quantizing the
weight wij is as follows:
Lij = (wij −Qij(wij))2
[H−1
F ]nijnij
,
(1)
where HF is a proxy Hessian matrix corresponding to unquantized weights. Let X ∈RN×P be the
input activation matrix to the layer, where P is the number of calibration samples. Then XF ⊂X
denotes the submatrix formed by selecting the rows corresponding to unquantized weights, and the
Hessian matrix corresponding to unquantized weights is approximated as HF = 2XF X⊤
F . The
diagonal entry [H−1
F ]nijnij quantifies the sensitivity of the loss with respect to perturbations in wij,
where nij denotes the index of wij among the unquantized weights in row i.
In the existing literature, the bitwidths Rij are typically chosen to be identical for all weights.
However, empirical results on OPT models [5] indicates that quantization noise is more influential
in terms of NN final performance for some weights. Therefore, there is an incentive to adapt Rij
to the weight to be quantized. One of the motivations of this paper is precisely to formulate and
solve an optimization problem which produces the optimal bitwidths to be used to quantize the NN.
By optimality, it is meant in terms of the global loss associated with the considered layer that is,
L = PM
i=1
PN
j=1 Lij. One of the major difficulties in doing so is that each component Lij depends
on Rij in a non-explicit and complicated way. To circumvent this difficulty, we propose the following
approximation:
Lij(Rij) ≈
1
[H−1
F ]nijnij
· ∆2
ij
12 =
1
[H−1
F ]nijnij
· (wmax
ij
−wmin
ij )2
12 · 22Rij
(2)
where: wmax
ij
and wmin
ij
denote the maximum and minimum bounds of the quantizer Qij, ∆ij =
wmax
ij
−wmin
ij
2Rij
is the quantization step for the uniform scalar quantizer Qij. To build this approximation,
the rationale is as follows. First, to allocate a bitwidth to a given weight, one considers the mean
of the loss Lij instead of the loss itself. Second, we exploit a high-resolution approximation of
this mean. Indeed, it is known from [18][19] that the distortion for a scalar uniform quantizer can
be approximated in the high-resolution regime by ∆2
12 , ∆being the quantization step. Remarkably,
as all our simulations have shown, this approximation remains relevant even when the bitwidth is
intermediate or low, which is also a behavior observed in image compression [20].
Finally, we introduce a total bit budget for the considered layer (or component of the NN), denoted
by Rsum. In practice, this constraint is key, for example, to enable the comparison of two model
compression techniques using the same resources, or to impose a given total size on the model (or
one of its components). The bit allocation problem to be solved thus writes as:
(OP-A)
minimize
R11,...,RMN
M
X
i=1
N
X
j=1
cij · 2−2Rij
(3)
subject to
M
X
i=1
N
X
j=1
Rij ≤Rsum,
Rij ≥0, ∀i, j
(4)
where
cij = (wmax
ij
−wmin
ij )2
12 · [H−1
F ]nijnij
.
In the proposed formulation, note that Rij is not required to be an integer. Therefore, the optimization
problem (OP-A) describes a relaxed version of the initial problem. In practice, (OP-A) is solved, and
3


--- Page 4 ---
the entries of the bit allocation vector are rounded according to a rule to be defined; one possible rule
is provided in the next section. The purpose of the next section is to solve the relaxed problem and
provide algorithms suitable for implementation.
4
Analytical solution and algorithm
4.1
Optimal solution
In (OP-A), the bitwidth Rij for weight wij is assumed to be continuous, and (OP-A) can be checked
to be a convex problem which is strongly dual. The optimal solution can be proved to be as follows
[21]:
R⋆
ij = max

0, 1
2 log2
cij
λ

,
(5)
where λ
∝
QMN
i=1 c1/MN
ij
is a normalization factor determined by enforcing the bud-
get constraint.
By imposing λ to meet the latter constraint and assuming that
Rsum
MN
≥
max(i,j)
1
2

−log2
cij
(
Q
(i,j) cij)
1/MN

, the interior solution can be rewritten as:
R⋆
ij = 1
2


log2
cij
Q
(i,j) cij
1/MN


+ Rsum
MN .
(6)
Three interesting observations can be made. First, the obtained solution is markedly different from
the uniform bit allocation rule (namely, the rule used by state-of-the art solutions such as GPTQ)
when the cij vary widely. By inspecting the expression of cij, it is seen that this typically happens
when the weight have different ranges or the eigenvalues of the matrix H are very different (which
has been observed in [5]). Second, it can be checked that the optimal interior solution satisfies the
equal-loss principle:
cij · 2−2R⋆
ij = ckℓ· 2−2R⋆
kℓ,
∀(i, j), (k, ℓ).
(7)
Third, the total quantization loss under the optimal allocation R⋆
ij is:
X
i,j
cij · 2−2R⋆
ij = MN

Y
i,j
cij


1
MN
· 2−2Rsum/MN,
(8)
while uniform allocation yields a loss proportional to the arithmetic mean of {cij}. Thus, the relative
gain of optimal over uniform allocation is:
Lossoptimal
Lossuniform
=
Q
i,j cij
1/MN
1
MN
P
i,j cij
.
(9)
This shows that larger variance in {cij} leads to greater improvement from optimal bit allocation,
aligning with the equal-loss principle and justifying BAQ’s efficiency.
This structure is exploited for the design of the following practical algorithm. More precisely, the
practical weight quantization technique we propose consists of three sub-algorithms. The motivation
behind these algorithms is twofold: to trade off between performance gains and complexity; to allow
the proposed allocation rule to be integrated in existing model compression techniques. The following
three subsections describe the three proposed algorithms which allow the proposed quantization
technique to be implemented for large NN models such as LLMs. Leveraging the results established
so far we will introduce the BAQ algorithm, which efficiently assigns bitwidths to each column of the
weight matrix W based on Hessian-informed sensitivity. The algorithm is grounded in the equal-loss
principle, which suggests that optimal quantization is achieved when each column of W contributes
equally to the total loss.
4


--- Page 5 ---
4.2
Column-wise Bit Allocation
Instead of assigning a distinct number of quantization bits to each individual weight, we consider a
simplified bit allocation scheme, in which a shared bitwidth is assigned to each entry of the column
of W . This approach is motivated by two key observations. First, the approximation of quantization
error by its expected value becomes more accurate at the column level, since the total quantization
error aggregates over multiple weights. Second, the bitwidth overhead required to encode per-weight
precision can be substantially reduced by sharing bitwidths across larger structures. The overhead
aspect is key for quantizing large models. Therefore, we introduce a reference quantization loss value
Lref, such that each column of W is quantized to ensure its individual loss equals Lref. Given the
column sensitivity coefficient Cj defined as:
Cj =
M
X
i=1
cij =
M
X
i=1
(wmax
i
−wmin
i
)2
12 · [H−1
F ]qijqij
,
(10)
the required bitwidth for every entry of column j, denoted by Rj and imposed to satisfy the equal-loss
equality Cj · 2−2Rj = Lref, is given by:
Rj = 1
2 log2
 Cj
Lref

.
(11)
In practice, we round Rj to the nearest integer to obtain a hardware-friendly bit assignment and also
ensure Rj is non-negative. The following procedure summarizes this bit allocation strategy:
Algorithm 1 Bit Allocation Given Reference Loss
Require: Sensitivity coefficients {Cj}N
j=1, reference loss Lref
Ensure: Integer bit allocations {Rj}N
j=1
1: for each column j = 1 to N do
2:
Rj ←1
2 log2

Cj
Lref

3:
Rj ←max(0, round(Rj))
4: return {Rj}N
j=1
4.3
Layer-wise Reference Loss Estimation
As shown in equation (8), the total quantization loss for a layer is exponentially dependent on the
average bitwidth. This observation enables us to estimate an appropriate Lref for each layer based
on a desired average bitwidth Rref. Specifically, we begin by selecting an initial value Linit, which
corresponds to an initial average bitwidth Rinit computed via Algorithm 1. To align the final bit
allocation with a target average bitwidth Rref, we adjust the reference loss according to:
Lref = Linit · 22(Rinit−Rref),
(12)
where Linit is an initial reference loss corresponding to an empirically estimated average bitwidth
Rinit. This formulation ensures that the resulting bit allocation is centered around the desired average
Rref.
It is important to note that different layers often exhibit vastly different sensitivity distributions and
quantization characteristics. As a result, achieving the same average bitwidth across layers typically
requires setting distinct values of Lref for each layer. Empirical observations confirm that using a
fixed global Lref can lead to substantial discrepancies in layer-wise average bitwidths Ravg, which in
turn causes notable degradation in model performance. To ensure consistent accuracy and reliable
compression, it is essential to control Ravg within a narrow range across layers.
5


--- Page 6 ---
Algorithm 2 Reference Loss Estimation for Target Average Bitwidth
Require: Sensitivity coefficients {Cj}N
j=1, initial reference loss Linit, target average bitwidth Rref
Ensure: Updated reference loss Lref
1: Use Algorithm 1 with Linit to compute {Rj}N
j=1
2: Rinit ←1
N
PN
j=1 Rj
3: Lref ←Linit · 22(Rinit−Rref)
4: return Lref
4.4
Full BAQ Workflow and Integration with Existing Quantization Techniques
The BAQ algorithm described above can be readily integrated into existing quantization pipelines by
replacing their static or heuristic bit assignment with our sensitivity-guided bit allocation strategy. In
particular, methods such as GPTQ [3], which apply fixed-bit quantization, can benefit significantly
from our layer-wise adaptive bitwidth assignment.
To demonstrate this, we present the full BAQ workflow as a drop-in replacement for the bit allocation
module in methods based on GPTQ. The workflow assumes access to a quantization-aware routine
(e.g., GPTQ) that performs weight quantization given a bitwidth configuration.
Algorithm 3 Full BAQ Workflow with GPTQ-based Quantization
Require: Weight matrix W ∈RM×N, inverse Hessian diagonal {[H−1
F ]qijqij}, target average
bitwidth Rref, initial reference loss Linit
Ensure: Quantized weights c
W
1: Compute sensitivity coefficients: Cj = PM
i=1
(wmax
i
−wmin
i
)2
12·[H−1
F ]qij qij
2: Estimate refined reference loss Lref using Algorithm 2
3: Compute optimal bitwidths {Rj} using Algorithm 1 with Lref
4: for each column j = 1 to N do
5:
Quantize column W:,j using GPTQ with bitwidth Rj
6: return Quantized weight matrix c
W
For other model compression methods, as long as the loss induced by weight compression can be
explicitly expressed, our BAQ framework can be similarly applied. The key difference lies in how the
sensitivity coefficients Cj are calculated, which may vary depending on the specific compression
strategy employed. For instance, pruning-based or low-rank approximation methods may define
Cj using different second-order metrics or task-specific criteria. Nonetheless, once a meaningful
per-component loss approximation is available, BAQ provides a general mechanism to optimally
allocate quantization precision under a global budget.
4.5
Complexity and Overhead Analysis
The BAQ algorithm brings minimal additional computational complexity, as it reuses quantities like
the inverse Hessian diagonal and weight statistics already computed in standard pipelines such as
GPTQ. Its core step, computing each bitwidth via equation (11), followed by rounding, involves only
cheap, element-wise operations, negligible compared to matrix or calibration computations.
In terms of encoding overhead, BAQ adopts a structured quantization scheme where all weights
in a column share the same bitwidth Rj. As such, it suffices to transmit one additional value per
column to indicate the bitwidth used. Since the number of bits typically ranges from 0 to 15, a 4-bit
header per column is sufficient to represent all bitwidths without loss. Given that a typical column
contains approximately 1000 weight elements, the overhead is only 0.004 bits per component, which
is negligible compared to the savings achieved through mixed-precision quantization.
6


--- Page 7 ---
5
Experiments
Overview. We evaluate BAQ on the OPT [22] model family across a wide range of sizes (from 125M
to 30B parameters), focusing exclusively on aggressive 2-bit weight-only quantization. Classical
methods such as GPTQ are known to perform well in the 4-bit setting, but often degrade significantly
at 2 bits. In contrast, BAQ enables high-accuracy quantization even at 2-bit precision across diverse
models. We show that (1) BAQ consistently outperforms GPTQ across OPT models on both perplexity
and accuracy metrics across diverse datasets, and (2) these gains primarily stem from BAQ’s ability
to allocate bits more efficiently by exploiting the heterogeneous sensitivity of individual weights.
Setup. Our experimental setup follows the GPTQ pipeline [3], and we use HuggingFace implemen-
tations of all models. We quantized all models using a single NVIDIA A100 GPU with 80GB of
memory. Our calibration dataset consists of 128 randomly sampled 2048-token segments from the
C4 dataset [23], without any retraining or task-specific tuning. We report perplexity on WikiText2
[24], PTB [25], and C4, and zero-shot accuracy on LAMBADA [26], PIQA [27] and ARC-Easy [28].
We use structured allocation to ensure each weight column uses the same number of bits.
Methods. We compare BAQ with GPTQ across OPT models. In all cases, BAQ uses our closed-form
allocation rule (Section 4) for 2-bit assignment.
125M
350M
1.3B
2.7B
6.7B
13B
30B
Number of parameters
101
102
103
104
Perplexity (WikiText2) (Log Y-Axis)
Perplexity Comparison on WikiText2
GPTQ-2bit
BAQ-2bit
GPTQ-3bit
RTN-16bit
(a) Perplexity on WikiText2 (log scale).
125M
350M
1.3B
2.7B
6.7B
13B
30B
Number of parameters
25
30
35
40
45
50
55
60
65
Accuracy (ARC-easy)
Accuracy Comparison on ARC-easy
RTN-16bit
GPTQ-3bit
BAQ-2bit
GPTQ-2bit
(b) Accuracy on ARC-easy
Figure 1: Comparison between BAQ-2bit and several baselines (GPTQ-2bit, GPTQ-3bit, RTN-16bit)
on two representative tasks: WikiText2 (left) and ARC-easy (right). BAQ-2bit consistently improves
over GPTQ-2bit.
Table 1: Comparison of BAQ and GPTQ on various OPT models and datasets with 2-bit quantization.
Perplexity (↓) and accuracy (↑) metrics are reported.
Perplexity (↓)
Accuracy (↑)
Method
Model
Avg. Bits
C4
WikiText2
PTB
LAMB
PIQA
ARC
BAQ
OPT-125M
2.05
207.66
460.76
518.59
3.41
56.2
30.18
GPTQ
OPT-125M
2.00
2260
4563
4410
0
52.23
28.62
BAQ
OPT-350M
2.08
301.69
915.45
816.51
1.29
54.46
31.23
GPTQ
OPT-350M
2.00
8418
18687
18161
0
51.25
26.3
BAQ
OPT-1.3B
2.08
121.31
253.64
236.46
5.23
56.2
33.86
GPTQ
OPT-1.3B
2.00
4028
7856
6858
0
49.73
25.46
BAQ
OPT-2.7B
1.92
126.50
282.47
326.53
5.76
55.17
32.11
GPTQ
OPT-2.7B
2.00
4388
8949
8281
0
48.42
26.94
BAQ
OPT-6.7B
2.07
33.64
52.71
70.84
26.17
63.87
43.16
GPTQ
OPT-6.7B
2.00
500.7
2958
2521
1.07
55.11
31.86
BAQ
OPT-13B
2.00
29.98
40.18
59.35
23.11
65.07
43.86
GPTQ
OPT-13B
2.00
135.48
372.68
344.44
25.77
66.05
42.47
BAQ
OPT-30B
1.95
24.21
31.05
47.98
30.42
66.97
48.6
GPTQ
OPT-30B
2.00
29.59
71.7
88.19
25.77
66.05
42.47
Main results. As illustrated in Figure 1, with 2-bit quantization, BAQ consistently outperforms
GPTQ across model scales on both WikiText2 (perplexity) and ARC-easy (accuracy), demonstrating
7


--- Page 8 ---
superior performance in language modeling and zero-shot reasoning tasks. Table 1 presents a more
detailed comparison between the proposed BAQ method and GPTQ across a range of OPT models
and evaluation tasks. Two metrics are reported: perplexity and accuracy. Perplexity measures
how well the model predicts the next token in a sequence—lower values indicate better language
modeling performance. Accuracy, in this context, refers to performance on zero-shot multiple-choice
benchmarks such as LAMBADA, PIQA, and ARC-easy. These tasks assess the model’s reasoning and
language understanding capabilities without any task-specific fine-tuning, and accuracy is computed
as the fraction of correct choices made over the evaluation set.
Several key observations emerge. First, BAQ consistently outperforms GPTQ in perplexity across
all OPT model sizes, indicating better preservation of token-level predictions under aggressive 2-bit
quantization. For example, on OPT-1.3B, BAQ reduces C4 perplexity from 4028 (GPTQ) to 121.31.
Second, BAQ yields substantial accuracy gains on downstream zero-shot tasks, particularly for larger
models. On OPT-2.7B, it improves ARC-easy accuracy from 26.94% to 32.11% and PIQA from
48.42% to 55.17%. These improvements demonstrate that BAQ not only minimizes quantization
distortion but also enhances the model’s zero-shot generalization ability. Overall, these results
validate the core design of BAQ: bitwidths are allocated according to Hessian-based sensitivity to
preserve model semantics under quantization. The strong gains in both perplexity and accuracy,
especially in low-bit regimes, highlight BAQ’s effectiveness as a scalable, high-performance solution
for post-training quantization.
Bitwidth histogram. To further understand the effectiveness of BAQ-2bit, we analyze the bit
allocation profile on OPT-2.7B, visualized in Figure 2. The figure plots the distribution of allocated
bitwidths across all weights in the model. While the majority of weights are still assigned the minimal
2-bit representation, a non-negligible portion of weights are allocated to lower or higher precision.
This illustrates that BAQ successfully integrates the bit allocation mechanism into the quantization
process, assigning fewer bits to more robust weights and more bits to sensitive ones.
0
2
4
6
8
Rate per element
0.0
0.5
1.0
1.5
2.0
Count
1e8
Histogram of Bitwidth
Figure 2: Distribution of assigned bitwidths Rj in OPT-2.7B under BAQ. While most weights are
quantized to 2 bits, BAQ adaptively allocates higher or lower precision to match weight sensitivity.
Layers with higher variance in sensitivity (as measured by {Cj}) exhibit broader bitwidth distribu-
tions, reflecting structural adaptivity.
Layer-wise loss analysis. Fig. 3 compares two metrics across transformer layers to explain the
effectiveness of BAQ from the layer-wise loss perspective: Ratio_L, the ratio of quantization loss
under BAQ to that under GPTQ (uniform 2-bit), and Ratio_C, the geometric-to-arithmetic mean
ratio of the sensitivity coefficients {Cj}. Quantization loss is approximated by P
j Cj2−2Rj, where
uniform bitwidth yields loss scaling with the arithmetic mean, while optimal allocation achieves
scaling with the geometric mean.
Fig. 3 reveals that layers with low Ratio_C, which indicate diverse sensitivities on weights, show
greater improvement under BAQ (lower Ratio_L). This confirms that bit allocation is especially
effective in layers with heterogeneous sensitivity. Since Cj can be computed from the diagonal entries
of the Cholesky decomposition of the inverse Hessian H−1 [3], a large variation in {Cj} typically
indicates that the original Hessian H has a wide spread of eigenvalues, meaning some directions in the
weight space are much more sensitive than others. In such layers, uniform quantization inefficiently
allocates bits to insensitive weights, while BAQ adaptively assigns higher precision where it matters
most. This adaptivity is reflected in the greater variance of bitwidths observed in those layers.
8


--- Page 9 ---
Figure 3: Layer-wise comparison of quantization efficiency with OPT-2.7B model. Ratio_C (geomet-
ric mean over arithmetic mean of {Cj}) characterizes the potential gain from optimal bit allocation.
Ratio_L measures the realized gain by BAQ compared to GPTQ. Layers with more dispersed sensi-
tivity (lower Ratio_C) benefit more from bit allocation.
Running time. BAQ incurs approximately 1.5× running time compared to GPTQ, mainly due to the
additional computation for layer-wise bit allocation.
Table 2: Running time (in seconds) of GPTQ and BAQ for different OPT models.
Method
OPT-2.7B
OPT-6.7B
OPT-13B
OPT-30B
GPTQ
517.08
1194.45
2423.57
6220.92
BAQ
797.07
1734.22
3336.18
8211.75
Integration to transformation-based quantization. As demonstrated in prior sections, our bit
allocation algorithm provides significant improvements over fixed-bit quantization schemes by
adapting bitwidths to the sensitivity of individual weight groups. A natural question arises: can BAQ
further enhance advanced quantization pipelines such as QuIP, which already use transformations to
improve quantization robustness? To investigate this, we integrated BAQ into QuIP by replacing its
uniform bitwidth assignment with BAQ’s optimal per-column bitwidths. However, we observed only
marginal gains. This is largely due to the effect of QuIP’s incoherence processing, which transforms
the weight matrix into a domain where the sensitivity coefficients Cj are nearly uniform. In this
transformed space, the geometric mean of {Cj} closely matches the arithmetic mean, which limits
the potential benefit of non-uniform bit allocation.
This observation also highlights an important advantage of BAQ: even without such transformations,
BAQ alone can achieve competitive results in the 2-bit regime. In contrast, QuIP-type methods depend
heavily on accurate construction of transformation matrices. If these are imperfectly estimated or
mismatched during inference, performance can degrade sharply. By avoiding this dependency, BAQ
offers a simpler, more robust, and computationally efficient solution for mixed-precision quantization
with low overhead and strong performance.
6
Conclusion
This paper introduced BAQ, a principled bit allocation framework for post-training model quantization.
By formulating bit allocation as a convex optimization problem over a sensitivity-aware loss model,
9


--- Page 10 ---
we derived a closed-form rule that assigns quantization precision to individual weights based on
their Hessian-informed importance. The resulting algorithm is simple, efficient, and compatible
with existing quantization pipelines such as GPTQ and QuIP. Experimental results demonstrate that
BAQ delivers substantial gains over uniform-bit quantization. Importantly, these improvements come
with negligible computational overhead, making BAQ highly practical for real-world deployment.
Our analysis also revealed a strong empirical correlation between the variance in Hessian-derived
sensitivity coefficients and the benefit of bit allocation, validating the theoretical underpinnings of
our approach. In addition, we showed that while transformation-based methods like QuIP tend
to homogenize weight sensitivities (thus limiting the gains from adaptive bitwidth), BAQ remains
effective and robust even without such preprocessing. Looking forward, an interesting direction
is to jointly optimize weight transformations and bit allocation to maximize their complementary
strengths. Overall, BAQ offers a lightweight, theoretically grounded, and high-performance solution
for aggressive quantization in modern LLMs.
10


--- Page 11 ---
References
[1] OpenAI. Introducing ChatGPT. OpenAI Blog.
[2] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient
finetuning of quantized llms. Advances in neural information processing systems, 36:10088–
10115, 2023.
[3] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization
for generative pre-trained transformers. In The Eleventh International Conference on Learning
Representations, 2023.
[4] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang,
Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language
models. arXiv preprint arXiv:2310.11453, 2023.
[5] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization
of large language models with guarantees. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023.
[6] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
Smoothquant: Accurate and efficient post-training quantization for large language models.
In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023.
[7] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng
Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quan-
tization for large language models. In The Twelfth International Conference on Learning
Representations, 2024.
[8] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Sophia Shao,
Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference
with kv cache quantization. Advances in Neural Information Processing Systems, 37:1270–1303,
2024.
[9] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan
Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization
for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems,
6:87–100, 2024.
[10] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Outlier-
aware weight quantization for efficient fine-tuning and inference of large language models. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13355–13364,
2024.
[11] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W
Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint
arXiv:2306.07629, 2023.
[12] Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh
Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. SpQR: A sparse-quantized
representation for near-lossless LLM weight compression. In The Twelfth International Confer-
ence on Learning Representations, 2024.
[13] Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In IEEE International Conference on Neural Networks, 1993.
[14] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural
information processing systems, 2, 1989.
[15] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned
in one-shot. In International Conference on Machine Learning, pages 10323–10337. PMLR,
2023.
11


--- Page 12 ---
[16] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-
training quantization and pruning. Advances in Neural Information Processing Systems, 35:4475–
4488, 2022.
[17] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele
Magno, and Xiaojuan Qi. BiLLM: Pushing the limit of post-training quantization for LLMs.
arXiv preprint arXiv:2402.04291, 2024.
[18] Robert M. Gray and David L. Neuhoff. Quantization. IEEE Transactions on Information
Theory, 44(6):2325–2383, 1998.
[19] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
[20] David S Taubman, Michael W Marcellin, and Majid Rabbani. Jpeg2000: Image compression
fundamentals, standards and practice. Journal of Electronic Imaging, 11(2):286–287, 2002.
[21] Allen Gersho and Robert M Gray. Vector quantization and signal compression, volume 159.
Springer Science & Business Media, 2012.
[22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam
Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke
Zettlemoyer. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068,
2022.
[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.
[24] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
[25] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark
Ferguson, Karen Katz, and Britta Schasberger. The Penn Treebank: Annotating predicate
argument structure. In Human Language Technology: Proceedings of a Workshop held at
Plainsboro, New Jersey, March 8-11, 1994, 1994.
[26] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA
dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1525–1534, Berlin, Germany, August 2016. Association for Computational Linguistics.
[27] Sandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein data sets. In
International Conference on Scientific and Statistical Database Management, 2003.
[28] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, An-
drew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei,
Ryan Musa, Kartik Talamadupula, and Michael Witbrock. A systematic classification of knowl-
edge, reasoning, and context within the ARC dataset. In Proceedings of the Workshop on
Machine Reading for Question Answering, pages 60–70, Melbourne, Australia, July 2018.
Association for Computational Linguistics.
12


--- Page 13 ---
Appendices
The appendices provide additional technical content that supports and extends the main paper. First,
we present rigorous derivations for the optimal bit allocation problem introduced in Section 4.1,
including the closed-form solution, the equal-loss principle, and the geometric-vs-arithmetic mean
quantization loss ratio. Second, we demonstrate the generalizability of BAQ by applying it to other
model families, such as LLaMA2, confirming its efficacy across different architectures. Lastly, we
show how BAQ can be integrated into advanced transformation-based quantization methods like QuIP.
Empirical results reveal that while QuIP alone benefits from incoherence-promoting transformations,
combining it with BAQ yields further improvements, especially when sensitivity varies significantly
across columns, highlighting the flexibility and broad applicability of BAQ across a wide range of
quantization settings.
A
Proof of Optimal Bit Allocation Results in Sec 4.1
In this section, we rigorously prove the key theoretical results from Section 4.1 of the main paper,
including:
• the closed-form solution for the optimal bit allocation R∗
ij,
• the equal-loss property,
• and the geometric-vs-arithmetic mean ratio for quantization loss.
We consider the relaxed optimization problem:
min
{Rij≥0}
X
i,j
cij · 2−2Rij
subject to
X
i,j
Rij ≤Rsum,
(13)
A.1
Closed-Form Expression for R∗
ij
We define the Lagrangian:
L({Rij}, λ) =
X
i,j
cij · 2−2Rij + λ

X
i,j
Rij −Rsum

.
(14)
Set the derivative with respect to Rij to zero:
∂L
∂Rij
= −2 ln(2) · cij · 2−2Rij + λ = 0.
(15)
Solving the above equation yields:
R∗
ij = max

0, 1
2 log2
cij
λ′

,
λ′ =
λ
2 ln 2.
(16)
If the total budget Rsum is sufficiently large such that the optimal solution satisfies R∗
ij > 0 for all
(i, j), we can omit the max operator. In this case, to satisfy the constraint P
i,j R∗
ij = Rsum, we
obtain:
λ′ =

Y
i,j
cij


1/MN
· 2−2Rsum/MN.
(17)
Substituting into the expression gives:
R∗
ij = 1
2 log2
cij
G

+ Rsum
MN ,
(18)
where G =
Q
i,j cij
1/MN
represents the geometry mean of {cij}.
13


--- Page 14 ---
A.2
Equal-Loss Property
From optimality:
2−2R∗
ij = λ′
cij
⇒
cij · 2−2R∗
ij = λ′.
(19)
This result implies that all quantization-induced loss terms are equal across weights:
cij · 2−2R∗
ij = ckℓ· 2−2R∗
kℓ,
∀(i, j), (k, ℓ).
(20)
A.3
Quantization Loss Ratio: Geometric vs. Arithmetic Mean
Under optimal allocation:
Lossoptimal =
X
i,j
cij · 2−2R∗
ij = MN · G · 2−2Rsum/MN.
(21)
Under uniform allocation Runi
ij = Rsum/MN:
Lossuniform =
X
i,j
cij · 2−2Rsum/MN = MN · A · 2−2Rsum/MN,
(22)
where A =
1
MN
P
i,j cij represents the arithmetic mean of {cij}. Therefore, the ratio becomes:
Lossoptimal
Lossuniform
= G
A = (Q cij)1/MN
1
MN
P cij
.
(23)
This confirms that the relative benefit of optimal allocation over uniform allocation increases with
greater variance in {cij}.
B
Results with LLaMA Models
To further validate the generality of BAQ, we evaluate its performance on LLaMA2-13B model. This
model differs architecturally from the OPT family and represents a distinct class of large language
models.
We apply BAQ under the standard 2-bit weight-only post-training quantization setting and compare
it against GPTQ, which serves as the baseline. We use the WikiText2 dataset as the calibration set.
Evaluation is conducted on standard benchmarks such as WikiText2, C4, and PTB for perplexity,
PIQA and ARC-easy for zero-shot accuracy.
Table 3: Comparison of BAQ and GPTQ on LLaMA2 models and datasets with 2-bit quantization.
Perplexity (↓) and accuracy (↑) metrics are reported.
Perplexity (↓)
Accuracy (↑)
Method
Model
Avg. Bits
C4
WikiText2
PTB
PIQA
ARC
BAQ
LLaMA2-13B
2.01
253
54.44
1806.89
52.77
27.19
GPTQ
LLaMA2-13B
2.00
1172.16
254.44
3264.04
51.41
26.67
The results in Table 3 demonstrate that BAQ continues to deliver strong performance gains over
GPTQ when applied to LLaMA2 models. On LLaMA2-13B, BAQ reduces WikiText2 perplexity
from 254.44 to 54.44, achieving over 4× improvement in language modeling quality under the same
2-bit quantization budget. Improvements are also observed on zero-shot downstream tasks.
These findings confirm that BAQ’s bit allocation mechanism generalizes well across architectures and
scales. By adapting precision according to sensitivity, BAQ effectively reduces quantization-induced
distortion, making it a robust and efficient method for compressing high-performance foundation
models like LLaMA2 under aggressive quantization regimes.
14


--- Page 15 ---
C
Integration with Transformation-Based Quantization
To study the interaction between BAQ and transformation-based quantization methods such as QuIP,
we consider the use of orthogonal linear transformations applied to the weight matrix W and its
corresponding Hessian approximation H, as originally proposed in QuIP. Specifically, QuIP leverages
transformation pairs (U, V) to map weights and curvature into an incoherent domain:
W 7→U⊤WV,
H 7→V⊤HV,
where U, V are blockwise orthogonal matrices.
The choice of transformation matrices U and V plays a crucial role in transformation-based quantiza-
tion, as it directly affects the distribution of sensitivity coefficients {Cj} in the transformed domain.
Following the QuIP framework, we construct U and V as block-diagonal matrices composed of
smaller orthogonal blocks. Specifically, we build U = diag(U1, . . . , UNp) and similarly for V,
where each Ui ∈Rp×p is an orthogonal matrix.
To evaluate how the structure of these orthogonal blocks impacts quantization performance, we
consider three construction strategies:
• Mild transformation (σ = 10−2): Each block Ui ∈Rp×p is constructed as the orthogonal
factor Q from the QR decomposition of a random matrix of the form I + σ · G, where
G ∈Rp×p is a matrix with i.i.d. entries sampled from N(0, 1). The resulting blocks are
close to identity and introduce limited incoherence.
• Moderate transformation (σ = 10−1): We apply the same procedure but increase the
noise level to σ = 10−1, generating blocks that are more randomized and less correlated
with the identity, thereby inducing stronger incoherence.
• Highly randomized transformation (σ →∞): Each block Ui is drawn as a fully random
orthogonal matrix of size p × p, typically sampled from the Haar distribution via QR
decomposition of a standard Gaussian matrix. This represents the limiting case of the
above construction with very large σ. This construction achieves high incoherence between
transformed features.
To assess the compatibility of BAQ with transformation-based quantization frameworks such as QuIP,
we apply linear transformations U and V to the input covariance matrix H and the weight matrix W,
respectively, following the design of QuIP. The bitwidths in QuIP are uniformly set per column, and
the quantization loss of each column is empirically measured. Using this, we estimate the sensitivity
coefficients Cj by rearranging the proxy loss expression Cj2−2R, since all columns use the same
bitwidth R in the QuIP baseline. This enables us to approximate the induced loss from quantizing
column j, which forms the basis for evaluating the potential benefit of BAQ’s bit allocation.
Table 4: Perplexity comparison of QuIP and QuIP+BAQ under three transformation settings (OPT-
125m), with an average bitwidth of 2 bits. BAQ is applied to adjust bitwidths per-column, while
preserving the same overall budget.
Transformation
Dataset
QuIP
QuIP+BAQ
Mild
C4
1760.70
253.49
WikiText2
3032.47
647.92
PTB
3067.05
590.97
Moderate
C4
348.15
326.76
WikiText2
638.39
530.00
PTB
1686.45
644.29
Highly Randomized
C4
48.79
47.91
WikiText2
81.09
79.14
PTB
214.09
223.14
We analyze how the effectiveness of BAQ varies under different transformation settings by comparing
QuIP and QuIP+BAQ across three increasingly incoherent configurations: mild, moderate, and highly
randomized. As shown in Table 4, applying BAQ yields substantial perplexity reductions in the
15


--- Page 16 ---
mild and moderate cases. For instance, under mild transformation, BAQ reduces the perplexity on
WikiText2 from 3032.47 to 647.92. This significant gain stems from the high variance in weight
sensitivities across columns, which allows BAQ to exploit bitwidth adaptation effectively.
To further interpret this behavior, we refer to the histograms of Ratio_C in Figure 4. Ratio_C
represents the geometric mean to arithmetic mean ratio of sensitivity coefficients {Cj}, which
approximates the percentage of loss reduction achievable by optimal bit allocation compared to
uniform allocation with the same average bitwidth. As shown in the histograms, under highly
randomized transformations, most Ratio_C values are close to 1, implying limited benefit from bit
reallocation. In contrast, with moderate or mild transformations, the Ratio_C values exhibit greater
variation and are frequently much less than 1, indicating that BAQ can yield substantial improvements.
This structural variance enables BAQ to provide meaningful improvements by assigning more bits to
sensitive directions.
Thus, while BAQ consistently preserves the bit budget, its relative impact depends strongly on the
underlying sensitivity structure shaped by the transformation. These findings not only validate the
use of Ratio_C as a diagnostic metric for allocation benefit, but also highlight the complementary
nature of BAQ and incoherence processing.
These findings suggest that BAQ is especially advantageous in settings where incoherence is difficult
to achieve or the transformation quality is uncertain. Unlike QuIP, which relies on the accurate
construction and inversion of transformation matrices, BAQ provides a lightweight and robust
precision scheduling mechanism that adapts to structural sensitivity without introducing additional
inference-time overhead.
16


--- Page 17 ---
(a) Mild transformation
0.0
0.2
0.4
0.6
0.8
Ratio_C
0
1
2
3
4
5
6
7
Frequency
(b) Moderate transformation
0.0
0.2
0.4
0.6
0.8
1.0
Ratio_C
0
2
4
6
8
10
Frequency
(c) Highly randomized transformation
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Ratio_C
0
5
10
15
20
25
Frequency
Figure 4: Histograms of Ratio_C = GM({Cj})/AM({Cj}) across different transformation strate-
gies. A lower value of Ratio_C indicates greater dispersion in the sensitivity coefficients {Cj},
which corresponds to higher potential gains from bit allocation. In mild and moderate transformations,
Ratio_C often falls significantly below 1, suggesting that BAQ can yield substantial improvement
over uniform quantization. In contrast, highly randomized transformations yield {Cj} distributions
that are nearly uniform, with Ratio_C ≈1, thus diminishing the relative benefit of bit allocation.
These patterns align with the performance differences observed in Table 4.
17
