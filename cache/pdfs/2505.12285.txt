--- Page 1 ---
arXiv:2505.12285v1  [cs.NE]  18 May 2025
CALM: Co-evolution of Algorithms and Language
Model for Automatic Heuristic Design
Ziyao Huang1, Weiwei Wu2, Kui Wu3, Jianping Wang1, Wei-Bin Lee4
1City University of Hong Kong 2Southeast University
3University of Victoria 4Hon Hai Research Institute
zhuang88-c@my.cityu.edu.hk, weiweiwu@seu.edu.cn, wkui@uvic.ca
jianwang@cityu.edu.hk, wei-bin.lee@foxconn.com
Abstract
Tackling complex optimization problems often relies on expert-designed heuristics,
typically crafted through extensive trial and error. Recent advances demonstrate that
large language models (LLMs), when integrated into well-designed evolutionary
search frameworks, can autonomously discover high-performing heuristics at a
fraction of the traditional cost. However, existing approaches predominantly rely
on verbal guidance, i.e., manipulating the prompt generation process, to steer the
evolution of heuristics, without adapting the underlying LLM. We propose a hybrid
framework that combines verbal and numerical guidance, the latter achieved by
fine-tuning the LLM via reinforcement learning based on the quality of generated
heuristics. This joint optimization allows the LLM to co-evolve with the search
process. Our method outperforms state-of-the-art (SOTA) baselines across various
optimization tasks, running locally on a single 24GB GPU using a 7B model with
INT4 quantization. It surpasses methods that rely solely on verbal guidance, even
when those use significantly more powerful API-based models.
1
Introduction
Complex optimization problems are prevalent in real-world applications, including logistics [1, 2],
scheduling [3, 4], and transportation [5, 6]. Traditionally, solving these problems relies heavily
on manually crafting high-quality heuristics, a labor-intensive process requiring substantial expert
knowledge. Given the limitations of this manual approach, Automatic Heuristic Design (AHD)
emerged to streamline heuristic generation. Nevertheless, classic AHD approaches like Genetic
Programming (GP) [7] still depend significantly on human-defined problem-specific components,
limiting the search space and flexibility.
Recently, the advent of Large Language Models (LLMs) has introduced promising avenues for AHD
by employing LLMs as heuristic generators and evolutionary computing (EC) techniques as a search
framework. In this paradigm, heuristics generated by LLMs are iteratively evaluated through a
predefined simulation framework, and superior heuristics inform subsequent generation prompts, thus
creating a feedback-driven evolutionary loop [8]. Nevertheless, existing LLM-based AHD methods
predominantly keep the underlying LLM untouched and merely guide heuristic evolution via textual
prompt manipulations, referred to as "verbal gradients" [9]. Consequently, these methods inherently
neglect the opportunity of tuning and enhancing the generative capability of LLM based on the
feedback from heuristic designs.
We propose Co-evolution of Algorithms and the Language Model (CALM) to capture this opportunity.
CALM fundamentally differs from the state-of-the-art [8‚Äì11] by enabling the LLM to co-evolve
alongside heuristic designs. This co-evolution is made possible by treating the heuristic generation
Preprint. Under review.


--- Page 2 ---
Task Description
Best Heuristic
(‚ùâfrozen) LLM
Evaluation Env.
perf.
Prompt
Existing LLM-based AHD Methods
(
üî•
learnable) LLM
‚Ä¶‚Ä¶
Operator Base
Evaluation Env.
Prompt
no new algo. 
no idea
no code
runtime error
random algo.
worse algo.
better algo.
prompt
response
Our CALM Framework
[
]
‚Ä¶
Heuristic Base
[
]
(selection)
perf-based
Diversity-based
perf.
perf.
Idea
Code
Crossover
Mutate
Simplify
Reflect
Initialize
‚Ä¶‚Ä¶
Operator Base
[
]
‚Ä¶
Heuristic Base
(selection)
perf-based
mcts-based
[
]
Crossover Simplify Initialize
Idea
Code
Task Description
Best Heuristic
Figure 1: Pipeline of existing LLM-based AHD methods [9‚Äì11, 13] under a fixed LLM and our new
approach CALM that enables the co-evolution of LLM in the iterative heuristic search process. New
components are presented in bright colors.
process not only as a target of optimization but also as a rich source of training data. As heuristics
are continually proposed, evaluated, and selected based on their performance, the evolutionary loop
naturally produces abundant prompt-response-performance triplets. These data points are highly
informative, as each heuristic‚Äôs effectiveness provides an implicit signal about the utility of the
underlying generation process. By using this signal as feedback for reinforcement learning, we can
fine-tune the LLM, thereby applying what we term "numerical gradients" to adapt the model itself.
This co-evolution approach unlocks a new dimension of adaptability, allowing the LLM to internalize
characteristics of successful heuristics and improve its future generations.
CALM is the first LLM-based AHD framework that jointly optimizes both the prompt generation
process and the LLM model itself, overcoming the limitations of fixed-model approaches. For prompt
generation, CALM introduces a suite of evolutionary operators, including fine-granularity mutation
operators (injection and replacement) and a diversity-aware crossover operator, that promote mean-
ingful and diverse heuristic variations while preserving structural coherence. Furthermore, a simple
yet effective collapse mechanism is developed to help escape the local optima. For model improve-
ment, CALM employs a memory-efficient reinforcement learning (RL) algorithm GRPO [12] with a
carefully designed reward function to enable efficient fine-tuning. Experimental results demonstrate
that our new approach can discover heuristics that beat existing state-of-the-art baselines [8, 9, 11],
while running entirely on a local computer with a single 24GB GPU, in contrast to prior methods that
depend heavily on commercial LLM APIs.
2
Related Work
As our approach centers on fine-tuning LLMs by RL for solving optimization problems, we review
relevant literature in both RL and LLMs applied to optimization. Additional related topics, including
LLMs for code generation and RL-based LLM fine-tuning, are discussed in Appendix A.
RL for Optimization Problems. Existing RL-based methods for optimization can be broadly
categorized by the role the learned policy plays: (1) Instance-Level Solution Generator. Deep
RL has been widely adopted to learn policies for solving specific optimization instances [14‚Äì16].
However, these methods differ fundamentally from LLM-based AHD methods, as they directly
produce solutions rather than design the algorithms that generate them. The LLM-based AHD
approach operates at a meta level, seeking to learn the algorithmic structure that produces solutions.
This distinction also applies to the broader class of Neural Combinatorial Optimization (NCO) [17‚Äì
20], where models are trained to directly solve instances. Moreover, NCO methods often require
explicit adaptation to handle problem scales not seen during training, whereas our method generalizes
more naturally to new scales. (2) Heuristic Generator. Some RL-based methods target meta-
level search to discover heuristics instead of instance-level solutions. For example, AlphaDev [21]
learns to combine low-level operations to discover faster sorting algorithms, and [22] searches for
high-performing metaheuristics from predefined algorithmic components. While having similar
goals, these approaches rely heavily on hand-engineered building blocks, akin to traditional AHD
2


--- Page 3 ---
frameworks [7, 23, 24]. In contrast, LLM-based method reduces manual intervention by leveraging
LLMS to explore an open-ended heuristic space with minimal prior specification.
LLM for Optimization Problems. Studies in this area fall into two categories depending on how
LLMs are employed: (1) Instance-Level Solution Generator. Several works [25‚Äì27] prompt LLMs
with instance-specific inputs for direct solution generation. LLM-based methods in this category
focus on discovering reusable heuristics. Moreover, methods such as [26, 27] keep LLM parameters
frozen, and [25] fine-tunes the model using supervised labels from an existing solver (OR-Tools [28]).
In contrast, our method fine-tunes the LLM solely by providing it with "how good is your current
response" instead of "how should you do", encouraging a more flexible exploration in heuristic
generation. (2) Heuristic Generator. LLM-based AHD methods [8‚Äì11, 13, 29‚Äì32] repeatedly ingest
information about the current elite heuristics‚Äîtypically their natural-language descriptions, source
code, and performance scores‚Äîand, via fixed prompt templates that mimic genetic operators, produce
new candidate heuristics. Those candidates are then executed and evaluated, and the resulting feedback
is fed back into the prompt, forming an evaluate‚Äìgenerate loop that continues until the evaluation
budget is exhausted. However, prior work keeps the LLM static. Our approach improves this by
continuously fine-tuning the LLM using prompt-response-performance tuples from the evolutionary
process, enhancing future heuristic generation. Notably, a very recent study EvoTune [33] also
fine-tunes LLMs within an evolutionary search framework, but our method differs in four key ways:
(i) We extra adapt prompt generation to better guide the LLM‚Äôs evolution; (ii) We continuously
reward the model by telling it how good each response is to encourage open-ended exploration, while
EvoTune offers a binary feedback by indicating the better of two responses; (iii) Our method, using a
compact LLM, outperforms several mainstream approaches [8‚Äì11, 13] that rely on more powerful
LLMs. In contrast, EvoTune only reports improvements over [13] when both use compact LLMs.
3
Preliminary
3.1
LLM-Based AHD
Let P be a problem with input space I and solution space S, and let a heuristic be a function
h : I ‚ÜíS. Given a training set D ‚äÇI and an objective f : S ‚ÜíR (lower is better), the performance
of a heuristic is g(h) = Ex‚ààD[‚àíf(h(x))]. Let H denote the space of all feasible heuristics. The
objective of AHD is to identify the optimal heuristic within this space, i.e., h‚àó= arg maxh‚ààH g(h).
LLM-based AHD is AHD where LLM serves as a heuristic generator. In practice, the LLM is
charged with designing the core decision function of a solver. For example, on tasks like the Traveling
Salesman Problem (TSP) or the Capacitated Vehicle Routing Problem (CVRP), an LLM-based AHD
method might generate a function, which selects the next city to visit or constructs an edge-desirability
matrix to guide solution search within an Ant Colony Optimization (ACO) framework.
3.2
GRPO
GRPO [12] is a recent reinforcement learning algorithm that has proven effective in training LLMs,
as evidenced by its application in models such as DeepSeek-R1. GRPO starts from an initial model
œÄŒ∏ and a reward function denoted by rœï(q, o) that maps the prompt q and the generated response o to
a scalar. At the beginning of each training round, it snapshots œÄŒ∏ as a reference model œÄref. Then, it
split all task prompts into multiple batches. When training for each prompt batch Db, it first snapshots
œÄŒ∏ as œÄold. For each task prompt q ‚ààDb, it samples a group of G responses {oi}G
i=1 ‚àºœÄŒ∏old and
computes rewards r = {ri = rœï(q, oi)}G
i=1 for each prompt-response pair. Subsequently, it computes
the advantage ÀÜAi,t for each token t in response i as the normalized reward (ri ‚àímean(r))/std(r).
The model parameters Œ∏ are updated by maximizing the following objective function:
JGRPO(Œ∏) = E[q‚àºQ,{oi}‚àºœÄŒ∏old]
1
G
G
X
i=1
1
|oi|
|oi|
X
t=1
n
min
h
ÀÜri,t ÀÜAi,t, clip (ÀÜri,t, 1 ‚àíŒµ, 1 + Œµ) ÀÜAi,t
i
‚àíŒ≤ DKL

œÄŒ∏‚à•œÄref
o
,
(1)
where œµ and Œ≤ are hyper-parameters, ÀÜri,t = œÄŒ∏(oi,t | q, oi,<t)/œÄold
Œ∏ (oi,t | q, oi,<t), and the KL
divergence term is computed using the unbiased estimator in [34] with guaranteed positivity. GRPO
3


--- Page 4 ---
uses the group mean reward as a baseline to eliminate the need for an auxiliary value network, thereby
reducing memory requirements. Additionally, the clipping mechanism combined with KL divergence
regularization ensures stable and conservative updates.
4
Methodology
To explore the benefit of RL-based fine-tuning for discovering higher-quality heuristics in LLM-based
AHD, we introduce CALM, a novel framework that integrates both verbal and numerical guidance in
evolutionary heuristic search. As shown in Fig. 1, CALM maintains a pool of heuristics, each with its
own idea, code, and performance. At every round, CALM draws a feasible evolutionary operator to
produce a new prompt q. Subsequently, G responses are sampled from the local LLM œÄŒ∏, which are
then evaluated. Based on the evaluation results, rewards are assigned to each response for GRPO to
train the LLM, and new feasible heuristics are added to the pool. Consequently, CALM returns the
best-so-far heuristic after running T rounds. Next, we elaborate on the critical techniques in CALM:
prompt generation, collapse mechanism, and the reward function.
4.1
Prompt Generation
CALM provides several evolutionary operators: injection, replacement, crossover, simplification, and
initialization. Prompts are predominantly generated by the selected operator and heuristics sampled
from maintained pools. The initialization operator is an exception, as it does not require heuristics
from the pool. A complete algorithm can be found in Appendix B. Next, we introduce the heuristic
sampling method and operators in detail1.
Heuristic Sampling Method. The heuristic sampling approach varies for the crossover operator,
details of which will be provided when introducing this operator. For the remaining operators, i.e.,
injection, replacement, and simplification, the heuristics are selected based on their performance
rankings like [8]. Specifically, the probability of sampling a heuristic h is inversely proportional to
its rank in the current pool (i.e., proportional to 1/rankp(h)). Heuristics ranked below a threshold,
defined as the population size, are assigned a probability of zero.
Fine-Granularity Mutation Operators: Injection & Replacement. GRPO assigns an advantage
score to each token based on the relative reward of the full response compared to others from the
same prompt. This means each part of a heuristic is encouraged or penalized depending on the quality
of the whole. However, heuristic performance can shift dramatically with changes to even a single
sub-component, making uniform treatment of all parts‚Äîin terms of gradient direction‚Äîunreliable.
While cumulative gradient updates can correct misattributed rewards or penalties for the same token
appearing in different responses, we aim to further boost this process. To this end, we introduce two
novel operators that enable more precise control over heuristic variations. These operators encourage
the LLM to retain more common parts while introducing meaningful modifications to the input
heuristic. Consequently, GRPO is expected to more effectively identify the contribution‚Äîpositive or
negative‚Äîof individual structural changes. The two newly designed operators are:
Injection. Given an existing heuristic, the injection operator prompts the LLM to incorporate a new
component into it. Additionally, a concise description of the new component must be included in the
response. All component descriptions are stored, and subsequent applications of the injection operator
require the LLM to introduce components distinct from those previously saved, promoting diversity
in generated heuristics. Unlike mutation operators in prior LLM-based AHD methods [8, 11], which
are fed with full heuristic codes: (1) Our approach uses compact summaries instead of full code,
allowing more heuristics to fit within the LLM‚Äôs context window; (2) Saved component descriptions
are globally accessible and not limited to the currently sampled heuristics; (3) Prior methods often
require entirely new heuristics, while our approach focuses on more granular modifications; (4)
When the number of heuristics is below the population size, the sampling probability of the injection
operator is increased to encourage exploration in the phase of population expansion.
Replacement. Given an existing heuristic, the replacement operator prompts the LLM to rewrite
an existing component under a specific instruction. There are three distinct instructions, and each
time the replacement operator is applied, one is randomly sampled for the given heuristic. While
1The prompts used in CALM are detailed in Appendix C due to space limit.
4


--- Page 5 ---
the "rewrite hyper-parameter" instruction is also present in prior studies [8, 11], CALM introduces
two novel instructions: (1) Rewrite an instance-independent decision rule as an instance-dependent
one‚Äîto improve the heuristic‚Äôs adaptability to varying problem contexts; (2) Rewrite a fragment that
assigns equal or near-equal credit to all candidates as one that differentiates credit based on contextual
performance‚Äîto encourage more effective prioritization and refined decision-making.
Diversity-Aware Crossover.
To balance exploitation and exploration, each crossover invocation
randomly chooses between (1) performance-based: sample both parents by performance rank;
and (2) diversity-based: sample the first parent hc,1 by performance rank and the second from all
retained heuristics with probability inversely proportional to diversity rank (larger diversity is better).
Specifically, let idea_token(¬∑) denote the set of unique tokens in a heuristic‚Äôs idea, the diversity
is: div(hc,1, h) = |idea_token(h) \ idea_token(hc,1)|/|idea_token(h)|. This hybrid mechanism
ensures that at least one parent heuristic is of high quality, while the second parent is either high-
performing or structurally novel. The diversity-aware selection expands the evolutionary search space
and leverages underutilized heuristics, potentially unlocking novel strategies that might otherwise be
overlooked due to suboptimal early performance. More discussions are moved to Appendix D.
Simplification Operator. As heuristic structures grow increasingly complex through repeated
applications of injection, crossover, and replacement, there is a risk of accumulating redundant or
unnecessarily verbose components. The simplification operator counterbalances this tendency by
prompting the LLM to produce a more concise and effective version of a given heuristic.
Initialization Operator. In cases where there is no heuristic in the pool (e.g., no initial/seeding
function is provided), this operator is invoked to prompt the LLM to generate new heuristics.
4.2
Collapse Mechanism
Why to Collapse. A key reason LLM-based evolutionary heuristic search can succeed is that prompts
containing better-performing heuristics tend to guide the LLM toward generating even stronger
ones. This creates a self-reinforcing feedback loop, gradually evolving a population of increasingly
effective heuristics. However, this process can also lead to inbreeding and premature convergence:
over time, the population becomes dominated by minor variations of the current best-performing
heuristic. When this state persists without meaningful breakthroughs, the search risks becoming
trapped in a local optimum, a classic challenge in evolutionary computing [35].
How to Collapse. As a remedy, CALM introduces a proactive collapse mechanism that resets the
search process when it detects stagnation, allowing the system to escape local optima and reinitiate
meaningful exploration. Specifically, when the search has plateaued‚Äîcharacterized by a prolonged
lack of performance improvement‚Äîwe reset the population by discarding all heuristics except two:
the original seed algorithm and the current best-performing heuristic. These two retained heuristics
jointly serve as the seed algorithms for the new search process, grounding it in past progress while
freeing it from the genetic redundancy accumulated in the previous population.
When to Collapse. Once the heuristic pool reaches its target population size, CALM begins tracking
stagnation using a no-breakthrough counter cn, initialized to zero. This counter records the number
of consecutive prompt rounds‚Äîeach involving G sampled responses‚Äîthat fail to yield a globally
superior heuristic. If any sampled heuristic in a round surpasses all previous ones in performance, cn
is reset to zero; otherwise, it increments by one.
To escape local optima, CALM introduces a probabilistic collapse mechanism based on this counter.
At the end of each round, collapse is triggered if: random(0, 1) < cnŒ¥0
or
cn ‚â•C, where
Œ¥0 ‚â™1 controls the rate at which collapse probability grows, and C is a hard cap ensuring collapse
happens by the C-th stagnation step at the latest. To aid in hyperparameter selection, we further
provide an analytical approximation for the expected number of rounds before collapse is triggered:
E

cn | collapse, C > 1
Œ¥0

‚âà
r œÄ
2Œ¥0
.
(2)
This collision of a rising-probability rule with a fixed maximum fosters a balance between giving
the search plenty of room to improve and ensuring it doesn‚Äôt stall infinitely. A detailed proof and
discussion about the benefit of the mechanism can be found in Appendix E.
5


--- Page 6 ---
4.3
Reward Function
When the LLM is prompted to generate a heuristic, the response quality varies considerably. CALM‚Äôs
reward function is designed to provide fine-grained guidance for GRPO-based learning by penalizing
infeasible outputs and reinforcing improvements over existing heuristics.
Infeasible Responses. The most penalized outcomes are those where the generated response fails to
constitute a valid heuristic. To diagnose such cases, we apply a hierarchy of failure modes, assigning
progressively higher (i.e., less negative) rewards to increasingly plausible but still unacceptable
outputs. These modes include: (1) omission of a required idea (reward: ‚àí1.0); (2) missing code
block (‚àí0.95); (3) improperly formatted function (‚àí0.9); (4) runtime errors or time budget violations
(‚àí0.85); and (5) detection of randomness in the heuristic (‚àí0.75), which incurs the mildest penalty
among infeasible cases. The reward for detecting randomized components, denoted as rrand < 0,
also serves as a ceiling for all other infeasibility penalties.This graded reward design encourages the
LLM to progress toward syntactic and semantic correctness even if full feasibility is not yet achieved.
Feasible Responses. For valid heuristics, performance becomes the primary signal for learning.
However, since the quality of generated heuristics is strongly influenced by the prompt‚Äîparticularly
the included base heuristics‚Äîwe avoid attributing full credit or blame for the resulting performance
solely to the LLM. Therefore, rewards depend not only on the absolute performance of the new
heuristic but also on its novelty and improvement relative to the best base heuristic in the prompt.
Specifically, let H denote the set of base heuristics used to construct prompt q, and hnew be the heuris-
tic parsed from the LLM‚Äôs output o. We define the top base heuristic as ht_base = arg maxh‚ààH g(h),
and measure relative performance via:
‚àÜ(hnew, ht_base) = clip

|g(hnew) ‚àíg(ht_base)|
min{|g(hnew)|, |g(ht_base)|}, 0, 1

.
(3)
The reward function rœï(q, o | hnew, ht_base) is then defined as:
rœï(q, o | hnew, ht_base) =
Ô£±
Ô£≤
Ô£≥
0.8rrand,
if ‚àÉh ‚ààH s.t. g(h) = g(hnew);
0.5rrand ¬∑ ‚àÜ(hnew, ht_base),
if g(hnew) < g(ht_base);
1 + ‚àÜ(hnew, ht_base),
if g(hnew) > g(ht_base).
(4)
This formulation satisfies two critical properties. First, any feasible heuristic is rewarded more than
any infeasible one, ensuring that basic correctness is prioritized. Second, the reward is primarily
determined by whether the new heuristic improves over the best base heuristic or not, with the relative
performance gap further modulating the strength of the reward or penalty. When the generated
heuristic is identical in performance to an existing base heuristic, a small but consistent reward
(0.8rrand) is given to discourage trivial reproduction. If the new heuristic underperforms relative to
the best base, a scaled negative reward is applied, while genuine improvements yield strictly positive
rewards starting from 1. In the special case of heuristics generated via the initialization operator
(which has no base references), the reward is simply set to zero to encourage feasible outputs.
5
Experiments
Implementation Details of CALM. We build CALM on Unsloth [36] and employ an INT4-quantized
Qwen2.5-7B-Instruct model [37], fine-tuning just 1.15% of its weights. INT4 compression cuts
memory usage up to 8√ó versus FP32 but degrades precision. According to [37], performance ranks
as follows: GPT-4o-mini ‚âàQwen2.5-Turbo > Qwen2.5-14B-Instruct > Qwen2.5-7B-Instruct >
Qwen2.5-7B-Instruct-INT4. The 14B and 7B Instruct models share the same architecture, so the
larger parameter count drives the 14B‚Äôs edge over the 7B, while quantization further reduces the 7B‚Äôs
accuracy. Consequently, GPT-4o-mini‚Äìbased baselines retain a clear advantage in raw accuracy over
our lean, resource-efficient setup. More implementation details can be found in Appendix G
Optimization Tasks. Existing LLM-based methods can demonstrate near-optimal or optimal perfor-
mance on some benchmark problems, such as TSP [8, 9, 11] (aided by ACO solvers) and knapsack
problem (KP) [11], leaving little room for further improvement. Therefore, we focus on tasks that
remain challenging for LLM-based AHD as follows: Online Bin Packing (OBP) problem and TSP
under step-by-step construction task, CVRP and Orienteering Problem (OP) under an ACO search
framework. Detailed problem descriptions can be found in Appendix G.3
6


--- Page 7 ---
Baselines. To evaluate CALM, we compare its designed heuristics against the following baselines:
(1) hand-crafted heuristics such as Best-Fit [38] for OBP, Greedy-Construct (GC) [39] for TSP, and
ACO [40] for CVRP and OP; (2) Nerual Combinatorial Optimization (NCO) methods including
POMO [14] and DeepACO [41]; and (3) LLM-based AHD approaches like FunSearch [13], EoH [8],
ReEvo [9], HSEvo [10], and MCTS-AHD [11]. To ensure a fair comparison, we align CALM and
all LLM-based AHD baselines with consistent settings, including shared seed heuristics, identical
training datasets for evaluating heuristic performance, and comparable evaluation budgets‚Äìspecifically,
1,000 heuristic evaluations for baselines and a fixed budget of 2,000 LLM queries for CALM across
all tasks except OBP. Notably, prior AHD methods typically conduct 2,000 heuristic evaluations
using over 4,000 queries for OBP, whereas CALM operates under a fixed budget of 2,000 queries.
5.1
Overall Results
Table 1: Optimality gaps of construction heuristics for OBP, averaged over three runs. All methods
use the same training and test datasets as [11]. Gaps are computed against a strong lower bound [42].
Test instances follow the Weibull distribution, with scales either matching (underlined) or differing
from training. Format: 1k_100 denotes instances with 1, 000 items and a bin capacity of 100.
Online Bin Packing (OBP)
Test sets
1k_100
1k_500
5k_100
5k_500
10k_100
10k_500
Avg.
Best Fit [38]
4.77%
0.25%
4.31%
0.55%
4.05%
0.47%
2.40%
First Fit
5.02%
0.25%
4.65%
0.55%
4.36%
0.50%
2.56%
LLM-based AHD: GPT-4o-mini (w/o. GRPO)
FunSearch [13]
2.45%
0.66%
1.30%
0.25%
1.05%
0.21%
0.99%
EoH [8]
2.69%
0.25%
1.63%
0.53%
1.47%
0.45%
1.17%
ReEvo [9]
3.94%
0.50%
2.72%
0.40%
2.39%
0.31%
1.71%
HSEvo [10]
2.64%
1.07%
1.43%
0.32%
1.13%
0.21%
1.13%
MCTS-AHD [11]
2.45%
0.50%
1.06%
0.32%
0.74%
0.26%
0.89%
CALM (Ours)
2.78%
0.29%
0.83%
0.28%
0.50%
0.24%
0.82%
LLM-based AHD: Phi3.5-mini-Instruct(w/. DPO, Over 10√ó LLM Queries)
EvoTune [33]
4.77%
0.25%
4.27%
0.40%
4.05%
0.42%
2.36%
LLM-based AHD: Qwen2.5-7B-Instruct-INT4 (w/. GRPO)
CALM (Ours)
2.55%
0.00%
0.85%
0.17%
0.56%
0.14%
0.71%
Table 2: Performance of step-by-step construction
heuristics on TSP, averaged over three runs. All
methods are evaluated on three test sets of 1,000
instances each, using the same training and testing
datasets as in [11]. Test sets with in-domain scales
(i.i.d. to the training data) are underlined. Optimal
tour lengths are obtained by LKH [43]. The best
LLM-based result in each column is shaded gray,
and the overall best result is shown in bold.
Traveling Salesman Problem (TSP)
N=50
N=100
N=200
Methods
Obj.‚Üì
Gap‚Üì
Obj.‚Üì
Gap‚Üì
Obj.‚Üì
Gap‚Üì
Optimal
5.675
‚Äì
7.768
‚Äì
10.659
‚Äì
GC [39]
6.959
22.62%
9.706
24.94%
13.461
26.29%
POMO [14]
5.697
0.39%
8.001
3.01%
12.897
20.45%
LLM-based AHD: GPT-3.5-turbo (w/o. GRPO)
FunSearch [13]
6.683
17.75%
9.240
18.95%
12.808
19.61%
EoH [8]
6.390
12.59%
8.930
14.96%
12.538
17.63%
MCTS-AHD [11]
6.346
11.82%
8.861
14.08%
12.418
16.51%
LLM-based AHD: GPT-4o-mini (w/o. GRPO)
FunSearch [13]
6.357
12.00%
8.850
13.93%
12.372
15.54%
EoH [8]
6.394
12.67%
8.894
14.49%
12.437
16.68%
MCTS-AHD [11]
6.225
9.69%
8.684
11.79%
12.120
13.71%
CALM (Ours)
6.273
10.54%
8.691
11.88%
12.104
13.56%
LLM-based AHD: Qwen2.5-7B-Instruct-INT4 (w. GRPO)
CALM (Ours)
6.244
10.04%
8.668
11.58%
12.088
13.41%
OBP. We train and evaluate CALM on the same
dataset used in [11], which includes four train-
ing instances at varying scales and five testing
instances spanning six different scales‚Äîtwo of
which are out-of-domain and not seen during
training. Results in Table 1 show that CALM
consistently outperforms all baseline methods in
terms of average optimality gap across the full
test set. It can achieve superior performance on
out-of-domain and in-domain scales. Remark-
ably, CALM achieves a zero gap in set 1k_500,
indicating exact optimal solutions at that scale.
As EvoTune [33] does not release its code, train-
ing procedure, or test data, we adapted its re-
ported heuristic to our context for comparison.
We used the hyperparameter optimization frame-
work Optuna [44] to tune all hyperparameters
over 2000 trials, selecting the best configura-
tion based on the average optimality gap across
all test sets. While EvoTune outperforms some
prior RL-free LLM-based baselines on certain
test sets, our method consistently achieves better
results than EvoTune across all test sets. Further-
more, according to EvoTune‚Äôs reported data, their approach improves FunSearch by 2.88%~15.29%
under the same LLM on OBP. In contrast, our CALM framework outperforms FunSearch by 38.69%
7


--- Page 8 ---
(averaged over the performance improvements across all test sets). These results highlight the greater
efficiency and robustness of our LLM-based AHD framework.
Table 3: Performance of ACO-based heuristics on CVRP and OP, averaged over three runs. All
methods are evaluated on three test sets of 64 randomly generated instances each, following the setup
in [11] and [10], respectively. Optimal solutions are approximated using DeepACO with significantly
more ants and iterations than those in the baseline configurations.
CVRP
OP
N=50
N=100
N=200
N=50
N=100
N=200
Methods
Obj.‚Üì
Gap‚Üì
Obj.‚Üì
Gap‚Üì
Obj.‚Üì
Gap‚Üì
Obj.‚Üë
Gap‚Üì
Obj.‚Üë
Gap‚Üì
Obj.‚Üë
Gap‚Üì
Optimal
8.888
‚Äì
14.932
‚Äì
27.159
‚Äì
19.867
‚Äì
36.392
‚Äì
63.380
‚Äì
ACO
18.581
109.05%
30.107
101.63%
37.590
40.69%
13.354
32.69%
24.131
33.69%
37.586
40.69%
LLM-based AHD: GPT-4o-mini (w/o. GRPO)
EoH [8]
9.894
11.32%
16.953
13.54%
30.314
11.62%
13.388
32.61%
24.154
33.63%
37.319
41.12%
ReEvo [9]
9.558
7.54%
16.350
9.50%
29.219
7.58%
15.103
23.98%
30.523
16.13%
53.807
15.10%
HSEvo [10]
9.431
6.11%
16.396
9.81%
29.520
8.69%
15.082
24.08%
30.454
16.32%
53.260
15.97%
MCTS-AHD [11]
9.372
5.44%
15.974
6.98%
28.434
4.70%
14.847
25.27%
30.163
17.12%
53.024
16.34%
CALM (Ours)
9.404
5.81%
16.046
7.46%
28.713
5.72%
15.017
24.41%
30.294
16.76%
53.098
16.22%
LLM-based AHD: Qwen2.5-7B-Instruct-INT4 (w/. GRPO)
CALM (Ours)
9.228
3.83%
15.745
5.44%
28.230
3.95%
15.054
24.22%
30.78
15.43%
55.406
12.58%
TSP. CALM is trained on the same dataset used by [11]: a training set of 64 TSP instances with
N = 50 nodes and three test sets of 1,000 instances each at N = 50, 100, and 200. As shown in
Table 2, CALM-constructed heuristics outperform all LLM-based baselines on both out-of-domain
test sets and achieve the second-best LLM-based result on the in-domain set. Notably, at the largest
scale, CALM surpasses the NCO baseline POMO, which requires per-scale training.
CVRP. CALM is trained on 10 instances as in [11] with N = 50 nodes using the ACO framework,
and evaluated on three test sets of 64 instances each at N = 50, 100, and 200, following the same
generation protocol. During both training and testing, the number of ants and iterations is fixed to
30 and 100, respectively. As shown in Table 3, CALM consistently outperforms all LLM-based
baselines across all test sets, including the in-domain one and both out-of-domain ones.
OP. CALM is trained 5 OP instances with N = 50 nodes using the ACO framework and evaluated
on three test sets of 64 instances each at N = 50, 100, and 200, following the generation protocol
in [11]. Both training and testing use a fixed configuration of 20 ants and 50 iterations. As reported in
Table 3, CALM consistently outperforms all other LLM-based baselines on the out-of-domain scales.
As for the in-domain scale, it still outperforms EoH and the most recent approach MCTS-AHD.
5.2
Discussion
Efficacy of our verbal gradient. For each problem instance, we further evaluate the design of our
verbal gradient in isolation (i.e., without GRPO) by (1) switching the backend to the GPT-4o-mini API,
(2) setting G = 1, and (3) using T = 4000 for OBP and T = 2000 for all other tasks‚Äîmatching the
query budgets of prior LLM-based AHD methods. As shown in Tables 1‚Äì3, this API-based variant of
CALM delivers performance on par with or superior to the recent MCTS-AHD approach: it achieves
the lowest optimality gaps on the 5k_100 and 10k_100 OBP datasets and ranks second on average
across all OBP test sets, matches MCTS-AHD and outperforms all other baselines on every CVRP
test set, consistently surpasses MCTS-AHD on all OP instances, and closely tracks MCTS-AHD
on TSP at N = 50 and 100 while outperforming all non-MCTS baselines at those scales and even
surpassing MCTS-AHD at N = 200. These results demonstrate that, even without reinforcement
learning or advanced techniques such as reflection [9, 10] and tree search [11], CALM‚Äôs verbal
guidance mechanism remains highly effective, placing the API-based CALM firmly within the top tier
of existing LLM-based AHD methods.
Power of RL. We have tested the performance of CALM without the GRPO algorithm and under many
ablation settings. As shown in Table 4, results demonstrate that disabling the GRPO module causes
the largest drop in performance across near all ablations. In other words, The reinforcement-learning
component has the most significant impact on overall performance among all ablation settings.
Morever, as illustrated in Table 1~3, with GRPO and our custom reward, the Qwen2.5-7B-Instruct-
INT4‚Äìderived heuristic not only closes the gap but actually outperforms the GPT-4o-mini‚Äìbased
heuristic. We have also visualized the training curve in Figure 2. Results show CALM‚Äôs heuristics lag
8


--- Page 9 ---
early‚Äîlikely due to GPT-4o-mini‚Äôs head start‚Äîbut as GRPO adapts the LLM, its heuristics converge
and outperform all baselines. This suggests the transformative power of RL in enhancing AHD.
Impact of reward design. Our feasible-response reward allocates credit by comparing each gen-
erated heuristic against its parent(s), rather than attributing full reward or blame solely to the
LLM. We evaluate two alternative schemes (keeping the infeasible-response penalty unchanged): (i)
performance-based reward, where a feasible heuristic receives a positive reward proportional to its
performance relative to the seed algorithm; and (ii) the {0.5 rrand, 1}-improvement reward, which
assigns reward 1 if the new heuristic outperforms all parent or baseline heuristics, and 0.5 rrand
otherwise. Both alternatives remove the trivial-reproduction penalty and mitigate the performance
bias present in Equation (4). As Table 4 demonstrates, neither variant beats our original design:
the performance-based scheme underperforms even the no-RL baseline on the OP problem, while
the {0.5 rrand, 1}-improvement strategy delivers closer but still inferior results compared to our
proposed reward function. This confirms the effectiveness of our original reward design.
0
500
1000
1500
2000
# LLM queries
10.0
12.5
15.0
17.5
20.0
Objective
CALM
EoH
ReEvo
MCTS-AHD
(a) CVRP.
0
500
1000
1500
2000
# LLM queries
13.5
14.0
14.5
15.0
Objective
CALM
MCTS-AHD
EoH
ReEvo
(b) OP.
Figure 2: Objective score of the best heuristic in
training averaged over 3 runs (std. dev. shaded).
Table 4: Optimality gap under ablation settings for
problem OBP and OP averaged over three runs.
Method
OBP
OP
CALM (local, w/ GRPO)
0.71%
17.41%
CALM (API, w/o GRPO)
0.82%
19.13%
RL-based Fine-tuning
local, w/o GRPO
1.78%
19.89%
rew‚àà{0.5rrand, 1}
1.04%
17.44%
rew=performance
1.24%
21.30%
Collapse Mechanism
w/o Collapse
0.98%
19.57%
Œ¥0 = 0.0005, C = 15
0.77%
18.31%
Œ¥0 = 0.005, C = 15
1.93%
27.22%
Œ¥0 = 0.0005, C = ‚àû
0.96%
19.50%
Œ¥0 = 0.005, C = ‚àû
0.98%
18.38%
Operators
w/o diversity
1.05%
19.44%
w/o crossover
0.88%
18.49%
w/o injection
1.11%
18.68%
w/o replacement
1.20%
17.57%
w/o simplification
1.35%
19.45%
Impact of collapse. We examine the impact of
the collapse mechanism by analyzing the heuris-
tics produced by CALM both without collapse
and under various hyperparameter configura-
tions that influence when collapse is triggered.
As shown in Table 4, incorporating the col-
lapse mechanism generally enhances the heuris-
tic search process. An exception arises in the
configuration with the strictest tolerance for not
discovering a breakthrough heuristic (i.e., when
Œ¥0 = 0.005 and C = 15). A detailed analysis
of the evolutionary trajectory under this setting
reveals a significantly reduced number of break-
throughs. In one run on the OP problem, no
breakthrough heuristic was identified after the
132nd LLM query. These findings suggest that
setting a reasonable tolerance for the absence
of breakthroughs‚Äîbalancing patience with the
benefits of early stopping‚Äîis important for sup-
porting a more effective evolution.
Impact of operators. We evaluate each opera-
tor‚Äôs contribution by measuring CALM‚Äôs perfor-
mance with that operator removed (Table 4). Re-
sults show all operators positively impact heuris-
tic quality. Crossover, injection, and replace-
ment are similarly critical‚Äîremoving any one
notably degrades performance in either OBP or
OP. Among all, removing simplification causes
the largest drop in both tasks, likely because it
uniquely reduces redundancy and curbs com-
plexity, counterbalancing other operators that
tend to increase heuristic length.
Moreover,
when crossover is applied without diversity-
based selection‚Äîusing only performance-based
sampling‚ÄîCALM performs worse than with no crossover at all, highlighting the importance of
diversity awareness in the most-used operator.
6
Conclusion
This paper introduces CALM, the first framework to marry prompt evolution with on-the-fly LLM
adaptation for AHD, freeing it from the constraints of fixed-model approaches. Running entirely on a
single 24 GB GPU with a compact foundation model, CALM autonomously uncovers heuristics that
outmatch SOTA API-based baselines across various challenging optimization scenarios. Moreover,
even without the power of RL, CALM matches or exceeds prior best results using the same LLM
9


--- Page 10 ---
API, demonstrating the potency of our verbal-gradient designs. In the future, we expect that scaling
CALM‚Äôs paradigm to larger models and extended post-training could further push the frontier of
automated algorithm discovery.
References
[1] Jiahui Duan, Xialiang Tong, Fei Ni, Zhenan He, Lei Chen, and Mingxuan Yuan. A data-driven
column generation algorithm for bin packing problem in manufacturing industry. arXiv preprint
arXiv:2202.12466, 2022.
[2] Giulia Tresca, Graziana Cavone, Raffaele Carli, Antonio Cerviotti, and Mariagrazia Dotoli.
Automating bin packing: A layer building matheuristics for cost effective logistics. IEEE
Transactions on Automation Science and Engineering, 19(3):1599‚Äì1613, 2022.
[3] Bachir Mihoubi, Brahim Bouzouia, and Mehdi Gaham. Reactive scheduling approach for
solving a realistic flexible job shop scheduling problem. International journal of production
research, 59(19):5790‚Äì5808, 2021.
[4] Jessica Coto Palacio, Yailen Mart√≠nez Jim√©nez, Leander Schietgat, Bart Van Doninck, and Ann
Now√©. A q-learning algorithm for flexible job shop scheduling in a real-world manufacturing
scenario. Procedia CIRP, 106:227‚Äì232, 2022.
[5] Nadia Dahmani, Ines Sbai, Takwa Tlili, and Saoussen Krichen. On solving the 2l-cvrp using an
adaptive chemical reaction algorithm: postal transportation real-case. International Journal of
System Assurance Engineering and Management, pages 1‚Äì25, 2024.
[6] Rafael HM Pereira, Marcus Saraiva, Daniel Herszenhut, Carlos Kaue Vieira Braga, and
Matthew Wigginton Conway. r5r: rapid realistic routing on multimodal transport networks with
r 5 in r. Findings, 2021.
[7] Edmund K Burke, Mathew R Hyde, Graham Kendall, Gabriela Ochoa, Ender Ozcan, and John R
Woodward. Exploring hyper-heuristic methodologies with genetic programming. Computational
intelligence: Collaboration, fusion and emergence, pages 177‚Äì201, 2009.
[8] Fei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and
Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using
large language model. In International Conference on Machine Learning, pages 32201‚Äì32223.
PMLR, 2024.
[9] Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo
Park, and Guojie Song. Reevo: Large language models as hyper-heuristics with reflective
evolution. In Advances in Neural Information Processing Systems, 2024.
[10] Pham Vu Tuan Dat, Long Doan, and Huynh Thi Thanh Binh. Hsevo: Elevating automatic
heuristic design with diversity-driven harmony search and genetic algorithm using llms. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 26931‚Äì26938,
2025.
[11] Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, and Bryan Hooi. Monte carlo tree search for
comprehensive exploration in llm-based automatic heuristic design. In International Conference
on Machine Learning. PMLR, 2025.
[12] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical
reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
[13] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog,
M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang,
Omar Fawzi, et al. Mathematical discoveries from program search with large language models.
Nature, 625(7995):468‚Äì475, 2024.
[14] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai
Min. Pomo: Policy optimization with multiple optima for reinforcement learning. Advances in
Neural Information Processing Systems, 33:21188‚Äì21198, 2020.
10


--- Page 11 ---
[15] Yuxin Pan, Yize Chen, and Fangzhen Lin. Adjustable robust reinforcement learning for online
3d bin packing. Advances in Neural Information Processing Systems, 36:51926‚Äì51954, 2023.
[16] Jieyi Bi, Yining Ma, Jianan Zhou, Wen Song, Zhiguang Cao, Yaoxin Wu, and Jie Zhang.
Learning to handle complex constraints for vehicle routing problems. Advances in Neural
Information Processing Systems, 37:93479‚Äì93509, 2024.
[17] Fu Luo, Xi Lin, Zhenkun Wang, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang.
Self-improved learning for scalable neural combinatorial optimization.
arXiv preprint
arXiv:2403.19561, 2024.
[18] Pei Xiao, Zizhen Zhang, Jinbiao Chen, Jiahai Wang, and Zhenzhen Zhang. Neural combinatorial
optimization for robust routing problem with uncertain travel times. Advances in Neural
Information Processing Systems, 37:134841‚Äì134867, 2024.
[19] Jingyan Sui, Shizhe Ding, Boyang Xia, Ruizhi Liu, and Dongbo Bu. Neuralgls: learning to
guide local search with graph convolutional network for the traveling salesman problem. Neural
Computing and Applications, 36(17):9687‚Äì9706, 2024.
[20] Zhi Zheng, Shunyu Yao, Genghui Li, Linxi Han, and Zhenkun Wang. Pareto improver: Learning
improvement heuristics for multi-objective route planning. IEEE Transactions on Intelligent
Transportation Systems, 25(1):1033‚Äì1043, 2023.
[21] Daniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin
Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster
sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257‚Äì263,
2023.
[22] Wenjie Yi, Rong Qu, Licheng Jiao, and Ben Niu. Automated design of metaheuristics using
reinforcement learning within a novel general search framework.
IEEE Transactions on
Evolutionary Computation, 27(4):1072‚Äì1084, 2022.
[23] Nelishia Pillay and Rong Qu. Hyper-heuristics: theory and applications. Springer, 2018.
[24] Xavier S√°nchez-D√≠az, Jos√© Carlos Ortiz-Bayliss, Ivan Amaya, Jorge M Cruz-Duarte, Santi-
ago Enrique Conant-Pablos, and Hugo Terashima-Mar√≠n. A feature-independent hyper-heuristic
approach for solving the knapsack problem. Applied Sciences, 11(21):10209, 2021.
[25] Henrik Abgaryan, Ararat Harutyunyan, and Tristan Cazenave. Llms can schedule. arXiv
preprint arXiv:2408.06993, 2024.
[26] Xia Jiang, Yaoxin Wu, Yuan Wang, and Yingqian Zhang. Unco: Towards unifying neural
combinatorial optimization through large language model. arXiv preprint arXiv:2408.12214,
2024.
[27] Duo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang Cui, and Fangxin
Wang. Netllm: Adapting large language models for networking. In Proceedings of the ACM
SIGCOMM 2024 Conference, pages 661‚Äì678, 2024.
[28] Laurent Perron and Vincent Furnon. Or-tools. URL https://developers.google.com/
optimization/.
[29] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large
language model. arXiv preprint arXiv:2311.15249, 2023.
[30] Zijie Chen, Zhanchao Zhou, Yu Lu, Renjun Xu, Lili Pan, and Zhenzhong Lan. Qube: Enhancing
automatic heuristic design via quality-uncertainty balanced evolution, 2025. URL https:
//arxiv.org/abs/2412.20694.
[31] Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang, Zhichao Lu, and
Qingfu Zhang. Llm4ad: A platform for algorithm design with large language model. arXiv
preprint arXiv:2412.17287, 2024.
11


--- Page 12 ---
[32] Alexander Novikov, Ng√¢n VÀúu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang,
Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas
Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies,
Sebastian Nowozin, Pushmeet Kohli, and Matej Balog.
AlphaEvolve: A coding agent
for scientific and algorithmic discovery.
Technical report, Google DeepMind, 05 2025.
URL
https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/
alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/
AlphaEvolve.pdf.
[33] Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel
Abbe, and Caglar Gulcehre.
Algorithm discovery with llms: Evolutionary search meets
reinforcement learning. arXiv preprint arXiv:2504.05108, 2025.
[34] John Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/
kl-approx.html. Accessed: 2025-05-11.
[35] Larry J Eshelman. Preventing premature convergence in genetic algorithms by preventing incest.
In Proceedings of Fourth International Conference on Genetic Algorithms, 1991, 1991.
[36] Michael Han Daniel Han and Unsloth team. Unsloth, 2023. URL http://github.com/
unslothai/unsloth.
[37] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint
arXiv:2412.15115, 2024.
[38] Claire Kenyon.
Best-fit bin-packing with random order.
PhD thesis, Laboratoire de
l‚Äôinformatique du parall√©lisme, 1995.
[39] Daniel J Rosenkrantz, Richard E Stearns, and Philip M Lewis, II. An analysis of several
heuristics for the traveling salesman problem. SIAM journal on computing, 6(3):563‚Äì581, 1977.
[40] Christian Blum. Ant colony optimization: Introduction and recent trends. Physics of Life
reviews, 2(4):353‚Äì373, 2005.
[41] Haoran Ye, Jiarui Wang, Zhiguang Cao, Helan Liang, and Yong Li. Deepaco: Neural-enhanced
ant systems for combinatorial optimization. Advances in neural information processing systems,
36:43706‚Äì43728, 2023.
[42] Silvano Martello and Paolo Toth. Lower bounds and reduction procedures for the bin packing
problem. Discrete applied mathematics, 28(1):59‚Äì70, 1990.
[43] Shen Lin and Brian W Kernighan. An effective heuristic algorithm for the traveling-salesman
problem. Operations research, 21(2):498‚Äì516, 1973.
[44] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
SIGKDD international conference on knowledge discovery & data mining, pages 2623‚Äì2631,
2019.
[45] Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Mohammad Bahrami Karkevandi, Gonzalo
De La Torre Parra, Elias Bou-Harb, and Peyman Najafirad. Llm-powered code vulnerability
repair with reinforcement learning and semantic reward. arXiv preprint arXiv:2401.03374,
2024.
[46] Yun-Da Tsai, Mingjie Liu, and Haoxing Ren. Code less, align more: Efficient llm fine-tuning
for code generation with data pruning. arXiv preprint arXiv:2407.05040, 2024.
[47] Junqiao Wang, Zeng Zhang, Yangfan He, Yuyang Song, Tianyu Shi, Yuchen Li, Hengyuan
Xu, Kunyu Wu, Guangwu Qian, Qiuwu Chen, et al. Enhancing code llms with reinforcement
learning in code generation. arXiv preprint arXiv:2412.20367, 2024.
[48] Wei Shen and Chuheng Zhang. Policy filtration in rlhf to fine-tune llm for code generation.
arXiv preprint arXiv:2409.06957, 2024.
12


--- Page 13 ---
[49] Junjie Li, Aseem Sangalay, Cheng Cheng, Yuan Tian, and Jinqiu Yang. Fine tuning large
language model for secure code generation. In Proceedings of the 2024 IEEE/ACM First
International Conference on AI Foundation Models and Software Engineering, pages 86‚Äì90,
2024.
[50] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[51] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-
hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425,
2023.
[52] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and
Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint
arXiv:2309.06657, 2023.
13


--- Page 14 ---
A
Extended Discussion about Related Work
LLM for Code Generation. Recent work has explored improving LLMs‚Äô code generation capabilities
through post-training [45‚Äì49]. For example,[45] employs RL and semantic feedback to repair
vulnerabilities, while[47] demonstrates RL‚Äôs effectiveness in enhancing code quality. Despite surface
similarities, our task differs fundamentally: in code generation, objectives often prioritize pass
rates [46‚Äì48] or safety [45, 49], whereas our goal is to maximize heuristic performance. Moreover,
in code generation, fine-tuning aims to produce a generally stronger model, while in our case, both
the model tuning and prompt evolution serve a singular goal‚Äîimproving the quality of generated
heuristics.
RL for LLM Fine-tuning. Reinforcement learning is a central technique for fine-tuning large
language models, with the RLHF paradigm commonly relying on Proximal Policy Optimization
(PPO)[50] to iteratively refine model outputs based on human feedback. Building on this, Group
Relative Policy Optimization (GRPO)[12] simplifies training by removing the need for a separate
value network, instead estimating baselines over groups of candidate completions‚Äîleading to
improved sample efficiency and stability. Other alternatives such as Direct Preference Optimization
(DPO), SLiC-HF [51], and Rejection Sampling Optimization (RSO)[52] offer off-policy mechanisms
that further reduce computational burden. While we do not aim to develop new fine-tuning algorithms,
our method integrates GRPO within the broader co-evolution framework to adapt the LLM in tandem
with heuristic evolution. We specifically adopt GRPO because it requires only a scalar signal per
prompt-response pair (in contrast to preference-based signals), making it suitable for our setting.
Moreover, we implement fine-tuning using Unsloth[36], a GPU-efficient open-source framework
that enables fast, memory-light training even on single consumer-grade GPUs‚Äîmaking our method
especially practical and accessible for researchers with limited hardware resources.
14


--- Page 15 ---
B
Complete Algorithm
Algorithm 1: CALM
Input
:LLM œÄŒ∏, Evaluation environment g, number G of responses to be sampled for one
prompt, maximum round number T, Population size Lp, Sampling weight w for each
operator, Hyperparameter Œ¥0 and C that control the collapse mechanism, set of seed
heuristic Hseed (set to be ‚àÖif not given any seed heuristic).
Initialize collapse counter tc = ‚àí1, best heuristic h‚àó= null, best performance g‚àó= ‚àí‚àû,
heuristic pool Hpool = Hseed, wi = winjection;
for t = 1, ¬∑ ¬∑ ¬∑ , T do
Operator base OPs ‚Üê{Initialization};
if |Hpool| ‚â•1 then
OPs ‚Üê{Injection, Replacement, Crossover, Simplification};
end
if |Hpool| ‚â•2 then
OPs ‚ÜêOPs ‚à™{Crossover};
end
if |Hpool| < Lp then
wInjection ‚Üêmax(w);
else
wInjection ‚Üêwi;
Hbase ‚Üê‚àÖ, op ‚ÜêDraw an operator from OPs with the probability proportional to w;
if op Ã∏= Initialization then
hc,1 ‚ÜêDraw an heuristic from top-Lp-performing heuristics in Hpool, where the
sampling probability of an heuristic h is proportional to 1/rankp(h) and rankp(h) is
the heuristic‚Äôs performance rank;
Hbase ‚ÜêHbase ‚à™{h};
if op = Crossover then
if random(0, 1) ‚â§0.5 then
hc,2 ‚ÜêDraw a heuristic from the population by performance rank as sampling
hc,1;
else
Calculate diversity metric
div(hc,1, h) = |idea_token(h)\idea_token(hc,1)|
|idea_token(h)|
, ‚àÄh ‚ààHpool;
hc,2 ‚ÜêDraw a heuristic from the pool by diversity rank where the sampling
probability is proportional to 1/rankd(h) (a larger diversity value yields a
higher probability);
Hbase ‚ÜêHbase ‚à™{hc,2};
end
end
q ‚ÜêGenerate prompt by the operator op and base heuristics Hbase;
O ‚ÜêSample G responses from œÄŒ∏ for q;
Hfeasible, ÀÜrO ‚ÜêTry extracting a feasible heuristic from each response o ‚ààO and assign
reward to each response following Section 4.3;
Œ∏ ‚ÜêUpdate the LLM by GRPO that optimizes Equation 1 with (q, O, ÀÜrO);
Hpool ‚ÜêHpool ‚à™H{‚åâ‚ä£‚à´‚ü©‚åä‚Üï‚åâ;
h‚àó‚Üêarg maxh‚ààHpool g(h);
if g(h‚àó) = g‚àóand |Hpool| ‚â•Lp then
/* If the population is full, the counter for collapse starts.
*/
tc ‚Üêmax(tc, 0) + 1;
else
g‚àó= (h‚àó), tc ‚Üêmin(tc, 0);
if random(0, 1) ‚â§Œ¥0tc or tc ‚â•C then
Hbase ‚Üê{h‚àó} ‚à™Hseed, tc ‚Üê‚àí1; /* Collapse
*/
end
end
Return :h‚àó
15


--- Page 16 ---
Searching superior heuristics on the {problem.name} problem in an evolutionary manner through 
conversation between User and Assistant. In this problem, {problem.description} The User 
provides existing algorithms and requests a new one.
## Your Task
You should first present a concise conceptual description, followed by a complete code 
implementation.
* The description must:
* Be enclosed with a double brace and starts with "The idea of the algorithm is to".
* Ensure it is self-contained, insightful, and creatively original.
* Not reference or rely on any prior ideas or existing code.
* The code must:
* Strictly follow the input-output variable names and types used in the provided implementation.
* Be a single Python function formatted within Python code blocks.
* Exclude any usage examples.
* Ensure the algorithm is deterministic.
* Avoid introducing unnecessary, arbitrarily-tuned hyperparameters; any parameters used should 
be essential and systematically derived from the input.
Overall, your response should be like:
{{The idea of the algorithm is to (sepcific description here)}}
```python
your code here
```
Except for the idea and code, do not give additional explanations or comments.
Figure 3: Template of the system prompt.
C
Prompts Used in CALM
System Prompt. The system prompt is generated by inserting the name and description into the
template shown in Figure 3. The specific prompt used for each problem can be found in Table 5.
Injection Prompt. The template used to generate injection prompts is shown in Figure 4. In the
prompt template, the algorithm details are generated by the given heuristics and the prompt template
in Figure 8. The description of the most recent injected components is created by (1) parsing the
string wrapped within "The new component ... has been introduced", (2) globally saving the historical
new components, and (3) picking the last 10 new components to be used.
Replacement Prompt. The replacement prompt is created by the template, some predefined compo-
nent Paris shown in Figure 5, and the algorithm detail template shown in Figure 8.
Crossover Prompt. The crossover prompt is generated by the template shown in Figure 6 and the
algorithm detail template shown in Figure 8.
Simplification Prompt. The simplification prompt is created by the template shown in Figure 7 and
the algorithm detail template shown in Figure 8.
Initialization Prompt. The initialization prompt is created by the template shown in Figure 9. The
algorithm template is a function signature.
16


--- Page 17 ---
Inject a novel, meaningful component into the following algorithm. The component may be self-
devised or inspired by ideas from other domains or problems.
{algorithm_details(given_heuristics)}
Use a concise noun phrase to describe the new component in the responded idea like "The new 
component ... has been introduced.". Exclude the following components that have already been 
explored: {description of most recent injected components}
Figure 4: Template of the injection prompt.
For the following algorithm, identify {old_component} and rewrite it to {new_component}.
{algorithm_details(given_heuristics)}
new_component
old_component
a deterministic rule that derives its value from the 
current input instance
a fixed, instance-independent decision rule
a more principled constant justified by theory or 
practice
a key hyper-parameter expressed as either a 
constant literal or a stationary variable
a fragment where credits are deterministically 
and reasonably differentiated
a fragment that assigns equal or near-equal 
credits to multiple elements
Figure 5: Template of the replacement prompt.
Please generate a new algorithm that is motivated by the following algorithms but performs better 
on any same instance.
{algorithm_details(given_heuristics)}
Figure 6: Template of the crossover prompt.
Please create a simplified and more elegant version of an algorithm by distilling and refining the 
core ideas from the following: 
{algorithm_details(given_heuristics)}
Figure 7: Template of the simplification prompt.
...
## Algorithm k
* Performance: {heuristic.performance} (Rank {heurisitc.rank})
* Idea: {heuristic.idea} {heuristic.unit}
* Code:
```python
{heuristic.code}
```
...
Figure 8: Template of algorithm details.
17


--- Page 18 ---
Be very creative and inventive. Generate an efficient algorithm following the template below: 
{algorithm_template}
Figure 9: Template of the initialization prompt.
Table 5: Information of each problem used in prompts
OBP
CVRP
OP
TSP
Name
Online Bin
Packing
Capacitated Vehicle
Routing
Orienteering
Traveling
Salesman
Unit
percent of
the gap to the
lower bound
units of travel distance
units of collected
reward
length units
of the tour
Description Items arrive
sequentially
and must be
placed imme-
diately into
bins only if
they fit within
the remaining
capacity. The
objective is to
minimize the
number of bins
used.
A fleet of vehicles with
limited carrying capacity
must deliver goods to a
set of geographically dis-
tributed customers with
known demands, mini-
mizing the total travel
distance while ensuring
that no vehicle exceeds
its capacity. The goal
is to design a heuristic
function that estimates
the desirability of mov-
ing between customers,
to be used within an Ant
Colony Optimization
(ACO) algorithm.
An agent must visit
a subset of locations,
each offering a re-
ward, within a max-
imum travel budget.
The objective is to
maximize the total
collected reward
while adhering to
the travel constraint.
The goal is to design
a heuristic function
that estimates the de-
sirability of moving
between locations, to
be used within an Ant
Colony Optimization
(ACO) algorithm.
The algorithm
must find a
tour that visits
each node
exactly once
and returns to
the start node.
The objective
is to minimize
the length of
the tour.
18


--- Page 19 ---
D
Discussion about the Diversity-Aware Crossover Operator
Notably,[11] allowed heuristic selection beyond the top-performing population, offering greater
exploration flexibility, though without explicitly modeling diversity. In contrast,[10] emphasized the
role of diversity in heuristic evolution but did not integrate it into crossover and operated within a
fixed-size population. Therefore, CALM‚Äôs crossover operator complements prior work by explicitly
incorporating diversity into the crossover process.
E
More Details For the Collapse Mechanism
E.1
Proof of Equation (2)
Let cn be the stagnation counter just before collapse. Under the collapse mechanism with per-round
hazard
pk = k Œ¥0,
k = 1, 2, . . . ,
the probability of surviving beyond k rounds is
Pr[cn > k] =
k
Y
i=1
 1 ‚àíi Œ¥0

,
which vanishes for k ‚â•‚åä1/Œ¥0‚åã.
By definition,
E[ cn ] =
‚àû
X
k=1
k Pr[cn = k].
Introduce the nonnegative array
aj,k =
Pr[cn = k],
k ‚â•j ‚â•1,
0,
otherwise.
Then
‚àû
X
k=1
k Pr[cn = k] =
‚àû
X
k=1
k
X
j=1
Pr[cn = k] =
‚àû
X
k=1
‚àû
X
j=1
aj,k.
Since aj,k ‚â•0, Tonelli‚Äôs theorem allows swapping the sums:
‚àû
X
k=1
‚àû
X
j=1
aj,k =
‚àû
X
j=1
‚àû
X
k=1
aj,k =
‚àû
X
j=1
‚àû
X
k=j
Pr[cn = k] =
‚àû
X
j=1
Pr[cn ‚â•j] =
‚àû
X
j=0
Pr[cn > j].
Hence the tail-sum identity
E[ cn ] =
‚àû
X
j=0
Pr[cn > j].
For Œ¥0 ‚â™1 we approximate the finite product by exponentiating its logarithm, using the Maclaurin
expansion
ln(1 ‚àíx) = ‚àí
‚àû
X
m=1
xm
m = ‚àíx ‚àíx2
2 ‚àí¬∑ ¬∑ ¬∑ ,
|x| < 1,
with x = iŒ¥0. Truncating at the linear term gives
k
X
i=1
ln
 1 ‚àíi Œ¥0

‚âà‚àí
k
X
i=1
i Œ¥0 = ‚àíŒ¥0
2 k(k + 1) ‚âà‚àíŒ¥0
2 k2,
so
Pr[cn > k] ‚âàexp
 ‚àíŒ¥0
2 k2
.
Substituting into the tail-sum and replacing the discrete sum by an integral yields
E[ cn ] ‚âà
‚àû
X
k=0
e‚àíŒ¥0
2 k2 ‚âà
Z ‚àû
0
e‚àíŒ¥0
2 x2 dx =
r œÄ
2 Œ¥0
,
which establishes Equation (2).
19


--- Page 20 ---
E.2
Goodness
Following this reset, the search effectively starts anew, but with a strategic advantage: it builds upon
the best insights discovered so far. Importantly, during the early stage of repopulation, the system
temporarily relaxes selection constraints. New heuristics generated via injection, replacement, or
crossover are allowed into the population regardless of performance, as long as the total number
of heuristics remains below the target population size. This gives structurally novel but potentially
suboptimal components the opportunity to propagate and evolve‚Äîsomething not feasible under
normal selection pressure, where only top-performing heuristics are retained and processed further.
F
Justification for Penalizing Random Modules
Randomized heuristics are excluded in this work because their stochastic behavior substantially
increases evaluation cost and noise. To enforce determinism, CALM penalizes responses that invoke
randomness (e.g., usage of random, np.random, etc.). The framework could support randomized
heuristics by relaxing this constraint, though evaluation overhead would increase.
G
More Experimental Details
G.1
Implementation Details
We build CALM on Unsloth [36], with two modifications: raising the learning rate to 5 √ó 10‚àí5 for
faster adaptation and sampling G = 4 responses per prompt to enable more evolutionary steps under
a fixed query budget.
We set the initial collapse growth rate to Œ¥0 = 0.0005 (max threshold C = 25), cap training at
T = 500 rounds, and assign operator sampling probabilities in the ratio 1 : 1 : 2 : 4 for simplification,
injection, modification, and crossover, respectively. Each heuristic is evaluated within 60 s [11]. All
experiments ran on a 24 GB NVIDIA A30 GPU with an Intel Xeon Gold 5220R CPU.
Under this configuration, the average running time of CALM for the OBP, CVRP, OP, and TSP is
about 6.8, 7.2, 5.3, and 5.5 hours, respectively, for T = 500 steps. However, it is important to note
that the actual running time for a single trial may vary considerably due to the stochastic nature of
the LLM and the potentially large number of heuristics generated, each requiring time-intensive
evaluation.
G.2
Justification For the Baseline Implementations
The source code, training dataset, and test dataset for AlphaEvolve [32] and EvoTune [33] are not
available. Besides, ReEvo [9] and its follow-up approach HSEvo [10] can stop at a very early stage
in evolution as found by [11]. Thus, the results of them on TSP are not reported.
G.3
Description of Problems in Experiments
Online Bin Packing (OBP). A sequence of items of varying sizes arrives one by one. Each bin has a
fixed capacity. Upon arrival of an item, the algorithm must immediately assign it to an existing bin
that has enough remaining space or open a new bin. The goal is to minimize the total number of bins
used. The input of the heuristic is the size of the current item and the remaining capacities of the bins.
The output of the heuristic is the priority score of each observed bin, where the feasible bin with the
highest score will be selected to accomodate the item.
Traveling Salesman Problem (TSP) under Step-by-Step Construction. Given a set of locations
with pairwise travel distances, the objective is to construct a tour that starts at one location, visits
each other location exactly once, and returns to the start. At each step the heuristic must choose the
next unvisited location based solely on the information gathered so far. The aim is to keep the total
travel distance as small as possible.
Capacitated Vehicle Routing Problem (CVRP) under ACO. A fleet of vehicles with identical
load capacity must serve a set of customers, each with a known demand, and all vehicles start and
end at a central depot. Under the Ant Colony Optimization framework, many artificial ‚Äúants‚Äù build
20


--- Page 21 ---
routes by moving from customer to customer. Each ant‚Äôs choice of next customer is guided by a
combination of pheromone trails‚Äîupdated based on previous high-quality solutions‚Äîand heuristic
scores provided by the LLM. The goal is to serve all customers while minimizing the total distance
traveled and respecting vehicle capacity limits.
Orienteering Problem (OP) under ACO. Starting from a given location (and possibly ending at
the same or another specified location), an agent may visit a subset of available sites, each offering
a reward, subject to an overall travel budget. Within the ACO framework, ants construct candidate
paths by choosing which site to visit next based on pheromone levels and LLM-generated heuristic
scores that estimate the benefit of each edge under the reward-and-budget trade-off. The aim is to
collect as much reward as possible without exceeding the travel budget.
G.4
Generated Heurisitcs
"""
The idea of the
algorithm is to refine the
scoring
mechanism by
introducing
logarithmic
adjustments
and a novel
scoring
component
that
captures
the
logarithmic
relationship
between
the
remaining
capacity
and the square of the item size , and an adjusted
logarithmic
density
term that
provides a more
refined
scoring
mechanism. This new
algorithm
aims to enhance
the
accuracy of bin
suitability
assessment by adding a component
that
adjusts
the
score
based on the
logarithmic
difference
between
the
remaining
capacity
and the
maximum
bin
capacity. The
algorithm
also
simplifies
the
scoring
steps to make it more
elegant
and
efficient.
"""
import
numpy as np
def step(item_size: float , remaining_capacity : np.ndarray) ->
np.ndarray:
max_bin_cap = max( remaining_capacity )
bin_density = np.sum( remaining_capacity ) / (item_size *
len( remaining_capacity ))
log_adj = np.log( remaining_capacity + 1) / np.log(max_bin_cap + 1)
score = ( remaining_capacity - max_bin_cap)**2 / item_size +
remaining_capacity **2 / (item_size **2) +
remaining_capacity **2 / (item_size **3) + bin_density *
remaining_capacity
score[ remaining_capacity > item_size] = -score[ remaining_capacity
> item_size]
score [1:]
-= score [:-1]
score *= log_adj
score += log_adj * remaining_capacity
score *= log_adj
new_component = remaining_capacity / (item_size -
remaining_capacity + 1)
score += new_component
new_component = remaining_capacity * np.log( remaining_capacity +
1) / (item_size * np.log(max_bin_cap + 1)) * (1 -
remaining_capacity / item_size)
score += new_component
new_adjustment = ( remaining_capacity / item_size) * log_adj
score += new_adjustment
remaining_capacity_adjusted = remaining_capacity / item_size
score += np.log( remaining_capacity_adjusted + 1) /
np.log(max_bin_cap + 1)
21


--- Page 22 ---
new_component = ( remaining_capacity - 1) / (item_size -
remaining_capacity + 1) * log_adj / np.log(max_bin_cap + 1)
score += new_component
new_component = log_adj * remaining_capacity / (item_size -
remaining_capacity )
score += new_component
new_component = remaining_capacity * np.log( remaining_capacity +
1) / (item_size **2) * (1 - remaining_capacity / item_size)
score += new_component
return
score
Heuristic 1: OBP, by CALM (local, w/ GRPO)
"""
The idea of the
algorithm is to introduce
the "Bin
Utilization
Diminution" component , which
assesses
the degree of bin usage
throughout
the
sequence of placements
and
introduces a
diminishing
incentive
for
overpopulating
any
particular
bin
beyond a certain
threshold. This
encourages a more even
distribution of item
placements
across all bins , thereby
reducing
the risk of reaching
capacity
too
quickly in any single bin ,
helping to extend the
lifespan
and
utility of each bin in the
packing
process. By dynamically
adjusting
the fit score to favor
items
that
contribute to a balanced
utilization , the
algorithm
aims to enhance
overall
bin
efficiency
and
minimize
the total bin
count.
"""
import
numpy as np
def step(item_size: float , remaining_capacity : np.ndarray) ->
np.ndarray:
avg_item_size = np.mean(item_size) if item_size > 0 else 1.0
adaptive_factor = avg_item_size / np.maximum(remaining_capacity ,
1e -10)
fit_score = np.maximum( remaining_capacity - item_size , 0) /
( remaining_capacity + 1e -10)
fit_score[ remaining_capacity < item_size] = -np.inf
sustainability_score = ( remaining_capacity
- avg_item_size ) ** 2
sustainability_score [ remaining_capacity < item_size] = np.inf
historical_fit_scores = np.cumsum(fit_score)
normalized_historical_fit_scores = historical_fit_scores /
(np.max( historical_fit_scores ) + 1e -10)
combined_scores = (
(0.5 * fit_score * adaptive_factor ) +
(0.3 / ( sustainability_score + 1e -10)) -
(0.2 * normalized_historical_fit_scores )
)
differentiation_factor = 1 / (1 +
np.arange(len( remaining_capacity )) * 0.1)
combined_scores *=
differentiation_factor
cumulative_fit_impact = np.cumsum(fit_score) / (np.arange (1,
len( remaining_capacity ) + 1) + 1)
22


--- Page 23 ---
cumulative_fit_adjustment = np.maximum(fit_score -
cumulative_fit_impact , 0)
combined_scores += 0.4 * cumulative_fit_adjustment
temporal_utilization_metric = np.arange(len( remaining_capacity ))
/ (np.maximum(remaining_capacity , 1e -10) + 1e -10)
combined_scores *= (1 + temporal_utilization_metric )
sequential_elasticity =
np.exp(-np.arange(len( remaining_capacity )) /
(np.mean(np.maximum(remaining_capacity , 1e -10)) + 1e -10))
combined_scores *=
sequential_elasticity
size_factor = 1 + (item_size / (np.sum(item_size) + 1e -10))
# New
Component: Bin
Utilization
Diminution
overutilization_penalty = np.maximum (0, np.cumsum(item_size) /
(np.maximum(np.cumsum( remaining_capacity ), 1e -10) + 1e -10) -
1)
combined_scores
-= 0.3 * overutilization_penalty
# Encourage
even
distribution
across
bins
# Eventual
Capacity
Influence
eventual_capacity_score = np.log(np.maximum(np.arange (1,
len( remaining_capacity ) + 1), 1)) /
(np.maximum(remaining_capacity , 1e -10) + 1e -10)
combined_scores
-= 0.3 * eventual_capacity_score
# Penalize
bins
that don‚Äôt contribute to optimal
utilization
distinct_scores = combined_scores * size_factor
return
distinct_scores
Heuristic 2: OBP, by CALM (API, w/o GRPO)
"""
The idea of the
algorithm is to further
refine the
savings
potential
calculation by emphasizing a more
adaptive
balance
factor
that is
influenced by the
current
instance ‚Äôs capacity
utilization
and the
diversity of capacity
usage
across the
routing
problem. By
leveraging a more
sophisticated
adaptive
balance
factor and
reducing
the
complexity of the
penalty
factor , we ensure
that
nodes
that are too close to each
other are
penalized
appropriately
without
overly
compounding
the impact. This
simplified
yet
adaptive
approach
allows for a nuanced
exploration
of the
solution space , enhancing
the ACO
algorithm ‚Äôs ability to
converge to high -quality
solutions
while
maintaining a balance
between
exploration
and
exploitation . Additionally , we introduce
a clustering -based
adjustment
factor
that
captures
the
overall
network
connectivity
and
adjusts
the
savings
potential
accordingly , leading to more
robust and
flexible
routing
plans.
"""
import
numpy as np
def
advanced_heuristics_v7 ( distance_matrix : np.ndarray , coordinates:
np.ndarray , demands: np.ndarray , capacity: int) -> np.ndarray:
capacity_prob = demands / capacity
distance_reciprocal = 1 / distance_matrix
proximity_factor = np.linalg.norm(coordinates [:, np.newaxis , :] -
coordinates[np.newaxis , :, :], axis =2)
proximity_factor /= np.max( proximity_factor )
# Normalize
between
0 and 1
23


--- Page 24 ---
proximity_factor = 1 - proximity_factor
# Invert for higher
penalty as proximity
increases
remaining_demands = capacity - demands
future_savings = ( remaining_demands [:, np.newaxis] *
remaining_demands ) / ( distance_matrix * ( remaining_demands [:,
np.newaxis] + remaining_demands ))
capacity_ratio = remaining_demands / capacity
proximity_savings = proximity_factor * capacity_ratio
# Cluster -based
proximity
adaptive
savings
potential
cluster_savings = np.zeros_like( distance_matrix )
cluster_distance = np.sum(distance_matrix , axis =1) /
np.linalg.norm(capacity_prob - 1, ord =1)
cluster_adj_factor = ( remaining_demands [:, np.newaxis] *
remaining_demands * cluster_distance ** 3.5) /
( distance_matrix * ( remaining_demands [:, np.newaxis] +
remaining_demands ))
# Adaptive
balance
factor
adjusted
based on remaining
capacity
and
cluster
adjustment
balance_factor = np.min([1, 0.975 + 0.05 * capacity_prob .mean () +
0.03 * cluster_adj_factor .mean () + 0.005 *
np.var(capacity_prob )])
# Penalty
factor
that
heavily
penalizes
nodes
that are too close
to each other , focusing on the
proximity to the next node
penalty_factor = proximity_factor ** 3
# Combine
all
components
probability = distance_reciprocal * capacity_prob *
proximity_factor * future_savings * proximity_savings *
cluster_adj_factor * (1 - balance_factor + proximity_savings
* balance_factor) * (1 - penalty_factor ) * (1 +
cluster_adj_factor * 0.6)
return
probability
Heuristic 3: CVRP, by CALM(local, w/ GRPO)
"""
The idea of the
algorithm is to refine the credit
allocation
process
in the
vehicle
routing
problem by implementing a deterministic
weighting
mechanism
that
assigns
distinct
credits to customers
based on their
delivery
demands , individual
distance
factors , and
their
influence on overall
routing
efficiency , thus
ensuring
that
credits
reflect
meaningful
differences
without
redundancy.
"""
import
numpy as np
from
sklearn.cluster
import
DBSCAN
def
heuristics( distance_matrix : np.ndarray , coordinates: np.ndarray ,
demands: np.ndarray , capacity: int) -> np.ndarray:
num_customers = demands.shape [0]
cumulative_penalty = np.zeros( num_customers )
# Calculate
baseline
scores
from
demand to distance
with
added
urgency
weighting
urgency_weight = np.linspace (1, 1.5, num_customers )
base_score = (demands * urgency_weight ) / ( distance_matrix + 1e-5)
base_score[np.isnan(base_score)] = 0
# Set
penalties
for
exceeding
capacity
based on cumulative
demands
for i in range(num_customers ):
24


--- Page 25 ---
current_demand = demands[i]
cumulative_penalty [i] = max(0, current_demand - capacity)
# Normalize
distances to emphasize
closer
customers to refine
scoring
normalized_distance_score = 1 / (np.clip(distance_matrix , 1e-5,
None) ** 2.5)
# Calculate
effective
capacity
utilization
adjustment
effective_capacity_utilization = np.clip (( capacity - demands) /
capacity , 0, 1)
# Historical
performance
adjustments
historical_performance_factor = np.zeros( num_customers )
for i in range(num_customers ):
historical_performance_factor [i] = np.mean ([ base_score[j] for
j in range(num_customers ) if distance_matrix [i][j] < 10
and j != i])
# Spatial
clustering
mechanism
clustering_model = DBSCAN(eps=5, min_samples =2).fit(coordinates)
labels = clustering_model .labels_
cluster_scores = np.zeros( num_customers )
# Calculate
cluster -based
scores
with
deterministic
differentiation
for
cluster_id in set(labels):
if cluster_id !=
-1:
# Ignore
noise
points
cluster_indices = np.where(labels == cluster_id)[0]
total_demand = demands[ cluster_indices ].sum()
for idx in cluster_indices :
# Implement
differentiated
scoring
based on demand ,
ensuring non -equal
credits
cluster_demand_factor = (demands[idx] / total_demand)
if total_demand > 0 else 0
distance_weight = 1 / (1 +
distance_matrix [idx].min())
# Closer
customers
get more
weight
cluster_scores [idx] = cluster_demand_factor *
distance_weight
# Mix demand and
distance
# New
resilience
score
based on historical
demand
variability
demand_variability = np.std(demands)
resilience_score = 1 / (1 + demand_variability )
# Compose
final
scores
combining
all
elements
including
the new
resilience
score
final_scores = base_score * normalized_distance_score *
effective_capacity_utilization * (1 +
historical_performance_factor + cluster_scores ) *
resilience_score
return
final_scores
Heuristic 4: CVRP, by CALM (API, w/o GRPO)
"""
The idea of the
algorithm is to refine the exploration -expemy
exploitation
trade -off by introducing a sinusoidal
decay
that
incorporates a sinusoidal
penalty
with a sinusoidal
smoothness
adjustment. This
adjustment
helps to smooth the
preference
for
both
recent and
distant nodes , leading to a more
balanced
and
improved
performance.
"""
25


--- Page 26 ---
import
numpy as np
def
enhanced_heuristics (prize: np.ndarray , distance: np.ndarray ,
maxlen: float) -> np.ndarray:
# Exponential
decay for
immediate
high
Subscription
nodes
exp_ratio = np.exp(prize[np.newaxis , :] / distance - maxlen)
# Logarithmic
scaling
for
exploration
log_ratio = np.log(prize[np.newaxis , :] + 1) / distance
# Sinusoidal
decay for recent
nodes
with a sinusoidal
smoothness
adjustment
sinusoidal_penalty = 0.5 * (1 + np.sin(np.pi * distance / (maxlen
+ 1))) * (distance / maxlen) * maxlen
# Combined
ratio
combined_ratio = exp_ratio * log_ratio * (1 - sinusoidal_penalty )
# Ensure the ratio is non -negative
combined_ratio [combined_ratio < 0] = 0
return
combined_ratio
Heuristic 5: OP, by CALM (local, w/ GRPO)
"""
The idea of the
algorithm is to introduce a novel
component
called
"reward
fluctuation
sensitivity" which
adjusts
the
desirability
of each
location
based on the
variability of rewards
over time.
This
component
accounts
for the
possibility
that
rewards
may
change or fluctuate
due to external
factors , thereby
allowing
the
agent to prioritize
locations
not only by their
current
rewards
but also by the
potential
volatility of those
rewards. This
sensitivity is integrated
into the
existing
framework , allowing
for a more
dynamic
response to the
changing
landscape of rewards ,
ultimately
enhancing
the agent\‚Äôs decision -making
process
and
route
optimization.
"""
import
numpy as np
def
heuristics(prize: np.ndarray , distance: np.ndarray , maxlen:
float) -> np.ndarray:
adjusted_distance = distance + 1e-10
# Avoid
division by zero
potential_reward = np.zeros_like(prize)
for i in range(len(prize)):
reachable_indices = np.where(distance[i] <= maxlen)[0]
potential_reward [i] = np.sum(prize[ reachable_indices ]) if
reachable_indices .size > 0 else 0
reward_hist_factor = potential_reward / (1 + np.sum(prize))
#
Shape
reward
based on historical
performance
reward_decay = np.exp(- adjusted_distance / maxlen)
# Decay
effect for
distant
rewards
proximity_factor = (maxlen - adjusted_distance ) ** 4
# Further
enhance
proximity
impact
with
quartic
distance
proximity_factor [ proximity_factor < 0] = 0
tiered_adjustment = (prize / ( adjusted_distance + 1e -10)) ** 2
#
Classify
rewards
into
categories
for
tiering
26


--- Page 27 ---
# Reward
volatility
assessment
component
volatility_factor = np.zeros_like(prize)
for i in range(len(prize)):
historical_rewards = prize[np.where(distance[i] <= maxlen)[0]]
if historical_rewards .size > 1:
volatility_factor [i] = np.std( historical_rewards ) /
np.mean( historical_rewards )
# Coefficient of
variation
# Risk -reward
analysis
component
variability_factor = np.zeros_like(prize)
for i in range(len(prize)):
historical_rewards = prize[np.where(distance[i] <= maxlen)[0]]
if historical_rewards .size:
variability_factor [i] = np.mean( historical_rewards ) -
np.std( historical_rewards )
# Basic
differentiation
final_heuristic = ( reward_hist_factor * reward_decay *
proximity_factor *
tiered_adjustment ) / (1 + volatility_factor +
variability_factor + 1e -10)
return
final_heuristic
Heuristic 6: OP, by CALM (API, w/o GRPO)
"""
The idea of the
algorithm is to select the next node by optimizing a
heuristic
that
considers
the
distance to the
current node , the
average
distance to unvisited nodes , the
variance of distances to
the
current
node from the
unvisited nodes , the
entropy of
distances to the
destination
node from each of the
unvisited
nodes , the
average
distance
from the
destination
node to each of
the
unvisited nodes , the
current
node ‚Äôs distance to the
destination node , and the
standard
deviation of the
overall
tour
distances. This
proposed
algorithm
aims to introduce a new term
that
captures
the
deviation of the
current
node from the
average
tour
length and
balances it with the
entropy
term to reduce the
overall
tour
length. Additionally , this
method
assigns
more
weight to the
standard
deviation of the
distances
from the
destination
node to each of the
unvisited nodes , which
helps in
reducing
the
variability of distances
and thus
leading to more
consistent
and
shorter
tour
lengths.
"""
import
numpy as np
def
select_next_node (current_node: int , destination_node : int ,
unvisited_nodes : set , distance_matrix : np.ndarray) -> int:
scores = {}
for node in unvisited_nodes :
all_distances = [ distance_matrix [node ][i] for i in
unvisited_nodes if i != node]
average_distance = np.mean( all_distances )
standard_deviation = np.std( all_distances )
variance_of_distances =
np.var([ distance_matrix [ current_node ][i] for i in
unvisited_nodes if i != node ])
entropy_of_distances =
-np.sum(np.log2 ([ distance_matrix [ destination_node ][i] for
i in unvisited_nodes if i != node ]) /
len( unvisited_nodes ))
27


--- Page 28 ---
average_distance_to_destination =
np.mean ([ distance_matrix [ destination_node ][i] for i in
unvisited_nodes if i != node ])
score = (
0.6 * distance_matrix [current_node ][ node]
- 0.4 * average_distance
+ 0.3 * standard_deviation
- 0.2 * entropy_of_distances
- 0.1 * distance_matrix [ destination_node ][ node]
- 0.08 * variance_of_distances
- 0.05 * average_distance_to_destination
- 0.01 * (np.mean ([ distance_matrix [current_node ][i] for i
in unvisited_nodes ]) - average_distance )
- 0.005 * entropy_of_distances
- 0.008 * distance_matrix [current_node ][ node] *
distance_matrix [node ][ destination_node ]
- 0.006 * standard_deviation *
distance_matrix [node ][ destination_node ]
)
scores[node] = score
next_node = min(scores , key=scores.get)
return
next_node
Heuristic 7: TSP, by CALM (local, w/ GRPO)
"""
The idea of the
algorithm is to select the next node to visit
from
the
unvisited nodes , incorporating a novel
component of dynamic
path
optimization
feedback. The new
component
analyzes
previous
decision
points in the tour to determine
the
effectiveness of the
routes taken , adjusting
future
node
selection to favor
pathways
that have
historically
resulted in lower
overall
traversal
costs.
This
method not only
enhances
the
algorithm ‚Äôs ability to learn
from its own
experiences
but also
promotes
the
selection of
routes
that
align
with
optimal
connectivity
patterns
established
during the tour.
"""
import
numpy as np
def
select_next_node (current_node: int , destination_node : int ,
unvisited_nodes : set , distance_matrix : np.ndarray) -> int:
threshold = 0.7
c1 , c2 , c3 , c4 , c5 = 0.4, 0.3, 0.2, 0.1, 0.1
scores = {}
for node in unvisited_nodes :
all_distances = [ distance_matrix [node ][i] for i in
unvisited_nodes if i != node]
average_distance_to_unvisited = np.mean( all_distances )
std_dev_distance_to_unvisited = np.std( all_distances )
# New
component: consider
dynamic
path
optimization
feedback
feedback_paths = [distance_matrix [i][ node] for i in
range(len(distance_matrix )) if i not in unvisited_nodes
and
distance_matrix [current_node ][i] < threshold]
average_feedback_distance = np.mean( feedback_paths ) if
feedback_paths
else 0
score = (
c1 * distance_matrix [ current_node ][ node]
- c2 * average_distance_to_unvisited
28


--- Page 29 ---
+ c3 * std_dev_distance_to_unvisited
- c4 * distance_matrix [ destination_node ][ node]
+ c5 * average_feedback_distance
)
scores[node] = score
next_node = min(scores , key=scores.get)
return
next_node
Heuristic 8: TSP, by CALM (API, w/o GRPO)
H
Limitations
A current limitation of our method is that the evolution of the LLM during the heuristic discovery
process depends heavily on performance signals derived from heuristics present in the prompt and
response. As a result, trajectories that do not contain explicit heuristics in either component provide
no reward signal, limiting the LLM‚Äôs ability to learn from such cases.
Another limitation is that we currently evaluate our method, CALM, using a compact LLM on a
single 24GB GPU. This restriction is primarily due to limited computational resources and the high
cost associated with high-accuracy, full-parameter fine-tuning on larger models. While this setup
demonstrates the feasibility of our approach in a resource-constrained environment, further evaluation
on larger-scale models and infrastructure would be valuable for understanding the method‚Äôs full
potential and scalability.
In future work, we aim to address these limitations by (1) exploring mechanisms for adapting the
LLM in the absence of explicit performance feedback, enabling more effective use of reinforcement
learning, and (2) extending evaluations to more powerful models and settings. These directions may
allow for better integration with techniques such as reflection [9, 10], which have shown promise in
enhancing LLM-based automated heuristic discovery.
I
Broader Impact
The CALM framework stands to greatly accelerate the pace of innovation in algorithm design by
seamlessly integrating prompt engineering and on-the-fly model adaptation. By enabling state-of-
the-art heuristic discovery on a single 24 GB GPU, CALM democratizes access to cutting-edge
Automatic Heuristic Design. This empowers research groups, startups, and educational institutions
with limited compute budgets to explore and deploy high-performance solutions in domains such as
logistics, scheduling, and resource allocation.
J
License
The licenses and URLs of baselines, models, and softwares are summarized in Table 6.
29


--- Page 30 ---
Table 6: A summary of licenses.
Resources
Type
License
URL
Unsloth
Code
Apache-2.0 License
https://github.com/unslothai/unsloth
Qwen2.5
Model
Apache-2.0 License
https://huggingface.co/Qwen/Qwen2.
5-7B-Instruct
Optuna
Code
MIT License
https://github.com/optuna/optuna
LKH3
Code
Available for academic research use
http://webhotel4.ruc.dk/~keld/
research/LKH-3/
OR-Tools
Code
MIT License
https://developers.google.com/
optimization/pack/knapsack?hl=zh-cn
POMO
Code
Available online
https://github.com/yd-kwon/POMO/tree/
master
DeepACO
Code
MIT License
https://github.com/henry-yeh/DeepACO
Funsearch
Code
Apache License
https://github.com/google-deepmind/
funsearch
EoH
Code
MIT License
https://github.com/FeiLiu36/EoH/tree/
main
ReEvo
Code
MIT License
https://github.com/ai4co/reevo
HSEvo
Code
Available online
https://github.com/datphamvn/HSEvo
MCTS-AHD
Code
MIT License
https://github.com/zz1358m/
MCTS-AHD-master
30
