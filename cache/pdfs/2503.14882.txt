--- Page 1 ---
1
Communication-Efficient Distributed On-Device
LLM Inference Over Wireless Networks
Kai Zhang, Hengtao He, Member, IEEE, Shenghui Song, Senior Member, IEEE,
Jun Zhang, Fellow, IEEE, and Khaled B. Letaief, Fellow, IEEE
Abstractâ€”Large language models (LLMs) have demonstrated
remarkable success across various application domains, but their
enormous sizes and computational demands pose significant
challenges for deployment on resource-constrained edge devices.
To address this issue, we propose a novel distributed on-device
LLM inference framework that leverages tensor parallelism to
partition the neural network tensors (e.g., weight matrices) of one
LLM across multiple edge devices for collaborative inference. A
key challenge in tensor parallelism is the frequent all-reduce
operations for aggregating intermediate layer outputs across
participating devices, which incurs significant communication
overhead. To alleviate this bottleneck, we propose an over-the-
air computation (AirComp) approach that harnesses the ana-
log superposition property of wireless multiple-access channels
to perform fast all-reduce steps. To utilize the heterogeneous
computational capabilities of edge devices and mitigate com-
munication distortions, we investigate a joint model assignment
and transceiver optimization problem to minimize the average
transmission error. The resulting mixed-timescale stochastic non-
convex optimization problem is intractable, and we propose an
efficient two-stage algorithm to solve it. Moreover, we prove that
the proposed algorithm converges almost surely to a stationary
point of the original problem. Comprehensive simulation results
will show that the proposed framework outperforms existing
benchmark schemes, achieving up to 5x inference speed acceler-
ation and improving inference accuracy.
Index Termsâ€”6G, distributed inference, large language mod-
els, over-the-air computation, tensor parallelism.
I. INTRODUCTION
The advent of large language models (LLMs) has marked a
significant breakthrough in artifical intelligence (AI), demon-
strating superior performance and adaptability in a wide range
of applications, such as natural language processing [2]â€“[4],
embodied intelligence [5]â€“[7], and wireless communications
[8]â€“[10]. The efficacy of LLMs is primarily attributed to
the vast model scale with billions of parameters, which en-
ables them to capture complex semantic relationships and
contextual nuances, leading to superior performance across
diverse tasks. However, the substantial computational and
memory requirements of LLMs present significant challenges
for the deployment on resource-constrained edge devices. For
instance, the LLaMA3 model [11] with 13 billion parameters
requires 40GB of RAM, which far exceeds the capabilities
of most edge devices. Consequently, most existing LLMs
Part of this work has been accepted for presentation at the 2025 IEEE Int.
Conf. Commun. (ICC), Montreal, Canada [1].
The authors are with the Department of Electronic and Computer En-
gineering, The Hong Kong University of Science and Technology, Clear
Water Bay, Hong Kong (email: kzhangbn@connect.ust.hk, eehthe@ust.hk,
eeshsong@ust.hk, eejzhang@ust.hk, eekhaled@ust.hk). (The corresponding
author is Hengtao He.)
rely on cloud-based infrastructure, which limits the feasibility
of LLM deployment and raises concerns about data privacy
and inference latency, especially in sensitive domains like
healthcare and finance. To address these challenges, distributed
LLM inference has recently been proposed as a promising
solution, which distributes the large models and computational
workloads across multiple devices [12]â€“[14]. This strategy
allows each device to handle smaller and more manageable
model segments, thereby reducing the burden on individual
devices and strengthening privacy protections. Furthermore,
advancements in communication technologies, such as the 5G
and future 6G wireless networks, enhance the feasibility of
distributed LLM inference for real-time applications [15], [16].
Communication overhead is a critical factor affecting the
performance of distributed LLM inference systems. To en-
hance communication efficiency, several recent studies have
been conducted [17]â€“[24]. In [17], Zhang et al. proposed a col-
laborative edge computing framework that distributes different
layers of LLMs across the edge device and cloud server. They
developed a joint device selection and model partitioning algo-
rithm to minimize inference latency and maximize throughput.
In [18], Yuan et al. considered splitting LLMs into several
sub-models, where the resource-intensive components were
offloaded to the server through non-orthogonal multiple-access
(NOMA) channels. They further proposed a gradient descent-
based algorithm to find the optimal trade-off between inference
delay and energy consumption. In [19], He et al. developed
an active inference method to address the joint task offloading
and resource allocation problem for distributed LLM inference
over cloud-edge computing frameworks. Similarly, Chen et
al. [20] proposed a reinforcement learning algorithm that
optimizes the splitting point of LLMs between the edge
device and cloud server to reduce the communication overhead
under varying wireless network conditions. Furthermore, task-
oriented communications have been utilized to optimize end-
to-end inference throughput, accuracy, and latency, which can
further enhance the communication efficiency of distributed
LLM inference systems [21]â€“[24].
Despite significant advances in distributed LLM inference,
most existing works [17]â€“[24] primarily focus on the device-
cloud collaborative inference. This architecture, however, faces
substantial challenges in terms of feasibility and scalability
due to its reliance on a powerful centralized cloud server
with high computational capability. Moreover, prior works
have generally employed the pipeline parallelism architectures,
which are associated with inherent disadvantages such as
pipeline bubbles [25]. These bubbles occur when downstream
arXiv:2503.14882v1  [cs.DC]  19 Mar 2025


--- Page 2 ---
2
Reference
Application
Scenario
Parallelism
Method
Antenna
Configuration
Optimization
Objective
Large-Scale
LLM
Device
Heterogeneity
K. Yang et al. (2020) [34]
Training
Data Parallelism
Multi-Antenna
Device Participation
Ã—
Ã—
X. Fan et al. (2021) [40]
Training
Data Parallelism
Single-Antenna
Convergence Rate
Ã—
âœ“
T. Sery et al. (2021) [36]
Training
Data Parallelism
Single-Antenna Communication Distortion
Ã—
Ã—
Y. Liang et al. (2024) [41]
Training
Data Parallelism
Single-Antenna
Training Latency and
Energy Consumption
Ã—
âœ“
H. Sun et al. (2024) [42]
Training
Data and
Model Parallelism
Multi-Antenna
Convergence Rate
âœ“
âœ“
Z. Zhuang et al. (2023) [43]
Inference
Data and
Model Parallelism
Multi-Antenna
Minimum Pair-Wise
Discriminant Gain
Ã—
âœ“
D. Wen et al. (2023) [24]
Inference
Data Parallelism
Multi-Antenna
Discriminant Gain
Ã—
âœ“
P. Yang et al. (2024) [44]
Inference
Data and
Model Parallelism
Multi-Antenna
Communication Distortion
Ã—
Ã—
This paper
Inference
Tensor Parallelism
Multi-Antenna
Communication Distortion
âœ“
âœ“
TABLE I. Overview of Over-the-Air Computation for Distributed Learning and Inference
devices are forced to remain idle while waiting for upstream
computations to complete, leading to poor utilization of com-
putational resources. To address these limitations, distributed
on-device LLM inference leveraging tensor parallelism has
recently been proposed as a promising solution [26]â€“[28]. This
approach divides large neural network tensors (e.g., weight
matrices) of LLMs into smaller segments and distributes them
across multiple edge devices. It not only eliminates the reliance
on a powerful central server but also enables concurrent
processing of model segments across devices, significantly
improving the utilization of computation and communica-
tion resources. Nevertheless, a critical challenge in tensor
parallelism is the frequent all-reduce operations required to
aggregate intermediate layer outputs across devices. These
communication-intensive all-reduce steps can cause substantial
latency in practical wireless networks and hinder real-time
inference, necessitating efficient communication strategies to
fully achieve the benefits of tensor parallelism.
In this paper, we propose a communication-efficient frame-
work for distributed on-device LLM inference with tensor par-
allelism. Specifically, we propose an over-the-air computation
(AirComp) approach to facilitate fast all-reduce operations.
AirComp leverages the superposition property of wireless
multiple-access channels, allowing simultaneous transmissions
from multiple devices to be naturally summed at the receiver
[29], [30]. This method reduces the communication latency
and bandwidth requirement compared to traditional techniques
that treat communication and computation separately. Most
recently, AirComp has gained popularity in various applica-
tions such as edge computing [31]â€“[33], federated learning
[34]â€“[36], and distributed sensing [37]â€“[39]. Table I shows
a thorough survey of recent state-of-the-art frameworks on
distributed parallel computing and AirComp for both model
training and inference tasks.
The performance of the proposed distributed LLM infer-
ence system, however, is heavily influenced by the com-
munication efficiency, particularly given the limited energy
resources of edge devices. Thus, to improve the inference
performance, we investigate a joint model assignment and
transceiver optimization problem aimed at minimizing the av-
erage transmission mean-squared error (MSE). The formulated
joint optimization is crucial considering the heterogeneous
computation capabilities of edge devices and varying wireless
channel conditions. Optimal model assignment ensures that
each device processes a suitable portion of the model based on
its computational capability (e.g., memory size and compute
power), while transceiver optimization minimizes the commu-
nication distortions during the AirComp process. To simplify
the problem and gain key insights, we initially consider the
scenario of single-antenna edge devices. We then extend the
framework to a multi-antenna configuration, leveraging spatial
multiplexing to further enhance communication efficiency and
reduce inference latency. Furthermore, the formulated joint
model assignment and transceiver optimization problem is
intractable due to its mixed-timescale, stochastic, and non-
convex property. Specifically, the model assignment policy
should be determined at the beginning of inference based on
long-term statistical channel state information (CSI), while
the transceiver design adapts dynamically to the CSI in each
all-reduce step. To address the mixed-timescale optimization
problem, we develop an efficient two-stage algorithm by
employing semidefinite relaxation (SDR) and stochastic suc-
cessive convex approximation (SCA). We note that although
existing wireless optimization techniques (e.g., SDR and SCA
algorithms) have been well studied, their tailored applica-
tion to distributed LLM inference brings unique challenges
and technical requirements. Specifically, our framework ad-
dresses unique challenges arising from large-scale distributed
LLM inference, including the frequent aggregation of high-
dimensional tensors, mixed-timescale optimization involving
long-term model assignment and short-term transceiver adap-
tation, handling of heterogeneous device capabilities, multi-
antenna AirComp beamforming designs, and stringent energy
constraints.


--- Page 3 ---
3
ğŠ!
ğ‘¸!
ğ•!
ğŠ"
ğ"
ğ•"
Device 1
CPU
RAM
SSD
Device ğ‘µ
CPU
RAM
SSD
(1) User initiates a task request.
Edge Server
(2) Edge server detects available devices 
and allocates model dynamically. 
(3) Load model segments
(4) Perform forward computation
(5) All-reduce operation
. . .
ğ—
(a) MLP
(b) Self-Attention
ğ—
ğ—
ğ—ğ–"
ğ—ğ–!
ReLU
ğ˜!
ğ˜"ğ”"
ğ˜!ğ”!
ğ™"
ğ™!
All-reduce
ğ™
Device 1
Device ğ‘µ
ğ—
ğ—
ğ—
ğ˜"
Softmax
ğ˜"
Softmax
ğ˜!
ğ˜"ğ”"
ğ˜!ğ”!
ğ™"
ğ™!
All-reduce
Device 1
Device ğ‘µ
ReLU
ğ™
Fig. 1. An illustration of the distributed on-device LLM inference system, showing the system workflow and visualizing tensor
parallelism for (a) MLP and (b) self-attention layers.
A. Contributions
The main contributions of this paper are summarized as
follows.
1) We propose a novel distributed on-device LLM inference
framework by employing tensor parallelism and AirComp.
While tensor parallelism effectively distributes computational
workloads across edge devices, its frequent all-reduce opera-
tions incur significant communication overhead, which offsets
the computational benefits and becomes a major bottleneck for
inference performance. To address this challenge, we develop a
communication-efficient AirComp all-reduce approach by ex-
ploiting the signal superposition property of wireless multiple-
access channels.
2) To utilize the heterogeneous computational capabilities
of edge devices and mitigate communication distortions, we
investigate a joint model assignment and transceiver opti-
mization problem to minimize the average transmission MSE.
The formulated mixed-timescale stochastic non-convex opti-
mization problem is inherently intractable. Thus, we develop
an efficient two-stage algorithm that decomposes the original
problem into short-term transceiver optimization and long-
term model assignment optimization subproblems. The result-
ing subproblems are further solved by employing SDR and
stochastic SCA, respectively. The proposed algorithm requires
no prior knowledge of channel statistics, and it converges
almost surely to a stationary point of the original problem.
3) We validate the effectiveness of the proposed frame-
work through simulations with two state-of-the-art open-
source LLMs and a real-world text dataset. Simulation results
demonstrate that the proposed algorithm outperforms bench-
mark schemes across various network settings, achieving up
to 5x inference speed acceleration and improving inference
accuracy.
B. Organization and Notations
The rest of this paper is organized as follows. In Section
II, we elaborate on the system model and present the problem
formulation. In Section III, we develop a two-stage algorithm
and prove its convergence. In Section IV, we extend the
algorithm for multi-antenna edge devices. Simulation results
are presented in Section V, and we conclude the paper in
Section VI.
Notations: Column vectors and matrices are denoted by
boldface lowercase and boldface capital letters, respectively.
The symbol R denotes the set of real numbers. CMÃ—N repre-
sents the space of the M Ã— N complex-valued matrices. (Â·)T
and (Â·)H stand for the transpose and the conjugate transpose of
their arguments, respectively. tr(A) denote the trace of matrix
A. E[Â·] denotes the expectation operation. âˆ‡represents the
gradient operator. | Â· | and âˆ¥Â· âˆ¥stand for the â„“1 and â„“2 norm
of vectors.
II. SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we first elaborate on the proposed distributed
on-device LLM inference system, followed by proposing
the communication-efficient AirComp all-reduce approach. To
minimize the average transmission MSE, we then formulate a
joint model assignment and transceiver optimization problem.
A. Distributed On-Device LLM Inference System
To deploy LLMs on resource-limited edge devices, dis-
tributed on-device inference with tensor parallelism has been
proposed. This method involves partitioning large neural net-
work tensors (e.g., weight matrices) of LLMs into smaller
segments and distributing them across multiple edge devices
for simultaneous processing. The complete workflow of the
proposed distributed on-device LLM inference system is illus-
trated in Fig. 1. When a device initiates an inference request,
the edge server dynamically identifies available local devices
and partitions the model parameters. Then, each device loads
its assigned model segment into memory and performs forward
computation. After each layer of the LLM is computed, an
all-reduce operation aggregates the intermediate layer outputs
from all devices, ensuring synchronization and consistency


--- Page 4 ---
4
across devices during inference. In the proposed distributed
inference framework, the device shares its input (typically
token embeddings rather than raw data) with other partici-
pating devices. For scenarios demanding strict confidential-
ity, encryption schemes (e.g., homomorphic encryption) or
secure enclaves can be adopted to mitigate privacy leakage.
Furthermore, we highlight two typical scenarios illustrating
real-world, trusted environments particularly suitable for our
distributed inference framework, as shown in the following.
â€¢ Organizational or HPC Clusters: Large institutions (e.g.,
corporate data centers, national labs, or university HPC
centers) often host massive LLMs that exceed the capacity
of a single node. In these clusters, multiple servers within
the same security domain can distribute model segments
or layers among them, securely exchanging raw input data
via internal networks. Since all compute nodes reside in
the same trusted infrastructure (with well-defined access
control, encryption, and compliance policies), they can fully
leverage parallelization to reduce per-inference latency and
alleviate memory bottlenecks, without risking data exposure
to external environments.
â€¢ Single-User or Local Edge Scenarios: Individual users or
small teams may possess multiple personal devices or local-
ized edge servers (e.g., the home server or on-premises GPU
node). These devices operate within a single-user network
or closed local environment, allowing them to share raw
inputs without breaching privacy. By splitting the LLMâ€™s
parameters or layers across these trusted devices, users can
achieve faster response times and reduced memory load per
device. These benefits are especially valuable for real-time
applications (e.g., smart home assistants or AR/VR), where
offloading data to external clouds may be undesirable or
impractical.
B. Tensor Parallelism
LLMs are primarily built on the Transformer architecture,
which typically consists of dozens of Transformer layers [45].
Each Transformer layer includes a self-attention mechanism
and a multi-layer perceptron (MLP). To achieve efficient
distributed inference, tensor parallelism partitions both the
self-attention and MLP layers within each Transformer block
into smaller tensor segments, as shown in Fig. 1. We note
that both pipeline parallelism and tensor parallelism are two
prevalent model partitioning strategies widely adopted in
distributed inference frameworks. While pipeline parallelism
partitions the model across layers, tensor parallelism partitions
computations within each layer across multiple devices. Tensor
parallelism is particularly attractive for on-device inference
due to its inherent advantages in significantly reducing idle
times (pipeline bubbles), achieving finer-grained memory allo-
cation, and, when combined with AirComp-based aggregation,
greatly minimizing communication overhead. These properties
highlight the practical benefits and superior suitability of ten-
sor parallelism for resource-constrained and latency-sensitive
inference scenarios considered in this work.
1) Tensor Parallelism for MLP Layer: For a typical 2-layer
MLP within the Transformer block, the forward computation
ğ—
Ã—
ğ–
Ã—
ğ”
=
ğ™
ğ—
Ã—
ğ–!
Ã—
=
ğ™!
ğ—
Ã—
Ã—
=
ğ—
=
=
=
ğ—ğ–
ğ–"
ğ”!
ğ”"
ğ—ğ–!
ğ—ğ–"
ğ™"
=
ğ™
Column-Wise 
Split
Row-Wise
Split
+
Device 1
Device 2
Fig. 2. Illustration of MLP matrix multiplication for conven-
tional unpartitioned approach and tensor parallelism with two
devices.
involves two main linear transformations, separated by a non-
linear activation function (e.g., ReLU or GeLU). We formulate
the computation of the MLP layer by taking the ReLU
activation as an example. Mathematically, it is expressed as
follows,
Z = max(0, XW)U,
(1)
where X is the input to the MLP layer, Z is the output, and W
and U are the weight matrices, respectively. Our framework
can be readily generalized to other activation functions, such
as GeLU function: Z = GeLU(XW)U, where GeLU(x) =
xÎ¦(x), with Î¦(x) representing the cumulative distribution
function of the standard Gaussian distribution. The traditional
centralized inference approach loads the entire weight matrices
W and U into memory and performs full matrix multipli-
cations on a single device, which is usually impractical for
resource-limited edge devices. To overcome this challenge,
tensor parallelism distributes the weight matrices W and
U across N devices. The weight matrices W and U have
dimensions d Ã— dhidden and dhidden Ã— d, respectively. As shown
in Fig. 2, the weight matrix W is partitioned column-wise into
multiple slices as
W
= [W1 âˆˆRdÃ—d1
hidden, W2 âˆˆRdÃ—d2
hidden, . . . , WN âˆˆRdÃ—dN
hidden],
(2)
where Wn represents the portion of the weight matrix W
assigned to device n and dhidden = PN
n=1 dn
hidden. Similarly,
the weight matrix U is partitioned row-wise as
U =
ï£®
ï£¯ï£¯ï£°
U1 âˆˆRd1
hiddenÃ—d
U2 âˆˆRd2
hiddenÃ—d
. . .
UN âˆˆRdN
hiddenÃ—d
ï£¹
ï£ºï£ºï£»,
(3)
where Un represents the portion of the weight matrix U
assigned to device n. Then, each device n can perform
the forward computation on its respective model segment as
follows,
Zn = max(0, XWn)Un,
(4)
where Zn is the partial output produced by device n. Once all
devices obtain their local outputs Zn, an all-reduce operation
is performed to aggregate the partial outputs from all devices


--- Page 5 ---
5
as follows,
Z =
N
X
n=1
Zn.
(5)
The validity of this aggregation can be explained by consid-
ering how the model parameters are partitioned across the
devices. Specifically, concatenating the column slices Wn
reproduces W, and stacking the row slices Un recovers U.
Consequently, the original unpartitioned MLP output Z can be
expressed as
Z = max(0, XW)U
= max(0, X[W1, W2, . . . , WN])
ï£®
ï£¯ï£¯ï£°
U1
U2
. . .
UN
ï£¹
ï£ºï£ºï£»
(a)=
N
X
n=1
max(0, XWn)Un
=
N
X
n=1
Zn,
(6)
where (a) follows from the element-wise property of activation
functions (e.g., ReLU, GeLU). Therefore, aggregating partial
results Zn reconstructs the original unpartitioned output Z
(i.e., Eq. (5) holds). After aggregation, the final output Z of the
MLP layer is broadcasted to all devices, ensuring synchroniza-
tion and consistency across devices for the subsequent layerâ€™s
computation.
2) Tensor Parallelism for Self-Attention Layer: For the self-
attention layer, tensor parallelism similarly partitions its query
(Q), key (K), value (V), and transformation (U) matrices
across edge devices. In the traditional centralized computation
of the self-attention layer, the output Z can be derived as
follows,
Z = softmax
XQ(XK)T
âˆšdk

VU,
(7)
where X denotes the input, and dk denotes the dimension of
the key vectors. In tensor parallelism, the memory-intensive
weight matrices are splited and distributed across N edge
devices as follows,
Q = [Q1, . . . , QN],
K = [K1, . . . , KN],
V = [V1, . . . , VN],
U = [UT
1 , . . . , UT
N]T.
(8)
Then, each device n performs local computation on its corre-
sponding portion of the query, key, value, and transformation
matrices as follows,
Zn = softmax
XQn(XKn)T
âˆšdk

VnUn.
(9)
Once all devices obtain their local outputs Zn, a similar all-
reduce operation is required to gather and combine the partial
outputs from devices as shown in (5).
C. Over-the-Air All-Reduce
Employing tensor parallelism for distributed LLM inference
requires frequent all-reduce operations, which cause signifi-
cant communication overhead in practical wireless networks.
To address this issue, we propose a communication-efficient
AirComp all-reduce approach. The AirComp aggregates dis-
tributed data efficiently by leveraging the signal superposi-
tion property of wireless multiple-access channels, allowing
simultaneous transmissions to compute nomographic functions
(e.g., arithmetic mean) [46]. In the proposed distributed LLM
inference system, the aggregation of intermediate layer outputs
in the all-reduce step aligns with this operation, making the
AirComp suitable to mitigate communication overhead. Note
that edge devices performing AirComp must achieve symbol-
level synchronization to ensure their transmitted signals arrive
concurrently at the receiver, minimizing aggregation errors due
to timing offsets. In our framework, synchronization among
edge devices can be practically realized through the well-
established timing advance (TA) mechanism. Specifically, the
edge server estimates each deviceâ€™s timing offset and instructs
each device to adjust its signal transmission timing via dedi-
cated TA commands. By aligning transmissions precisely, edge
devices can ensure simultaneous arrival and accurate signal
aggregation at the receiver.
We consider a wireless network consisting of an edge server
with Nr antennas and N single-antenna edge devices. We
further extend the proposed framework to a more general
scenario invloving multi-antenna edge devices in Section IV.
The uplink channels from edge devices to the server are block-
fading, where channel statistics remain constant throughout the
inference process, with channel states varying independently
across different time intervals. Let zn denote the per-round
transmitted entry of device nâ€™s intermediate layer output
Zn, which has a complete dimensionality of L0. To reduce
transmission power, the transmitted symbols are normalized to
have zero mean and unit variance, i.e., E[âˆ¥znâˆ¥2] = 1, where
the normalization factor is uniform for all devices and can be
inverted at the server. Given synchronized symbol boundaries,
all devices transmit their intermediate layer outputs simulta-
neously. To mitigate the distortion of received signals caused
by channel noise, aggregation beamforming is adopted. Let
a âˆˆCNrÃ—1 denote the aggregation beamforming vector at
the edge server. After the AirComp, the received signal at the
server is given by,
Ë†z = aH
N
X
n=1
hnbnzn + aHn,
(10)
where hn âˆˆCNrÃ—1 denotes the uplink channel from device n
to the server, bn is the transmit power of device n, and n âˆ¼
CN
 0, Ïƒ2I

denotes the additive white Gaussian noise vector
with Ïƒ2 being the noise power. In the single-antenna setting,
each device employs only a scalar transmit-power coefficient
bn (instead of a beamforming vector) to scale its transmitted
scalar entry zn. The distortion of Ë†z with respect to the desired
target summation z = PN
n=1 zn is measured by the MSE,
which is defined as
MSE(Ë†z, z) = E

âˆ¥Ë†z âˆ’zâˆ¥2
.
(11)


--- Page 6 ---
6
The MSE serves as a metric to evaluate the performance of the
AirComp all-reduce operations. As shown in the simulations
later, the inference accuracy of the distributed on-device LLM
inference system is greatly influenced by the transmission error
during the AirComp phase. By substituting (10) into (11), the
MSE can be explicitly represented as a function of aggregation
beamforming vector a and transmitter scalars {bn}N
n=1 as
follows,
MSE(a, {bn}) =
N
X
n=1
aHhnbn âˆ’1
2 + Ïƒ2aHa.
(12)
Edge devices involved in inference tasks typically have
limited energy supply. Thus, we assume that for each device
n, the energy consumption for both the forward computation
of each LLM layer and the transmission of the intermediate
output cannot exceed the maximum power budget P max
n
. To
model the computation energy consumption, we first introduce
a model assignment vector m = [m1, . . . , mN] with its entry
mn âˆˆ[0, 1] representing the proportion of model allocated to
device n. Consequently, the computation energy consumption
for device n is given by enmnstot, where en denotes the
device-specific energy coefficient that reflects the energy cost
associated with accessing and processing each weight during
computation, and stot is the number of parameters (weights)
for each layer. The communication energy consumption of
device n can be derived as L0âˆ¥bnâˆ¥2. Accordingly, the power
constraint is given by
enmnstot + L0âˆ¥bnâˆ¥2 â‰¤P max
n
, âˆ€n.
(13)
D. Problem Formulation
In the proposed distributed LLM inference system, the
overall performance is determined by the model assignment
policy m and the transceiver design a, {bn}. Optimal model
assignment ensures that each device processes a suitable
portion of the model based on its computational capability
(e.g., memory size and compute power). Meanwhile, efficient
transceiver optimization can reduce signal misalignment error
and suppress channel noise, thereby improving inference accu-
racy. Thus, to improve inference performance, we formulate a
joint model assignment and transceiver optimization problem
that aims to minimize the average MSE, subject to the per-
device power constraints. Importantly, the transceiver design
can adapt dynamically to instantaneous CSI. In contrast,
adapting the model assignment policy to instantaneous CSI
in a real-time manner is impractical due to the significant
latency caused by loading different model segments. Thus,
model assignment should be finished before inference based
on the long-term channel statistics.
The resulting problem is therefore formulated as a mixed-
timescale joint optimization of the short-term transceiver vari-
ables a, {bn} and the long-term model assignment policy m
as follows,
P1 : min
m
Eh

min
a,{bn} MSE(a, {bn})

s.t. enmnstot + L0âˆ¥bnâˆ¥2 â‰¤P max
n
, âˆ€n,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n,
(14)
where the expectation Eh [Â·] is taken over all random channel
realizations h = {hn}N
n=1. However, the problem P1 is
challenging to be solved due to the following three reasons.
â€¢ Non-convexity: The objective function is inherently non-
convex due to the coupling between the receiver aggre-
gation beamformer a and the transmitter scalars {bn}.
â€¢ Expectation over Random Channels: The objective in-
volves an expectation over random CSI, which requires
prior knowledge of channel statistics.
â€¢ Interdependence of Timescales: The per-device power
constraints link the short-term transceiver variables with
the long-term model assignment policy, leading to a
complex interplay between the two timescales.
To address these challenges, we develop a two-stage algorithm
that separately solves the short-term transceiver optimization
and the long-term model assignment optimization in the fol-
lowing section.
III. ALGORITHM DEVELOPMENT
In this section, we develop an efficient two-stage algorithm
to solve the joint model assignment and transceiver optimiza-
tion problem P1. Then, we show that the proposed algorithm
can converge to a stationary point of the original problem P1.
A. Problem Decomposition
We start by decomposing problem P1 into a family of short-
term transceiver optimization problems and a long-term model
assignment optimization problem as follows.
1) Short-term transceiver optimization for given model as-
signment policy m and channel condition h:
Ps : min
a,{bn} MSE(a, {bn})
s.t.
enmnstot + L0âˆ¥bnâˆ¥2 â‰¤P max
n
, âˆ€n.
(15)
2) Long-term model assignment optimization based on the
optimal solution aâˆ—(m), {bâˆ—
n(m)} to problem Ps:
Pl : min
m
Eh [MSE(aâˆ—(m), {bâˆ—
n(m)})]
s.t. enmnstot + L0âˆ¥bâˆ—
n(m)âˆ¥2 â‰¤P max
n
, âˆ€n,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n.
(16)
The short-term transceiver optimization problem Ps remains
non-convex, and we address it using the SDR technique.
The long-term model assignment optimization problem Pl
is similarly challenging, as the optimal transceiver variables


--- Page 7 ---
7
aâˆ—(m), {bâˆ—
n(m)} cannot be derived in closed form. Addition-
ally, the distribution of CSI is difficult to obtain in practical
wireless systems. To address these challenges, we propose a
stochastic SCA algorithm that operates without requiring prior
knowledge of channel statistics. In the following subsections,
we provide a detailed implementation of the proposed algo-
rithms.
B. Short-Term Transceiver Optimization for Ps
The short-term problem Ps is challenging to be solved due
to the inherent non-convexity caused by the coupling between
receiver aggregation beamformer a and the transmitter scalers
{bn}N
n=1. Thus, we first simplify problem Ps by demonstrating
that the channel inversion precoding is optimal conditioned on
the aggregation beamformer.
Lemma 1. For a given aggregation beamformer a, the trans-
mission MSE is minimized by using the zero-forcing precoders
bâˆ—
n =
1
aHhn , âˆ€n.
Proof. Lemma 1 can be proved by following the same steps
as in [47, Appendix A] and we omit it for brevity.
Let g represent the normalized aggregation beamformer that
satisfies gHg = 1, and consequently a = âˆšÎ±g where Î± is
optimized to satisfy the power constraints of edge devices.
By applying Lemma 1, problem Ps can be reformulated as
follows,
min
Î±,g
Î±
s.t. enmnstot +
L0
Î±âˆ¥gHhnâˆ¥2 â‰¤P max
n
, âˆ€n,
gHg = 1.
(17)
Then, by employing the equation âˆ¥gHhnâˆ¥2 = tr
 hnhH
nggH
,
an equivalent formulation of problem (17) is obtained as
follows,
min
Î±,g
Î±
s.t. enmnstot +
L0
Î±tr (hnhHnggH) â‰¤P max
n
, âˆ€n,
gHg = 1.
(18)
The problem (18) remains intractable due to the non-convex
norm constraint on g. To address this issue, we apply the
SDR approach that relaxes the non-convex norm constraint by
employing its convex hull.
Lemma 2. (Convex Hull Relaxation [48]) Suppose the set
â„¦1 = {Y : Y = XXH, XHX = Id} and set â„¦2 =
{Y : tr(Y) = d, 0 âª¯Y âª¯I}, where Y is of the size m by
m while X is of the size m by d. The condition of the set â„¦2
indicates that both Y and I âˆ’Y are positive semi-definite.
Then, â„¦2 is the convex hull of â„¦1, and â„¦1 is the set of extreme
points of â„¦2.
By applying Lemma 2, we can replace the non-convex norm
constraint by its convex hull and reformulate a relaxed version
of problem (18) as follows,
min
Î±, Ë†G
Î±
s.t. enmnstot +
L0
Î±tr (hnhHnggH) â‰¤P max
n
, âˆ€n,
tr( Ë†G) = 1,
0 âª¯Ë†G âª¯I,
(19)
where Ë†G = ggH. The problem (19) can be proved to be
convex, and the globally optimal solution Ë†Gâˆ—can be obtained
by using a convex solver (e.g., the CVX toolbox in MATLAB
[49]).
We note that the optimal solution Ë†Gâˆ—has a high probability
to satisfy the rank-one constraint [47]. If a rank-one solution
Ë†Gâˆ—is obtained, the optimal solution gâˆ—of the original problem
(18) can be immediately achieved by extracting the dominant
eigenvector of Ë†Gâˆ—as gâˆ—= [V Ë†Gâˆ—]:,1. Otherwise, if the rank
of Ë†Gâˆ—is larger than 1, we apply the Gaussian randomization
algorithm [50] to map the solution to a feasible, near-optimal
solution for the original non-convex problem.
C. Long-Term Model Assignment Optimization for Pl
In this subsection, we propose a stochastic SCA algorithm
to solve the long-term model assignment problem Pl. The
proposed algorithm requires no prior knowledge of channel
statistics. For clearer algorithmic description, we first refor-
mulate the long-term problem Pl into an equivalent form as
follows,
min
m
f0(m) = EH [MSE(aâˆ—(m), {bâˆ—
n(m)})]
s.t. f1(m) = stotdiag(emT) + L0ec (m) â‰¤pmax,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n,
(20)
where
ec(m)
=
[âˆ¥bâˆ—
1(m)âˆ¥2, . . . , âˆ¥bâˆ—
N(m)âˆ¥2]T,
pmax
=
[P max
1
, . . . , P max
N ]T, and e = [e1, . . . , eN]T. The proposed
stochastic SCA algorithm iteratively performs the following
two steps: First, quadratic surrogate functions Ë†f0(m), Ë†f1(m)
are constructed to approximate the non-convex components of
the original objective and constraint functions f0(m), f1(m),
respectively. Then, the resulting convex quadratic approxima-
tion problem is solved, and the long-term model assignment
policy is updated based on the solution. The details of these
two steps are illustrated as follows.
1) Step 1: In each iteration Ï„, the edge server first generates
a channel sample hÏ„, and then calculates the short-term
transceiver variables aâˆ—(mÏ„) and {bâˆ—
n(mÏ„)}N
n=1 by solving
the short-term problem Ps. Then, the recursive convex ap-
proximation of the original objective function f0(m) can be
derived as [51]
Ë†f Ï„
0 (m) = Â¯f0(mÏ„) + (uÏ„
0)T (m âˆ’mÏ„) + Î·0 âˆ¥m âˆ’mÏ„âˆ¥2 ,
(21)
where Î·0 is a constant that ensures convexity, Â¯f0(mÏ„) =
MSE(aâˆ—(mÏ„), {bâˆ—
n(mÏ„)}) denotes the sample-wise approx-
imation of the average MSE and is computed by using


--- Page 8 ---
8
the specific channel realization hÏ„. Furthermore, uÏ„
0 is an
approximation of the gradient âˆ‡f0(mÏ„), which is updated
recursively as
uÏ„
0 = (1 âˆ’ÏÏ„)uÏ„âˆ’1
0
+ ÏÏ„âˆ‡m Â¯f0(m; aâˆ—(mÏ„), {bâˆ—
n(mÏ„)}),
(22)
and uâˆ’1
0
= 0 [51]. The algorithm parameter ÏÏ„ is de-
creasing in Ï„, satisfying limÏ„â†’âˆÏÏ„ = 0, Pâˆ
Ï„=0 ÏÏ„ = âˆ,
Pâˆ
Ï„=0(ÏÏ„)2 < âˆ, and Pâˆ
Ï„=0 ÏÏ„Ï„ âˆ’1/2 < âˆ. Similarly,
the recursive convex approximation of the power constraint
function f1(m) is given by
Ë†f Ï„
1 (m) = f1(mÏ„) + (uÏ„
1)T (m âˆ’mÏ„) + Î·1 âˆ¥m âˆ’mÏ„âˆ¥2 ,
(23)
where Î·1 > 0 is a constant, and uÏ„
1 is updated recursively as
follows [51],
uÏ„
1 = (1 âˆ’ÏÏ„)uÏ„âˆ’1
1
+ ÏÏ„âˆ‡mf1(m; aâˆ—(mÏ„), {bâˆ—
n(mÏ„)}).
(24)
It is noted that the surrogate functions Ë†f Ï„
0 (m) and Ë†f Ï„
1 (m) are
quadratic approximations of the original nonconvex objective
f0(m) and constraint f1(m) around the current iterate mÏ„.
Specifically, at iteration Ï„, each surrogate function is con-
structed using first-order Taylor expansions of the correspond-
ing function fi(m) (for i = 0, 1), along with an additional
quadratic regularization term controlled by convexity constants
Î·0 and Î·1. The convexity constants Î·0 and Î·1 serve to ensure
strong convexity and numerical stability of the surrogate
functions. Specifically, larger values of Î·0 and Î·1 enhance
numerical stability but may slow convergence, whereas smaller
values permit larger update steps but require careful tuning to
prevent instability. In practice, setting these constants within
the range 10âˆ’2
âˆ¼
10âˆ’1 (e.g., around 0.05) achieves a
favorable balance between stability and convergence speed.
2) Step 2: After obtaining the convex approximations of
the objective and constraint functions, we formulate a convex
approximation of the original problem (20) to solve the
optimal Ë†mÏ„ as follows,
Ë†mÏ„ = min
m
Ë†f Ï„
0 (m)
s.t.
Ë†f Ï„
1 (m) â‰¤pmax,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n.
(25)
If problem (25) turns out to be infeasible, the optimal solution
Ë†mÏ„ is obtained by solving the following feasibility problem,
Ë†mÏ„ = min
m,Âµ Âµ
s.t.
Ë†f Ï„
1 (m) â‰¤pmax + Âµ,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n.
(26)
After solving for Ë†mÏ„, the model assignment policy is updated
as
mÏ„+1 = (1 âˆ’Î³Ï„)mÏ„ + Î³Ï„ Ë†mÏ„,
(27)
Algorithm 1: Mixed-Timescale Model Assignment
and Transceiver Optimization Algorithm
1 Initialize: Model assignment policy m0, iteration index
Ï„ = 0, and convergence tolerance Ïµ;
2 Step 1 (long-term model assignment optimization at the
beginning of inference task)
3 repeat
4
Obtain a channel sample hÏ„ = {hÏ„
1, . . . , hÏ„
N} and cal-
culate the short-term transceiver variables aâˆ—(mÏ„),
{bâˆ—
n(mÏ„)} by solving the short-term problem Ps;
5
Update the surrogate functions Ë†f Ï„
i (m) according to (21)
and (23);
6
if problem (25) is feasible then
7
Solve problem (25) to obtain the optimal Ë†mÏ„
8
else
9
Solve problem (26) to obtain the optimal Ë†mÏ„
10
end
11
Update mÏ„ according to (27);
12
Ï„ â†Ï„ + 1;
13 until âˆ¥mÏ„ âˆ’mÏ„âˆ’1âˆ¥â‰¤Ïµ;
14 Step 2 (short-term transceiver optimization at each all-
reduce step):
15 Obtain the channel condition h, and apply the short-term
algorithm to solve the optimal transceiver variables with
the determined model assignment policy m.
where Î³Ï„ âˆˆ(0, 1) satisfies limÏ„â†’âˆÎ³Ï„ = 0, Pâˆ
Ï„=0 Î³Ï„ = âˆ,
and Pâˆ
Ï„=0(Î³Ï„)2 < âˆ[51]. To facilitate practical implemen-
tation and reproducibility, we provide explicit heuristics for
choosing the hyperparameters ÏÏ„and Î³Ï„. We suggest setting
these hyperparameters as ÏÏ„ =
1
(Ï„+1)Î± and Î³Ï„ =
c
Ï„+câ€² , where
the parameter Î± typically ranges from 0.5 to 1 to satisfy
the convergence conditions outlined in Lemma 3. In practice,
setting Î± â‰ˆ0.8, c = 15, and câ€² = 14 has been found
through simulations to achieve an effective balance between
convergence speed and numerical stability.
The above two steps iterate until convergence, i.e., âˆ¥mÏ„ âˆ’
mÏ„âˆ’1âˆ¥â‰¤Ïµ, where Ïµ is the convergence tolerance. The overall
algorithm is outlined in Algorithm 1, and the block diagram
of the proposed algorithm is illustrated in Fig. 3.
Remark 1. The computational complexity of solving the
short-term transceiver optimization problem Ps is at most
O
 NN 3
r + N 2N 2
r + N 3
, and is usually much lower in prac-
tice [52, Theorem 3.12]. Moreover, the most computation-
expensive steps for the long-term optimization problem Pl
are solving the constructed convex quadratic approxima-
tion problems (25) and (26). Specifically, the computa-
tional complexity of solving problems (25) and (26) is
at most in the order of O
 2N 4 + N 3
. Then, the to-
tal computational complexity of Algorithm 1 is given by
O
 Ï„ max  NN 3
r + N 2N 2
r + 2N 4 + 2N 3
, where Ï„ max is the
maximum iteration number of Algorithm 1. Both optimization
and model assignment are performed only once at the begin-
ning of the inference (or after substantial changes in channel
or device conditions). Hence, while solving the optimization
and loading model segments introduce a non-negligible one-


--- Page 9 ---
9
Update the surrogatre functions 
according to (21) and (23)
If (25) is feasible
Objective Update
(Solving the convex 
approximation problem (25))
Feasiblity Update
(Solving the feasible problem 
in (26))
Initialize 
Update                   
according to (27)
Yes
No
Fig. 3. Block diagram of Algorithm 1
time cost, the subsequent benefits from parallelized forward
computation significantly outweigh this initial overhead, re-
sulting in increased inference speed in practical settings.
Remark 2. In the proposed framework, the edge server,
which possesses limited but non-negligible computational and
memory resources, collects the channel state information and
device capability information. Leveraging these data, the
server solves the mixed-timescale optimization problem and
assigns each participating edge device its respective portion
of the model parameters.
D. Convergence Analysis
In this subsection, we analyze the asymptotic convergence
performance of Algorithm 1 to a stationary point of the
original problem P1.
We first show the convergence of the surrogate functions in
the following lemma.
Lemma 3. Consider a sequence {mÏ„}âˆ
Ï„=0 converging to a
limiting point mâˆ—, and define
Ë†fi(m) = fi(m) + âˆ‡fi(mâˆ—)T (m âˆ’mâˆ—) + Î·i âˆ¥m âˆ’mâˆ—âˆ¥2 ,
(28)
which
satisfies
Ë†fi(mâˆ—)
=
fi(mâˆ—)
and
âˆ‡Ë†fi(mâˆ—)
=
âˆ‡fi(mâˆ—), âˆ€i âˆˆ{0, 1}. Then, if the algorithm parameter Ï
satisfies Pâˆ
Ï„=0 ÏÏ„Ï„ âˆ’1/2 < âˆ, we have
lim
Ï„â†’âˆ
Ë†f Ï„
i (m) = Ë†fi(m),
(29)
almost surely [51].
Proof. The proof is presented in Appendix A.
To elaborate the convergence result, we need to introduce
the Slaterâ€™s condition for the converged surrogate function in
the following.
Definition
1.
(Slaterâ€™s
Condition)
Given
the
sequence
{mÏ„}âˆ
Ï„=1 converging to a limiting point mâˆ—and let Ë†f1(m)
be the converged surrogate function as defined in (28). The
Slaterâ€™s condition holds at mâˆ—if there exists a constant m
such that
Ë†f1(m) < pmax,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n.
(30)
The Slaterâ€™s condition is widely used in constrainted opti-
mization algorithms (e.g., the majorization-minimization algo-
rithm [53] and virtual queue-based online convex optimization
algorithm [54]). With Lemma 3 and the Slaterâ€™s condition, we
are ready to show the main convergence result of Algorithm
1 in the following theorem.
Theorem 1. Let {mÏ„}âˆ
Ï„=1 denote the sequence of model
assignment policies generated by Algorithm 1. If the Slaterâ€™s
condition is satisfied at the limiting point mâˆ—of the sequence
{mÏ„}âˆ
Ï„=1, then mâˆ—is a stationary point of problem P1 almost
surely.
Proof. The proof is presented in Appendix B.
In Section V-B, we further verify the convergence of Algo-
rithm 1 through simulations.
IV. EXTENSION TO MULTI-ANTENNA DEVICES
In the previous sections, we analyzed the scenario of
single-antenna edge devices to establish foundational insights
for optimizing communication efficiency in distributed LLM
inference. In this section, we extend the proposed framework
and algorithms to the multi-antenna setting. By leveraging
spatial multiplexing, the multi-antenna configuration further
enhances communication efficiency and reduces inference
latency, providing a more general and scalable solution.
A. Problem Formulation
Building upon the single-antenna setting, we now consider
a more generalized scenario where edge devices in the dis-
tributed LLM inference system are equipped with multiple
antennas. Thus, the spatial diversity and spatial multiplex-
ing are utilized to further improve communication efficiency.
Specifically, we consider the server and each edge device are
equipped with Nr and Nt antennas, respectively. Similar to the
single-antenna case, all devices simultaneously upload their
intermediate layer outputs through wireless multiple-access
channels. Let zn = [zn,1, . . . , zn,L]T denote the per-round
transmitted L entries of device nâ€™s intermediate output Zn.
Let A âˆˆCNrÃ—L and Bn âˆˆCNtÃ—L denote the aggregation
beamforming matrix at the edge server and the data precoding
matrix at device n, respectively. Then, the received signal at
the server after the AirComp can be derived as follows,
Ë†z = AH
N
X
n=1
HnBnzn + AHn,
(31)
where Hn âˆˆCNrÃ—Nt denotes the uplink MIMO channel from
device n to the edge server. In the multi-antenna setting, each


--- Page 10 ---
10
device employs the precoding (beamforming) matrix to map its
transmitted vector zn onto multiple antennas for simultaneous
transmission. The distortion of Ë†z with respect to the desired
target vector z = PN
n=1 zn is measured by the MSE, defined
as
MSE(Ë†z, z) = E

(Ë†z âˆ’z)H(Ë†z âˆ’z)

.
(32)
By substituting (31) into (32), the MSE can be explicitly
represented as a function of transceiver beamforming matrices
as follows,
MSE(A, {Bn})
=
N
X
n=1
tr
 AHHnBn âˆ’I
 AHHnBn âˆ’I
H
+Ïƒ2
ztr
 AHA

.
(33)
To effectively utilize the heterogeneous computational ca-
pabilities of edge devices and mitigate communication dis-
tortions, we similarly investigate a joint model assignment
and transceiver optimization problem. Specifically, the joint
optimization problem in the multi-antenna scenario can be
formulated as follows,
P2 : min
m
EH

min
A,{Bn} MSE(A, {Bn})

s.t. enmnstot + L0
L tr
 BnBH
n

â‰¤P max
n
, âˆ€n,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n,
(34)
where the expectation EH [Â·] is taken over all random channel
realizations H = {Hn}N
n=1.
B. Algorithm Development
In this subsection, we extend Algorithm 1 to a more
general case involving multi-antenna edge devices. Similarly,
we first decompose problem P2 into a family of short-term
subproblems and a long-term subproblem as follows.
1) Short-term transceiver optimization for given model as-
signment policy m and channel condition H:
Ps :
min
A,{Bn} MSE(A, {Bn})
s.t.
enmnstot + L0
L tr
 BnBH
n

â‰¤P max
n
, âˆ€n.
(35)
2) Long-term model assignment optimization based on the
optimal solution Aâˆ—(m), {Bâˆ—
n(m)} to problem Ps:
Pl : min
m
EH [MSE(Aâˆ—(m), {Bâˆ—
n(m)})]
s.t. enmnstot + L0
L tr
 Bâˆ—
n(m)Bâˆ—
n(m)H
â‰¤P max
n
, âˆ€n,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n.
(36)
To solve the short-term problem Ps, we first simplify it by
demonstrating that the zero-forcing (channel inversion) pre-
coder is optimal conditioned on the aggregation beamformer.
Lemma 4. For a given aggregation beamformer A, the trans-
mission MSE is minimized by using the zero-forcing precoders
as follows,
Bâˆ—
n =
 AHHn
H  AHHnHH
nA
âˆ’1 , âˆ€n.
(37)
Proof. The proof of Lemma 4 is similar to that of Lemma 1
and thus omitted for brevity.
Let G represent the normalized aggregation beamformer
that satisfies tr(GGH) = 1, and consequently A = âˆšÎ±G with
Î± denoting the norm of A. By employing (37), the problem
Ps can be reformulated as follows,
min
Î±,G
Î±
s.t. enmnstot + L0
Î±Ltr
 GHHnHH
nG
âˆ’1
â‰¤P max
n
, âˆ€n,
tr
 GGH
= 1.
(38)
The
problem
(38)
remains
challenging
to
be
solved
due
to
its
non-convex
constraints
involving
the
term
tr((GHHnHH
nG)âˆ’1). To address this issue, we develop a
tractable approximation of the problem by employing the
following inequality,
tr
 GHHnHH
nG
âˆ’1
â‰¤
L
Î»min (HHnGGHHn),
(39)
where the equality holds when the channel is well-conditioned,
i.e., the singular values of Hn are identical. By utilizing (39),
we reformulate an approximated version of problem (38) as
follows,
min
Î±,G
Î±
s.t.
L0
Î±Î»min (HHnGGHHn) â‰¤P max
n
âˆ’enmnstot, âˆ€n,
tr
 GGH
= 1.
(40)
Then, by introducing a new variable Ë†G = GGH, an equivalent
formulation of problem (40) is obtained as follows,
min
Î±, Ë†G
Î±
s.t.
L0
Î±Î»min

HHn Ë†GHn
 â‰¤P max
n
âˆ’enmnstot, âˆ€n,
tr( Ë†G) = 1, rank( Ë†G) = L, Ë†G âª°0.
(41)
We observe that the only non-convex constraint in problem
(41) is rank( Ë†G) = L. Therefore, we remove this constraint to
obtain a relaxed version of problem (41) as follows,
min
Î±, Ë†G
Î±
s.t.
L0
Î±Î»min

HHn Ë†GHn
 â‰¤P max
n
âˆ’enmnstot, âˆ€n,
tr( Ë†G) = 1, Ë†G âª°0.
(42)
The problem (42) can be proved to be a convex problem. After
solving problem (42) using a convex solver (e.g., the CVX
toolbox in MATLAB [49]) and obtaining the globally optimal
solution Ë†Gâˆ—, we apply the Gaussian randomization algorithm


--- Page 11 ---
11
[50] to map the solution to a feasible, near-optimal solution
for the original non-convex problem.
Next, we solve the long-term model assignment problem Pl.
The proposed stochastic SCA algorithm, initially introduced
for the single-antenna case in Section III-B, can be directly
extended to the multi-antenna scenario without requiring fur-
ther modifications. For clearer algorithmic description, we first
reformulate the long-term problem Pl into an equivalent form
as follows,
min
m
f0(m) = EH [MSE(Aâˆ—(m), {Bâˆ—
n(m)})]
s.t. f1(m) = stotdiag(emT) + L0
L ec (m) â‰¤pmax,
mT1 = 1, m â‰¥0,
(43)
where
ec(m)=[tr
 Bâˆ—
1(m)(Bâˆ—
1(m))H
,. . ., tr
 Bâˆ—
N(m)(Bâˆ—
N(m))H
]T,
pmax = [P max
1
, . . . , P max
N ]T, and e = [e1, . . . , eN]T. The main
structure of the proposed stochastic SCA algorithm remains
intact, and it iteratively performs the following two steps: First,
quadratic surrogate functions Ë†f0(m), Ë†f1(m) are constructed
to approximate the non-convex components of the original
objective and constraint functions f0(m), f1(m), respectively.
Then, the resulting convex quadratic approximation problem is
solved, and the long-term model assignment policy is updated
based on the solution. Here, we omit the details of these two
steps for brevity. In Section V-B, we also demonstrate the
convergence of the proposed algorithm for the multi-antenna
scenario through simulations.
V. SIMULATION RESULTS
A. Simulation Setups
1) LLM Inference Model Setting: All simulations are per-
formed on a desktop server equipped with Nvidia GeForce
RTX 4070Ti GPU and Intel Core i9 CPU, using PyTorch
2.0 with CUDA 11.7. We set up N virtual machines (VMs)
with each VM simulating a distinct edge device. Each VM
is allocated 4 CPU cores, 16 GB RAM, and 128 GB storage
space, ensuring efficient utilization of computational resources
and optimized parallel processing. For evaluation, we utilize
the LLaMA2 [55] and LLaMA3 [11] models due to their
state-of-the-art performance among open-source models. Ad-
ditionally, we employ the WikiText-2 dataset [56], which is
widely used in the field of LLM inference for benchmark-
ing and evaluation purposes. We have released our imple-
mentation on GitHub: https://github.com/zklasd24/distributed
llama AirComp, which builds upon the open-source project
Distributed Llama [57].
The primary performance metric for inference accuracy is
perplexity [58], which is a widely recognized measure of a
LLMâ€™s capability to predict the next word in a sequence. It is
defined mathematically as follows,
Perplexity = exp
 
âˆ’1
Ltxt
Ltxt
X
k=1
log P (wk |w1, . . . , wkâˆ’1)
!
,
(44)
where P (wk | w1, . . . , wkâˆ’1) denotes the modelâ€™s predicted
probability for the next word wk, and Ltxt is the text length.
Fig. 4. Convergence of Algorithm 1 for the scenarios of single-
antenna devices (Top) and multi-antenna devices (Bottom).
Lower perplexity values indicate better inference performance,
reflecting the modelâ€™s accuracy in generating subsequent to-
kens.
2) Communication Model Setting: The number of antennas
at the edge server is Nr = 16, and each edge device has single
antenna or Nt = 4 antennas for different cases. The bandwidth
between the edge server and edge devices is B = 10 MHz. The
uplink channels are assumed to be independent and identically
distributed (i.i.d.) Rician fading [59], modeled as i.i.d. complex
Gaussian random variables with non-zero mean Âµ = 1 and
variance Ïƒ2 = 1. Moreover, the maximum power budget is
set as P max
n
= 10 and the noise variance at the edge server is
assumed to be 1.
B. Algorithm Convergence
In this subsection, we analyze the convergence behavior of
the propose d algorithm for both single-antenna and multi-
antenna scenarios. The parameter parameters are set as Ïµ =
0.001, ÏÏ„ = [1/((Ï„ + 1)4/5)], and Î³Ï„ = 15/(14 + Ï„). As
illustrated in Fig. 4, the proposed algorithm demonstrates rapid
convergence, reaching a stationary point within approximately
100 iterations. The swift convergence speed ensures that the
distributed LLM inference system can quickly adapt to varying
network conditions, enabling real-time inference especially in
latency-sensitive applications. Moreover, the consistent perfor-
mance across both single-antenna and multi-antenna settings
suggests the robustness of the proposed algorithm to various
network scenarios.
C. Performance Evaluation
In this subsection, we compare the performance of the
proposed AirComp all-reduce approach with the following two


--- Page 12 ---
12
(a)
(b)
(c)
Fig. 5. The average MSE (a), perplexity (b), and average generation time (c) versus the number of edge devices for the scenario
of single-antenna d evices.
Transmission Scheme
Transmission Time
Digital All-Reduce
NL0Q
B log2(1+SNRrxN)
Uncoded FDMA
NL0
B
AirComp All-Reduce
L0
B
TABLE II. Transmission time for different transmission
schemes.
benchmark schemes.
â€¢ Digital All-Reduce: All devices upload intermediate
layer outputs using a traditional broadband digital
multiple-access scheme, with each transmitted symbol
quantized to Q = 8 bits. To prevent multi-user inter-
ference, orthogonal frequency division multiple-access
(OFDMA) is employed, assigning each sub-channel to
one device [60].
â€¢ Uncoded FDMA: This scheme similarly employs the
OFDMA technique, with each device occupying a dedi-
cated sub-channel to upload intermediate layer outputs in
an uncoded analog manner.
In Fig. 5, we compare the inference performance of different
transmission schemes using the LLaMA3 model with 8 billion
parameters, across three key performance metrics: transmis-
sion MSE, perplexity, and average generation time. In Fig.
5(a), the proposed AirComp all-reduce approach consistently
achieves low MSE across all device counts, significantly
outperforming the uncoded FDMA scheme, which exhibits
a near-linear increase in MSE as the number of devices
grows. The digital all-reduce method achieves near-zero MSE
across all configurations. However, it has significantly higher
communication latency. In Fig. 5(b), perplexity follows the
same trend as the transmission MSE. The AirComp all-
reduce method maintains stable, low perplexity across all
device configurations, while the perplexity of uncoded FDMA
rises sharply with more devices. Digital all-reduce performs
similarly to AirComp all-reduce, maintaining low perplexity.
Turning to the average generation time in Fig. 5(c), we observe
a notable distinction among the three methods. Here, the
total inference time is defined as the sum of local compu-
tation time and the time taken to transmit the local outputs.
The local computation time is obtained through experimental
measurements, while the communication time is estimated
based on different transmission methods as outlined in Table
II, where SNRrx denotes the average receive signal-to-noise
ratio (SNR). We observe that AirComp all-reduce consistently
demonstrates the lowest latency, particularly as the number
of edge devices grows. The digital all-reduce scheme shows
a significant increase in generation time with more devices
due to increased communication overhead, while the uncoded
FDMA method provides moderate improvements but still lags
behind AirComp all-reduce. The proposed AirComp all-reduce
approach exhibits superior scalability compared to traditional
communication strategies. Specifically, by exploiting analog
signal superposition inherent in wireless channels, AirComp
enables simultaneous aggregation of signals from multiple
devices within a single communication slot. Consequently,
unlike traditional communication schemes, whose overhead
increases linearly with the number of participating devices, the
AirComp all-reduce approach maintains low communication
latency even as device count grows.
Fig. 6 expands on the simulation results by evaluating the
performance of the proposed method in a more general setting
of multi-antenna devices. In this scenario, the digital all-reduce
scheme maintains the lowest MSE and perplexity. However, its
average generation time grows considerably with an increasing
number of devices, indicating scalability limitations in prac-
tice. The proposed AirComp all-reduce scheme, while exhibit-
ing a slight increase in MSE compared to digital all-reduce, re-
mains competitive in terms of perplexity and demonstrates the
shortest generation time across all configurations. This makes
it an promising choice for applications where low latency is
critical, and slight trade-offs in accuracy are acceptable. On
the other hand, the uncoded FDMA schemeâ€™s performance
degrades significantly with more devices, reflected by steep


--- Page 13 ---
13
(a)
(b)
(c)
Fig. 6. The average MSE (a), perplexity (b), and average generation time (c) versus the number of edge devices for the scenario
of multi-antenna devices.
Average generation time per token (ms)
Model
LLaMA2-7B
LLaMA2-13B
LLaMA2-70B
LLaMA3-70B
Device Number
1
2
4
8
1
2
4
8
1
2
4
8
1
2
4
8
Digital All-Reduce
114.2 85.2 79.5 108.3 217.3 174.0 176.6 261.4
N/Aâˆ—807.3 729.7 981.6 N/A 893.2 783.8 1033.6
AirComp All-Reduce 114.2 69.7 45.7
37.8
217.3 128.5
81.3
66.4
N/A
660.9 423.0 354.2 N/A 746.8 477.1
406.0
*: Not available due to insufficient memory.
TABLE III. Average generation time for different models across varying device numbers, with the shortest average generation
time for each model being highlighted in bold.
increases in both MSE and perplexity.
To further validate the effectiveness of the proposed algo-
rithm, we conduct additional experiments using larger models,
including LLaMA2 with 7, 13, and 70 billion parameters,
and LLaMA3 with 70 billion parameters. In Table III, it is
observed that AirComp all-reduce method consistently demon-
strates superior performance in terms of reduced generation
time, particularly as the number of devices increases. Across
various device and model configurations, AirComp all-reduce
achieves up to 4x faster generation speed, demonstrating its
significant advantages for distributed LLM inference, espe-
cially with large-scale models.
Overall, the AirComp all-reduce approach emerges as a
balanced and scalable solution, effectively managing the trade-
offs between latency, accuracy, and scalability in both single-
antenna and multi-antenna environments. This highlights its
potential for deployment in practical, large-scale wireless
scenarios.
D. Comparison with Centralized Inference Approach
In this subsection, we compare the proposed AirComp-
based distributed inference framework with the traditional
centralized inference approach. Table IV compares the per-
token generation latency for centralized versus distributed
LLM inference across different large models. As shown in
the table, although the centralized inference does not incur a
communication overhead, it suffers from significantly higher
per-token computation time. In contrast, the proposed dis-
tributed inference approach partitions the model across multi-
ple devices, substantially reducing each deviceâ€™s computational
load. Despite introducing modest communication overhead,
the proposed distributed scheme achieves significantly lower
total inference latency per token. Hence, for large-scale LLMs
with billions of parameters, distributing both the model storage
and compute cost across multiple devices proves far more
feasible and efficient than hosting the entire model on a single
node. Moreover, both per-token local computation time and
communication overhead increase substantially as the number
of transformer layers grows. However, it is noteworthy that
the distributed inference approach consistently maintains a
significant latency advantage over centralized inference across
all models with different number of layers.
VI. CONCLUSION
In this paper, we proposed a novel distributed on-device
LLM inference framework employing tensor parallelism. To
mitigate the communication overhead from frequent all-reduce
steps in tensor parallelism, we proposed a communication-
effcient AirComp all-reduce approach. Moreover, to minimize
the average transmission MSE, we formulated a joint model
assignment and transceiver design problem, which can be de-
rived as a mixed-timescale stochastic non-convex optimization.


--- Page 14 ---
14
Model
Number of
Transformer
Layers
Method
Per-Token Local
Computation Time (ms)
Per-Token
Communication
Time (ms)
Per-Token Total
Generation Time (ms)
LLaMA2-7B
32
Centralized
114.2
0
114.2
Distributed
26.0
11.8
37.8
LLaMA2-13B
40
Centralized
217.3
0
217.3
Distributed
38.5
27.9
66.4
LLaMA2-70B
80
Centralized
1152.6
0
1152.6
Distributed
264.6
89.6
354.2
TABLE IV. Comparison of Centralized v.s. Decentralized Inference across Different Models: Per-Token Computation,
Communication, and Total Latency.
We further developed an efficient two-stage algorithm that
decomposed the original problem in short-term transceiver
optimization and long-term model assignment optimization
problems, which were solved by leveraging the SDR and
stochastic SCA, respectively. We proved that the proposed
algorithm can converge almost surely to a stationary point
of the original problem. Simulation results demonstrated that
the proposed approach significantly reduced inference latency
while improving inference accuracy, making distributed on-
device LLM inference feasible for resource-constrained edge
devices.
There are several promising directions for further advancing
distributed on-device LLM inference systems. One important
research direction is experimentally validating the proposed
AirComp-based distributed inference framework using real-
world wireless hardware setups, further assessing practical per-
formance and robustness. In addition, exploring cluster-based
hierarchical AirComp designs and distributed transceiver opti-
mization methods can effectively address potential scalability
bottlenecks arising from synchronization overhead, channel
estimation complexity, and computational demands in large-
scale device networks.
APPENDIX
A. Proof of Lemma 3
According to the assumption that channel statistics remain
constant throughout the inference process, we have that the
sample-wise approximation of the average MSE,
Â¯f0(mÏ„),
satisfying
Â¯f0(mÏ„)
a.s.
âˆ’âˆ’â†’f0(mÏ„),
(45)
E[âˆ¥Â¯f0(mÏ„) âˆ’f0(mÏ„)âˆ¥] = O
 1
âˆšÏ„

,
(46)
which follow from the law of large numbers and the central
limit theorem, respectively. Then, combining (45) and (46) into
(21), we have
lim
Ï„â†’âˆ| Ë†f Ï„
i (mÏ„) âˆ’fi(mÏ„)| = 0,
(47)
for i = 0, 1. Equation (47) indicates the convergence of Ë†f Ï„
i ,
and we then need to prove the convergence of âˆ‡Ë†f Ï„
i as follows,
lim
Ï„â†’âˆ|âˆ‡Ë†f Ï„
i (mÏ„) âˆ’âˆ‡fi(mÏ„)| = 0.
(48)
It is easy to verify that the MSE funtion Â¯f0 and its derivation
âˆ‡Â¯f0 are Lipschitz continuous, according to the fact that the
channel sample is always bounded in practice. Then, we can
obtain that
âˆ¥E[uÏ„
0] âˆ’âˆ‡f0(mÏ„)âˆ¥
â‰¤E[âˆ¥âˆ‡Â¯f0(mÏ„) âˆ’âˆ‡f0(mÏ„)âˆ¥]
(a)
â‰¤O
 1
âˆšÏ„

,
(49)
where (a) holds since âˆ‡Â¯f0 is Lipschitz continuous. From
Pâˆ
Ï„=0 ÏÏ„Ï„ âˆ’1/2 < âˆ, we can obtain that
âˆ
X
Ï„=0
ÏÏ„âˆ¥E[uÏ„
0] âˆ’âˆ‡f0(mÏ„)âˆ¥< âˆ,
(50)
which indicates the convergence of uÏ„
0. Then, according to
[61, Lemma 1], equation (48) holds.
Next, according to the fact that Â¯f0 is Lipschitz continuous,
it directly follows that there exists a constant l such that
lim
Ï„1,Ï„2â†’âˆ
Ë†f Ï„1
i (mÏ„1) âˆ’Ë†f Ï„2
i (mÏ„2) â‰¤lâˆ¥mÏ„1 âˆ’mÏ„2âˆ¥.
(51)
Finally, from (47), (48) and (51), we can obtain that the
sequences of the surrogate functions Ë†f Ï„
i (m) converge to Ë†fi(m)
almost surely.
B. Proof of Theorem 1
Let {mÏ„}âˆ
Ï„=1 denote the sequence of model assignment
policies generated by Algorithm 1. According to [51, Lemma
4], we have
lim
Ï„â†’âˆf1(mÏ„) â‰¤pmax,
(52)
lim
Ï„â†’âˆâˆ¥mÏ„ âˆ’Ë†mÏ„âˆ¥= 0,
(53)
where Ë†mÏ„ is obtained by solving problem (25) or (26). Then,
we introduce an auxiliary variable ËœmÏ„, which is the optimal
solution of the following problem,
ËœmÏ„ = min
m
Ë†f Ï„
0 (m)
s.t.
Ë†f Ï„
1 (m) â‰¤pmax + ÂµÏ„,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n,
(54)


--- Page 15 ---
15
where limÏ„â†’âˆÂµÏ„ = 0. Letting Ï„ â†’âˆin (54) and combining
(47) and (53) into (54), we have
mâˆ—= min
m
Ë†f0(m)
s.t.
Ë†f1(m) â‰¤pmax,
N
X
n=1
mn = 1,
0 â‰¤mn â‰¤1, âˆ€n.
(55)
Then, if mâˆ—satisfies the Slaterâ€™s condition, we have that the
KKT condition of problem (55) holds, i.e., there exists Î» such
that
âˆ‡Ë†f0(mâˆ—) + Î»âˆ‡Ë†f1(mâˆ—) = 0.
(56)
Finally, it follows from Lemma 1 and (56) that mâˆ—satisfies
the KKT condition of the original problem Pl as follows,
âˆ‡f0(mâˆ—) + Î»âˆ‡f1(mâˆ—) = 0.
(57)
This completes the proof.
REFERENCES
[1] K. Zhang, H. He, S. Song, J. Zhang, and K. B. Letaief, â€œDistributed on-
device LLM inference with over-the-air computation,â€ in Proc. IEEE
Int. Conf. Commun. (ICC), Montreal, Canada, Jun. 2025, to appear,
https://arxiv.org/abs/2502.12559.
[2] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,
E. Agirre, I. Heintz, and D. Roth, â€œRecent advances in natural language
processing via large pre-trained language models: A survey,â€ ACM
Comput. Surv., vol. 56, no. 2, pp. 1â€“40, 2023.
[3] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,
C. Wang, Y. Wang, et al., â€œA survey on evaluation of large language
models,â€ ACM Trans. Intell. Syst. Technol., vol. 15, no. 3, pp. 1â€“45,
2024.
[4] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann,
J. Gao, and H. Poon, â€œDomain-specific language model pretraining for
biomedical natural language processing,â€ ACM Trans. Comput. Healthc.,
vol. 3, no. 1, pp. 1â€“23, 2021.
[5] H. Fan, X. Liu, J. Y. H. Fuh, W. F. Lu, and B. Li, â€œEmbodied intelligence
in manufacturing: leveraging large language models for autonomous
industrial robotics,â€ J. Intell. Manuf., pp. 1â€“17, 2024.
[6] Y. Yang, T. Zhou, K. Li, D. Tao, L. Li, L. Shen, X. He, J. Jiang, and
Y. Shi, â€œEmbodied multi-modal agent trained by an LLM from a parallel
textworld,â€ in IEEE Conf. Comput. Vis. Pattern Recognit., pp. 26275â€“
26285, 2024.
[7] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., â€œPalm-e: An embodied
multimodal language model,â€ arXiv preprint arXiv:2303.03378, 2023.
[8] L. Bariah, Q. Zhao, H. Zou, Y. Tian, F. Bader, and M. Debbah, â€œLarge
generative AI models for telecom: The next big thing?,â€ IEEE Commun.
Mag., 2024.
[9] J. Shao, J. Tong, Q. Wu, W. Guo, Z. Li, Z. Lin, and J. Zhang,
â€œWirelessLLM: Empowering large language models towards wireless
intelligence,â€ J. Commun. Inf. Netw., vol. 9, pp. 99â€“112, 2024.
[10] J. Tong, J. Shao, Q. Wu, W. Guo, Z. Li, Z. Lin, and J. Zhang,
â€œWirelessAgent: Large language model agents for intelligent wireless
networks,â€ arXiv preprint arXiv:2409.07964, 2024.
[11] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
A. Mathur, A. Schelten, A. Yang, A. Fan, et al., â€œThe llama 3 herd of
models,â€ arXiv preprint arXiv:2407.21783, 2024.
[12] B. Wu, Y. Zhong, Z. Zhang, G. Huang, X. Liu, and X. Jin, â€œFast
distributed inference serving for large language models,â€ arXiv preprint
arXiv:2305.05920, 2023.
[13] A.
Borzunov,
M.
Ryabinin,
A.
Chumachenko,
D.
Baranchuk,
T. Dettmers, Y. Belkada, P. Samygin, and C. A. Raffel, â€œDistributed
inference and fine-tuning of large language models over the internet,â€
Adv. Neural Inf. Process. Syst., vol. 36, 2024.
[14] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang,
S. Wang, Y. Bao, et al., â€œInference without interference: Disaggre-
gate llm inference for mixed downstream workloads,â€ arXiv preprint
arXiv:2401.11181, 2024.
[15] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, â€œThe
roadmap to 6G: AI empowered wireless networks,â€ IEEE Commun.
Mag., vol. 57, no. 8, pp. 84â€“90, 2019.
[16] K. B. Letaief, Y. Shi, J. Lu, and J. Lu, â€œEdge artificial intelligence for
6G: Vision, enabling technologies, and applications,â€ IEEE J. Sel. Areas
Commun., vol. 40, no. 1, pp. 5â€“36, 2021.
[17] M. Zhang, J. Cao, X. Shen, and Z. Cui, â€œEdgeshard: Efficient
LLM inference via collaborative edge computing,â€ arXiv preprint
arXiv:2405.14371, 2024.
[18] X. Yuan, N. Li, T. Zhang, M. Li, Y. Chen, J. F. M. Ortega, and S. Guo,
â€œHigh efficiency inference accelerating algorithm for noma-based edge
intelligence,â€ IEEE Trans. Wireless Commun., 2024.
[19] Y. He, J. Fang, F. R. Yu, and V. C. Leung, â€œLarge language models
inference offloading and resource allocation in cloud-edge computing:
An active inference approach,â€ IEEE Trans. Mobile Comput., 2024.
[20] Y. Chen, R. Li, X. Yu, Z. Zhao, and H. Zhang, â€œAdaptive layer
splitting for wireless LLM inference in edge computing: A model-based
reinforcement learning approach,â€ arXiv preprint arXiv:2406.02616,
2024.
[21] J. Shao, Y. Mao, and J. Zhang, â€œLearning task-oriented communication
for edge inference: An information bottleneck approach,â€ IEEE J. Sel.
Areas Commun., vol. 40, no. 1, pp. 197â€“211, 2021.
[22] H. Li, W. Yu, H. He, J. Shao, S. Song, J. Zhang, and K. B. Letaief,
â€œTask-oriented communication with out-of-distribution detection: An
information bottleneck framework,â€ in Proc. IEEE Global Commun.
Conf. (GLOBECOM), Kuala Lumpur, Malaysia, Dec. 2023.
[23] H. Li, J. Shao, H. He, S. Song, J. Zhang, and K. B. Letaief, â€œTackling
distribution shifts in task-oriented communication with information
bottleneck,â€ arXiv preprint arXiv:2405.09514, 2024.
[24] D. Wen, X. Jiao, P. Liu, G. Zhu, Y. Shi, and K. Huang, â€œTask-oriented
over-the-air computation for multi-device edge AI,â€ IEEE Trans. Wire-
less Commun., 2023.
[25] F. Brakel, U. Odyurt, and A.-L. Varbanescu, â€œModel parallelism on
distributed infrastructure: A literature review from theory to LLM case-
studies,â€ arXiv preprint arXiv:2403.03699, 2024.
[26] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-
zaro, â€œMegatron-lm: Training multi-billion parameter language models
using model parallelism,â€ arXiv preprint arXiv:1909.08053, 2019.
[27] H. Dong, T. Johnson, M. Cho, and E. Soroush, â€œTowards low-bit
communication for tensor parallel LLM inference,â€ arXiv preprint
arXiv:2411.07942, 2024.
[28] J. Hansen-Palmus, M. Truong-Le, O. HausdÂ¨orfer, and A. Verma, â€œCom-
munication compression for tensor parallel LLM inference,â€ arXiv
preprint arXiv:2411.09510, 2024.
[29] B. Nazer and M. Gastpar, â€œComputation over multiple-access channels,â€
IEEE Trans. Inf. Theory, vol. 53, no. 10, pp. 3498â€“3516, 2007.
[30] S. Cui, J.-J. Xiao, A. J. Goldsmith, Z.-Q. Luo, and H. V. Poor, â€œEnergy-
efficient joint estimation in sensor networks: Analog vs. digital,â€ in IEEE
International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), vol. 4, pp. 745â€“748, IEEE, 2005.
[31] F. Wang and V. K. Lau, â€œMulti-level over-the-air aggregation of mobile
edge computing over d2d wireless networks,â€ IEEE Trans. Wireless
Commun., vol. 21, no. 10, pp. 8337â€“8353, 2022.
[32] M. Frey, I. BjelakoviÂ´c, and S. StaÂ´nczak, â€œOver-the-air computation in
correlated channels,â€ IEEE Trans. Signal Process., vol. 69, pp. 5739â€“
5755, 2021.
[33] X. Cao, G. Zhu, J. Xu, and K. Huang, â€œOptimized power control
for over-the-air computation in fading channels,â€ IEEE Trans. Wireless
Commun., vol. 19, no. 11, pp. 7498â€“7513, 2020.
[34] K. Yang, T. Jiang, Y. Shi, and Z. Ding, â€œFederated learning via over-
the-air computation,â€ IEEE Trans. Wireless Commun., vol. 19, no. 3,
pp. 2022â€“2035, 2020.
[35] J. Zhu, Y. Shi, Y. Zhou, C. Jiang, W. Chen, and K. B. Letaief, â€œOver-
the-air federated learning and optimization,â€ IEEE Internet Things J.,
2024.
[36] T. Sery, N. Shlezinger, K. Cohen, and Y. C. Eldar, â€œOver-the-air fed-
erated learning from heterogeneous data,â€ IEEE Trans. Signal Process.,
vol. 69, pp. 3796â€“3811, 2021.
[37] Z. Liu, Q. Lan, A. E. Kalor, P. Popovski, and K. Huang, â€œOver-the-
air multi-view pooling for distributed sensing,â€ IEEE Trans. Wireless
Commun., 2023.


--- Page 16 ---
16
[38] Z. Wang, A. E. Kalor, Y. Zhou, P. Popovski, and K. Huang, â€œUltra-
low-latency edge inference for distributed sensing,â€ arXiv preprint
arXiv:2407.13360, 2024.
[39] C. Feres, B. C. Levy, and Z. Ding, â€œOver-the-air multi-sensor collabora-
tion for resource efficient joint detection,â€ IEEE Trans. Signal Process.,
2023.
[40] X. Fan, Y. Wang, Y. Huo, and Z. Tian, â€œJoint optimization of com-
munications and federated learning over the air,â€ IEEE Trans. Wireless
Commun., vol. 21, no. 6, pp. 4434â€“4449, 2021.
[41] Y. Liang, Q. Chen, G. Zhu, H. Jiang, Y. C. Eldar, and S. Cui,
â€œCommunication-and-energy efficient over-the-air federated learning,â€
IEEE Trans. Wireless Commun., 2024.
[42] H. Sun, H. Tian, W. Ni, J. Zheng, D. Niyato, and P. Zhang, â€œFed-
erated low-rank adaptation for large models fine-tuning over wireless
networks,â€ IEEE Trans. Wireless Commun., 2024.
[43] Z. Zhuang, D. Wen, Y. Shi, G. Zhu, S. Wu, and D. Niyato, â€œIntegrated
sensing-communication-computation for over-the-air edge AI inference,â€
IEEE Trans. Wireless Commun., vol. 23, no. 4, pp. 3205â€“3220, 2023.
[44] P. Yang, D. Wen, Q. Zeng, Y. Zhou, T. Wang, H. Cai, and Y. Shi, â€œOver-
the-air computation empowered vertically split inference,â€ IEEE Trans.
Wireless Commun., 2024.
[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ Adv. Neural
Inf. Process. Syst., 2017.
[46] M. Goldenbaum, H. Boche, and S. StaÂ´nczak, â€œHarnessing interference
for analog function computation in wireless sensor networks,â€ IEEE
Trans. Signal Process., vol. 61, no. 20, pp. 4893â€“4906, 2013.
[47] X. Li, G. Zhu, Y. Gong, and K. Huang, â€œWirelessly powered data
aggregation for iot via over-the-air function computation: Beamforming
and power control,â€ IEEE Trans. Wireless Commun., vol. 18, no. 7,
pp. 3437â€“3452, 2019.
[48] M. L. Overton and R. S. Womersley, â€œOn the sum of the largest
eigenvalues of a symmetric matrix,â€ SIAM J. Matrix Anal. Appl., vol. 13,
no. 1, pp. 41â€“45, 1992.
[49] M. Grant and S. Boyd, â€œCvx: Matlab software for disciplined convex
programming, version 2.1,â€ 2014.
[50] Z.-Q. Luo, W.-K. Ma, A. M.-C. So, Y. Ye, and S. Zhang, â€œSemidefinite
relaxation of quadratic optimization problems,â€ IEEE Signal Process.
Mag., vol. 27, no. 3, pp. 20â€“34, 2010.
[51] A. Liu, V. K. Lau, and B. Kananian, â€œStochastic successive convex ap-
proximation for non-convex constrained stochastic optimization,â€ IEEE
Trans. Signal Process., vol. 67, no. 16, pp. 4189â€“4203, 2019.
[52] I. M. Bomze, V. F. Demyanov, R. Fletcher, T. Terlaky, I. PÂ´olik,
and T. Terlaky, â€œInterior point methods for nonlinear optimization,â€
Nonlinear Optimization, pp. 215â€“276, 2010.
[53] M. Razaviyayn, Successive convex approximation: Analysis and appli-
cations. PhD thesis, University of Minnesota, 2014.
[54] K. Zhang and X. Cao, â€œOnline power control for distributed multitask
learning over noisy fading wireless channels,â€ IEEE Trans. Signal
Process., 2023.
[55] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., â€œLlama
2: Open foundation and fine-tuned chat models,â€ arXiv preprint
arXiv:2307.09288, 2023.
[56] S. Merity, N. S. Keskar, and R. Socher, â€œRegularizing and optimizing
lstm language models,â€ arXiv preprint arXiv:1708.02182, 2017.
[57] B.
Tadych,
â€œDistributed
llama.â€
https://github.com/b4rtaz/
distributed-llama, 2024.
[58] G. Alon and M. Kamfonas, â€œDetecting language model attacks with
perplexity,â€ arXiv preprint arXiv:2308.14132, 2023.
[59] D. Tse and P. Viswanath, Fundamentals of wireless communication.
Cambridge university press, 2005.
[60] A. Goldsmith, Wireless communications. Cambridge university press,
2005.
[61] A. RuszczyÂ´nski, â€œFeasible direction methods for stochastic programming
problems,â€ Math. Program., vol. 19, pp. 220â€“229, 1980.
