--- Page 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
1
LLM-Enhanced Multi-Agent Reinforcement
Learning with Expert Workflow for Real-Time P2P
Energy Trading
Chengwei Lou, Zekai Jin, Wei Tang, Guangfei Geng, Jin Yang, Senior Member, IEEE, Lu Zhang, Senior
Member, IEEE
Abstract‚ÄîReal-time peer-to-peer (P2P) electricity markets dy-
namically adapt to fluctuations in renewable energy and varia-
tions in demand, maximizing economic benefits through instanta-
neous price responses while enhancing grid flexibility. However,
scaling expert guidance for massive personalized prosumers poses
critical challenges, including diverse decision-making demands
and a lack of customized modeling frameworks. This paper
proposes an integrated large language model-multi-agent rein-
forcement learning (LLM-MARL) framework for real-time P2P
energy trading to address challenges such as the limited technical
capability of prosumers, the lack of expert experience, and
security issues of distribution networks. LLMs are introduced as
experts to generate personalized strategies, guiding MARL under
the centralized training with decentralized execution (CTDE)
paradigm through imitation. To handle the scalability issues
inherent in large-scale P2P networks, a differential attention-
based critic network is introduced to efficiently extract key
interaction features and enhance convergence. Experimental
results demonstrate that LLM-generated strategies effectively
substitute human experts. The proposed imitative expert MARL
algorithms achieve significantly lower economic costs and voltage
violation rates on test sets compared to baseline algorithms, while
maintaining robust stability. This paper provides an effective
solution for the real-time decision-making of the P2P electricity
market by bridging expert knowledge with agent learning.
Index Terms‚ÄîP2P Energy Trading, Large Language Model,
Multi-Agent Reinforcement Learning, Imitation Learning, Atten-
tion Mechanism
I. INTRODUCTION
T
HE rise of peer-to-peer (P2P) energy trading has shifted
electricity users from traditional consumers to ‚Äùpro-
sumers,‚Äù combining both production and consumption [1].
However, this development faces two challenges: on the
virtual layer, prosumers often lack the technical capability
for repeated trading and efficient energy management [2];
on the physical level, ensuring system security during the
transmission of electricity transactions from the virtual layer in
actual distribution networks remains a challenge [3]. Although
the majority of energy trading is settled in the day-ahead
stage, actual load and renewable generation often deviate
This work was supported by Smart Grid-National Science and Technol-
ogy Major Project(2024ZD0800500). (Corresponding authors: Zekai Jin; Lu
Zhang). Chengwei Lou, Zekai Jin, Wei Tang, and Lu Zhang are with the Col-
lege of Information and Electrical Engineering, China Agricultural University,
Beijing, China. Guangfei Geng is with the School of Information Science and
Technology, Guangdong University of Foreign Studies, Guangzhou, China. Jin
Yang is with the James Watt School of Engineering, University of Glasgow,
Glasgow, United Kingdom.
substantially from the scheduled profiles due to inherent uncer-
tainties. Consequently, real-time mechanisms are indispensable
for rapidly rescheduling trades to restore power balance and
enabling distributed resources to provide immediate balancing
services [4]. At present, electricity markets such as Nord Pool
in the Nordic region, PJM and ERCOT in the United States,
and Australia‚Äôs National Electricity Market mainly operate
under a two-stage framework with separate settlement in the
day-ahead and real-time markets. As the real-time market is
a critical component of electricity market operations, it is
necessary to operate P2P market mechanisms in a real-time
operational mode as well [5].
In response to this requirement, recent studies have fo-
cused on developing real-time P2P energy trading. Specifi-
cally, studies such as [6] and [7] employed Lyapunov opti-
mization to relax temporal coupling constraints and utilized
the Alternating Direction Method of Multipliers to achieve
distributed solutions for real-time P2P energy rescheduling. In
parallel, [8] proposed a real-time P2P energy trading method
grounded in Model Predictive Control (MPC). Nevertheless,
these approaches have inherent limitations. Lyapunov opti-
mization inevitably introduces approximation errors due to
the relaxation of temporal coupling constraints. Similarly, the
open-loop nature of MPC policies renders them intrinsically
suboptimal in the presence of stochastic noise
[9]. Further-
more, such conventional model-driven methods rely heavily
on precise system modeling and parameterization, which are
typically difficult to obtain in practice [10]. Consequently, they
often struggle to accommodate the highly dynamic nature of
real-time P2P energy trading, creating a trade-off between
computational efficiency and decision-making flexibility [11].
To address these challenges, reinforcement learning (RL) has
emerged as a promising solution, owing to its robust adapt-
ability to uncertain environments and its ability to facilitate
rapid strategy generation [12].
RL relies heavily on trial-and-error interactions with the
environment, faces fundamental challenges when applied to
P2P energy trading systems with strict operational constraints.
RL methods struggle to efficiently converge to a global market
equilibrium [13], particularly in large-scale P2P markets char-
acterized by highly personalized prosumers, whose preferences
and capacities demand individualized and incentive-compat-
ible decision-making rather than uniform policies [14]. To
mitigate these challenges, optimization-based solvers and imi-
tation learning techniques have been introduced to incorporate
arXiv:2507.14995v3  [cs.MA]  26 Jan 2026


--- Page 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
2
expert knowledge into the learning process, thereby improving
training efficiency and solution feasibility in related power
system applications, such as microgrid operation [15] and
distribution network reconfiguration [16]. For instance, [17]
proposed a multi-virtual-agent imitation learning framework
that leverages adversarial imitation across multiple virtual
environments to achieve robust microgrid energy scheduling
under uncertain power supply interruptions. However, imita-
tion learning approaches driven by a single expert‚Äôs decision-
making experience remain limited in their ability to adapt to
the diverse personalities and strategic preferences of prosumers
in P2P energy trading. Standardized expert strategies often fail
to capture individualized behaviors and incentive structures.
Moreover, relying on a human Distribution System Opera-
tor (DSO) as the expert decision-maker introduces practical
constraints, including high labor costs and limited real-time
responsiveness, while the lack of generalization capability
further amplifies operational risks [18].
With the emergence of large language models (LLMs) like
ChatGPT, LLMs showcase strong reasoning, decision-making,
and generalization abilities, addressing the shortcomings of
using single experts in RL to handle the heterogeneity of pro-
sumers, as well as the challenges of human expert labor costs
and generalization. While LLMs have begun to be applied in
the power and energy sector‚Äîsuch as in [19], where an RL2
mechanism is designed with LLM-assisted safe reinforcement
learning for active distribution network energy management
via an iterative penalty function, and in [20], where the
LLM is used to interpret linguistic operational stipulations
and generate reward signals, alleviating the need for manually
designing explicit reward functions for qualitative objectives.
[21] evaluates LLMs‚Äô performance in various power system
tasks, highlighting their potential in complex system modeling
and reasoning. The study in [22] introduces a power multi-
agent framework with a feedback mechanism, validated on
the DALINE and MATPOWER platforms. However, the inte-
gration of LLMs as expert systems for assisting prosumers in
RL training for P2P energy trading remains underexplored.
Currently, in non-power system domains LLMs have been
successfully used as expert guides in autonomous driving
RL training [23], [24], providing valuable insights for the
application of LLMs in the P2P energy trading domain.
Nevertheless, applying single-agent RL to P2P energy trad-
ing entails inherent limitations. When consumers seek individ-
ual economic optimality, uncoordinated decisions can cause an
excessive concentration of load or generation, posing risks to
the security of the distribution network [25]. Furthermore, the
concurrent and independent policy updates of multiple pro-
sumers render the learning environment highly non-stationary
from the perspective of any single agent [26]. To address these
challenges, multi-agent reinforcement learning (MARL) has
emerged as a promising paradigm for P2P energy trading, as
it explicitly captures strategic interactions among distributed
prosumers in continuous action spaces, supports decentralized
decision-making, and promotes cooperative behaviors. [27]
proposed a multi-agent adversarial reinforcement learning to
solve the active voltage control problem in peer-to-peer en-
ergy trading-enabled distribution networks. [28] formulates the
network-constrained MARL P2P energy trading problem as a
cooperative Markov game.
However, the complexity of managing numerous agents with
high-dimensional actions under the Centralized Training and
Decentralized Execution (CTDE) framework [29] limits the
ability to exploit global collaborative information [30]. To
improve learning efficiency, attention mechanisms have been
introduced into MARL models [31] to better extract relevant
features. Empirical studies confirm their effectiveness in power
system applications such as voltage regulation [32], microgrid
trading [33], and community-based P2P trading [34]. While
attention is inherently a neural network architecture rather
than a market mechanism, it is particularly well-suited for
modeling the uneven interaction patterns in P2P energy trading
[35]. Nevertheless, conventional attention mechanisms often
lacks a holistic understanding of the entire sequence and may
ignore critical information [36]. In a distribution network,
a prosumer‚Äôs state is significantly affected only by a subset
of relevant peers, rather than the entire population. Standard
attention mechanisms may fail to distinguish these critical
trading partners from irrelevant ones, leading to inefficient
coordination in large-scale P2P systems, which constrains
MARL‚Äôs performance in real-world P2P trading.
In summary, this paper addresses the P2P energy trading
limitations of existing reinforcement learning methods identi-
fied in TABLE I. It proposes a novel integrated framework in
which LLM-based experts guide the training of MARL agents,
with the goal of maximizing social welfare in local real-
time P2P electricity markets featuring prosumer collaboration.
During the initial stage of training, the LLM generates model
code tailored to differentiated prosumers‚Äô demands. Specif-
ically, each prosumer‚Äôs expert is embedded within a multi-
LLM workflow, where real-time states are fed into solvers,
indirectly generating expert strategies for each prosumer. For
the training phase, a CTDE-based imitative expert MARL
algorithm is proposed. Furthermore, inspired by the recent
application of the Differential Transformer in LLMs [37],
an enhanced critic network architecture utilizing differential
attention is designed. This architecture is specifically devel-
oped to mitigate irrelevant information interference from other
agents among large-scale prosumers during training and to
enhance the overall convergence performance of the algorithm.
The main contributions of this article are summarized as
follows:
TABLE I: Comparison limitations in P2P energy trading
reference
Ref.
Prosumer
Personalized
Network
Constraints
Large
Scalability
Method
[14]
‚úì
√ó
‚Äî
RL
[28]
√ó
‚úì
√ó
MARL
[34]
√ó
√ó
‚úì
MARL
[35]
√ó
√ó
‚úì
MARL
This Paper
‚úì
‚úì
‚úì
LLM-MARL
1) A novel LLM-MARL integrated framework is proposed
for the real-time P2P electricity market. By introducing
LLMs as expert in the P2P energy trading, the framework


--- Page 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
3
replaces human experts to guide MARL agents during
training. This directly substitutes the process of human
expert code generation or expert guidance and achieves
a deep integration of expert knowledge and LLM-based
reasoning.
2) An LLM expert workflow tailored to local P2P energy
trading is developed for each prosumer. This workflow
transforms prosumer natural language input into exe-
cutable actions through model generation, tool retrieval,
code generation, and code correction. By processing state
information, it dynamically generates prosumer strategies
that balance economic performance and distribution net-
work security, thereby providing reliable expert guidance
during training.
3) A novel imitative expert MARL algorithm is proposed.
It introduces the Wasserstein metric to measure the simi-
larity between expert strategy and agent policy, enabling
effective guidance from the LLM experts‚Äô workflow. Fur-
thermore, a differential multi-head attention-based Critic
network is designed to improve policy evaluation accu-
racy and accelerate the learning process in the large-scale
P2P energy trading, thereby boosting overall algorithmic
performance.
This paper is structured as follows: Section 2 introduces the
system model and decentralized partially observable Markov
decision process (Dec-POMDP) formulation for P2P energy
trading. Section 3 details the integration of LLM and MARL.
Section 4 presents numerical study and result analysis. Section
5 concludes the paper with key findings.
II. PRELIMINARIES
A. Prosumer energy management and P2P Trading
Prosumers are modeled as independent energy units with
conventional distributed generators (CDGs), renewable dis-
tributed generators (RDGs), battery energy storage systems
(BESSs), and controllable loads (CLs). This study adopts a
15-minute interval for optimization to align with real-time P2P
energy trading settlement practices [6].
CDG output is limited by physical and safety constraints,
including ramp rate limits on power variation. RDGs, such
as Photovoltaic (PV) and Wind Turbine (WT), adjust active
and reactive power via inverter control. BESS regulates energy
flow within power and State of Charge (SOC) limits to pre-
vent overcharging or deep discharge. CLs offer demand-side
flexibility, adjusting consumption based on load characteristics
and user preferences.
P CDG
i,min ‚â§P CDG
i,t
‚â§P CDG
i,max,
(1)
|P CDG
i,t
‚àíP CDG
i,t‚àí1 | ‚â§RCDG
i,max,
(2)
0 ‚â§P RDG
i,t
‚â§P RDG
i,t,max,
(3)
P RDG,2
i,t
+ QRDG,2
i,t
‚â§SRDG,2
i,max ,
(4)
P BESS
i,min
‚â§P BESS
i,t
‚â§P BESS
i,max ,
(5)
SOCBESS
i,min ‚â§SOCBESS
i,t
‚â§SOCBESSS
i,max
,
(6)
SOCBESS
i,t
=

SOCBESS
i,t‚àí1
+ P BESS
i,t
/Œ∑, P BESS
i,t
< 0
SOCBESS
i,t‚àí1
+ Œ∑P BESS
i,t
, P BESS
i,t
‚â•0
(7)
0 ‚â§P CL
i,t
‚â§Œ±P Load
i,t
,
(8)
where P CDG
i,t
is the CDG output, bounded by [P CDG
i,min , P CDG
i,max]
with ramp limit RCDG
i,max; P RDG
i,t
,QRDG
i,t
is the active / reactive
power of RDG, limited by its maximum active power P RDG
i,t,max
and apparent power rating SRDG
i,max, P BESS
i,t
is the power
of BESS bounded by [P BESS
i,min , P BESS
i,max ]; SOCBESS
i,t
obeys
efficiency Œ∑ and bounded by [SOCBESS
i,min , SOCBESS
i,max ]; P CL
i,t
is the controllable load up to fraction Œ± of its demand.
Prosumers at different nodes engage in P2P energy trading
to increase their revenue. Each prosumer must satisfy an
internal power balance, ensuring that generation, consumption,
and storage remain aligned.
P EX
i,t
= ‚àíP Grid
i,t
‚àíP P 2P
i,t
Pi,t = P CDC
i,t
+ P RDG
i,t
+ P CL
i,t ‚àíP Load
i,t
‚àíP BESS
i,t
,
(9)
QEX
i,t
= QRDG
i,t
‚àíQLoad
i,t
,
(10)
where P P 2P
i,t
is the P2P electricity trading; P Grid
i,t
is the active
power purchase and sale with the grid; P Load
i,t
,QLoad
i,t
are the
active and reactive loads of prosumer.
Power flow constraints in the distribution network guarantee
the safe, stable, and efficient operation of the power system.
P EX
i,t
= Vi,t
X
j‚ààN
Vj,t(Gij cos Œ∏ij,t + Bij sin Œ∏ij,t),
(11)
QEX
i,t
= Vi,t
X
j‚ààN
Vj,t(‚àíBij cos Œ∏ij,t + Gij sin Œ∏ij,t),
(12)
Vmin ‚â§Vi,t ‚â§Vmax,
(13)
where Vi,t is the node voltage magnitude and bounded by
[Vmin, Vmax]; Gij,Bij is the conductance and susceptance of
branch ij; Œ∏ij is the voltage phase angle difference.
The total operational cost for a single prosumerCCost
i
consists of the power purchase or sale cost from the grid
CGrid
i
, CDG operational costs CCDG
i
, BESS maintenance
costs CBESS
i
, CL compensation costs CCL
i
, and P2P energy
trading costs CP 2P
i
.
CCost
i
= CGrid
i
+ CCDC
i
+ CBESS
i
+ CCL
i
+ CP 2P
i
. (14)
Prosumers‚Äô electricity purchase and sale costs follow
time-of-use pricing. CDG operational costs are modeled as
quadratic functions of fuel consumption. BESS costs depend
on charging and discharging power, while CL costs reflect
user dissatisfaction from load reduction. P2P trading incurs
additional trading costs.
CGrid
i
=
X
t
 ŒªS
t P Grid
i,t
, P Grid
i,t
< 0
ŒªB
t P Grid
i,t
, P Grid
i,t
‚â•0
(15)
CCDG
i
=
X
t
cCDGP CDG,2
i,t
+ bCDGP CDG
i,t
,
(16)
CBESS
i
=
X
t
Œ≥|P BESS
i,t
|,
(17)
CCL
i
=
X
t
œÅ|P CL
i,t |,
(18)
CP 2P
i
=
X
t
ŒªDSO|P P 2P
i,t
| + ŒªP 2P
t
P P 2P
i,t
,
(19)
where ŒªB
t , ŒªS
t are the time-of-use electricity purchase and sales
price for the grid; cCDG, bCDG are the quadratic and linear


--- Page 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
4
cost coefficients of CDG fuel cost; Œ≥ is the maintenance cost
coefficient; œÅ is the compensation cost coefficient; ŒªDSO
t
is the
P2P compensate fees charged by DSO; ŒªP 2P
t
is the real-time
P2P price.
The real-time P2P trading price ŒªP 2P
t
is set as a fixed
proportion between the grid‚Äôs time-of-use buying and selling
prices [38]. It is expressed as:
ŒªP 2P
t
= Œ∫P 2P (ŒªB
t ‚àíŒªS
t ) + ŒªS
t ,
(20)
where Œ∫P 2P ‚àà[0, 1] is a price coefficient that determines the
fairness of the P2P market. This pricing scheme incentivizes
local trading by offering sellers a higher price than grid export
rates and buyers a lower price than grid retail rates.
This paper aims to maximize the social welfare of pro-
sumers by assuming that each prosumer makes rational trad-
ing decisions and accepts a centrally coordinated P2P price
determined by the DSO. Whenever a prosumer experiences a
surplus or deficit of electricity, it first seeks to balance supply
and demand through local P2P energy trading.
X
i
X
t
ŒªP 2P
t
P P 2P
i,t
= 0,
(21)
However, within P2P energy trading, the total revenue of
prosumers is 0 [39].
B. Dec-POMDP for P2P Energy Trading
In P2P energy trading, the inherent uncertainty challenges
traditional mathematical optimization methods in meeting the
precision and real-time demands of prosumer optimization
control. RL methods can address these limitations, enabling
data-driven optimization decisions. This paper models the
P2P trading problem for multiple prosumers in a distribution
network as a Dec-POMDP, represented as an eight-tuple
‚ü®I, A, S, O, P, r, œÄ, Œ≥‚ü©, where S is the global state space, A
is the joint action space, r is the global reward function based
on state transitions P, O is the observation space, and Œ≥ is the
discount factor. The model captures the features of information
asymmetry and decentralized decision-making through partial
observability and decentralized architecture.
1) Agent: Each prosumer participating in P2P energy trading
at a node in the distribution network is considered an
agent. The set of agents is defined as I.
2) Action A: The joint action space at time t is represented
by at = {ai,t |i ‚ààI}, where ‚àÄat ‚ààA. The action space
of each agent, ai,t, consists of the actions of controllable
devices within the prosumer.
ai,t =

P CDG
i,t
, P RDG
i,t
, QRDG
i,t
, P BESS
i,t
, P CL
i,t

.
(22)
3) State S: The global state at time t, denoted as st =
{si,t |i ‚ààN}, for ‚àÄst ‚ààN, encompasses the operational
conditions of all nodes in the distribution network. Vi,t‚àí1
captures node voltage magnitudes via phasor measure-
ment units and wireless sensors [40], enabling realistic
prosumer state observation. Specifically, si,t includes
the operational state of the prosumer, the distribution
network‚Äôs interaction state, and the previous action of the
prosumer:
si,t =

t, ŒªB
t , ŒªS
t , P RDG
i,t,max, P Load
i,t
, QLoad
i,t
P CDG
i,t‚àí1 , SOCi,t‚àí1, P P 2P
i,t‚àí1, Vi,t‚àí1

.
(23)
4) Observation O:
The joint observation at time t is represented by ot =
{oi,t |i ‚ààI}, for ‚àÄot ‚ààI. The observation of the i-th
agent at time t is the state of the corresponding node,
i.e., oi,t = si,t.
5) State Transition Probability P: The state transition is
described by the conditional probability distribution
P
 st+1
st, at

, which represents the probability of transi-
tioning to the next time step. This transition process con-
siders power flow distribution, load demand fluctuations,
and renewable energy output uncertainty. The power flow
distribution is driven by the actions at of the controlled
devices.
6) Reward Function r:
Given the strong coupling between active power injec-
tions and nodal voltages in P2P energy trading, ignor-
ing physical limits renders market outcomes infeasible
[27]. Following [28], we adopt a unified global reward
function shared among all agents. This design is crucial
because, in physically coupled networks, a shared signal
is necessary to guide agents toward a Nash Equilibrium
that maximizes social welfare while strictly adhering to
voltage safety limits. To this end, the reward function is
designed as:
r = rCost + rP en,
(24)
rCost = Œ¥
X
I
CCost
i
,
(25)
rP en = aP en + cP enmax

0,
 Vbase ‚àíVi,t
 ‚àíVmax‚àíVmin
2

,
(26)
where Œ¥ is the weight coefficient for operational costs, and
rPen represents the penalty cost for voltage violations in
the distribution network.
III. METHODOLOGY
This paper focuses on P2P energy trading in distribution
networks and introduces a novel MARL framework con-
strained by expert strategies. The proposed framework adopts
an off-policy RL paradigm to enable efficient online updates.
In this approach, LLM serves as an expert, generating per-
sonalized strategies to guide prosumers in energy trading.
Fundamentally, it constitutes a form of ‚Äúphase-triggered code
synthesis training‚Äù enabled by LLM. Specifically, at the onset
of each training phase, the LLM synthesizes optimization code
tailored to the current system configuration. During subsequent
online learning iterations, this code is executed in a loop
to produce state-dependent, personalized strategies for each
prosumer agent. To ensure a principled integration of expert
knowledge with agent learning, the imitative expert MARL
algorithm is enhanced using the Wasserstein metric, which


--- Page 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
5
Fig. 1: Our proposed LLM-MARL framework
promotes alignment between the LLM strategy and the learned
agent policies. When prosumer devices are modified or newly
added, the LLM regenerates the code to adapt to the updated
environment. The overall framework is illustrated in Fig. 1.
A. LLM Expert Workflow
1) Knowledge Enhancement Methods: Knowledge injec-
tion enhances the capabilities of LLMs and help mitigate infor-
mation hallucination when these models act as domain experts.
The main knowledge enhancement approaches include Super-
vised Fine-Tuning (SFT) and Retrieval-Augmented Generation
(RAG). Unlike SFT, which requires full parameter fine-tuning,
RAG decouples the knowledge storage from the inference
generation. It only requires maintaining an external knowledge
base. This characteristic not only ensures the real-time model
outputs but also improves the interpretability of the generated
results through explicit knowledge tracing. Recent research
from Microsoft has demonstrated that RAG outperforms SFT
in integrating domain-specific knowledge into LLMs [41],
making RAG the core knowledge enhancement framework in
this paper.
In constructing the knowledge system for LLM-based expert
systems, this paper creates a structured JSON-format external
knowledge base. JSON‚Äôs inherent facilitation of data extraction
and processing, relative to alternative formats, establishes a
robust foundation for retrieving broader and more precise
knowledge during search operations [42]. The knowledge
base integrates power domain expert knowledge and tool
documentation, including system optimization models, de-
vices, objective functions, constraints, and support for dynamic
updates. Additionally, it includes detailed descriptions of the
core classes, functions, and constraints in the cvxpy library
[43]. Furthermore, this paper designs a multi-level prompt
engineering strategy, which first clarifies the roles of various
domain experts, then uses Chain-of-Thought techniques to
guide experts in step-by-step reasoning, and ensures that the
LLM structures content according to predefined rules.
2) Prosumer-Centric LLM Expert Strategies: This paper
presents an LLM-based expert execution workflow to sup-
port personalized operational strategy for each prosumer. The
system includes four complementary LLM agent experts and
a module for distribution networks security verification via
DSO, All optimization models are formulated in cvxpy and
solved using mathematical optimization solvers such as Gurobi
[44]. The overall process is shown in Fig. 2, as described
below:
‚Ä¢ Model Generation Expert: This module LLM extracts
key devices and optimization requirements from the input
of the prosumer‚Äôs natural language, generates the corre-
sponding model knowledge, and predicts relevant cvxpy
Atoms and Constraints based on retrieval results and tool
documentation.
‚Ä¢ Code Generation Expert: This module LLM constructs
the optimization model in the cvxpy framework using
knowledge of the power domain, the cvxpy programming
syntax, and the device parameters and state data of the
prosumer. It outputs DCP-rule-compliant and feasible
modeling code.
‚Ä¢ Iterative Correction Expert: This module LLM runs the
model in a sandbox environment using cvxpy together
with commercial solvers, detecting and correcting syntax
errors and runtime issues, ensuring that the model is
executable and complete.
‚Ä¢ Energy Trading Integration Expert: After prosumer
modeling and validation, this module LLM integrates the
P2P energy trading variables into the optimization model,
adding the necessary objective functions and constraints,
and outputs personalized objective function, constraint
list, and node injection of active and reactive power.
‚Ä¢ Distribution Networks Security Verification: After all
prosumer objectives and constraints are submitted, the
DSO adds the objective functions and constraint sets of
all prosumers. It then incorporates the active and reactive
power node injections into the branch flow model [45]
and invokes a commercial optimization solver to verify
the global power flow correction across the distribution
network, thereby generating optimized operating strate-
gies for the prosumers.
B. Multi-Agent Imitation Learning Algorithm
For the prosumer collaborative learning problem, this paper
proposes an expert strategy-constrained MARL Algorithm
based on the CTDE framework. The Algorithm constructs
a joint state-action value function (Q-function) and state
value function (V-function) through a centralized evaluation
network, integrating the global environmental state and the
behavior policies of all agents during the training phase,
thereby guiding the differential optimization of individual
agent policies. During execution, a decentralized approach


--- Page 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
6
Fig. 2: Our proposed LLM expert workflow in P2P energy trading
is adopted, where each agent independently makes decisions
based on local observations through independently trained
networks, balancing cooperative benefits and decision-making
efficiency.
1) Preparation: The expert strategy constraints are embed-
ded within a multi-agent actor-critic framework to solve the
constrained optimization problem. Based on the Lagrangian
dual theory, this problem is first transformed into its La-
grangian dual form. For each prosumer agent i, the multi-agent
formulation can be expressed as:
min
œÄ
Est‚àºB,at‚àºœÄœïi(¬∑|oi,t)
h
‚àímin
z‚àà1,2QŒ∏i,z(st, at)
i
s.t. ÀÜW2
 œÄœïi(¬∑|oi,t), œÄLLM(¬∑|oi,t)

‚â§œµ ,
(27)
where B is the experience replay buffer; œï is the policy net-
work parameters and the output sampling from the Gaussian
distribution ai,t ‚àºN(¬µœïi, œÉ2
œïi); œµ is the policy deviation; Œ∏
is the Q-function network parameters and z = 1,2; œà is the
V-function network parameters. The policy network outputs
a distribution over actions, which inherently facilitates ex-
ploration during training by enabling diverse action sampling
under identical states.
Since the LLM-based expert strategies can only generate the
mean parameters of the policy distribution, without estimating
the standard deviation, the output is modeled as a Dirac delta
function representing a degenerate distribution. To measure the
similarity during policy iterations, the Wasserstein-2 metric,
known for its distributional robustness, is used. In the case
of a one-dimensional Gaussian distribution, ÀÜW 2
2 has a closed-
form analytical solution [46]:
ÀÜW 2
2 (œÄœïi(¬∑|oi,t), œÄLLM(¬∑|oi,t) =
Z 1
0
F ‚àí1(q) ‚àíG‚àí1(q)
2 dq,
(28)
Z 1
0
 ¬µœïi + œÉœïiŒ¶‚àí1(q) ‚àíaLLM
i,t
2 dq =
Z 1
0
h
(¬µœïi ‚àíaLLM
i,t
)2
+ 2(¬µœïi ‚àíaLLM
i,t
)œÉœïiŒ¶‚àí1(q) + œÉ2
œïi
 Œ¶‚àí1(q)
2 i
dq.
(29)
The resulting Wasserstein-2 metric between the expert strat-
egy and the policy network is:
ÀÜW2(œÄœïi(¬∑|oi,t), œÄLLM(¬∑|oi,t) =
q
(¬µœïi ‚àíaLLM
i,t
)2 + œÉ2
œïi.
(30)
2) Multi-Head Differential Attention: To enhance the mod-
eling capability of the centralized critic in capturing inter-agent
interactions, a multi-head differential attention mechanism is
integrated into the critic network. This design aims to improve
the accuracy of global value estimation. For each agent i, the
input is first transformed into an embedding ei and then E
represents the traversal of these embedded vectors. Matrix E
is then split into h attention heads and linearly projected into
the query, key, and value spaces for each head, as follows:
[Qh
1, Qh
2] = EW Q, [Kh
1 , Kh
2 ] = EW K, V h = EW V , (31)
where Qh
1, Qh
2, Kh
1 , Kh
2 , V h ‚ààRdmodel. The differential atten-
tion mechanism operates by subtracting two softmax-based
attention maps, aiming to eliminate redundant information


--- Page 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
7
Fig. 3: Multi-head differential attention critic network
among agents and emphasize critical dependencies. The at-
tention output for each head is computed as:
headh =

softmax(Qh
1Kh‚ä§
1
‚àödk
) ‚àíŒæhsoftmax(Qh
2Kh‚ä§
2
‚àödk
)

V h,
(32)
X = Concat(head1, ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ , headh)W O,
(33)
where W O is a project matrix; scaling factor Œæh is a learnable
scalar and dynamically computed as:
Œæh = exp(Œæq1 ¬∑ Œæk1) ‚àíexp(Œæq2 ¬∑ Œæk2) + Œæinit,
(34)
where Œæq1, Œæq2, Œæk1, Œæk2 are trainable vectors that vary with the
head index.
To enhance the representational capacity of critic networks
while preserving ego-specific information, we employ a fea-
ture fusion mechanism that concatenates the intermediate out-
put xi with the original agent embedding ei. This ensures
that agent-specific features remain directly accessible to subse-
quent layers, mitigating potential information loss during inter-
mediate transformations. The fused representation is processed
by an MLP of fixed dimensionality, guaranteeing that its size
remains invariant to agent population growth. This fusion facil-
itates flexible feature interactions and refines the representation
prior to generating the critic‚Äôs final value estimate. The overall
architecture is illustrated in Fig. 3.
The differential attention mechanism is motivated by the
observation that, in real-time P2P energy trading, only a subset
of prosumers significantly influences an agent‚Äôs decision. For
example, when an industrial prosumer exhibits high energy de-
mand, cooperating with nearby prosumers possessing renew-
able surpluses can significantly enhance the social welfare. In
contrast, during low-demand periods, such trading interactions
are less influential to the cooperative outcome. Conventional
attention mechanisms may assign non-negligible weights to
local irrelevant prosumers, thereby diluting focus on critical
trading relationships. To address this, the proposed mechanism
computes two attention maps, one capturing general relevance
and the other suppressing redundant interactions, and subtracts
them. This operation sharpens the critic network‚Äôs focus on
discriminative inter-agent dependencies, enabling more accu-
rate value estimation.
3) Learning the Critics: In scenarios where agents share
a global reward, a major challenge is to reduce the variance
of policy gradient estimates in interactive multi-agent envi-
ronments. To address this issue, a double Q-function network
and a target V-function network are employed. The mini-
mum selection operation in the double Q-function network
effectively mitigates overestimation bias. The incorporation
of a V-function network contributes to a significant reduction
in estimation variance [47]. All agents share critic networks
with shared parameters; this design significantly reduces the
number of trainable parameters and improves training stability
and scalability in large-scale multi-agent systems.
For V-function network Vœà, the loss function is calculated to
approximate the given the current state and Q-function values:
LV (œà) = Est‚àºB,at‚àºœÄœïi(¬∑|oi,t)
h
(Vœà(st) ‚àímin
z‚àà1,2QŒ∏z(st, at))2i
.
(35)
For the Q-function network QŒ∏, the training target yt is
computed using the a delay updated target network Vœà. The
loss function is defined as follows:
LQ(Œ∏) =E(st,at,rt,st+1)‚àºB
1
2 (QŒ∏(st, at) ‚àíyt)2

yt = ri,t + Œ≥V ¬Ø
œà(st+1) ,
(36)
where the target V-function network œà is updated via Polyak
averaging rather than direct copying to enhance stability:
¬Øœà ‚ÜêœÑœà + (1 ‚àíœÑ) ¬Øœà ,
(37)
where œÑ ‚â™1 is the smoothing coefficient.
4) Learning the Actors: Following the critic network up-
date, the policy constraint in (27) can be expressed as a
Lagrangian dual problem [48], where Lagrangian multipliers
Œªi > 0. The formulation becomes:
max
Œª
min
œÄ
Est‚àºB,at‚àºœÄœïi(¬∑|oi,t)

‚àímin
z‚àà{1,2} QŒ∏i,z(st, at)+
Œªi

ÀÜW2
 œÄœï(¬∑|oi,t), œÄLLM(¬∑|oi,t)

‚àíœµ

.
(38)
Policy improvement serves to optimize and update the
MARL policy. œïi updated by minimizing the following loss
function:
LœÄ(œïi) = Est‚àºB,at‚àºœÄœïi(¬∑|oi,t)

‚àímin
z‚àà{1,2} QŒ∏i,z(st, at)+
Œªi

ÀÜW2
 œÄœï(¬∑|oi,t), œÄLLM(¬∑|oi,t)

‚àíœµ

.
(39)


--- Page 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
8
By updating Œªi, the degree of constraint violation can be
mitigated. This is achieved by minimizing the following loss
function:
L(Œªi) = Est‚àºB
h
‚àíŒªi
  ÀÜW2(œÄ(¬∑|oi,t), œÄLLM(¬∑|oi,t

‚àíœµ)
i
.
(40)
In the initial phase of training, a low policy deviation is
employed to guide the agent‚Äôs learning; during the middle and
later phases, a larger policy deviation is introduced to sustain
the agent‚Äôs exploration. This framework ultimately enables
efficient and stable policy optimization.
5) Prioritized Experience Replay: A sample loss-based pri-
ority evaluation mechanism is introduced. Samples with higher
losses are considered more valuable for the learning process
and are assigned higher priority, increasing their sampling
probability.
Two experience replay buffers are defined: the normal
operation experience replay buffer stores experiences collected
during time steps where the agent‚Äôs actions do not cause
voltage violations, reflecting typical operating conditions; the
constraint violation experience replay buffer stores experiences
from extreme renewable energy time steps in which the agent‚Äôs
actions lead to grid voltage limit violations. During training,
experiences are sampled from both buffers separately with a
loss-based priority evaluation mechanism, drawn in a ratio of
k : (1 ‚àík), to form training batches.
IV. NUMERICAL STUDY
To validate the effectiveness of the proposed framework, a
numerical study is carried out on a modified IEEE 141-bus
distribution network, the voltage level is 12.47kV. As shown
in Fig. 4, 20 prosumers are selected to participate in local
P2P energy trading, and the characteristics of five prosumer
types are summarized in TABLE II. The renewable genera-
tion outputs and load demand curves of the prosumers are
extracted from a real-world dataset published by the Belgian
Transmission System Operator Elia [49].
Node
Devices Portfolio
Prosumer
Scenario
CDG
WT
PV
BESS
CL
48,78,102,127,
‚úì
‚Äì
‚úì
‚úì
‚úì
Commercial
59,109,130,140
‚Äì
‚úì
‚úì
‚úì
‚úì
Rural
67,95,133,136
‚úì
‚úì
‚Äì
‚Äì
‚úì
Industrial
62,86,106,138
‚Äì
‚Äì
‚úì
‚úì
‚úì
Residential
74,100,116,134
‚úì
‚úì
‚úì
‚úì
‚úì
Energy Hub
TABLE II: Personalized configurations for 20 prosumers
A. Comparison Baselines
To evaluate the performance of the proposed algorithm,
we compare it against the following baselines while also
introducing:
1) MADDPG [50]: A CTDE-based multi-agent extension of
DDPG employing a centralized critic and decentralized
actors.
2) MAAC [51]: A CTDE framework augmented with a soft
attention mechanism that adaptively weights and filters
inter-agent information.
Fig. 4: IEEE141-bus distribution networks with twenty
prosumers
3) MATD3+BC [52]: Enhances MATD3 by incorporating a
behavior-cloning loss to align each agent‚Äôs policy with
LLM-generated expert actions.
4) MAGAIL [53]: A multi-agent extension of GAIL, in
which an adversarial discriminator is trained to distin-
guish expert trajectories generated by LLM from the
agents‚Äô joint state‚Äìaction trajectories. The output of the
discriminator is subsequently transformed into a shaped
imitation reward, which replaces the environment reward.
5) Our Proposed: Introduces a Lagrange multiplier during
training to progressively constrain agent behaviors toward
the LLM expert‚Äôs strategies.
6) Our Proposed-MH: Extends the Our Proposed algorithm
by integrating a differential multi-head attention mecha-
nism into the critic to improve global value estimation.
B. Implementation Details
In terms of neural network architecture design, aside from
the variants incorporating attention mechanisms, all baseline
algorithms share a unified network architecture under identical
hyperparameter settings. The detailed hyperparameter settings
are provided in TABLE III. Training was conducted for 5000
episodes with the same random seed initialization and up-
date frequency. All experiments were implemented in Python
3.11.10 under the PyTorch 2.7, with parameters updated via
the Adam optimizer. Computations were performed on a
platform equipped with an NVIDIA RTX 5070Ti GPU, an
AMD Ryzen ThreadRipper 3970X CPU, and 64GB of RAM.
Using year-round data, we designate the first day of each
month as the validation set, the last week of each month as the
test set, and the remaining days for training. A dynamic valida-
tion strategy is employed during training. After each training
step, the current policy is evaluated using the validation set.
The average cumulative discounted reward across all validation


--- Page 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
9
TABLE III: Hyperparameters used in the MARL
Symbol
Meaning
Value
lr
Learning rate
1e-4
Nepisode
Maximum episode
5000
Œ≥
Discount factor
0.99
NB
Replay buffer size
1e5
B
Training batch size
128
Œª
Initial Lagrange Multiplier
0.02
œµ
Policy deviation
0.1
k
Proportion of normal replay buffer
0.8
sets is recorded as a performance metric. To ensure statistical
robustness, each algorithm is executed independently five
times, and both the mean reward and the standard deviation
of the resulting rewards are recorded.
In the present study, the large parameter LLM workflow is
implemented through online API interfaces for experimental
validation. Nevertheless, the proposed architecture is fully
modular, allowing seamless substitution with locally hosted or
SFT LLM instances to ensure data privacy and reduce latency.
The LLM workflow is activated after every 200 episodes dur-
ing the MARL training phases or system reconfiguration stages
(e.g., when new devices are added or topology changes occur).
This approach effectively mitigates the decline in learning
performance caused by any poor generation by the LLM in
a given episode. During every training step, each prosumer
executes the LLM pre-generated optimization model code
in parallel, while the DSO merely aggregates the objective
functions and constraints from all prosumers and utilizes a
commercial solver to generate secure operational strategies for
each prosumer. To address potential runtime burdens, efficient
memory management is implemented to ensure the computa-
tional tractability of the iterative training process. In real-time
operation, lightweight MARL agents autonomously perform
decision-making without additional LLM guidance or global
DSO validation. In particular, when the LLM expert workflow
encounters a device that is not available in the knowledge base,
the policy deviation is deliberately increased to a very large
value. Under this setting, the agent in our proposed algorithm
that lacks valid LLM expert demonstrations effectively degen-
erates into a reward-driven reinforcement learning process, as
the influence of expert guidance is implicitly suppressed.
C. Performance Comparison
1) Analysis of the LLM Expert Strategy: In this evalua-
tion, the human expert baseline refers to the optimal strat-
egy obtained by solving a deterministic convex optimization
model‚Äîformulated in accordance with the problem setting
described in Section II‚Äîusing the Gurobi. This model fully
incorporates all physical constraints of the prosumers and
minimizes the total operational cost under perfect forecasts of
renewable generation and load demand. The resulting solver-
based optimal policy serves as the ground-truth reference for
assessing the accuracy of LLM-generated strategies.
Since the actions produced by the LLM expert directly
influence subsequent MARL training, we evaluate the LLMs
on the prosumer task using the following four key metrics:
‚Ä¢ Pass Rate: The success rate of error-free, executable
outputs, reflecting the LLM‚Äôs ability to generate valid
code.
‚Ä¢ Accuracy: For successful executions, accuracy quantifies
the similarity to human expert actions based on cost
deviation and action gap:
Deviation = |CLLM ‚àíCHuman|
CHuman
√ó 100%,
(41)
Gap = 1
T
T
X
t=1

aLLM
t
‚àíaHuman
t
aHuman
t
 √ó 100%,
(42)
Accuracy = 100% ‚àíGap + Deviation
2
.
(43)
‚Ä¢ Correction: The average number of code-fix iterations
required before a successful execution, indicating gen-
eration efficiency.
‚Ä¢ Tokens: The average number of completion tokens per
successful run, reflecting the computational cost of gen-
erating expert policies.
The workflow is implemented using LangGraph [54], where
the number of code generation iterations per execution cycle
is capped at a maximum of 5 iterations. The temperature
parameter is set to 0.5 to balance randomness and determinism
in the model output. All experiments utilize the latest publicly
accessible LLMs via official API interfaces. For each model,
10 experimental trials are conducted per type of prosumer
request, resulting in 50 total trials per model. The detailed
results are as follows:
TABLE IV: Performance comparison of different LLMs in
workflow without prosumer preference
LLM
Pass Rate(%)
Accuracy (%)
Correction
Tokens
Chatgpt-4o
88
92.76
1.38
4727
Claude-3.5-Sonnet
94
99.41
0.95
5929
Gemini-2.5-Flash
92
98.65
1.24
18856
DeepSeek-V3
96
96.45
0.43
4039
Qwen-2.5-Max
90
99.62
1.54
5302
Chatgpt-o3
100
98.52
0.28
17195
Claude-4-Opus
100
99.93
0.20
9454
Gemini-2.5-pro
100
99.86
0.48
7558
DeepSeek-R1
100
98.31
0.54
16687
Qwen-3
100
99.84
0.33
8157
TABLE IV provides a detailed comparison of the four met-
rics proposed in this study, where the LLMs above the dashed
line deactivates advanced reasoning capabilities, while those
below activate it. The latency of a large model depends on
the network speed when API calls are used, but is effectively
eliminated when the model is deployed locally. Specifically,
LLMs without advanced reasoning capabilities are generally
more cost-efficient; among these, Google Gemini-2.5-Flash
shows the highest average cost at 2.43 CNY (Derived from
the official billing statements of different LLMs after running
the workflow once, and converted into CNY based on the
prevailing exchange rate), mainly because it generates a larger
number of output tokens per query, leading to higher overall
billing despite its relatively low per-token price. In contrast,
among reasoning-enabled models, Claude 4-Opus is the most
expensive, with an average price of 6.27 CNY, primarily
due to its higher per-token pricing structure reflecting the
premium positioning of its advanced reasoning capabilities.


--- Page 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
10
Open-source LLMs such as DeepSeek and Qwen can also
completely eliminate API-related expenses when deployed
locally.
The proposed multi-agent framework demonstrates supe-
rior model compatibility, enabling seamless integration with
diverse mainstream LLMs rather than being constrained to
the performance of a single model. A key finding is that
the workflow exhibits exceptional performance in LLMs with
advanced reasoning capabilities, achieving a pass rate of 100%
in the evaluated tasks; this underscores that while prompt
engineering merely mimics the structure of reasoning, rein-
forcement learning effectively internalizes the genuine instinct
for self-correction. Notably, Claude-4-Opus attains a level of
proficiency that in these scenarios fully substitutes for human
experts. The complete prompt for our proposed LLM workflow
is publicly accessible in the supplementary materials [55].
Fig. 5: Comparative experiments on the LLM framework
without prosumer preference
As demonstrated in Fig. 5, the results validate the effec-
tiveness of the proposed LLM-expert workflow. Comparative
experiments were conducted where LLMs could not access
Atomic Function retrieval content. In this scenario, LLMs were
required to generate code directly based on input models,
data, and user information, and then perform power flow
verification without any iterative code correction. Cross com-
parisons of two core metrics‚ÄîPass Rate and Accuracy‚Äîreveal
that models without the integrated framework exhibit severe
performance degradation. For deactivated advanced reasoning
LLMs, pass rates approach zero due to primary failure modes
such as ‚ÄùModel Infeasible or Unbounded‚Äù and ‚ÄùNumerical
trouble encountered,‚Äù indicating significant gaps from human-
expert-level performance.
To further investigate the performance of personalized
prosumer preferences within the LLM expert workflow, we
consider the potential conflicts between prosumer preferences
and distribution network voltage constraints. Accordingly, the
prompt instructs the LLM to formulate prosumer preferences
as soft constraints in the objective function. This design
ensures that distribution network security is always prioritized
when conflicts arise, while prosumer preferences are satisfied
to the greatest extent possible. The results are illustrated in
Fig. 6.
TABLE V: Performance comparison of different LLMs in
workflow with prosumer preference
LLM
Pass Rate(%)
Accuracy(%)
Correction
Tokens
Chatgpt-4o
86
92.27
1.56
4904
Claude-3.5-Sonnet
92
99.14
1.01
6223
Gemini-2.5-Flash
90
98.34
1.47
19341
DeepSeek-V3
96
96.01
0.54
4362
Qwen-2.5-Max
88
99.39
1.75
5638
Chatgpt-o3
100
98.35
0.38
17773
Claude-4-Opus
100
99.81
0.28
13320
Gemini-2.5-pro
100
99.79
0.57
8365
DeepSeek-R1
100
98.01
0.65
17322
Qwen-3
100
99.66
0.43
9010
TABLE V evaluates the performance of LLMs in handling
prosumer preferences using 50 randomly generated, unam-
biguous profiles per prosumer type to ensure a fair comparison
with human experts. Compared to TABLE IV, the Pass Rate
remains unchanged, while the Accuracy decreases slightly.
This slight decline occurs because incorporating preferences
increases the workflow complexity, necessitating more iterative
refinements; consequently, both the Correction metric and
Token count increase, although this does not compromise
the LLMs‚Äô ability to successfully generate executable code.
Further demonstrating the method‚Äôs robustness, the results in
Fig. 7 highlight the advantages of the proposed workflow when
handling the increased complexity of prosumer-defined opti-
mization models. In contrast to Fig. 5, omitting the proposed
workflow in this setting leads to significant degradation: the
lowest Pass Rate drops to only 30%, while Accuracy falls
to 85.43%, proving the workflow‚Äôs superior effectiveness in
managing tasks with heightened complexity.
TABLE VI: Performance Comparison of LLMs across
Different Parameter in workflow with prosumer preference
LLM
Pass Rate(%)
Accuracy(%)
Correction
Tokens
without preferences
Qwen-3-32B
82
89.34
2.21
6361
Qwen-3-14B
76
85.63
4.03
6908
Qwen-3-8B
44
76.85
1.27
6005
Qwen-3-4B
2
82.47
1.78
6187
with preferences
Qwen-3-32B
78
87.41
2.63
6398
Qwen-3-14B
64
85.08
4.17
7036
Qwen-3-8B
26
74.86
1.61
6121
Qwen-3-4B
0
78.12
1.74
6202
To address API connection limitations, we evaluated locally
deployed open-source Qwen-3 models with reasoning acti-
vated across various parameter scales in TABLE VI, testing
50 tasks with user preferences and 50 without. Workflow
effectiveness declines with model size; reducing the scale
from 14B to 8B makes matching human-expert Pass Rate and
Accuracy unattainable.LangGraph workflow analysis reveals
that models under 8B parameters consistently fail to transform
input specifications. These models omit key user devices,
preferences, and Model Generation Expert-derived constraints,


--- Page 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
11
Fig. 6: Our proposed LLM expert workflows for processing personalized prosumer preferences
while also failing to select appropriate RAG-retrieved Atomic
Functions. Furthermore, these models have nearly lost the
capability to rectify erroneous code. Consequently, in suc-
cessful instances, the number of correction rounds and token
consumption actually decrease due to the lack of iterative
refinement. Thus, Qwen-3 models with at least 14B parameters
are recommended for local deployment.
The proposed LLM workflow demonstrates strong scalabil-
ity within the scope of its structured knowledge base. Specifi-
cally, the LLM-based expert workflow can effectively accom-
modate different prosumer types, provided that the correspond-
ing device models are already included in the knowledge base
(note that constructing a comprehensive model repository is
beyond the scope of this work, although research institutes,
large utility companies and tech giants are already making
efforts to build model libraries [56]). For each prosumer, the
LLM dynamically generates personalized trading strategies
based on its specific configuration. However, it is essential to
recognize that when faced with requests involving device types
or operational scenarios not covered by its knowledge base,
the LLM may generate plausible but inaccurate or unsupported
models or code, potentially compromising the accuracy and
reliability of the resulting expert strategies.
2) Performance Analysis of MARL Algorithm: Claude-4-
Opus is used as the expert LLM for algorithm performance
comparison in this section. It uses three indicators: daily
average reward, average operational cost, and average voltage
violation rate to compare the proposed algorithm with the
baseline algorithms:
During the training of 20 prosumers, the proposed LLM
expert workflow average replaced 3,039 lines of manually
written human expert code. The results in Fig. 8 demonstrate
the performance of the proposed algorithm during training on
the validation set. As shown, ‚ÄôOur proposed-MH‚Äô achieves
faster convergence to a low operational cost with minimal
fluctuations, demonstrating enhanced reward stability for guid-
ing agents when LLM outputs expert actions. Furthermore,
the curve of average voltage violation rate for ‚ÄôOur proposed-
MH‚Äô rapidly declines and maintains a low level, highlighting
its strength in constraint satisfaction and ability to maintain
secure grid operations.
After completing model training, we applied the proposed
algorithm to a test set to evaluate the practical applicability. As
shown in TABLE VII, the proposed algorithm demonstrates
significant advantages in both operational cost and voltage
violation rate. Specifically, the average cost reached 4840.61


--- Page 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
12
Fig. 7: Comparative experiments on the LLM framework
with prosumer preference
TABLE VII: Comparison of different algorithms on test set
Algorithm
Operational Cost (CNY)
Voltage Violation Rate (p.u)
Mean
Std. Dev.
Mean
Std. Dev.
MADDPG
6586.45
340.76
3.66 √ó 10‚àí3
8.47 √ó 10‚àí4
MAAC
5940.32
220.49
2.02 √ó 10‚àí3
7.29 √ó 10‚àí4
MATD3+BC
5489.20
207.88
1.61 √ó 10‚àí3
2.82 √ó 10‚àí4
MAGAIL
6380.96
164.23
2.82 √ó 10‚àí3
9.01 √ó 10‚àí4
OP
5146.27
146.07
1.39 √ó 10‚àí3
4.05 √ó 10‚àí4
OP-MH
4840.61
135.31
1.06 √ó 10‚àí3
3.03 √ó 10‚àí4
Note: OP stands for ‚ÄúOur Proposed.‚Äù
CNY, while the voltage violation rate was 1.06 √ó 10‚àí3.
Through comprehensive analysis of mean-standard deviation
comparisons with baseline algorithms, the proposed approach
achieves optimal results across all three evaluation metrics. It
maintains the lowest mean values while exhibiting minimal
standard deviations, indicating not only superior average per-
formance but also robust stability.
During the training process, the evolution of the agent
average Lagrange multiplier Œª and the average Wasserstein-
2 metric provides key insight into the dynamic balance be-
tween expert imitation and autonomous policy optimization.
As shown in Fig. 9, Œª initially increases rapidly, reflecting
the strong constraint effect imposed by the LLM expert
strategy to guide the early-phase exploration of the MARL
agents. With the gradual improvement of policy learning, Œª
subsequently decreases and eventually stabilizes, indicating
that the agents have sufficiently aligned with the expert be-
havior and require less external constraint. In contrast, the
Wasserstein-2 metric between the agent policy and the expert
policy exhibits a consistently decreasing trend, demonstrating
continuous convergence of the learned policy toward the expert
distribution. Once the Wasserstein-2 metric approaches the
predefined threshold Policy deviation œµ, fluctuations remain
bounded, signifying that the imitation constraint has reached
equilibrium and the policy optimization process has entered a
stable regime.
To further investigate the sensitivity of the Wasserstein-
based regularization, a comparative analysis was conducted
between two formulations of the Wasserstein-2 metric in
(30): one with the variance term œÉ and one without it. As
illustrated in Fig. 10, when the œÉ is incorporated, it converges
smoothly to approximately 10‚àí5 after around 500 episodes.
This indicates that the stochastic exploration of the policy
network is gradually reduced as the learned policy approaches
the expert strategy, achieving a stable and near-deterministic
policy behavior. In contrast, when the œÉ is excluded, the
implied policy variance remains large and fluctuates around 1.2
throughout training, suggesting incomplete convergence, the
agent‚Äôs actions are not stable enough. These results highlight
that explicitly learning and regularizing the œÉ is crucial for
maintaining exploration in the early phase while ensuring
eventual convergence to a stable policy distribution.
To investigate the prosumer preference issue in P2P en-
ergy trading, heterogeneous preferences are assigned to 20
prosumers on the IEEE 141-bus topology shown in Fig.
4.
For each prosumer‚Äôs personalized preference, Our proposed
method is guided by an LLM-based expert workflow.
The results in Fig.
11 present a comparative analysis
for prosumers exhibiting preferences toward BESS and CL
operation under the proposed method and MAAC. Conven-
tional MARL algorithms, represented by MAAC, are limited to
conventional optimization behavior due to their strict reliance
on the predefined reward function. In contrast, the proposed
method further aligns the learned policy with expert strategies
beyond purely reward-driven optimization, thereby enabling
the consideration of prosumer preferences while simultane-
ously maintaining the maximization of social welfare.
When the device type requested by a prosumer is not in-
cluded in the knowledge base, or in local deployment set-
tings, the use of LLMs with fewer than 8B parameters may
lead to suboptimal expert actions. A comparative experiment
is conducted between the proposed method and the existing
baseline algorithm under a Qwen-3-8B model. After 1,000
training episodes, we explicitly schedule an increase in the
policy deviation to allow the algorithm to perform autonomous
exploration, while enforcing a lower bound policy variance to
prevent premature policy determinism.
As shown in Fig. 12, conventional imitation learning (e.g.,
BC, GAIL) excessively constrains policies, causing agents to
inherit expert bias and converge to inferior local optimal. In
contrast, our method gradually relaxes the imitation constraint
by increasing policy deviation. This expands the exploration
space and reintroduces reward signals, enabling agents to es-
cape LLM expert-induced suboptimal equilibrium. The early-
phase alignment provides stable initialization, while later-
phase self-exploration compensates for expert inaccuracies.
This two-phase process effectively balances expert knowledge
exploitation with autonomous exploration. Conversely, BC-
and GAIL-based methods enforce strict adherence throughout,
rendering them unable to recover from suboptimal expert
guidance.
To assess the scalability of Our proposed-MH algorithm un-
der large-scale prosumer participation, further experiments are


--- Page 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
13
(a) The smoothed curves of the average
rewards
(b) The smoothed curves of the average
operation cost
(c) The smoothed curves of the average
voltage violation rate
Fig. 8: Comparison chart of the performance of baseline algorithms
Fig. 9: Average Lagrange multiplier and Wasserstein metric
during training episode
Fig. 10: Sensitivity analysis of Wasserstein metric variance
term
conducted on the IEEE 141-bus distribution network with 100
prosumers, the prosumer details are reported in TABLE VIII.
Fig.
13 demonstrates the proposed method‚Äôs scalability,
with the LLM workflow replacing 14,874 lines of manual
code. Conventional CTDE baselines (e.g., MATD3+BC, MA-
GAIL) fail in this setting as they require training 100 distinct
critics with linearly growing input dimensions. In contrast,
our proposed-MH algorithm addresses these limitations using
a shared critic and Differential Attention, which ensures the
concatenated representation (Concat(ei, xi)) remains fixed
regardless of system scale. Together with agent-specific LLM
Fig. 11: The operate results of our proposed algorithm on
BESS and CL preference prosumer
guidance, this guarantees effectiveness in large-scale scenarios.
To evaluate the effectiveness of the proposed differential
attention mechanism in multi-agent P2P energy trading, ab-
lation studies were conducted, with results shown in Fig. 14.
The experimental results indicate that the number of atten-
tion heads substantially affects the cost deviation in the LLM
expert. Increasing the number of heads from very few (e.g.,
1‚Äì2) leads to a notable reduction in cost deviation, suggest-
ing that introducing a small number of attention heads sig-
nificantly enhances the model‚Äôs ability to capture key agent
interactions. However, further increasing the head count (e.g.,
from 8 to 16) yields only marginal improvements, indicat-
ing performance saturation likely due to redundancy in the
captured features. Crucially, the differential attention mech-
anism outperforms standard attention in scheduling accuracy
while preserving similar computational complexity. Its two se-
quential softmax operations incur roughly the same cost as a


--- Page 14 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
14
Fig. 12: Training results of baseline algorithm under
Qwen-3-8B strategies
TABLE VIII: Personalized configurations for 100 prosumers
on IEEE-141 system
Node
CDG
WT
PV
BESS
CL
Prosumer Type
10, 12, 18, 24, 28, 33
‚úì
‚Äì
‚úì
‚úì
‚úì
Commercial
45, 48, 61, 78, 94, 102
15, 21, 30, 37, 52, 59
‚Äì
‚úì
‚úì
‚úì
‚úì
Rural
71, 84, 96, 109, 123, 130
17, 26, 36, 44, 54, 67
‚úì
‚úì
‚Äì
‚Äì
‚úì
Industrial
75, 88, 95, 111, 124, 133
19, 29, 39, 47, 57, 62
‚Äì
‚Äì
‚úì
‚úì
‚úì
Residential
76, 86, 99, 106, 121, 138
22, 31, 41, 50, 60, 74
‚úì
‚úì
‚úì
‚úì
‚úì
Energy Hub
83, 100, 112, 116, 128, 134
6, 13, 23, 34, 46, 58, 68
‚Äì
‚úì
‚úì
‚úì
‚Äì
Renewable
82, 97, 108, 114, 126, 135, 141
9, 16, 27, 35, 49, 57, 70
‚Äì
‚Äì
‚úì
‚úì
‚Äì
Storage-centric
81, 91, 101, 113, 125, 139
38, 56, 63, 72, 80, 90, 103
‚úì
‚Äì
‚Äì
‚Äì
‚úì
Data Center
110, 118, 129, 131, 136, 140
standard multi-head attention with twice the number of heads,
thus delivering superior performance without computational
overhead.
TABLE IX: Ablation Study of Initial Œæ
Initial
Œæ
Episode 100
Episode 1000
Episode 5000
Mean
Std. Dev.
Mean
Std. Dev.
Mean
Std. Dev.
0.2
-82.65
3.74
-20.54
3.27
-16.56
1.17
0.5
-85.23
4.13
-20.20
3.31
-17.30
1.22
0.8
-92.91
3.95
-21.16
2.99
-16.89
1.24
An ablation study is conducted to investigate the impact of
the initialization of the learnable vector Œæ in the Differential
Attention mechanism. As shown in TABLE IX, the choice of
the initial value has only a minor effect on training perfor-
mance during the early phase (within the first 100 training
steps). After 1,000 training steps, the initialization of Œæ has a
negligible influence on both the average training reward and its
variance. These results indicate that Œæ can consistently adapt
during training and gradually overcome the bias introduced by
its initial value, demonstrating the robustness of the proposed
Differential Attention mechanism with respect to parameter
initialization.
It is important to note that existing RL methods typically
separate the training phase from real-world deployment, uti-
lizing simulation environments rather than interacting directly
with the real-world power system [12]. In this simulation-
based training phase, traditional optimization methods are
feasible as global information is accessible. However, they
are often infeasible for real-time deployment due to the
lack of accurate forecast parameters [16]. Consequently, the
LLM-generated optimization models are employed strictly to
accelerate MARL training convergence. The trained agents
are capable of autonomously executing energy trading tasks
based solely on local observations, adaptively handling real-
time uncertainties without further LLM intervention.
It should be noted that, like most RL approaches, the
proposed framework exhibits limited generalization capabil-
ity and requires retraining [57] whenever the environment
undergoes substantial changes, such as major topology re-
configuration or the introduction of entirely new market
mechanisms. Despite this inherent limitation of RL, the
proposed LLM-MARL framework significantly outperforms
conventional MARL methods in both convergence speed and
final performance. To ensure reliability, all LLM generated
strategies undergo DSO-based security verification, which
guarantees their feasibility. Furthermore, the differential atten-
tion‚Äìbased critic effectively mitigates performance degradation
caused by increased agent interactions or system scale-up. As a
result, the framework greatly reduces reliance on human expert
guidance and significantly lowers the overall cost associated
with full retraining in practical deployments.
V. CONCLUSION
This paper addresses the collaborative decision-making
challenges among multiple prosumers in local real-time P2P
electricity markets by proposing a framework that integrates
LLM expert guidance with MARL. This approach effec-
tively overcomes limitations inherent in traditional optimiza-
tion methods‚Äìparticularly their inability to achieve real-time
decision-making‚Äìas well as limitations of MARL without
LLM guidance, particularly high manual labor costs associated
with human expert involvement. The framework innovatively
employs LLMs as experts to generate personalized strategies
for guiding MARL training, combined with the Wasserstein
metric and an enhanced Critic network, achieving deep integra-
tion of expert knowledge and agent learning. This significantly
reduces manual costs while enhancing policy optimization
performance.
Experimental validation demonstrates the method‚Äôs superior
performance. In model compatibility tests, the proposed frame-
work exhibits universality across mainstream LLMs, with the
Claude-4-Opus model achieving 100% pass rate and 99.93%
accuracy in an expert workflow task, effectively substituting
human experts. In the modified IEEE 141-bus distribution
network, the proposed method achieves a remarkably low
average operational cost of 4840.61 CNY and a voltage viola-
tion rate of only 1.06√ó10‚àí3 p.u., significantly outperforming
conventional baseline methods.
Computationally, the framework‚Äôs overhead is concentrated
in the training stage, while real-time operation remains highly
efficient using lightweight networks. The highest average cost


--- Page 15 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
15
(a) The smoothed curves of the average
rewards
(b) The smoothed curves of the average
operation cost
(c) The smoothed curves of the average
voltage violation rate
Fig. 13: Comparison chart of the performance of baseline algorithms with large-scale prosumers
Fig. 14: Ablation study on the number of attention heads in
20 agent prosumer
of the LLM expert workflow is less than 6.27 CNY. Retraining
is infrequent, triggered only by structural changes like topol-
ogy modification or market adjustments, rather than routine
fluctuations. Crucially, the training duration is negligible com-
pared to the long intervals of these structural evolutions. Fur-
thermore, the system supports efficient continual learning and
knowledge transfer; the shared critic preserves learned network
constraints, and the modular code repository enables direct
reuse, significantly reducing redundancy and ensuring scala-
bility.
Future work will extend to larger-scale prosumer groups,
expand external expert knowledge repositories, and explore
the adaptability of diverse LLM workflow architectures in
complex scenarios to strengthen the framework‚Äôs practical
value.
REFERENCES
[1] T. Capper, A. Gorbatcheva, M. A. Mustafa, M. Bahloul, J. M. Schwid-
tal, R. Chitchyan, M. Andoni, V. Robu, M. Montakhabi, I. J. Scott,
C. Francis, T. Mbavarira, J. M. Espana, and L. Kiesling, ‚ÄúPeer-to-peer,
community self-consumption, and transactive energy: A systematic liter-
ature review of local energy market models,‚Äù Renewable and Sustainable
Energy Reviews, vol. 162, p. 112403, 2022.
[2] C. Feng and A. L. Liu, ‚ÄúPeer-to-peer energy trading of solar and energy
storage: A networked multiagent reinforcement learning approach,‚Äù
Applied Energy, vol. 383, p. 125283, 2025.
[3] C. Feng, B. Liang, Z. Li, W. Liu, and F. Wen, ‚ÄúPeer-to-peer energy
trading under network constraints based on generalized fast dual ascent,‚Äù
IEEE Transactions on Smart Grid, vol. 14, no. 2, pp. 1441‚Äì1453, 2023.
[4] Z. Guo, P. Pinson, S. Chen, Q. Yang, and Z. Yang, ‚ÄúOnline optimization
for real-time peer-to-peer electricity market mechanisms,‚Äù IEEE Trans-
actions on Smart Grid, vol. 12, no. 5, pp. 4151‚Äì4163, 2021.
[5] Z. Guo, P. Pinson, Q. Wu, S. Chen, Q. Yang, and Z. Yang, ‚ÄúAn
asynchronous online negotiation mechanism for real-time peer-to-peer
electricity markets,‚Äù IEEE Transactions on Power Systems, vol. 37, no. 3,
pp. 1868‚Äì1880, 2022.
[6] J. Liu, Q. Long, R.-P. Liu, W. Liu, and Y. Hou, ‚ÄúOnline distributed opti-
mization for spatio-temporally constrained real-time peer-to-peer energy
trading,‚Äù Applied Energy, vol. 331, p. 120216, 2023. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0306261922014738
[7] B. Zheng and W. Wei, ‚ÄúReal-time peer-to-peer energy trading for
networked multi-energy systems with hybrid energy storage,‚Äù Journal
of Energy Storage, vol. 105, p. 114530, 2025. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S2352152X24041161
[8] X. Wang, H. Jia, Z. Wang, X. Jin, Y. Deng, Y. Mu, and X. Yu, ‚ÄúA real
time peer-to-peer energy trading for prosumers utilizing time-varying
building virtual energy storage,‚Äù International Journal of Electrical
Power & Energy Systems, vol. 155, p. 109547, 2024. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S014206152300604X
[9] D. Ernst, M. Glavic, F. Capitanescu, and L. Wehenkel, ‚ÄúReinforcement
learning versus model predictive control: A comparison on a power
system problem,‚Äù IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics), vol. 39, no. 2, pp. 517‚Äì529, 2009.
[10] T. Li, W. Hu, B. Zhang, G. Zhang, J. Li, Z. Chen, and F. Blaabjerg,
‚ÄúMechanism analysis and real-time control of energy storage based
grid power oscillation damping: A soft actor-critic approach,‚Äù IEEE
Transactions on Sustainable Energy, vol. 12, no. 4, pp. 1915‚Äì1926,
2021.
[11] X. Xu, K. Xu, Z. Zeng, J. Tang, Y. He, G. Shi, and T. Zhang,
‚ÄúCollaborative optimization of multi-energy multi-microgrid system: A
hierarchical trust-region multi-agent reinforcement learning approach,‚Äù
Applied Energy, vol. 375, p. 123923, 2024.
[12] T. Su, T. Wu, J. Zhao, A. Scaglione, and L. Xie, ‚ÄúA review of safe re-
inforcement learning methods for modern power systems,‚Äù Proceedings
of the IEEE, vol. 113, no. 3, pp. 213‚Äì255, 2025.
[13] T. L. Paine, C. Gulcehre, B. Shahriari, M. Denil, M. Hoffman,
H. Soyer, R. Tanburn, S. Kapturowski, N. Rabinowitz, D. Williams,
G. Barth-Maron, Z. Wang, N. de Freitas, and W. Team, ‚ÄúMaking
efficient use of demonstrations to solve hard exploration problems,‚Äù
2019. [Online]. Available: https://arxiv.org/abs/1909.01387
[14] M. Zhang, F. Eliassen, A. Taherkordi, H.-A. Jacobsen, Y. Li, and
Y. Zhang, ‚ÄúSelf-determination theory and deep reinforcement learning
for personalized energy trading in smart grid,‚Äù IEEE Transactions on
Systems, Man, and Cybernetics: Systems, vol. 55, no. 6, pp. 4216‚Äì4229,
2025.
[15] S. Gao, C. Xiang, M. Yu, K. T. Tan, and T. H. Lee, ‚ÄúOnline optimal
power scheduling of a microgrid via imitation learning,‚Äù IEEE Transac-
tions on Smart Grid, vol. 13, no. 2, pp. 861‚Äì876, 2022.
[16] Y. Zhang, F. Qiu, T. Hong, Z. Wang, and F. Li, ‚ÄúHybrid imitation learn-
ing for real-time service restoration in resilient distribution systems,‚Äù
IEEE Transactions on Industrial Informatics, vol. 18, no. 3, pp. 2089‚Äì
2099, 2022.
[17] Y. Lin, Z. Ni, and Y. Tang, ‚ÄúAn imitation learning method with
multi-virtual agents for microgrid energy optimization under interrupted
periods,‚Äù in 2024 IEEE Power & Energy Society General Meeting
(PESGM), 2024, pp. 1‚Äì5.


--- Page 16 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
16
[18] C. Huang, Z. Tang, S. Hu, R. Jiang, X. Zheng, D. Ge, B. Wang, and
Z. Wang, ‚ÄúOrlm: A customizable framework in training large models
for automated optimization modeling,‚Äù 2025. [Online]. Available:
https://arxiv.org/abs/2405.17743
[19] X. Yang, C. Lin, H. Liu, and W. Wu, ‚ÄúRl2: Reinforce large language
model to assist safe reinforcement learning for energy management of
active distribution networks,‚Äù IEEE Transactions on Smart Grid, vol. 16,
no. 4, pp. 3419‚Äì3431, 2025.
[20] Z. Yan and Y. Xu, ‚ÄúReal-time optimal power flow with linguistic
stipulations: Integrating gpt-agent and deep reinforcement learning,‚Äù
IEEE Transactions on Power Systems, vol. 39, no. 2, pp. 4747‚Äì4750,
2024.
[21] C. Huang, S. Li, R. Liu, H. Wang, and Y. Chen, ‚ÄúLarge foundation
models for power systems,‚Äù in 2024 IEEE Power & Energy Society
General Meeting (PESGM), 2024, pp. 1‚Äì5.
[22] M. Jia, Z. Cui, and G. Hug, ‚ÄúEnhancing llms for power system
simulations: A feedback-driven multi-agent framework,‚Äù 2025. [Online].
Available: https://arxiv.org/abs/2411.16707
[23] C.
Xu,
J.
Liu,
S.
Fang,
Y.
Cui,
D.
Chen,
P.
Hang,
and
J. Sun, ‚ÄúTell-drive: Enhancing autonomous driving with teacher
llm-guided deep reinforcement learning,‚Äù 2025. [Online]. Available:
https://arxiv.org/abs/2502.01387
[24] H. Pang, Z. Wang, and G. Li, ‚ÄúLarge language model guided deep
reinforcement learning for decision making in autonomous driving,‚Äù
2024. [Online]. Available: https://arxiv.org/abs/2412.18511
[25] D. Papadaskalopoulos and G. Strbac, ‚ÄúNonlinear and randomized pricing
for distributed management of flexible loads,‚Äù IEEE Transactions on
Smart Grid, vol. 7, no. 2, pp. 1137‚Äì1146, 2016.
[26] P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. M. de Cote, ‚ÄúA
survey of learning in multiagent environments: Dealing with non-
stationarity,‚Äù CoRR, vol. abs/1707.09183, 2017. [Online]. Available:
http://arxiv.org/abs/1707.09183
[27] P. Chen, S. Liu, X. Wang, and I. Kamwa, ‚ÄúPhysics-guided multi-
agent adversarial reinforcement learning for robust active voltage control
with peer-to-peer (p2p) energy trading,‚Äù IEEE Transactions on Power
Systems, vol. 39, no. 6, pp. 7089‚Äì7101, 2024.
[28] X. Liu, Y. Ye, S. Li, C. Zhang, Q. Ma, and J. Zhu, ‚ÄúCarbon-aware
peer-to-peer energy trading in an unbalanced distribution network via
a nash equilibrium discovery deep reinforcement learning approach,‚Äù
IEEE Transactions on Smart Grid, vol. 16, no. 4, pp. 3392‚Äì3407, 2025.
[29] L. Kraemer and B. Banerjee, ‚ÄúMulti-agent reinforcement learning as
a rehearsal for decentralized planning,‚Äù Neurocomputing, vol. 190, pp.
82‚Äì94, 2016.
[30] Y. Zhou, S. Liu, Y. Qing, K. Chen, T. Zheng, J. Song, and
M.
Song,
‚ÄúIs
centralized
training
with
decentralized
execution
framework centralized enough for marl?‚Äù 2025. [Online]. Available:
https://arxiv.org/abs/2305.17352
[31] Y. Wang, D. Shi, C. Xue, H. Jiang, G. Wang, and P. Gong, ‚ÄúAhac: Actor
hierarchical attention critic for multi-agent reinforcement learning,‚Äù in
2020 IEEE International Conference on Systems, Man, and Cybernetics
(SMC), 2020, pp. 3013‚Äì3020.
[32] X. Yang, H. Liu, and W. Wu, ‚ÄúAttention-enhanced multi-agent reinforce-
ment learning against observation perturbations for distributed volt-var
control,‚Äù IEEE Transactions on Smart Grid, vol. 15, no. 6, pp. 5761‚Äì
5772, 2024.
[33] F. Yang, D. Huang, D. Li, S. Lin, S. M. Muyeen, and H. Zhai, ‚ÄúData-
driven load frequency control based on multi-agent reinforcement learn-
ing with attention mechanism,‚Äù IEEE Transactions on Power Systems,
vol. 38, no. 6, pp. 5560‚Äì5569, 2023.
[34] Y. Ye, Y. Tang, H. Wang, X.-P. Zhang, and G. Strbac, ‚ÄúA scalable
privacy-preserving multi-agent deep reinforcement learning approach for
large-scale peer-to-peer transactive energy trading,‚Äù IEEE Transactions
on Smart Grid, vol. 12, no. 6, pp. 5185‚Äì5200, 2021.
[35] H. Xiao, X. Pu, W. Pei, and L. Ma, ‚ÄúPeer-to-peer energy transactions
for prosumers based on improved deep deterministic policy gradient
algorithm,‚Äù IEEE Transactions on Smart Grid, vol. 15, no. 6, pp. 5910‚Äì
5922, 2024.
[36] S. Savino, T. Minella, Z. Nagy, and A. Capozzoli, ‚ÄúA scalable demand-
side energy management control strategy for large residential districts
based on an attention-driven multi-agent drl approach,‚Äù Applied Energy,
vol. 393, p. 125993, 2025.
[37] T.
Ye,
L.
Dong,
Y.
Xia,
Y.
Sun,
Y.
Zhu,
G.
Huang,
and
F.
Wei,
‚ÄúDifferential
transformer,‚Äù
2025.
[Online].
Available:
https://arxiv.org/abs/2410.05258
[38] Y. Cui, Y. Xu, Y. Wang, Y. Zhao, H. Zhu, and D. Cheng, ‚ÄúPeer-to-
peer energy trading with energy trading consistency in interconnected
multi-energy microgrids: A multi-agent deep reinforcement learning
approach,‚Äù INTERNATIONAL JOURNAL OF ELECTRICAL POWER &
ENERGY SYSTEMS, vol. 156, FEB 2024.
[39] C. Mu, T. Ding, Y. Huang, S. Zhu, P. Siano, M. Shahidehpour, and
X. Shen, ‚ÄúDistributed collaboration method for peer-to-peer transactions
in reconfigurable distribution network,‚Äù IEEE Transactions on Power
Systems, vol. 40, no. 4, pp. 3029‚Äì3042, 2025.
[40] Q. Ma, Z. Liu, Y. Ye, and X. Liu, ‚ÄúNetwork-constrained p2p trad-
ing: A safety-aware decentralized multi-agent reinforcement learning
approach,‚Äù IEEE Transactions on Smart Grid, vol. 16, no. 6, pp. 5573‚Äì
5588, 2025.
[41] T. Xiao and P. Xu, ‚ÄúExploring automated energy optimization with
unstructured building data: A multi-agent based framework leveraging
large language models,‚Äù Energy and Buildings, vol. 322, p. 114691,
2024.
[42] G. Dong, X. Li, Y. Zhang, and M. Deng, ‚ÄúLeveraging llm-assisted
query understanding for live retrieval-augmented generation,‚Äù 2025.
[Online]. Available: https://arxiv.org/abs/2506.21384
[43] S. Diamond and S. Boyd, ‚ÄúCVXPY: A Python-embedded modeling lan-
guage for convex optimization,‚Äù Journal of Machine Learning Research,
vol. 17, no. 83, pp. 1‚Äì5, 2016.
[44] Gurobi Optimizer Reference Manual, Gurobi Optimization, LLC, 2024.
[Online]. Available: https://www.gurobi.com
[45] M. Farivar and S. H. Low, ‚ÄúBranch flow model: Relaxations and
convexification-part i,‚Äù IEEE TRANSACTIONS ON POWER SYSTEMS,
vol. 28, no. 3, pp. 2554‚Äì2564, AUG 2013.
[46] G. Peyr¬¥e and M. Cuturi, ‚ÄúComputational optimal transport,‚Äù Foundations
and Trends in Machine Learning, vol. 11, no. 5-6, pp. 355‚Äì607, 2019.
[47] X.
Lyu,
A.
Baisero,
Y.
Xiao,
and
C.
Amato,
‚ÄúA
deeper
understanding
of
state-based
critics
in
multi-agent
reinforcement
learning,‚Äù
Proceedings
of
the
AAAI
Conference
on
Artificial
Intelligence, vol. 36, no. 9, pp. 9396‚Äì9404, Jun. 2022. [Online].
Available: https://ojs.aaai.org/index.php/AAAI/article/view/21171
[48] Z. Huang, J. Wu, and C. Lv, ‚ÄúEfficient deep reinforcement learning with
imitative expert priors for autonomous driving,‚Äù IEEE Transactions on
Neural Networks and Learning Systems, vol. 34, no. 10, pp. 7391‚Äì7403,
2023.
[49] Elia, ‚ÄúElia wind/solar power/grid data set,‚Äù https://www.elia.be, 2025,
accessed: May 02, 2025.
[50] M. Bettini, A. Prorok, and V. Moens, ‚ÄúBenchmarl: Benchmarking
multi-agent
reinforcement
learning,‚Äù
2024.
[Online].
Available:
https://arxiv.org/abs/2312.01472
[51] S. Iqbal and F. Sha, ‚ÄúActor-attention-critic for multi-agent reinforcement
learning,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1810.02912
[52] S.
Fujimoto
and
S.
S.
Gu,
‚ÄúA
minimalist
approach
to
offline
reinforcement
learning,‚Äù
2021.
[Online].
Available:
https://arxiv.org/abs/2106.06860
[53] J.
Song,
H.
Ren,
D.
Sadigh,
and
S.
Ermon,
‚ÄúMulti-agent
generative adversarial imitation learning,‚Äù 2018. [Online]. Available:
https://arxiv.org/abs/1807.09936
[54] LangChain Inc. LangGraph. [Online]. Available: https://langchain-
ai.github.io/langgraph/
[55] [Online]. Available: https://github.com/jzk0806/P2P-llm-supplementary
[56] E.
P.
R.
I.
(EPRI),
‚ÄúOpen
power
ai
consortium,‚Äù
https://msites.epri.com/opai, 2025, accessed: 2025-10-21.
[57] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, ‚ÄúQuantifying
generalization in reinforcement learning,‚Äù in Proceedings of the 36th
International Conference on Machine Learning, ser. Proceedings of
Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds.,
vol. 97.
PMLR, 09‚Äì15 Jun 2019, pp. 1282‚Äì1289. [Online]. Available:
https://proceedings.mlr.press/v97/cobbe19a.html
