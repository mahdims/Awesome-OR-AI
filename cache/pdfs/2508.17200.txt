--- Page 1 ---
Large Language Model-Based Automatic Formulation for Stochastic
Optimization Models
Amirreza Talebia,∗
aDepartment of Integrated Systems Engineering, The Ohio State University, Columbus, 43210, OH, USA
Abstract
This paper presents an integrated systematic study of the performance of large language models
(LLMs), specifically ChatGPT, for automatically formulating and solving Stochastic Optimization
(SO) problems from natural language descriptions. Focusing on three key categories, individual
chance-constrained models, joint chance-constrained models, and two-stage stochastic mixed-integer
linear programming models, we design several prompts that guide ChatGPT through structured
tasks using chain-of-thought and agentic reasoning. We introduce a novel soft-scoring metric that
evaluates the structural quality and partial correctness of generated models, addressing the limi-
tations of canonical and execution-based accuracy metrics. Across a diverse set of SO problems,
GPT-4-Turbo achieves better partial scores than GPT-3.5 variants except for individual chance-
constrained problems.
Structured prompts significantly outperform simple prompting, reducing
extra-element generation and improving objective matching, although extra-element generation re-
mains a nontrivial task. Our findings reveal that with well-engineered prompts and multi-agent
collaboration, LLMs can facilitate SO formulations, paving the way for intelligent, language-driven
modeling pipelines for SO in practice.
Keywords:
stochastic optimization, ChatGPT, chain-of-thought, multi-agent prompting, soft
scoring
1. Introduction
1.1. Motivation
At the intersection of decision science and artificial intelligence in a rapidly changing research
landscape, large language models (LLMs) such as ChatGPT [38] offer new opportunities to automate
operations research (OR) tasks. Prior research has shown promise in translating natural language
(NL) descriptions of mathematical problems into formulations, especially for practitioners (e.g.,
[53, 57, 62]). In the optimization domain, most of this work has concentrated on deterministic
optimization formulations or robust optimization (e.g., [7, 30, 61]).
In contrast, Stochastic Optimization (SO), particularly in problems involving chance constraints,
and two-stage stochastic mixed-integer linear programming models (SMILP-2), has received less
attention, despite its central role in modeling uncertainty in real-world decision-making problems.
Many real-world decision problems involve uncertainty, which necessitate the use of complex SO
models. For example, SMILP-2 formulations govern long-term commitments followed by adaptive
recourse actions, as in supply-chain design [47], inventory management [10], or disaster logistics
∗Corresponding author
Email address: talebi.14@osu.edu (Amirreza Talebi)
arXiv:2508.17200v3  [cs.AI]  14 Jan 2026


--- Page 2 ---
[48]. Chance-constrained variants capture risk-controlled feasibility with broad applications in real-
world engineering problems such as portfolio optimization [54], air traffic flow management [39],
vehicle routing [13], and renewable energy integration into power systems [16, 55]. More real-world
applications of SO, such as hydraulic control of underground water plume contamination, can be
found in [58]. Table 1 summarizes representative applications spanning these categories.
While deterministic optimization models can often be specified directly from known parame-
ters, SO introduces additional layers of uncertainty, recourse structures, and probabilistic feasibility
in reality that make these models more challenging to formulate and implement in practice. De-
spite their prevalence, constructing SO formulations requires knowledge in multiple fields, such as
optimization and probability, which limits their further use and applicability for many practitioners.
LLMs could help remove such barriers for practitioners. However, this needs to be done carefully
and systematically to ensure correctness. This work is motivated by making SO more accessible to
practitioners by enabling LLMs to automatically formulate and evaluate these classes of stochastic
models from NL descriptions, bridging NL reasoning and formal optimization. Leveraging LLMs
for modeling complex SO problems in a structured manner is crucial, as they provide quick, step-
by-step guidance and clear explanations, making complex modeling and implementation tasks more
comprehensible. This capability enables rapid testing of multiple scenarios and real-time analysis,
while enhancing decision-making with improved explainability and iterative refinement. However,
even doing this in practice is not straightforward. Correct and structured prompts are needed, and
subsequently, there is a need to evaluate the performance of the prompts and state-of-the-art LLMs,
particularly ChatGPT, and their accuracy in formulating and coding SO problems.
Table 1: Representative real-world applications of SO by model type.
Real world problem
Model type
Reference
Relevance to SO
Inventory management
SMILP-2
[10]
First stage: decisions regarding the target level of
inventory in the bike sharing system, second stage:
re-balancing the inventory levels to meet uncertain
demand
Supply
chain
network
design
SMILP-2
[47]
First stage: contract decisions, such as the amount
of grains purchased; second stage:
resource flows
after demand, price, and revenue under uncertainty
realization.
Disaster
relief
/
Hu-
manitarian logistics
SMILP-2
[48]
First stage:
selection of suppliers, agencies, criti-
cal facilities, and pre-positioning of supplies; sec-
ond stage:
resource allocation, relief distribution,
and supply of extra resources.
Hydraulic
control
of
underground
water
plume contamination
SMILP-2
[58]
First stage:
where to build a well; second stage:
handling
hydraulic
conductivity,
plume
contain-
ment constraints, and pumping rates.
Air traffic flow manage-
ment
Chance-
constrained
[39]
Control the probability of loss of separation at
shared way-points (avoiding conflicts between air-
planes at way-points with high probability)
Energy
unit
commit-
ment with renewables
Chance-
constrained
[16]
Wind power load balance constraints must jointly
hold with high probability at all periods for each
bus.
Integrating
renewable
energy into power grids
Chance-
constrained
[55]
Enforces that all spinning reserve constraints hold
simultaneously with a specified probability thresh-
old.
Chance-constrained ve-
hicle routing problem
Chance-
constrained
[13]
Build minimum length routes so that, based on cus-
tomer demand and the vehicle capacity, such that
every customer is met exactly once, and each vehi-
cle’s capacity is exceeded with a specified probabil-
ity threshold.
Portfolio optimization
Chance-
constrained
[54]
Maximize profit given that the return at the end of
a period is less than a value with a specified proba-
bility threshold.
2


--- Page 3 ---
1.2. Research gap and novelty
In addition to the gap in considering automatic formulation of SO problems mentioned earlier,
existing prompt-based and multi-agent frameworks (e.g., [30, 53, 57]) evaluate canonical1 accuracy
quality but do not assess to what extend LLMs can formulate SO problems containing probabilistic
constraints, and recourse actions.
This absence of benchmarks, evaluation metrics, and structured methodologies for SO formu-
lation constitutes a key gap in the current literature. Therefore, there is currently no systematic
methodology for evaluating the correctness of LLM-generated SO formulations.
This paper fills this gap by introducing a new framework that combines prompt engineering,
multi-agent reasoning, and algebraic and structural evaluation for formulating two commonly used
SO models: chance-constrained optimization models and SMILP-2.
We highlight several novel outcomes:
(i) the discovery of consistent reasoning patterns across GPT-4 and three models when handling
expectation-based objectives and probabilistic constraints;
(ii) the design of a multi-agent prompting system that relatively mitigates hallucinations by de-
composing model synthesis into specialized roles, and multiple reviewers, and
(iii) the development of a soft scoring metric that provides partial scores for correct components
of LLM-generated code outputs.
1.3. Contributions
This paper makes the following key contributions:
(i) First systematic application of LLMs to formulate SO problems, including SMILP-2 and
chance-constrained formulations, directly from NL descriptions, helping practitioners.
(ii) Customized prompting strategies tailored for practitioners, including stepwise reasoning and
problem-specific instructions.
(iii) A soft scoring metric for model evaluation that accounts for structural similarity, notational
variation, and component-level permutations, offering a more robust alternative to canonical
or execution-based scoring.
(iv) Building several test instances, including NL descriptions of problems, input data, ground
truth code, and output optimal objective solution.
(v) Extensive experimental evaluation across GPT-3.5 and GPT-4 variants and several prompt
designs, analyzing performance trends, error types, and consistency and reproducibility across
problem categories.
Our experiments demonstrate that GPT-4 models, particularly GPT-4-Turbo, achieve better
structural matching scores than GPT-3 models. Additionally, structured prompting strategies con-
sistently outperform simple prompts across various problem classes. The proposed scoring met-
ric further captures partial correctness that canonical and execution-based metrics miss, revealing
meaningful progress even when exact accuracy remains zero.
1Canonical accuracy measures whether the generated formulation matches a canonical representation of the opti-
mization problem (a standardized linear programming (LP) model form, e.g., see [45, Figure 2]).
3


--- Page 4 ---
2. Related work
2.1. Paradigms for automatic formulation of OR problems using LLMs
Optimization modeling and SO have traditionally required significant domain expertise, but
LLMs now offer a path toward automation, although it is in an early stage. Two main paradigms
exist leveraging LLMs to formulate OR problems: (i) prompt-based methods, which guide pretrained
LLMs via structured prompts or multi-agent workflows, and (ii) learning-based methods, which fine-
tune models2 on synthetic datasets (e.g., [21]) to generate variations of mathematical formulations,
problem instances, input data (data synthesis), and so on [64]. These two paradigms, in particular
the prompt-based approach, build the grounds for the following line of research in LLM-based
formulation of OR problems.
2.2. Prompt-based formulation of OR problems
Prompt-based approaches use NL problem descriptions to generate or refine optimization for-
mulations, heuristics, or solver code on pre-trained LLMs. Zero-shot prompting relies solely on
pretrained knowledge, i.e., a single command to render the results given a problem, while few-shot
prompting3 provides examples that implicitly define structure and notation [9]. For mathemat-
ically intensive tasks, Chain-of-Thoughts (CoT) prompting elicits intermediate and step-by-step
reasoning to reduce shortcut errors and clarify logic [60]. The Chain-of-Experts (CoE) framework,
on the other hand, decomposes problems into specialized roles such as extractor, formulator, and
verifier, coordinated by a central controller [61]. Extending this idea, multi-agent prompting lets
several LLM agents critique, repair, and validate each other’s work or invoke external tools for more
reliable formulations [27].
In this paper, we developed zero-shot and few-shot CoT prompts for a range of SO problems. We
have also included prompts with instructions inspired by the work of [7] in SO, which also improves
the performance of zero- and few-shot prompts [31]. There are other prompting methods, such as
Reflection, progressive hinting, Tree-of-Thought, ReAct, etc. We refer the reader to [61] for formal
definitions of these prompts. While some of these methods outperformed CoT on deterministic
mixed-integer linear programming (MILP) problems, they were capable of solving only a few simple
deterministic optimization problems. CoT provides the most controlled and interpretable baseline
for evaluating whether LLMs can formulate SO models as applied in the recent literature (e.g., [7]),
independent of reasoning heuristics or external tool usage.
2.3. Deterministic (MI)LP problem formulations using LLMs
As for examples of formulating deterministic (MI)LP problems, initially, [43] proposed OptGen,
an augmented intelligence system that automatically generates LP models from NL problem descrip-
tions. OptGen uses a two-stage pipeline, mapping text to an intermediate representation and then
converting it into a canonical LP, and introduced the first dataset of LP Word Problems (LPWP).
Building on this, [45] organized the NL4Opt Competition, where they also proposed a canonical-
form metric to assess deterministic LP problem formulations generated by LLMs. Subsequently,
[28] extended the NL4Opt dataset and designed a 3-stage MILP synthesis pipeline (variables, con-
straints, and model) for the formulation of deterministic MILP problems.
[4] evaluated LLMs
on the NL4Opt dataset. Similar to this paper, they found that few-shot prompting outperforms
zero-shot prompting, and fine-tuning using instructions can reduce hallucinations of LLMs, but
2Fine-tuning updates parameters of a pretrained model to make the model focus on a specific domain. Some
variants, such as LoRa [18], only fine-tune a subset of parameters to save time and costs.
3One-shot prompt is a variant of few-shot prompts with only one example [9].
4


--- Page 5 ---
the size of instructions is important. [15] integrated LLMs with optimization algorithms to create
adaptive decision-support systems. Their method interprets NL instructions, adjusts algorithmic
components, and explains decisions. In contrast, we have developed a scoring metric evaluating
the performance of LLMs on SO problems rather than decision-making criteria. In this work, for
the first time, we developed and gathered SO problems for testing LLMs, whereas the mentioned
literature is designed for deterministic optimization problems.
2.4. Multi-agent and heuristic OR frameworks with LLMs
Several works extended the formulation of deterministic (MI)LP problems with single-agent
prompts to multi-agent prompting.
The reference [37] introduced a multi-agent framework with relation-identification and veri-
fication agents that operate without solver execution on deterministic LP, MILP, and quadratic
programming (QP) problems. Their evaluation, however, depends on embedding-based similarity
between ground-truth and generated codes and comments, i.e., converting comments and codes into
vectors and finding similarity of these embeddings by the cosine-similarity approach. The OptiMUS
framework by [3] employs a multi-agent architecture to formulate and solve deterministic (MI)LP
problems from NL descriptions. Similar to this paper, they introduced a single evaluator agent to
capture and report errors of generated code by LLMs. [2] extended OptiMUS. In particular, they
included a self-reflective error correction method, enhancing the LLM’s output for deterministic
(MI)LP problems formulation. [63] presented OR-LLMAgent, a reasoning-based framework that
decomposes OR workflows into modeling, code generation, and debugging agents. On LP problem
dataset benchmarks, they achieved over 7% accuracy improvement, confirming the value of task
decomposition or a multi-agent framework. This literature, unlike this paper, does not contain
multiple reviewers, stochastic problems, and granular evaluation metrics or instructions-included
prompts.
As for designing heuristics using LLMs, [33] proposed an LLM-driven Evolutionary Algorithm
(LMEA) which constructs prompts to guide LLM for choosing algorithmic decisions, such as parent
solutions in a genetic algorithm, or [29] addressed the lot-streaming hybrid job shop scheduling prob-
lem (LHJSV) using a memetic algorithm guided by an LLM-designed heuristic, combining problem
decomposition with prompt learning. [20] proposed an LLM-based framework that autonomously
designs multi-objective evolutionary algorithms (MOEAs). The framework uses runtime and exe-
cution errors to refine prompts and reproduction methods automatically by LLMs. Although the
heuristic approach in these papers inherently contains stochasticity, they do not address the SO
problem formulations.
2.5. SO and decision-making under uncertainty using LLMs
A limited literature has worked on formulating SO problems by LLMs. For example, [7] showed
that ChatGPT can formulate and solve adaptive robust optimization (ARO) problems under un-
certainty using CoT prompts in which instructions are provided. Their single-agent design does
not address stochastic frameworks such as SMILP-2 or chance-constrained problems. [25] evalu-
ated LLMs’ capabilities in stochastic modeling using graduate-level exam problems and the SimOpt
benchmark, showing near-expert proficiency in decision-making under uncertainty. In particular,
they evaluated student-submitted solutions and ground-truth using LLMs. The work of [32] de-
veloped a Decision-making Large Language model Assistant (DeLLMa). In this framework, given
a problem context, a user goal, and a set of actions, the LLM agent is asked to develop a policy
function, a probabilistic function mapping states to actions. Furthermore, the agent is asked to
develop a utility function assigning values to states and actions. Finally, it will return the action
maximizing the utility function. Another work focusing on stochastic processes rather than SO
5


--- Page 6 ---
problem formulations, [46], introduced an LLM framework that embeds a Monte Carlo simulation
module to run simulations on an optimization problem obtained by LLMs for a problem description.
Then, an embedded optimization engine analyses the simulation outputs and renders an optimal
solution.
In contrast to these approaches, which focus on robust optimization, decision-making under
uncertainty, or simulation-based solution methods, this paper aims at automatic formulation of
SMILP-2 and chance-constrained SO models directly from NL descriptions using LLMs.
2.6. Evaluation metrics and reliability of LLMs for OR problems
A separate line of work uses LLMs for the reliability and interpretability of the outputs. [11]
developed OptiChat, a GPT-4-based framework for diagnosing infeasible MILP models by explain-
ing structure, locating errors, and proposing corrective actions. Another aspect of our work is the
granular evaluation metric. Evaluation methods in the literature largely depend on canonical and
execution accuracy [40, 43]. Canonical accuracy counts matched elements between generated and
ground-truth models but fails to handle permutation and notation variations. Execution accuracy,
while practical, cannot capture structural correctness when different formulations yield identical op-
timal solutions. [30] addressed permutation issues via bipartite-graph matching, but their method
remains sensitive to notation inconsistencies, computational overhead, and partial-correctness ig-
norance. These limitations motivate the need for granular evaluation metrics for SO formulations
using LLM.
Table 2 provides a summary of related works. Existing frameworks either remain fully determin-
istic optimization or rely on ARO or stochastic heuristic algorithms, such as the design of genetic or
evolutionary methods using LLMs. They lack granular evaluation metrics or multi-agent reviewer
frameworks. Besides, we consider a variety of prompting methods for SO problems that integrate
instructions and are easy to implement and adapt for practitioners.
Table 2: Summary of related literature on LLM-based optimization and formulation.
Category & reference
Main contribution
Notes
NL-to-optimization formulation frameworks (deterministic (MI)LP problems)
[43]
Introduced
OptGen,
a
two-stage
pipeline mapping NL to canonical LPs;
created LPWP dataset.
Focused on deterministic LP problems.
[45]
Organized NL4Opt competition defining
entity recognition and logical-form gen-
eration tasks with a canonical accuracy
metric.
Focused on deterministic LP problems.
[15]
Integrated LLMs with optimization al-
gorithms for adaptive decision-support
and interpreting NL instructions, ad-
justing algorithmic components, and ex-
plaining decisions.
Focused on deterministic (MI)LP, and heuristics con-
taining stochasticity.
[28]
Designed
a
3-stage
MILP
synthesis
pipeline
(variables,
constraints,
and
model).
Focused on deterministic (MI)LP problems.
[4]
Benchmarked LLMs for NL2OPT trans-
lation; found GPT-4 best in one-shot
settings.
Focused on deterministic (MI)LP problems.
Multi-agent frameworks
[37]
Proposed
multi-agent
relation-
identification and verification agents,
without solver execution;
introduced
new dataset.
Relies on embedding-based similarity rather than alge-
braic model checking. Focused on deterministic (MI)LP,
and QP problems.
Continued on next page
6


--- Page 7 ---
Category & reference
Main contribution
Notes
[3]
Developed OptiMUS, a modular multi-
agent MILP formulation and solving
framework using Python/Gurobi.
Focused on deterministic MILPs.
[2]
Extended OptiMUS with debugging and
refinement for complex NL MILP prob-
lems.
Focused on deterministic (MI)LP problems.
[63]
Introduced
OR-LLMAgent-reasoning-
based for modeling, code generation,
and
debugging;
developed
BWOR
dataset.
Focused on reasoning and deterministic (MI)LP prob-
lems.
[61]
Proposed CoEs architecture with feed-
back and reflection for (MI)LP prob-
lems.
Focused on deterministic (MI)LP problems based on
canonical evaluation.
LLM-assisted heuristic, evolutionary
[20]
Introduced LLM-driven MOEA design
using
error-driven
dialogue,
prompt-
based evolution, and dynamic strategy
selection for tuning heuristics.
Focused on evolutionary heuristic algorithms containing
stochasticity.
[29]
Applied
LLM-designed
heuristics
to
solve LHJSV scheduling via memetic al-
gorithms.
Covers heuristics containing stochasticity and determin-
istic problems.
[33]
Proposed LMEA constructing prompts
to guide LLM for choosing algorithmic
decisions, such as parent solutions in a
genetic algorithm.
Focused on LLM hyperparameter tuning and heuristics
containing stochasticity.
Stochastic and simulation-oriented
[7]
Demonstrated ChatGPT solving ARO
with CoT prompts and instructions.
Single-agent; Focused on ARO.
[25]
Evaluated LLMs in stochastic model-
ing and simulation-optimization using
SimOpt.
Assessment-focused rather than an SO modeling frame-
work.
[32]
Introduced DeLLMa, integrating utility
theory for decision-making under uncer-
tainty.
Focused on reasoning-level rather than SO problem for-
mulation.
[46]
Introduced an LLM framework that em-
beds a Monte Carlo simulation module
to run simulations on an optimization
problem obtained by LLMs for a prob-
lem description.
Focused on Monte Carlo simulation for generated for-
mulations using LLMs, assessing results based on an
embedded optimization engine.
Evaluation metrics, debugging, and verification
[11]
Developed OptiChat, a GPT-4 system
diagnosing infeasible optimization mod-
els.
Focused on post-solution debugging and deterministic
(MI)LP problems.
[30]
Used
bipartite-graph
matching
for
canonical-to-ground-truth model com-
parison.
Focused on evaluation metric based on bipartite graph
matching for generated and ground truth models.
[35]
Introduced
feedback-based
refinement
and self-correction for LLM reasoning
tasks.
Applied to general reasoning in LLMs rather than test-
ing in the OR domain.
3. Approach
In this section, we introduce formulations, prompts, justification for our framework design,
motivating examples, and formal representation and justification of the scoring system. For each
problem category, a general formulation is presented below in its standard analytical form, which is
used as a reference and as an input structure for prompts with instructions. These materials have
been adapted from [8, 51]. It is to be mentioned that our test cases include cases in which integer
variables also exist.
Throughout this paper, the ith row of a matrix A is denoted by Ai·, and the ith element of a vector
x is denoted by xi. Let ( ˜A,˜b) ∈Rm×n ×Rm denote random elements defined on a probability space
(Ξ, F, P) where Ξ is the set of all possible realizations of the stochastic parameters, F is a σ-algebra
7


--- Page 8 ---
of measurable events on Ξ, and P is a probability measure on (Ξ, F). Chance-constrained models
provide a probabilistic framework for stochastic decision-making by allowing controlled constraint
violations. A joint chance-constrained program requires that all constraints hold simultaneously
with probability at least 1 −α:
min
x∈X
c⊤x
s.t.
P
  ˜Ax ≤˜b

≥1 −α,
where c ∈Rn is a cost vector, vector x ∈X ⊆Zk
+ × Rn−k
+
contains non-negative decision variables
(with first k variables being integer) and α ∈[0, 1] is the prescribed confidence level. Equivalently,
the probability that any constraint is violated is limited by a risk budget:
P
 ∃i : ( ˜Ai·x > ˜bi)

≤α.
In this paper, we assume the probability law P is approximated by a finite empirical distribution
supported on a finite set of sample scenarios. Thus, one can replace the chance constraints with
finite-scenario approximations by sample-average approximation techniques, which can be formu-
lated linearly using integer variables (e.g., see [34], [41, p. 351]).
By contrast, an individual chance-constrained formulation enforces probabilistic feasibility sep-
arately for each constraint:
min
x∈X
c⊤x
s.t.
P
  ˜Ai·x ≤˜bi

≥1 −αi,
i = 1, . . . , m,
where the confidence level αi may vary across constraints to represent heterogeneous reliability or
safety requirements [41]. While the joint model enforces global reliability, the individual model
allows constraint-wise flexibility at the expense of potentially weaker overall guarantees.
The distinction between joint and individual chance-constrained formulations is central to un-
derstanding how language models interpret and encode probabilistic logic, specifically, whether they
recognize collective feasibility events or treat constraints independently in mathematical syntax.
A two-stage deterministic mixed-integer linear programming (DMILP-2) formulation is a lin-
ear recourse model without uncertainty. First-stage variables x are chosen initially; second-stage
variables y ∈Y ⊆Zp
+ × Rr−p
+
adjust subsequently without uncertainty, and given D ∈Rz×r, B ∈
Rz×n, q ∈Rr and d ∈Rz, DMILP-2 is formulated as follows:
min
x∈X
c⊤x + h(x)
s.t.
Ax = b,
where h(x) := min
y∈Y{q⊤y|Dy = Bx + d}, represents the optimal cost of the second-stage response for
a given first-stage decision x. This deterministic formulation serves as a baseline for comparing how
LLMs can recover a hierarchical decision structure with random parameters when facing SMILP-
2 problems. The simplest SMILP-2 model generalizes the DMILP-2 formulation by allowing the
second-stage data to be random. A scenario ξ ∈Ξ specifies a complete realization of the random
second-stage problem data. We denote this data by the random vector ˜ξ = vec(˜q, ˜D, ˜B, ˜d), where
ξ ∈Ξ is a realization of ˜ξ, ˜q is a random cost vector for the recourse decision, ˜D is a random
constraint matrix for the recourse system, ˜B is a random matrix that couples the first-stage decision
8


--- Page 9 ---
to the second-stage system, and ˜d is a random right-hand side vector with dimensions defined for
their deterministic counterpart above.
Given this setup, the SMILP-2 is
min
x∈X
c⊤x + E
h
h(x, ˜ξ)
i
(1)
s.t.
Ax = b,
where h(x, ˜ξ) := min
y∈Y{˜q⊤y| ˜Dy = ˜Bx + ˜d}. We assume relatively complete recourse and the h(x, ˜ξ)
is integrable for all x ∈X.
The expectation E[h(x, ˜ξ)] in (1) is taken with respect to the probability measure P on (Ξ, F),
i.e., E[h(x, ˜ξ)] =
R
ξ∈Ξ h(x, ˜ξ))dP(ξ), and represents the expected optimal recourse cost associated
with x.
In many practical settings, the uncertainty set Ξ is modeled as a finite scenario set {ξ1, . . . , ξs}
with strictly positive scenario probabilities p1, . . . , ps satisfying Ps
i=1 pi = 1. In that case, each
scenario ξi induces deterministic data vec(qi, Di, Bi, di), and the expectation operator in (1) reduces
to a finite weighted sum. The equivalent extensive-form deterministic model is then given by
min
x∈X, yi∈Y(i=1,...,s)c⊤x +
s
X
i=1
pi (qi)⊤yi
s.t.
Ax = b,
Diyi = Bix + di,
i = 1, . . . , s.
In this deterministic equivalent, the variable yi represents the recourse action that would be taken if
scenario ξi materializes. The linear system Diyi = Bix+di enforces scenario-wise feasibility, and the
weighted sum Ps
i=1 pi(qi)⊤yi captures the expected recourse cost under the scenario probabilities.
In the present context, it also provides a clean syntactic target for automatic model extraction and
evaluation.
Next, we present prompts and the multi-agent framework.
3.1. Design justification
Our prompting methodology is deliberately hybrid because distinct prompt styles mitigate dif-
ferent, well-documented failure modes in NL-to-optimization translation and are supported by prior
evidence, e.g., multi-agent prompting enhances reasoning [17, 61] and mitigates extra-element gen-
eration [19], and CoT improves LLMs’ performance and reasoning capabilities [49, 60, 61]. We use
the following methods:
(i) CoT makes intermediate reasoning explicit, which is essential when expressing complex SO
elements such as expectations, recourse mappings, and chance-constraints. Empirically, CoT im-
proves multi-step mathematical derivations [49, 60, 61]. Although we mentioned prompting methods
in Section 2 that slightly outperform CoT on deterministic optimization problems, CoT remains
more practitioner-friendly. It is more interpretable and adaptable across problem settings without
the need for external tools practiced by the recent works for SO problems (e.g., [7]).
(ii) Concise exemplars (sets →parameters →variables →objective/constraints) bias the model
toward the correct algebraic skeleton, reducing notation drift and omitted elements. Recent evalu-
ations of LLMs report consistent gains from such structured prompting over pure zero-shot [53, 57,
62].
9


--- Page 10 ---
(iii) We decompose the pipeline into extractor →formulator →reviewer →updater roles so
that each stage can be checked locally (e.g., variables/sets before constraints; constraints). Modular,
agentic workflows have shown higher reliability than single-pass prompting in complex OR pipelines
[30, 53, 57]. In particular, agentic frameworks have been shown to exhibit greater robustness against
forgetting and unsupported extra-element generation, as discussed in detail in Section 3.2.
(iv) We request Python/Gurobi code (rather than textual math) to (a) demand explicit decla-
rations of variables/constraints/objective, (b) avoid ambiguity in notation, (c) enable compile/run-
time checks, and (d) gain better performance (see chain-of-code prompting in [49]). Code-based
evaluation has proved to be easier than text-only outputs in optimization tasks [44, 59].
(v) Scenario expansion surfaces the block-angular structure and per-scenario feasibility that
are hidden in compact SMILP-2 notation, making it possible to verify that (a) the expectation is
correctly formed and (b) recourse constraints are indexed by scenario. Besides, this structure is
easier to code as it is a linear model, which can help LLMs reduce compile errors.
Collectively, these choices operationalize three principles: (a) make reasoning explicit (CoT/few-
shot), (b) separate tasks and verify locally (multi-agent roles), and (c) evaluate in code and in
algebra (Gurobi execution plus our soft-scoring metric).
3.2. Multi-agent framework
To further utilize the reasoning abilities of LLMs, we designed a multi-agent framework. In this
framework, distinct ChatGPT agents collaborate on solving parts of an SO problem. Each agent
is prompted with tailored context and roles. Our multi-agent framework is depicted in Figure 1.
Three important agents in this framework are the data extractor, the mathematical formulators,
and the reviewers.
Data extractor elicits deterministic and stochastic variables and parameters
together with sets and indices from the problem description and passes them to the next agent.
The mathematical formulator receives the previous agent’s outputs and problem description to code
the model in Python (Gurobi execution). Reviewer agents evaluate the consistency and coherence
of the output code from the mathematical formulator. They identify formulation issues or logical
gaps. Even with identical prompts, four reviewer agents introduce stochastic diversity in reasoning
paths and token sampling. The updating agent incorporates only those revisions justified by the
reviewer feedback, closing a targeted feedback loop.
Problem
Description
Data
Extractor
Mathematical
Formulator
Reviewer
Reviewer
Reviewer
Reviewer
Mathematical
Formulator
Output
Figure 1: Multi-agent prompting framework.
Agents’ independent and collective analyses can reduce hallucinations, i.e., the generation of
unsupported elements which are factually unsupported [19] and improve reasoning accuracy [17],
10


--- Page 11 ---
and increase the likelihood that at least one agent identifies subtle algebraic or modeling errors
overlooked by others. It also mitigates catastrophic forgetting, i.e., the loss of prior logical consis-
tency during extended reasoning [19], as agents can focus on specific tasks rather than the entire
problem-solving process. Each agent has access to the problem description, which helps them be
aware of the context and detect any potential errors caused by other agents, thereby mitigating the
cascading errors phenomenon.
3.3. Prompts
In this section, we present the engineered prompts used in our experiments. We present three
versions of CoT prompts, i.e., cot_s, cot_s2, and cot_instructions. Our standard CoT prompt,
cot_s, is an adapted version of the CoT prompt taken from [61]. We refer to it as standard since
it is the simplest stochastic version of it in [61]. We developed a sophisticated version of CoT,
which includes additional hints, particularly for SMILP-2 problems. Finally, cot_instructions
includes instructions from Section 3 for each of the stochastic problems.
The simplest prompt
is standard_s, which is the adapted version of its deterministic counterpart in [61]. It is called
standard as it contains no hints or step-by-step reasoning procedure. Lastly, we present agentic
prompt where each agent is specialized in a specific task. In what follows, we present the prompts
with a brief discussion on them.
Prompts 3.1 and 3.2 refer to standard_s and cot_s. These are the simplest in this work, and we
will compare the performance of the sophisticated prompts with the performance of these prompts.
Prompt 3.1: standard prompt for SO problems (standard_s) adapted from [61]
You are a Python programmer in the field of Operations Research and Stochastic Optimization.
Your proficiency in utilizing third-party libraries such as Gurobi is essential. In addition to your
expertise in Gurobi, it would be great if you could also provide some background in related
libraries or tools, like NumPy and SciPy. You are given a specific problem. You aim to develop
an efficient Python program that addresses the given problem. Now the original problem is as
follows:{problem_description}. Here is a starter code: {code_example}. Give your Python
code directly.
Prompt 3.2: standard CoT prompt for SO problems adapted (cot_s) from [61]
You are a Python programmer in the field of Stochastic Optimization. Your proficiency in
utilizing third-party libraries such as Gurobi is essential.
In addition to your expertise in
Gurobi, it would be great if you could also provide some background in related libraries or
tools, like NumPy and SciPy. You are given a specific problem. You aim to develop an efficient
Python program that addresses the given problem. Now the original problem is as follows:
{problem_description}. Let’s analyse the problem step by step, and then give your Python
code directly. Here is a starter code: {code_example}.
The code_example provided in the prompts refers to the code template.
Providing a code
template can give structure to the outputs generated by LLMs and pave the way for the evaluation
of the GPT-generated codes. The advanced prompt 3.3 introduces more granular and structured
steps with hints, making it worthwhile to investigate whether such precision leads to improved
performance.
For each problem category, it directs the model to decompose the description of
the problem and extract sets and indices, parameters, stochastic and deterministic variables. In
11


--- Page 12 ---
particular, for the SMILP-2 problems, it requires identifying first- and second-stage objectives and
constraints and an extensive form of the problem. Prompt 3.4 includes steps and hints of prompt 3.3
except that we have added instructions on the general structure of each type of problem category
provided in Section 3. We hypothesize that these instructions may further help LLMs to focus
on essential parts of the extraction of the elements and formulation. The multi-agent prompt is
presented at 3.5. We hypothesize that this framework, beyond the known benefits of CoT, further
reduces hallucinations by assigning each agent a narrowly tailored task and introducing a control
loop that prevents cascading errors. Each prompt corresponds to a specific agent and is designed
to operate independently; agents do not have access to one another’s prompts.
A summary of the prompting methods and specifications is provided in Table 3. Note that
prompting methods have been introduced in prior works. Following the structure of these methods,
we generated SO-specific prompts.
Table 3: Prompting methods used in this paper with acronyms.
Prompt
Acronym
Instructions
Notes
Zero-shot
standard_s
✗
Stochasticity-aware version of [61]
Multi-agent
agentic
✗
Explained in detail at 3.2
CoT
cot_s
✗
Stochasticity-aware version of [61]
CoT
cot_s2
✗
More precise guidance and steps than cot_s
Few-shot
cot_s_instruction
✓
Includes instructions
12


--- Page 13 ---
Prompt 3.3: Advanced CoT prompt for SO (cot_s2)
extract_elements : You are an expert in Gurobi and Stochastic Optimization (SO). You will
be given a verbal description of an SO problem. It is either a two-stage stochastic mixed-integer
linear programming (SMILP-2), its two-stage deterministic mixed-integer linear programming
(DMILP-2) counterpart, or an individual or joint chance-constrained optimization problem.
Your task is to identify and extract the following:
- Sets and indices (e.g., time periods, locations, resources, scenarios, etc.)
- Deterministic and stochastic parameters (e.g., costs, capacities, probabilities, uncertain de-
mand values, etc.)
- Variables (e.g., first- and second- stage decisions in an SMILP-2 problem, integer variables,
etc.)
Present your Python code for the elements above. Here is a code template: {code_example}.
Your extraction will serve as the foundation for subsequent code implementation.
formulate_model : Now, you are provided with the extracted components, including sets and
indices, deterministic or stochastic parameters, and variables. Your task is to formulate the
complete model in Python Gurobi. Specifically:
Clearly define the objective function, including the first-stage costs and the expected recourse
(second-stage) costs for SMILP-2 and DMILP-2 problems.
Write all relevant constraints of the model. Use scenario-based indicator reformulations by in-
teger variables and big-M to linearize chance constraints.
extensive_form : Now, you are given the Python Gurobi formulation of the problem (includ-
ing sets, indices, parameters, decision variables, objective function, and constraints). If it is an
SMILP-2 or DMILP-2 problem, your task is to construct the extensive form of this problem in
Python using Gurobi by modifying the previous code. Specifically:
Enumerate all possible scenarios, associating each with its corresponding probability.
Replace the expected value term in the objective function with scenario-specific expressions to
form a deterministic equivalent if it is an SMILP-2 problem by replicating and customizing the
second-stage constraints and variables for each scenario.
Present the full model in a single-stage linear programming format suitable for direct input into
a Gurobi solver in Python. Note the code example.
Now the original problem is as follows:
{problem_description}.
Let’s analyse the problem step by step, then give your final Python code. Here is a starter code:
{code_example}.
13


--- Page 14 ---
Prompt 3.4: CoT with instructions (cot_instructions)
extract_elements: You will be given a verbal description of a two-stage stochastic mixed-integer
linear programming (SMILP-2) problem, or a two-stage deterministic mixed-integer linear pro-
gramming (DMILP-2) problem, or an individual or joint chance-constrained problem. Your
task is to identify and extract the following components from the text:
- Sets and indices (e.g., time periods, locations, resources, scenarios, etc.)
- Deterministic and stochastic parameters (e.g., costs, capacities, probabilities, uncertain de-
mand values, etc.)
- Variables (e.g., first- and second- stage decisions in an SMILP-2 problem, integer variables,
etc.)
Present your answer in Python using Gurobi.
The original problem is as follows: {problem_description}.
Let’s analyse the problem step by step. Here is a starter code: {code_example}.
Learn the following instructions to guide you further if it is an SMILP-2:
{Instructions for SMILP-2}.
If it is a chance-constraint model, use the following instructions:
{Instructions for chance-constrained models}.
If it is a DMILP-2 model, use the following instructions:
{Instructions for DMILP-2 models}.
formulate_model: Now, given the sets, indices, variables, and stochastic and deterministic
parameters from the previous step, Your task is to formulate the complete model. Specifically:
- Clearly define the objective function, including the first-stage objective and the expected
recourse (second-stage) objective if it is an SMILP-2 problem.
- Code all constraints. Use scenario-based indicator reformulations by integer variables and
big-M to linearize chance constraints.
extensive_form (only if it is an SMILP-2 problem): Now, Your task is to construct the extensive
form of this SMILP-2 model. Specifically:
- Enumerate all possible scenarios, associating each with its corresponding probability.
- Replace the expected value term in the objective function with scenario-specific expressions
to form a deterministic equivalent by replicating and customizing the second-stage constraints
and variables for each scenario.
- Present the full model in a single-stage linear programming format suitable for direct input
into an LP solver. Clearly label all variables and constraints by scenario.
Read the instructions below for further guidance on the extensive counterpart of an SMILP-2:
{Extensive form generation}.
python_code: Now, you are provided with the complete mathematical formulation of the SO
problem or DMILP-2 problem.
Your task is to generate the complete Python code using Gurobi solver.
14


--- Page 15 ---
Prompt 3.5: multi-agent prompt for SO (agentic)
Data extractor agent : You will be given a verbal description of a Stochastic Optimization
problem, such as a two-stage stochastic mixed-integer linear programming (SMILP-2) problem
(or its two-stage deterministic mixed-integer linear programming counterpart) or individual and
joint chance-constrained problems. Your task is to identify and extract the following compo-
nents from the text:
- Sets and indices (e.g., time periods, locations, resources, scenarios, etc.)
- Deterministic and stochastic parameters (e.g., costs, capacities, probabilities, uncertain de-
mand values, etc.)
- Variables (e.g., first- and second- stage decisions in an SMILP-2 problem, integer variables,
etc.)
Present your answer in Python code using Gurobi. Below is also a code template, and problem
description:
{problem_description} + {code_example}.
Formulation agent : You will be given a verbal description of a Stochastic Optimization prob-
lem, such as a two-stage stochastic mixed-integer linear programming (SMILP-2) problem (or
its two-stage deterministic mixed-integer programming counterpart), individual or joint chance-
constrained problems, along with the extracted components including sets, indices, stochastic
and deterministic parameters, and variables. Your task is to code the complete model in Python
using Gurobi. Specifically:
Clearly define the objective function.
Code all relevant constraints. Specifically, if it is an SMILP-2 problem, you need to code the
extensive form of the problem. Use scenario-based indicator reformulations by integer variables
and big-M to linearize chance constraints.
New problem description:
{problem_description}.
Below is the extraction output:
{extraction_output}.
Below is the code template you should follow:
{code_example}.
Reviewer agent : You are a reviewer agent specialized in Stochastic Optimization problems.
You are provided with a problem description and a final Python code using Gurobi solver.
Review them carefully for potential mistakes (such as variables, objective function, constraints,
parameters, and syntax errors) and any additional elements that might be required. Provide
concise and precise feedback.
Problem description:
{problem_description}.
Mathematical formulation:
{code_formulation_output_1}.
Formulation agent 2 :
You are a specialized updating agent in Stochastic Optimization. You are provided with the
following information:
Problem description:
{problem_description}.
Current code:
{current_code}.
Feedback from reviewer agents:
{reviewers_feedback}.
Review the feedback carefully. If the feedback indicates valid improvements, update the code
accordingly. Return the updated final code. Do not include any additional text.
15


--- Page 16 ---
3.4. Motivating examples
We provide several examples where text output is desired.
Below is an example of a joint
chance-constrained problem.
Prompt: joint chance-constrained, adapted from [6, 12]
You are an expert in analyzing joint chance-constrained models. Here is our problem. You are
managing a resource allocation problem. You have to allocate amounts to two projects. The
actual resource needs are uncertain, and you want to ensure both project demands are met
simultaneously with 95% probability.
Read the following instructions to guide you and give the mathematical formulation. {Prompt:
Instructions for joint chance-constrained models}
ChatGPT-3.5-Turbo’s response for the joint chance-constrained modeling:
Joint Chance-Constrained Resource Allocation Decision Variables:
• x1, x2 ∈R+: amount of resource allocated to Project 1 and Project 2
Random Variables:
• ˜d1, ˜d2: random demands for Project 1 and Project 2
Model:
min
x1,x2
x1 + x2
s.t.
P
x1 ≥˜d1
x2 ≥˜d2

≥0.95
x1 ≥0,
x2 ≥0
The strengths of the solution are that the model correctly interprets the requirement as a joint
chance-constrained, and the response provides a separation between decision variables and random
parameters.
The following example is for an individual chance-constrained model.
16


--- Page 17 ---
Prompt: individual chance-constrained, adapted from [6]
You are an expert in analyzing individual chance-constrained models. Here is our problem: You
are responsible for allocating delivery trucks from two warehouses to fulfill uncertain demands
at two retail stores, Store A and Store B. The number of trucks allocated to each store is
denoted by x1 and x2, respectively. However, the exact demand at each store is not known in
advance and is modeled as a normal random variable:
• ˜d1 ∼N(100, 102) for Store A,
• ˜d2 ∼N(150, 152) for Store B.
Each truck delivers exactly one unit of goods. The goal is to minimize the total number of
trucks dispatched, while ensuring that the supply meets the demand at each store with high
probability:
• Store A should receive enough goods with at least 95% probability.
• Store B should receive enough goods with at least 90% probability.
Read the following instructions to guide you and give the mathematical formulation. {Prompt:
Instructions for individual chance-constrained models}
ChatGPT-4’s response for the individual chance-constrained modeling:
Mathematical Formulation: Individual Chance-Constrained Model
min
x1,x2
x1 + x2
s.t.
P

x1 ≥˜d1

≥0.95
(Store A)
P

x2 ≥˜d2

≥0.90
(Store B)
x1 ≥0,
x2 ≥0
The solution’s strengths are that the model correctly interprets the requirements as individual
chance-constrained, the objective is consistent with the problem statement, and the formulation is
compact and clearly structured. The weakness is that trucks are indivisible units; x1 and x2 should
be modeled as integers.
The following example is for an SMILP-2 model.
17


--- Page 18 ---
Prompt: SMILP-2, adapted from [36]
You are an expert in analyzing two-stage stochastic mixed-integer linear programming models
(SMILP-2). Here is our problem: We consider a supply-planning problem in which a producer
books transportation capacity in advance to satisfy uncertain factory demand. After demand
is realized, unused vehicle bookings may be cancelled at a fee, and any remaining shortage can
be covered by purchasing from an external supplier at a higher, uncertain cost. The goal is to
determine how many vehicles to book from each plant to minimize the expected total cost.
Read the following instructions to guide you and give the mathematical formulation. {Prompt:
Instructions for SMILP-2 models}
ChatGPT-4’s response for the SMILP-2 modeling:
Sets:
I: set of origins (plants)
Ω: set of scenarios, with probability pω for ω ∈Ω
Parameters:
dω: demand at factory under scenario ω
ci: cost to book one vehicle from plant i
¯cω
i : cost of actually using a booked vehicle from i in ω
γi: cancellation fee per unused vehicle at plant i
qω: unit cost to purchase shortage from external supplier in ω
Q: vehicle capacity (fixed, full-load)
First-stage decision:
xi: number of vehicles to book from plant i
Second-stage decisions:
yω
i : number of booked vehicles used from i under ω
zω: quantity purchased from external supplier under ω
min
x
X
i∈I
cixi + Eω [ϕ(x, ω)]
s.t.
xi ∈Z+
∀i ∈I
where
ϕ(x, ω) := min
yω,zω
X
i∈I
(¯cω
i yω
i + γi(xi −yω
i )+) + qωzω
s.t.
X
i∈I
Qyω
i + zω ≥dω
0 ≤yω
i ≤xi, ∀i ∈I,
zω ≥0
The strengths of the solution are that the formulation correctly captures the two-stage structure,
the model explicitly links first- and second-stage decisions through the constraints 0 ≤yω
i ≤xi, and
the recourse function is well-structured. The weaknesses are that the recourse cost function is not
linearized, and the demand-satisfaction constraint assumes continuous or fractional vehicle usage.
The prompt below is for the DMILP-2 problem.
18


--- Page 19 ---
Prompt: DMILP-2 problem, adapted from [6]
You are an expert in analyzing two-stage deterministic mixed-integer linear programming
(DMILP-2) models.
Here is our problem: You’re managing inventory for two warehouses.
In the first stage, you decide how much product to order for each warehouse. In the second
stage, once customer demands from two regions are revealed (and they’re deterministic), you
decide how to ship products from the warehouses to meet those demands. The objective is
to minimize total ordering and shipping costs while satisfying all customer demand and not
exceeding warehouse inventory levels. Read the following instructions to guide you and give the
mathematical formulation. {Prompt: Instructions for DMILP-2 models}
ChatGPT-4’s response for the DMILP-2 modeling:
The goal is to minimize ordering and shipping costs.
min
x
c⊤x + h(x)
(2)
s.t.
x ≥0
where
h(x) =
min
y
2
X
i=1
2
X
j=1
sijyij
s.t.
2
X
j=1
yij ≤xi
(do not exceed warehouse stock)
2
X
i=1
yij = dj
(meet regional demand)
yij ≥0
∀i, j
(3)
where xi: quantity ordered for warehouse i (first-stage decision), yij: quantity shipped from
warehouse i to region j (second-stage decision), ci: cost per unit ordered at warehouse i, sij:
shipping cost per unit from warehouse i to region j and dj: known demand in region j
The strength of the solution is that it correctly identifies and applies a DMILP-2 structure. The
weakness is in the declaration of index sets for constraints.
These observations motivate us to develop a scoring system to automate the evaluation and
quantification of solutions at scale.
3.5. Scoring
The scoring system evaluates ChatGPT’s generated model’s Python code based on output code
correctness and structural matching to a ground-truth code.
Exact matches receive full credit,
while variables, constraints, and objectives are compared using a matching strategy that assigns
partial credit and reports any runtime or compile-time errors.
The scoring system’s operations
are illustrated by Algorithm 1. The indicator function used in this paper is defined as 1{P} =
(
1,
if P is true,
0,
otherwise.
, and |.| indicates the cardinality of a set.
19


--- Page 20 ---
Algorithm 1 Scoring system
Require: true_code, generated_code
Ensure: (Ptotal, Pextra, Pvar, Pcons, Pobj) ∈[0, 100]5
1: If running generated code on a sample equals true optimal objective value then return accuracy = 100.
2: Parse variables V T, V G via .addVars, with arg strings AT, AG.
3: Parse constraints CT, CG via first arg of .addConstr(s)(·) (drop outer quicksum).
4: Parse objective OT, OG via .setObjective(·,GRB.(MAX)MINIMIZE); inline if a bare name; drop outer
sums.
5: Define normalizers:
Nidx : [·]7→[idx],
Nvar(s; υ) : υ7→_X_, others 7→_Yk_, K = 0, · · · , k,
R(r) =









lhs −rhs,
r : lhs ≤rhs,
rhs −lhs,
r : lhs ≥rhs,
lhs −rhs,
r : lhs = rhs,
r,
otherwise.
6: Define occ(s, υ) as the count of occurrences of υ plus occurrences of υ[·].
7: Define SymPy equivalence Eq(a, b) as: replace each sum/quicksum sub-expression by a canonical symbol
determined by its normalized body and sorted generator clauses, then equivalence is true if Eq(a, b) = 0.
8: for g ∈V G do
9:
for t ∈V T do
10:
S(g, t) ←0
11:
S(g, t) +=|{k ∈{lb, ub, vtype} : k = · appears identically in AG(g), AT(t)}|
(args)
12:
S(g, t) += min {P
s∈CT∪{OT}occ(s, t), P
s∈CG∪{OG}occ(s, g)}
(frequency)
13:
for cT ∈CT such that t occurs in cT do
(constraints)
14:
for cG ∈CG such that g occurs in cG do
15:
˜cT ←Nvar(Nidx(cT); t), ˜cG ←Nvar(Nidx(cG); g)
16:
S(g, t) += 2 · 1{˜cT =˜cG} + 2 · 1{Eq(R(˜cT), R(˜cG))} + 2 · 1{rmsp(˜cT)=rmsp(˜cG)}
17:
(rmsp is a normalization function deleting whitespace and punctuation)
18:
end for
19:
end for
20:
if t ∈OT and g ∈OG then
(objective)
21:
˜OT ←Nvar(Nidx(OT); t),
˜OG ←Nvar(Nidx(OG); g)
22:
S(g, t) += 2 · 1{ ˜OT = ˜OG} + 2 · 1{Eq(R( ˜OT), R( ˜OG))} + 2 · 1{rmsp(˜OT)=rmsp(˜OG)}
23:
S(g, t) += 2 · | {coef · t[·]⊂OT} ∩{coef · g[·]⊂OG} |
(term align)
24:
end if
25:
end for
26: end for
27: Build a greedy partial one-to-one mapping µ : V G ⇀V T by selecting pairs (g, t) in descending S(g, t)
without reuse; ignore S(g, t) = 0.
28: Rename whole-word g 7→µ(g) in generated_code (longest-first) to obtain aligned_generated_code.
29: Re-parse bV G, bCG, bOG from aligned_generated_code.
30: Mvar ←|V T ∩bV G|.
31: Mcons ←

n
cT ∈CT : ∃ˆc ∈bCG s.t. Nidx(cT) = Nidx(ˆc) or algebraic/structural equality holds after applying µ
o.
32: Mobj ←1
h
∃ˆO ∈{ bOG} s.t. Nidx(OT) = Nidx( ˆO) or algebraic/structural equality holds
i
.
33: Tvar ←|V T|, Tcons ←|CT|, Tobj ←|OT|
34: Pvar =100 · Mvar
Tvar ,
Pcons =100 · Mcons
Tcons ,
Pobj =100 · Mobj
Tobj
35: Ptotal =100 · Mvar+Mcons+Mobj
Tvar+Tcons+Tobj
36: Pextra =100 · E
G where G=|bV G| + | bCG| + | bOG| and E is number of unmatched generated components.
37: return (Ptotal, Pextra, Pvar, Pcons, Pobj)
20


--- Page 21 ---
3.6. Formal specification of the scoring metric
We evaluate a candidate (generated) model’s code against a reference (true) model’s code by
parsing their Gurobi code strings and computing matching scores.
While a matching optimal
objective solution suggests a high-quality model, it may still lack essential variables or constraints.
To prevent false positives, we can investigate matching scores for variables, constraints, and the
objective function.
Inputs and outputs: Let true_code and generated_code be the two input strings. The proce-
dure returns the following percentages: Pvar, Pcons, Pobj, Ptotal, Pextra ∈[0, 100].
If a preliminary execution of the generated code on a sample instance produces output exactly
equal to the ground-truth output (optimal objective solution), the procedure returns Accuracy
= 100. Yet, we check for structural matching to alarm false positives.
Parsed components: From each code string, we extract:
• Variables: every assignment v = model.addVars(args) yields a variable name v and its raw
argument string args (which may contain lb= ·, ub= ·, vtype= ·). Let V T (respectively V G)
be the set of true (respectively generated) variable names, with argument strings argsT
t for
t ∈V T and argsG
g for g ∈V G.
• Constraints: from every call model.addConstr(s)( · ), we take its first argument as a single
constraint string (e.g., a[i]*x[i] <= b[i]). We denote these sets by CT and CG.
• Objective function: from model.setObjective( · , GRB.(MAX)MINIMIZE) we extract the first
argument as a single expression OT or OG. If the argument is a bare identifier (e.g., obj),
its right-hand side is inlined from its assignment.
For example, if the output is obj =
gp.quicksum(cost[i] * x[i] for i in I) and model.setObjective(obj, GRB.MINIMIZE),
the expression for objective function is cost[i] * x[i]. Outermost wrappers of the form
quicksum( . ) or sum( . ) are dropped prior to comparison.
The implementation considers addVars, addConstr, and setObjective( ·, GRB.MINIMIZE) only.
Normalizations and equivalence: We use three normalizations:
1. Index normalization, Nidx: we replace any bracketed index by a placeholder, [·] 7→[idx], and
normalize simple generator phrases (e.g., for k in range(...)).
2. Variable-centric structural normalization, Nvar( · ; υ): given a target symbol υ, we map υ7→_X_
and every other symbol to placeholders _Y0_, _Y1_, . . . deterministically by first occurrence.
3. Relational residualization, R(·): for a string r of the form lhs <= rhs, lhs >= rhs, or lhs
== rhs, we define
R(r) =











lhs −rhs,
r : lhs ≤rhs,
rhs −lhs,
r : lhs ≥rhs,
lhs −rhs,
r : lhs = rhs,
r,
otherwise.
Using SymPy’s sympify and simplify, we declare:
(structural equivalence)
Nvar
 Nidx(s1); υ1

= Nvar
 Nidx(s2); υ2

,
(algebraic equivalence)
R
 Nvar(Nidx(s1); υ1)

and R
 Nvar(Nidx(s2); υ2)

simplify to equal forms.
We also use whitespace-insensitive equality as a tie-breaker on normalized strings.
21


--- Page 22 ---
Per-variable pair score: For each (g, t)∈V G×V T we compute following score:
S(g, t) = Svar(g, t) + Scons(g, t) + Sobj(g, t) + Sfreq(g, t) + Sterm(g, t),
With the following components:
1. Argument match, Svar ∈{0, 1, 2, 3}: for each k ∈{lb, ub, vtype}, we add 1 if the sub-string k
appears in both argsG
g and argsT
t and these sub-strings are identical.
2. Constraint structure, Scons ∈Z≥0: for every cT ∈CT containing t and every cG ∈CG containing
g, we set ˜cT = Nvar(Nidx(cT); t), ˜cG = Nvar(Nidx(cG); g). Then, we add





+2
if ˜cT = ˜cG,
+2
if R(˜cT) and R(˜cG) simplify to equal forms,
+2
if rmsp(˜cT) = rmsp(˜cG).
(All three checks are performed and accumulated independently. The operator rmsp(.) elim-
inates white-spaces).
3. Objective structure, Sobj ∈{0, 2, 4, 6}: if t occurs in OT and g occurs in OG, we define
˜OT = Nvar(Nidx(OT); t), ˜OG = Nvar(Nidx(OG); g).
Then, we add





+2
if ˜OT = ˜OG,
+2
if R( ˜OT) and R( ˜OG) simplify to equal forms,
+2
if remove-spaces( ˜OT) = remove-spaces( ˜OG).
4. Usage frequency, Sfreq ∈Z≥0: let occ(s, υ) be the exact token count of υ (bare or indexed) in
a string s. Then
Sfreq(g, t) = min
n
X
s∈CT∪OT
occ(s, t),
X
s∈CG∪OG
occ(s, g)
o
.
5. Term matching, Sterm ∈Z≥0: we define
T T(t) = {“coef ∗t[·]” subterms in OT}
and
T G(g) = {“coef ∗g[·]” subterms in OG}.
Then
Sterm(g, t) = 2 | T T(t) ∩T G(g) |.
Variable matching and renaming: we form all pairs (g, t) with scores S(g, t) and sort them in
descending order. Then, we build a partial one-to-one mapping µ : V G ⇀V T greedily by selecting
the highest-scoring pairs without reuse of g or t (pairs with zero scores are ignored). We call it
a partial injective or one-to-one mapping because there may be extra elements in V G which do
not match any element in V T or vice-versa. Then, we replace whole-word occurrences of each g
by µ(g) in generated_code, yielding aligned_generated_code. We sort longer names first. For
example, let’s assume the mapping x →demand, and x
′ →demand_i. If we replace x first, then
x
′ →demand
′, which is wrong.
22


--- Page 23 ---
Category-wise matching: we extract components from aligned_generated_code: bV G, bCG, and
bOG. Counts are defined as follows:
• Variables: Mvar =
 V T ∩bV G .
• Constraints: a true constraint cT ∈CT is matched if there exists ˆc∈bCG such that either
Nidx(cT) = Nidx(ˆc),
or, for some (g, t) ∈graph(µ),
Nvar(Nidx(cT); t) = Nvar(Nidx(ˆc); g),
or
R(·) of the two sides simplify to equal forms.
Let Mcons be the number of matched constraints.
• Objective: treat OT and bOG as singleton sets and apply the same logic to obtain Mobj ∈{0, 1}.
Let Tvar = |V T|, Tcons = |CT|, Tobj = |OT|. The percentages are
Pvar = 100 · Mvar
Tvar
,
Pcons = 100 · Mcons
Tcons
,
Pobj = 100 · Mobj
Tobj
.
The total matched percentage is Ptotal = 100 · Mvar+Mcons+Mobj
Tvar+Tcons+Tobj . Finally, let G = |bV G| + | bCG| + | bOG|
be the number of generated components, and let E be the number of generated components that
did not participate in any match; we report Pextra = 100 · E
G.
4. Solution and analysis
In this section, we analyze the solutions obtained from four recent ChatGPT models: GPT-
3.5-Turbo, GPT-3.5-16K, GPT-4, and GPT-4-Turbo, using the prompting methods listed in Table
3. The input problems are summarized in Table 4. We use zero temperature4 hyperparameter
throughout the experiments to limit superficial variability that would otherwise confound structural
comparison and soft scoring, following best practices in recent LLM benchmarks (e.g., [26, 61]). We
conducted 360 runs, i.e., a combination of nine problems, two instances for each problem, four
ChatGPT models, and five prompting methods, and 200 runs for analyzing the reproducibility of
results for electricity planning problem instances, i.e., a combination of two problem instances, four
GPT models, and five prompting methods, and 10 runs for each combination.
Parts of the implementation code build upon open-source code from the publication [61]. In
interpreting the results, we compare model and prompt performance only relative to one another.
All statements are therefore strictly relative to the specific models and prompting methods evaluated
in this paper, not absolute claims about broader performance.
4Temperature is a hyperparameter with a range in [0,2], influencing the randomness of the model. Lower temper-
atures result in more deterministic outputs, and higher temperatures increase randomness [1].
23


--- Page 24 ---
Table 4: Overview of stochastic problem used in the experiments.
Problem Name
Instances
Category
Adapted
Taken
Electricity planning
2
SMILP-2
[56]
Biweekly fleet size planning
2
SMILP-2
[56]
Weekly fleet size planning
2
SMILP-2
[56]
Equipment leasing
2
DMILP-2
[56]
Production planning
2
DMILP-2
[56]
Emergency supply
2
Joint chance-constrained
[23]
Production demand
2
Joint chance-constrained
[23]
Cold storage
2
Individual chance-constrained
[52]
Supply allocation
2
Individual chance-constrained
[42]
Figure 2 summarizes the average performance (across instances and problem categories) of the
four ChatGPT models across the problem categories.
The accuracy metric remained zero for all problem instances and categories across all prompting
and GPT models.
On average, GPT-4-Turbo achieved the best performance for SMILP-2 problems among GPT
models with the highest partial score, matching scores for variables and constraints, the least extra
element generation, and runtime error scores. Objective matching score remained zero for all GPT
models in this problem category. Three other GPT models showed fluctuating results in terms
of matching scores, but GPT-4 achieved the second-best scores in terms of variable matching and
partial scores.
The GPT-3.5-Turbo-16k performs better than the other models on individual chance-constrained
problems in terms of partial score, variable, and constraints matching scores. However, the partial
score remains below 20% for this problem category across all GPT models.
On average, GPT-4-Turbo achieved the best performance for DMILP-2 problems among GPT
models with the highest partial score, matching scores for variables and constraints, and the least
extra element generation score. GPT-4 performed almost as well as GPT-4-Turbo, with a lower
constraint matching score and a higher objective matching score.
Again, on average, GPT-4-Turbo outperformed other models on joint chance constrained models
in terms of partial score, objective, and constraints matching scores.
GPT-4 scored highest in
variable matching.
Across categories, the lowest extra element generation rates vary by model. For DMILP-2, the
lowest rate is achieved by GPT-4-Turbo. In the individual chance-constrained category, GPT-3.5-
Turbo-16k performs best. The joint chance-constrained category shows the lowest extra element
generation levels with GPT-3.5-Turbo. For SMILP-2, GPT-4-Turbo offers the most reliable behav-
ior.
According to Figure 3, the prompting methods exhibit clear differences in modeling quality.
On average, agentic outperforms other methods in terms of partial score and constraint matching
scores, with the highest extra element generation score (negative effect) on the SMILP-2 problem
category.
On the DMILP-2 problem category, cot_s achieves the highest partial and variable
matching scores. Highest matching scores for objective and constraints obtained by agentic and
cot_s2, respectively.
As for individual chance-constrained problems, cot_instructions outperforms among models
with the highest partial, variable, and objective matching scores, while agentic, as second second-
best model, achieves the highest constraint matching score. Similarly, on joint chance-constrained
models, these two prompting methods achieved the highest partial scores and variable matching
24


--- Page 25 ---
score. In particular, cot_s2 outperformed agentic in constraint matching score, and cot_s achieved
the highest objective matching score among others.
Finally, the standard_s baseline performs substantially worse than all other methods, with low
matching scores throughout. In many runs, this prompt did not return a complete Python code,
which resulted in low extra-element generation rates.
Figure 2: Average performance metrics of ChatGPT models across problem categories.
Figure 4 reports instance-level performance across the four problem categories. The variation
in results is less than 5% in all problem instances and metrics except for runtime error and compile
error. Runtime and compile errors’ variation is most pronounced for DMILP-2 and joint chance-
constrained instances.
Figure 5 reports the average performance of the prompting methods on all problem instances and
across all metrics. The agentic prompting method achieves the highest partial and matching scores
across all problem instances on average. It achieves the second-lowest extra-element generation rate
among prompting methods.
It is to be mentioned that we do not consider the extra element
generation score of standard_s, as in most runs, it did not generate model code output, thereby
having the smallest score among prompting methods. The second-best prompting method is the
cot_instructions with the lowest extra element generation score.
The cot_s_instructions method achieves the highest variable-matching score, though it also
produces a relatively high amount of extra elements. The cot_s2 method shows a balanced pattern,
with steady partial scores and moderate objective matching.
The agentic approach performs
reasonably well, with higher partial scores and a lower compilation error rate compared to the other
structured prompts. cot_s2 and cot_s performed similarly with cot_s being better in objective
matching score and cot_s2 outperforming in constraint matching score.
25


--- Page 26 ---
In contrast, standard_s has the weakest performance. It obtains the lowest matching scores
and partial scores. Overall, complex prompting methods provided better outputs in comparison to
simpler prompting methods.
Figure 3: Average performance metrics of prompting methods across problem categories.
Figure 4: Average metrics by problem category instance.
26


--- Page 27 ---
Figure 5: Comparison of average performance metrics across different prompting methods.
Figure 6 shows the average performance of GPT models on all problem instances and across
all metrics. The GPT-4-Turbo model performs the best overall, except for the average variable
matching score which GPT-4 is the best.
It reaches the highest partial score and the highest
objective matching. The GPT-4 model performs moderately well, though it produces more extra
elements than GPT-4-Turbo.
The two GPT-3.5 variants perform weaker on most metrics.
They show lower partial and
matching scores and generate more extra elements compared to the GPT-4 and GPT-4-Turbo
models. However, they show similar scores on extra element generation.
Figure 6: Average metrics per GPT model.
27


--- Page 28 ---
Figure 7 shows the consistency of the prompting methods across ten runs, averaged over the
SMILP-2 instances of the electricity planning problem. Variable matching and partial scores re-
main relatively stable over runs, with cot_s_instructions and cot_s showing the most consistent
behavior with less than 10% variation, while agentic demonstrates variations above 20% in runs
seven and 10.
Objective matching stays zero in all runs for all prompting methods. Constraint matching is
highly unstable and often variations pass 20% in a number of runs for agentic and cot_instructions.
Figure 7: Prompting method performance trends across runs (averaged over instances).
Figure 8 shows the stability of the GPT models across ten runs, averaged over the electricity
planning SMILP-2 instances. GPT-4-Turbo and GPT-4 remain stable in partial score and variable
matching scores with only small variations across runs.
However, they show high variations in
constraint matching scores.
GPT-3.5 and GPT-3.5-Turbo-16k maintain a consistent partial score and constraint matching
score, but show noticeable variation in variable matching score.
Figure 9 reports the reproducibility of several metrics across ten runs for two instances of the
electricity planning SMILP-2 problem. Partial score remains relatively stable for both instances,
with only fluctuations of less than 10% across runs.
Variable matching is also consistent with
variations less than 10% except for runs seven and 10. Extra element generation shows moderate
variation but follows similar patterns across the two instances. Constraint matching remains below
20% in all runs for both instances.
Objective matching and accuracy remained zero for both
instances in all runs. The extra element generation score also remained consistent across runs for
both instances, with higher fluctuations for the second instance, especially in run five. The error
metrics display larger discrepancies between instances. Both instances exhibit high runtime and
compile error rates depending on the runs.
In summary, the results show that partial score, variable, and constraint matching scores are
comparatively reproducible across runs.
28


--- Page 29 ---
Figure 8: Model-wise performance trends across runs (averaged over instances).
Figure 9: Metric reproducibility across runs for two instances of the SMILP-2 problem.
29


--- Page 30 ---
4.1. Discussion on the results
4.1.1. Limitations
While LLMs show the ability to translate verbal problem descriptions into formal SO formula-
tions, they face notable limitations when applied to complex instances. We were unable to generate
fully correct and executable code formulations.
Despite achieving a range of structural and al-
gebraic similarity scores, many outputs contained syntax errors, incomplete solver definitions, or
invalid variable declarations that led to compile or runtime errors (e.g., Figure 4).
Multi-stage stochastic programs (more than two stages) remain beyond the current scoring
system scope.
Furthermore, correct formulations with different algebraic formulations can still
hold true for an optimization problem, in which case, matching scores will be underrepresented.
Additionally, throughout our experiments, we observed that indices, coefficients, and variables often
employ text-intensive indexing and naming, which requires more sensitive parsing of model elements
to avoid underestimation of scores.
Third, reproducibility of results remains an ongoing challenge. Due to Figure 9, different runs
of the same instance or different instances of a problem can show low or high fidelity depending on
components of the problem.
4.1.2. Real-world applicability of our method and an example of an SMILP-2 supply chain model
Our framework enables real-world industries to model and evaluate SO problems using NL
descriptions alone rapidly. This is particularly valuable in sectors such as supply chains and trans-
portation, where decisions must be made under uncertainty and under time pressure. Our soft
scoring metric allows practitioners to quantitatively evaluate LLM-generated implementation code
even when exact syntax or notation varies to some extent. It measures structural correctness, vari-
able and objective matching, and highlights errors like extra elements or constraint mismatches.
This enables transparent model validation. Additionally, it enables the identification and fixation
of the errors in the generated code quickly by spotting the deficient parts of the code.
We take an SMILP-2 model with adaptations from [50] for a supply chain network. [50] imple-
mented the model for a company shipping cardboard packages to breweries and soft-drink producers,
adapted from [14]. Similar problems to this case can be found in recent literature (e.g., [5, 24]).
Let P be the set of potential facilities (e.g., plants, finishers, or warehouses) where in- and
outflow are allowed for each facility in this set, S ⊂P be the set of suppliers, C be the set of
customer nodes. The A ⊆(P × P) ∪(P × C) is the set of feasible arcs such that shipments within
facilities, and from facilities to customers are allowed, K, is the product set, and Ωis a finite scenario
set with probability pω for scenario ω ∈Ω.
Fixed opening costs for facilities are ci, i ∈P. Scenario ω provides transportation and processing
costs qω
ijk for product k ∈K shipped on arc (i, j), supply limits sω
ik of product k at supplier i ∈S,
demand of product dω
jk and shortage penalties hω
jk of product k at customer j ∈C.
Binary variables yi indicate which facilities are opened, i.e., yi = 1, if facility i is opened and
0 otherwise. For each scenario ω ∈Ω, the model chooses flows xω
ijk, i.e., flow of product k on arc
(i, j) under scenario ω, and shortage quantities zω
jk, i.e., shortage of product k at customer j under
scenario ω. uω
ik is the injected supply of product k by supplier i to the supply network at scenario
ω. The compact SMILP-2 formulation is
min
y
X
i∈P
ciyi + E

Q(y, ω)

s.t.
yi ∈{0, 1}, ∀i ∈P,
30


--- Page 31 ---
where the recourse function Q(y, ω) is
Q(y, ω) :=
min
xω, zω,uω
X
k∈K
X
(i,j)∈A
qω
ijkxω
ijk +
X
j∈C,k∈K
hω
jkzω
jk
s.t.
X
i:(i,j)∈A
xω
ijk −
X
l:(j,l)∈A
xω
jlk = 0, ∀j ∈P\S, ∀k ∈K(flow conservation at non-suppliers),
X
i:(i,j)∈A
xω
ijk + uω
jk −
X
l:(j,l)∈A
xω
jlk = 0, ∀j ∈S, ∀k ∈K(flow conservation at suppliers),
uω
ik ≤sω
ikyi, ∀i ∈S, k ∈K(injected supply availability),
X
i:(i,j)∈A
xω
ijk + zω
jk ≥dω
jk, ∀j ∈C, ∀k ∈K(demand satisfaction with allowed shortage),
X
j:(i,j)∈A
xω
ijk ≤M · yi, ∀i ∈P, ∀k ∈K, (no outflow for un-chosen supplier),
X
i:(i,j)∈A
xω
ijk ≤M · yj, ∀j ∈P, ∀k ∈K, (no inflow for un-chosen supplier),
xω
ijk ≥0, ∀(i, j) ∈A,
zω
jk ≥0, ∀j ∈C, k ∈K
uω
ik ≥0, ∀i ∈P, k ∈K.
First-stage decisions select facilities. Second-stage decisions adapt flows and shortages to uncer-
tainty in demand, supply availability, and transportation costs. Here is the problem description we
ask from GPT-4-Turbo using agentic prompting method:
Problem description: supply chain SMILP-2 problem, adapted from [50]
Consider the following two-stage stochastic mixed-integer linear programming (SMILP-2) for
the design of a supply chain network under uncertainty:
The supply chain consists of a set of potential facilities P, a subset S ⊂P of supplier facilities,
a set of customer nodes C, and a set of feasible arcs A ⊆(P × P) ∪(P × C) through which
shipments within facilities and from facilities to customers may flow. The system handles a
set of products K, and uncertainty is represented through a finite scenario set Ω, where each
scenario ω ∈Ωoccurs with probability pω.
Each potential facility i ∈P has a fixed opening cost ci. In scenario ω, the uncertain data
include transportation and processing costs qω
ijk for shipping product k through arc (i, j) ∈A,
supply availability sω
ik at supplier i ∈S, customer demand dω
jk at j ∈C, and shortage penalties
hω
jk for unsatisfied demand at customer j ∈C.
First-stage decision variables yi indicate whether facility i is opened. After scenario ω is realized,
second-stage decisions include shipment flows xω
ijk, injected supplies uω
ik at supplier nodes, and
shortage amount zω
jk at customers. These second-stage decisions must satisfy flow conservation
at all facilities, supplier supply availability limits, demand satisfaction with allowed shortages,
and linking constraints that prevent any inflow or outflow through facilities that are not opened.
The objective is to minimize the sum of fixed facility opening costs and the expected second-
stage operational cost, which consists of transportation and processing costs and penalties for
unmet customer demand.
We include the generated model’s code in Listing 1. We conducted 15 runs and selected the run
with the highest partial score.
Table 5 summarizes its evaluation results.
31


--- Page 32 ---
Table 5: Evaluation metrics for GPT-4-Turbo generated implementation code.
Metric
Value (%)
Percent extra components in generated model
36.36
Percent match in variables
100.00
Percent match in constraints
0.00
Percent match in objective
0.00
Compile error rate
100.00
Runtime error rate
0.00
The evaluation shows that the LLM correctly identified all variables and index sets. This is
useful because variable and index set extraction are often a time-consuming part for non-experts.
The main issue appears in the objective function and constraint set. Putting indexing of variables
and sum loops in the objective function aside, the objective function correctly interprets elements
of expected costs to be minimized. However, the inconsistency in indexing variables and summing
over the wrong indices and loops resulted in a zero objective matching score, although index sets
and variable indices have been defined correctly in the first place. The performance of the metric
in the evaluation of these issues has been shown in 2.
As for constraints, structurally or algebraically, outflow and inflow constraints are wrong. This
is again due to the wrong indexing of variables and the misinterpretation of outflow and inflow logic.
Regarding flow conservation at suppliers, again generated code uses inconsistent feasible arc syntax
and references the wrong variable keys. However, if the metric were index-agnostic, i.e., masking
indices completely, this constraint set would match the true constraint set, but in this version, it is
very sensitive to such inconsistencies. This applies to demand satisfaction constraints as well, where
wrong scenario data indexing has happened in the generated code. For supply limits constraints, the
generated code misses supplier capacity limit parameters in addition to wrong indexings. Finally,
inflow and outflow gating constraints for chosen and unchosen facilities have been conceptually
modeled differently from the true code. Although indexing is wrong for this constraint set in the
generated code, the metric will not be able to capture algebraic equivalence for this constraint set.
For the industry, this means LLMs can help produce an initial model quickly, but the model
logic still needs expert review. The benefit is speed and accessibility; the limitation is reliability
in structurally complex parts of the formulation. A helpful reference on how to model SO models
applied in industry is [22]. LLMs can be trained and fine-tuned on such resources to effectively
model SO problems.
Listing 1: GPT-4-Turbo generated Gurobi model for the SMILP-2 supply chain instance.
from gurobipy import Model, GRB, quicksum
def Industrial_example(P, S, C, K, A, c, scenarios, M):
model = Model("SMILP2_Extensive")
model.setParam("OutputFlag", 0)
model.setParam("MIPGap", 0.01)
# Setting a MIP gap for early stopping if the gap is
small enough
# Sets and indices
P = list(P)
# Potential facilities
S = list(S)
# Supplier facilities
C = list(C)
# Customer nodes
K = list(K)
# Products
A = [tuple(e) for e in A]
# Feasible arcs
32


--- Page 33 ---
W = range(len(scenarios))
# Scenarios
# Variables
y = model.addVars(P, vtype=GRB.BINARY, name="y")
# Facility opening decisions
x = model.addVars(A, K, W, vtype=GRB.CONTINUOUS, name="x")
# Shipment flows
u = model.addVars(S, K, W, vtype=GRB.CONTINUOUS, name="u")
# Injected supplies
z = model.addVars(C, K, W, vtype=GRB.CONTINUOUS, name="z")
# Shortage amounts
# Objective function
model.setObjective(
quicksum(c[i] * y[i] for i in P) +
quicksum(scenarios[w][’probability’] * (
quicksum(scenarios[w][’q’][i, j, k] * x[i, j, k, w] for i, j, k in A if (i,
j) in A) +
quicksum(scenarios[w][’h’][j, k] * z[j, k, w] for j in C for k in K)
) for w in W),
GRB.MINIMIZE
)
# Constraints
# Flow conservation at non-supplier facilities
model.addConstrs(
(quicksum(x[i, j, k, w] for i, j in A if j == p) == quicksum(x[j, i, k, w] for
j, i in A if j == p)
for p in P if p not in S for k in K for w in W),
name="flow_conservation_non_suppliers"
)
# Flow conservation at supplier facilities
model.addConstrs(
(quicksum(x[i, j, k, w] for i, j in A if j == s) + u[s, k, w] == quicksum(x[j,
i, k, w] for j, i in A if j == s)
for s in S for k in K for w in W),
name="flow_conservation_suppliers"
)
# Demand satisfaction
model.addConstrs(
(quicksum(x[i, j, k, w] for i, j in A if j == c) + z[c, k, w] >=
scenarios[w][’d’][c, k]
for c in C for k in K for w in W),
name="demand_satisfaction"
)
# Supply limits
model.addConstrs(
(u[s, k, w] <= scenarios[w][’s’][s, k] * y[s] for s in S for k in K for w in W),
name="supply_limits"
)
# Linking constraints (both from and to facilities)
model.addConstrs(
(x[i, j, k, w] <= M * y[i] for i, j, k in A for w in W),
name="linking_constraints_from"
33


--- Page 34 ---
)
model.addConstrs(
(x[j, i, k, w] <= M * y[i] for i, j, k in A for w in W),
name="linking_constraints_to"
)
# Optimize the model
try:
model.optimize()
if model.Status == GRB.OPTIMAL:
return model.ObjVal
else:
print("Optimization was stopped with status:", model.Status)
except Exception as e:
print("An error occurred during optimization:", str(e))
return None
# Example usage
P = [’Facility1’, ’Facility2’]
S = [’Facility1’]
C = [’Customer1’, ’Customer2’]
K = [’Product1’, ’Product2’]
A = [(’Facility1’, ’Customer1’), (’Facility1’, ’Customer2’), (’Facility2’, ’Customer1’)]
c = {’Facility1’: 1000, ’Facility2’: 1500}
scenarios = [
{’probability’: 0.5, ’q’: {(’Facility1’, ’Customer1’, ’Product1’): 2, (’Facility1’,
’Customer2’, ’Product2’): 3},
’h’: {’Customer1’: {’Product1’: 10}, ’Customer2’: {’Product2’: 15}},
’d’: {’Customer1’: {’Product1’: 30}, ’Customer2’: {’Product2’: 20}},
’s’: {’Facility1’: {’Product1’: 40, ’Product2’: 50}}
}
]
M = 10000
optimal_cost = Industrial_example(P, S, C, K, A, c, scenarios, M)
print("Optimal expected total cost:", optimal_cost)
Listing 2: GPT-4-Turbo generated Gurobi model for the SMILP-2 supply chain instance evaluation.
Mapping (generated -> true): {’x’: ’x’, ’y’: ’y’, ’z’: ’z’, ’u’: ’u’}
Matched variables:
4/4 -> [’u’, ’x’, ’y’, ’z’]
Extra generated vars (0): []
Matched constraints: 0/6
Constraint matches (true -> gen): (none)
TRUE constraints (6):
[T1] UNMATCHED: quicksum(x[a, k, w] for a in inP[p]) + u[S_index[p], k, w] -
quicksum(x[a, k, w] for a in outP[p]) == 0
[T2] UNMATCHED: u[i, k, w] <= scenarios[w]["s"][i][k] * y[P_index[S[i]]]
[T3] UNMATCHED: quicksum(x[a, k, w] for a in inP[p]) - quicksum(x[a, k, w] for a in
outP[p]) == 0
[T4] UNMATCHED: quicksum(x[a, k, w] for a in outP[p]) <= M * y[P_index[p]]
[T5] UNMATCHED: quicksum(x[a, k, w] for a in inC[C[j]]) + z[j, k, w] >=
scenarios[w]["d"][j][k]
34


--- Page 35 ---
[T6] UNMATCHED: quicksum(x[a, k, w] for a in inP[p]) <= M * y[P_index[p]]
GEN constraints (6):
[G1] UNMATCHED: quicksum(x[j, i, k, w] for j in P if (j, i) in A) <= M * y[i]
[G2] UNMATCHED: u[i, k, w] <= scenarios[w][’s’][i, k] * y[i]
[G3] UNMATCHED: quicksum(x[i, j, k, w] for j in P if (i, j) in A) <= M * y[i]
[G4] UNMATCHED: quicksum(x[i, j, k, w] for i, j in A if j == node) == quicksum(x[j, i,
k, w] for i, j in A if i == node)
[G5] UNMATCHED: quicksum(x[i, j, k, w] for i, j in A if j == node) + u[node, k, w] ==
quicksum(x[j, i, k, w] for i, j in A if i == node)
[G6] UNMATCHED: quicksum(x[i, j, k, w] for i, j in A if j == node) + z[node, k, w] >=
scenarios[w][’d’][node, k]
Matched objective:
0/1
Objective match (true -> gen): (none)
Unmatched TRUE objective(s):
TRUE: c[i] * y[i] for i in range(len(P))) + quicksum( scenarios[w]["probability"] * (
quicksum( scenarios[w]["q"][a][k] * x[a, k, w] for a in range(len(A)) for k in
range(len(K)) ) + quicksum( scenarios[w]["h"][j][k] * z[j, k, w] for j in
range(len(C)) for k in r...
Unmatched GEN objective(s):
GEN : c[i] * y[i] for i in P) + quicksum(scenarios[w][’probability’] * (
quicksum(scenarios[w][’q’][i, j, k] * x[i, j, k, w] for i, j, k in A for k in K if
(i, j, k) in scenarios[w][’q’]) + quicksum(scenarios[w][’h’][j, k] * z[j, k, w] for
j in C for k in K if (j...
Partial score (eval1): 36.36%
Scores: P_total=36.36% P_extra=63.64% P_var=100.00% P_cons=0.00% P_obj=0.00%
Passed: 0/1
Accuracy: 0.00%
Partial score (eval1 fallback): 36.36
>> percent_extra_in_generated: 63.64%
>> percent_match_in_variables: 100.00%
>> percent_match_in_constraints: 0.00%
>> percent_match_in_objective: 0.00%
Compile error: 100.00%
Runtime error: 0.00%
5. Conclusion
Recent advances in large language models (LLMs) have created new opportunities to automate
model formulation in operations research. While most prior studies have focused on deterministic
(mixed-integer) linear programming models, this paper presents a systematic investigation of LLMs
applied to Stochastic Optimization (SO), encompassing joint and individual chance-constrained
programs, two-stage stochastic mixed integer linear programs (SMILP-2), and their deterministic
mixed-integer linear programming (DMILP-2) counterparts.
A suite of customized prompting strategies, combining stepwise reasoning, chain-of-thought
(CoT), and multi-agent (agentic) approaches, was designed to capture probabilistic feasibility in
chance-constrained models and recourse logic in SMILP-2.
A soft scoring metric was proposed to quantify algebraic and structural equivalence, addressing
notational variation and permutation invariance that conventional metrics overlook. In addition,
a standardized benchmark of natural language problem descriptions, input instances, and ground-
truth Gurobi code was developed to ensure reproducibility and conduct experiments on SO problems.
35


--- Page 36 ---
Extensive experiments across GPT-3.5, GPT-4, and GPT-4-Turbo variants revealed reasonable
performance patterns and error behaviors across prompt designs, providing the first quantitative
benchmark for LLM-based SO problem formulations. Experimental results show that the full ac-
curacy score was achieved by none of the GPT models or prompting methods. Nevertheless, the
proposed scoring system paved the way to identify performance discrepancies among GPT mod-
els and prompting methods in this paper. GPT-4-Turbo outperformed all other models in most
cases. It attained the highest partial scores for SMILP-2, DMILP-2, and joint chance-constrained
problems, while GPT-3.5-Turbo-16k performed best on individual chance-constrained models.
Structured prompting improved formulation quality, with the agentic and cot_instructions
strategies achieving the best matching performance, whereas the standard baseline prompt fre-
quently failed to produce complete or reliable results.
Future research will focus on extending this framework in several important directions:
The whole proposed framework can be expanded to include a broader range of SO problems as
well as contextual SO problems, distributionally robust and chance-recourse hybrids models, with
a framework to customize structured prompts in accordance.
In particular, we demonstrated that variants of prompts exhibit different performance depend-
ing on the problem type. There is a need to develop a tool that automatically analyzes a problem
and renders the best prompts customized to that problem. Indeed, a prompt design problem re-
quires rigorous probabilistic analysis based on the stochastic behavior of LLMs embedded inherently
in them. Hence, developing frameworks for designing efficient multi-agent structures can further
improve the results. For instance, in our multi-agent framework, designing the architecture inter-
actions and deciding the number of independent agents to maximize the performance is essential.
Finally, the soft scoring can be enhanced to evaluate multi-stage stochastic programs and capture
recursive decision structures. It can also be further developed to neutralize the effect of indexing
errors in analyzing constraint and objective function correctness.
6. Declaration of competing interest
Authors affirm that there are no identifiable conflicting financial interests or personal associations
that could be perceived as impacting the work described in this article.
7. Data and code availability
Data and code are available at https://github.com/Amirreza-96/LLM-SO-manuscript.
8. Acknowledgments
The authors thank Dr. Güzin Bayraksan for her insightful and constructive feedback.
References
[1] Chat completions api — openai platform, 2025.
Accessed:
2025-11-29.
URL: https:
//platform.openai.com/docs/api-reference/chat/create.
[2] Ali AhmadiTeshnizi, Wenzhi Gao, Herman Brunborg, Shayan Talaei, Connor Lawless, and
Madeleine Udell. Optimus-0.3: Using large language models to model and solve optimization
problems at scale, 2025. URL: https://arxiv.org/abs/2407.19633, arXiv:2407.19633.
36


--- Page 37 ---
[3] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. Optimus: Scalable optimization mod-
eling with (MI) LP solvers and large language models. arXiv preprint arXiv:2402.10172, 2024.
[4] Tasnim Ahmed and Salimur Choudhury. Lm4opt: Unveiling the potential of large language
models in formulating mathematical optimization problems. INFOR: Information Systems and
Operational Research, 62(4):559–572, 2024.
[5] Masoud Alinezhad, Iraj Mahdavi, Milad Hematian, and Erfan Babaee Tirkolaee.
A fuzzy
multi-objective optimization model for sustainable closed-loop supply chain network design in
food industries. Environment, Development and Sustainability, 24(6):8779–8806, 2022.
[6] Arash Asadpour, Xuan Wang, and Jiawei Zhang.
Online resource allocation with limited
flexibility. Management Science, 66(2):642–666, 2020.
[7] Dimitris Bertsimas and Georgios Margaritis. Robust and adaptive optimization under a large
language model lens. arXiv preprint arXiv:2501.00568, 2024.
[8] John R Birge and Francois Louveaux. Introduction to stochastic programming. Springer, 1997.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models
are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[10] Rossana Cavagnini, Francesca Maggioni, Luca Bertazzi, and Mike Hewitt. A two-stage stochas-
tic programming model for bike-sharing systems with rebalancing. EURO Journal on Trans-
portation and Logistics, 13:100140, 2024.
[11] Hao Chen, Gonzalo E Constante-Flores, and Can Li. Diagnosing infeasible optimization prob-
lems using large language models. INFOR: Information Systems and Operational Research,
62(4):573–587, 2024.
[12] Yuwei Chen, Zengde Deng, Yinzhi Zhou, Zaiyi Chen, Yujie Chen, and Haoyuan Hu. An online
algorithm for chance constrained resource allocation, 2023. URL: https://arxiv.org/abs/
2303.03254, arXiv:2303.03254.
[13] Thai Dinh, Ricardo Fukasawa, and James Luedtke. Exact algorithms for the chance-constrained
vehicle routing problem. Mathematical Programming, 172(1):105–138, 2018.
[14] Koray Dogan and Marc Goetschalckx.
A primal decomposition method for the integrated
design of multi-period production-distribution systems. Iie Transactions, 31(11):1027–1036,
1999.
[15] Gianpaolo Ghiani, Emanuele Manni, and Sandro Zacchino.
Improving adaptability in
optimization-based decision support systems through large language models. IEEE Access,
2025.
[16] Ge Guo, Luckny Zephyr, Jose Morillo, Zongjie Wang, and C Lindsay Anderson.
Chance
constrained unit commitment approximation under stochastic wind energy. Computers & Op-
erations Research, 134:105398, 2021.
[17] Mahmood Hegazy. Diversity of thought elicits stronger reasoning capabilities in multi-agent
debate frameworks, 2025. URL: https://arxiv.org/abs/2410.12853, arXiv:2410.12853.
37


--- Page 38 ---
[18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models, 2021.
URL: https://arxiv.org/abs/2106.09685, arXiv:2106.09685.
[19] Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng
Yao, and Jinsong Su. Mitigating catastrophic forgetting in large language models with self-
synthesized rehearsal, 2024. URL: https://arxiv.org/abs/2403.01244, arXiv:2403.01244.
[20] Yuxiao Huang, Shenghao Wu, Wenjie Zhang, Jibin Wu, Liang Feng, and Kay Chen Tan.
Autonomous multi-objective optimization using large language model. IEEE Transactions on
Evolutionary Computation, pages 1–1, 2025. doi:10.1109/TEVC.2025.3561001.
[21] Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, and Yang Yu.
Llmopt: Learning to define and solve general optimization problems from scratch.
arXiv
preprint arXiv:2410.13213, 2024.
[22] Alan J King and Stein W Wallace. Modeling with stochastic programming, volume 4. Springer,
2012.
[23] Willem K Klein Haneveld, Maarten H van der Vlerk, Ward Romeijnders, Willem K Klein Han-
eveld, Maarten H van der Vlerk, and Ward Romeijnders.
Chance constraints.
Stochastic
Programming: Modeling Decision Problems Under Uncertainty, pages 115–138, 2020.
[24] Esra Koca.
Two-stage stochastic facility location problem with disruptions and restricted
shortages. Computers & Industrial Engineering, 183:109484, 2023.
[25] Akshit Kumar, Tianyi Peng, Yuhang Wu, and Assaf Zeevi.
Performance of LLMs on
stochastic modeling Operations Research problems: From theory to practice. arXiv preprint
arXiv:2506.23924, 2025.
[26] Jeong Hyun Lee and Jaeseung Shin. How to optimize prompting for large language models in
clinical research. Korean Journal of Radiology, 25(10):869, 2024.
[27] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
Ghanem. Camel: Communicative agents for "mind" exploration of large language model soci-
ety, 2023. URL: https://arxiv.org/abs/2303.17760, arXiv:2303.17760.
[28] Qingyang Li, Lele Zhang, and Vicky Mak-Hau. Synthesizing mixed-integer linear programming
models from natural language descriptions, 2023. URL: https://arxiv.org/abs/2311.15271,
arXiv:2311.15271.
[29] Rui Li, Ling Wang, Hongyan Sang, Lizhong Yao, and Lijun Pan. LLM-assisted automatic
memetic algorithm for lot-streaming hybrid job shop scheduling with variable sublots. IEEE
Transactions on Evolutionary Computation, 2025.
[30] Xing Linzi, Xinglu Wang, Fan Zhenan, Feng Yuxi, Jing Xiong, Zhijiang Guo, FU Xiaojin,
Rindranirina Ramamonjison, Mahdi Mostajabdaveh, Han Xiongwei, et al. Systems and meth-
ods for towards human-aligned evaluation for auto-formulating optimization modeling with
large language models, April 24 2025. US Patent App. 18/381,239.
[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Ad-
vances in neural information processing systems, 36:34892–34916, 2023.
38


--- Page 39 ---
[32] Ollie Liu, Deqing Fu, Dani Yogatama, and Willie Neiswanger. Dellma: Decision making under
uncertainty with large language models. arXiv preprint arXiv:2402.02392, 2024.
[33] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models
as evolutionary optimizers. In 2024 IEEE Congress on Evolutionary Computation (CEC), pages
1–8. IEEE, 2024.
[34] James Luedtke and Shabbir Ahmed. A sample approximation approach for optimization with
probabilistic constraints. SIAM Journal on Optimization, 19(2):674–699, 2008.
[35] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,
Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refine-
ment with self-feedback. Advances in Neural Information Processing Systems, 36:46534–46594,
2023.
[36] Francesca Maggioni, Florian A Potra, and Marida Bertocchi.
A scenario-based framework
for supply planning under uncertainty: stochastic programming versus robust optimization
approaches. Computational Management Science, 14(1):5–44, 2017.
[37] Mahdi Mostajabdaveh, Timothy T Yu, Rindranirina Ramamonjison, Giuseppe Carenini, Zirui
Zhou, and Yong Zhang. Optimization modeling and verification from problem specifications
using a multi-agent multi-stage LLM framework. INFOR: Information Systems and Operational
Research, 62(4):599–617, 2024.
[38] OpenAI. ChatGPT. https://chat.openai.com/, 2025. Accessed: 2025-04-25.
[39] Bizhao Pang, Kin Huat Low, and Vu N Duong. Chance-constrained uam traffic flow optimiza-
tion with fast disruption recovery under uncertain waypoint occupancy time. Transportation
Research Part C: Emerging Technologies, 161:104547, 2024.
[40] Ganesh Prasath and Shirish Karande. Synthesis of mathematical programs from natural lan-
guage specifications, 2023. URL: https://arxiv.org/abs/2304.03287, arXiv:2304.03287.
[41] András Prékopa. Stochastic programming, volume 324. Springer Science & Business Media,
2013.
[42] Feng Qiu, Shabbir Ahmed, Santanu S Dey, and Laurence A Wolsey. Covering linear program-
ming with violations. INFORMS Journal on Computing, 26(3):531–546, 2014.
[43] Rindranirina Ramamonjison, Haley Li, Timothy T Yu, Shiqi He, Vishnu Rengan, Amin
Banitalebi-Dehkordi,
Zirui Zhou,
and Yong Zhang.
Augmenting Operations Research
with auto-formulation of optimization models from problem descriptions.
arXiv preprint
arXiv:2209.15565, 2022.
[44] Rindranirina Ramamonjison, Haley Li, Timothy T. L. Yu, Shiqi He, Vishnu Rengan, Amin
Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang.
Augmenting Operations Research with
auto-formulation of optimization models from problem descriptions. ArXiv, abs/2209.15565,
2022. URL: https://api.semanticscholar.org/CorpusID:252668654.
[45] Rindranirina Ramamonjison, Timothy T. L. Yu, Raymond Li, Haley Li, Giuseppe Carenini,
Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi, Zirui Zhou,
and Yong Zhang.
Nl4opt competition: Formulating optimization problems based on their
39


--- Page 40 ---
natural language descriptions. In Neural Information Processing Systems, 2023. URL: https:
//api.semanticscholar.org/CorpusID:257532906.
[46] Sumedh Rasal and EJ Hauer. Optimal decision making through scenario simulations using
large language models. arXiv preprint arXiv:2407.06486, 2024.
[47] Silvia Araújo dos Reis, José Eugenio Leal, and Antônio Márcio Tavares Thomé. A two-stage
stochastic linear programming model for tactical planning in the soybean supply chain. Logis-
tics, 7(3):49, 2023.
[48] Oscar Rodríguez-Espíndola. Two-stage stochastic formulation for relief operations with multiple
agencies in simultaneous disasters. OR Spectrum, 45(2):477–523, 2023.
[49] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman
Chadha. A systematic survey of prompt engineering in large language models: Techniques and
applications. arXiv preprint arXiv:2402.07927, 2024.
[50] Tjendera Santoso, Shabbir Ahmed, Marc Goetschalckx, and Alexander Shapiro. A stochastic
programming approach for supply chain network design under uncertainty. European Journal
of Operational Research, 167(1):96–115, 2005.
[51] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic pro-
gramming: modeling and theory. SIAM, 2021.
[52] Alexander Shapiro and Andy Philpott. A tutorial on stochastic programming. Manuscript.
Available at www2. isye. gatech. edu/ashapiro/publications. html, 17, 2007.
[53] Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D Doan,
and Chandan K Reddy. LLM-srbench: A new benchmark for scientific equation discovery with
large language models. arXiv preprint arXiv:2504.10415, 2025.
[54] Kiyoharu Tagawa and Yukiko Orito. Prospect theory-based formulation of chance constrained
portfolio optimization problem using loan. SICE Journal of Control, Measurement, and System
Integration, 17(1):176–193, 2024.
[55] Zengyao Tian, Chunshan Li, Mingze Sun, and Shaowu Liu. Chance-constrained optimization
of multi-energy complementary power generation system considering multiple uncertainties.
In 2024 IEEE 4th International Conference on Electronic Technology, Communication and
Information (ICETCI), pages 531–536. IEEE, 2024.
[56] Huseyin Topaloglu.
Stochastic programming test problems.
Accessed: 2025-05-21.
URL:
https://people.orie.cornell.edu/huseyin/research/sp_datasets/sp_datasets.html.
[57] George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings,
Amitayush Thakur, and Swarat Chaudhuri. Putnambench: Evaluating neural theorem-provers
on the Putnam mathematical competition. arXiv preprint arXiv:2407.11214, 2024.
[58] Stein W Wallace and William T Ziemba. Applications of stochastic programming. SIAM, 2005.
[59] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171, 2022.
40


--- Page 41 ---
[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems, 35:24824–24837, 2022.
[61] Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han,
Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, et al. Chain-of-experts: When LLMs meet
complex Operations Research problems. In The twelfth international conference on learning
representations, 2023.
[62] Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean work-
book: A large-scale lean problem set formalized from natural language math problems. arXiv
preprint arXiv:2406.03847, 2024.
[63] Bowen Zhang and Pengcheng Luo.
OR-LLM-agent: Automating modeling and solving of
Operations Research optimization problem with reasoning large language model. arXiv preprint
arXiv:2503.10009, 2025.
[64] Yisong Zhang, Ran Cheng, Guoxing Yi, and Kay Chen Tan. A systematic survey on large
language models for evolutionary optimization: From modeling to solving.
arXiv preprint
arXiv:2509.08269, 2025.
41
