--- Page 1 ---
Optimal Resource Allocation for ML Model Training and
Deployment under Concept Drift
HASAN BURHAN BEYTUR, The University of Texas at Austin, USA
GUSTAVO DE VECIANA, The University of Texas at Austin, USA
HARIS VIKALO, The University of Texas at Austin, USA
KEVIN S CHAN, Devcom Army Research Laboratory, USA
We study how to allocate resources for training and deployment of machine learning (ML) models under
concept drift and limited budgets. We consider a setting in which a model provider distributes trained models
to multiple clients whose devices support local inference but lack the ability to retrain those models, placing
the burden of performance maintenance on the provider. We introduce a model-agnostic framework that
captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We
show that optimal training policies depend critically on the aging properties of concept durations. Under
sudden concept changes, we derive optimal training policies subject to budget constraints when concept
durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics
are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment
under communication constraints, prove that the associated optimization problem is quasi-convex under mild
conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance.
These results offer theoretical and algorithmic foundations for cost-efficient ML model management under
concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.
CCS Concepts: â€¢ Do Not Use This Code â†’Generate the Correct Terms for Your Paper; Generate the
Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your
Paper.
Additional Key Words and Phrases: Concept Drift, Resource Allocation, Model Training, Model Deployment,
MLOps, Optimal Control, Functional Optimization
ACM Reference Format:
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan. 2025. Optimal Resource Allocation
for ML Model Training and Deployment under Concept Drift. 1, 1 (December 2025), 25 pages. https://doi.org/
XXXXXXX.XXXXXXX
1
Introduction
The rapid advancement of machine learning (ML), particularly in generative image and large
language models (LLMs), is transforming how we carry out both professional and personal tasks
[17, 19, 24]. As such models grow in complexity, their training and inference require increasingly
more data and computational power, rendering their management resource-intensive. While these
demands have largely been met by centralized cloud infrastructure, there is a notable shift toward
Authorsâ€™ Contact Information: Hasan Burhan Beytur, The University of Texas at Austin, Austin, Texas, USA, hbbeytur@
utexas.edu; Gustavo de Veciana, The University of Texas at Austin, Austin, Texas, USA, deveciana@utexas.edu; Haris Vikalo,
The University of Texas at Austin, Austin, Texas, USA, hvikalo@ece.utexas.edu; Kevin S Chan, Devcom Army Research
Laboratory, USA, .
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM XXXX-XXXX/2025/12-ART
https://doi.org/XXXXXXX.XXXXXXX
, Vol. 1, No. 1, Article . Publication date: December 2025.
arXiv:2512.12816v1  [cs.LG]  14 Dec 2025


--- Page 2 ---
2
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
localized inference on end-user devices. This transition improves scalability by reducing reliance
on shared resources and enhances user privacy [7, 32, 42]. However, despite the rise of on-device
inference, training remains predominantly cloud-based due to its substantial computational and
coordination costs.
In real-world applications, ML model performance often degrades over time due to shifts in
underlying data distributions â€“ a phenomenon known as concept drift. Such drift may arise from
changes in the relationship between input features and target variables, evolving class distributions,
or the emergence of out-of-distribution data. For instance, a rental price predictor may lose accuracy
as market conditions evolve; a clinical model may degrade due to demographic changes; and an LLM
may fail to answer questions about recent events. In all cases, performance degradation stems from
a growing mismatch between training and deployment data [12, 14, 25]. Maintaining performance
under concept drift requires continuous monitoring, retraining, and redeployment â€“ an operational
challenge addressed by the field of MLOps [22, 31, 38]. These tasks add considerable overhead to an
already costly model development cycle. As ML adoption grows and models become more complex,
efficient resource allocation within MLOps workflows becomes critical for scalable and sustainable
deployment.
We consider a setting in which a service provider trains ML models and distributes them to
client devices for local inference. While clients are capable of performing inference, they often
lack the computational and data resources needed for retraining. As a result, the responsibility for
maintaining model accuracy under concept drift falls on the provider, who must judiciously allocate
limited resources for retraining and model updates as performance degrades. We focus on scenarios
involving sudden, discrete drift events and introduce a model-agnostic framework that captures how
performance depends jointly on training resource allocation and the dynamics of concept change.
Since client-side utility hinges on timely model refresh, we further study drift-aware deployment
strategies aimed at optimizing long-term average performance. Our results offer practical guidance
for designing resilient, resource-efficient ML systems that adapt to evolving data.
1.1
Related Works
Extensive empirical studies have investigated the trade-offs among compute, model size, and
data volume in ML training. A foundational work in this area [18] demonstrated that the cross-
entropy loss in autoregressive language models follows power-law relationships with model size,
dataset size, and total compute. These scaling laws, observed across several orders of magnitude,
offer simple prescriptions for allocating fixed compute budgets to achieve target performance.
Building on this, [15] showed that language models are most compute-efficient when model size
and training token count are scaled proportionally. This insight led to the development of Chinchilla
(70B parameters, 1.4T tokens), which outperforms much larger models under the same compute
constraints. Complementary work on learning curve modeling has enabled more cost-effective
training via early stopping and budget-aware learning rate schedules [10, 23, 39]. For example,
[10] proposed a probabilistic model to extrapolate learning curves and terminate unpromising
hyperparameter configurations early. Similarly, [23] provided empirical evidence that budget-aware
learning rate schedules can improve efficiency of training in resource-constrained settings. Inspired
by these insights into training-time efficiency, our work extends this line of research to include
deployment-time considerations in nonstationary environments. Specifically, we study how training
resource allocation and model deployment strategies affect performance under sudden concept
drift â€“ an important yet underexplored regime in the context of scaling laws and resource-aware
training.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 3 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
3
Concept drift challenges the standard assumption of stationary data distributions in ML. Seminal
surveys classify drift as either gradual (incremental) or sudden (abrupt) changes in data distribu-
tion [12, 25]. Gradual drift causes slow, predictable performance degradation and is commonly
addressed through sliding-window retraining [1], instance reweighting [21], or time-varying op-
timization frameworks [29, 37]. For example, [37] introduced a two-stage online algorithm that
tracks a time-varying optimum, providing theoretical bounds that link drift rate to the number of
required optimization stepsâ€”and, by extension, to resource demand. In contrast, sudden concept
drift, characterized by its abrupt and unpredictable nature, typically requires immediate model
replacement or resets triggered by change-point detection [5, 6, 27, 30, 34, 43]. Due to their distinct
dynamics, gradual and sudden drift require different mitigation strategies [14, 25]. While several
recent studies [27, 34, 43] have explored the trade-offs between model staleness and retraining
costs, they primarily target drift detection or updating strategies tailored to specific models or
domains. In all, existing approaches rarely consider how the statistical properties of drift events
influence elastic compute allocation and deployment scheduling â€“ gaps that our work addresses
through a general, model-agnostic framework.
Finally, our problem lies within the scope of MLOps â€“ the practice of reliably developing,
deploying, and maintaining machine learning systems at scale [22, 31, 38]. While MLOps primarily
aims to establish best practices for ML system operation, the growing complexity and escalating costs
of modern ML pipelines [9] have spurred interest in cost-aware and compute-efficient retraining
and deployment strategies [26, 33, 36, 40, 41]. These developments further underscore the need
for formal frameworks that link resource allocation to model performance under dynamic data
conditions â€“ the focus of our study.
1.2
Contributions
Motivated by the need to sustain performance in distributed ML systems operating under resource
constraints and non-stationary data conditions, we develop a model-agnostic framework that
captures the interplay between elastic resource allocation and sudden concept drift. Our main
contributions are:
(1) We propose a model-agnostic analytical framework that characterizes the performance of
both client-side and server-side models, capturing the effects of training resource allocation,
sudden concept drift, and deployment policy. The proposed framework enables formal analysis
of how these factors jointly influence model performance.
(2) We show that optimal training resource allocation policiesâ€”those that maximize the long-
term performance of server-side modelsâ€”are of the bang-bang control type, i.e., they switch
between extreme values. We prove that a single-switch front-loading policy is optimal if
and only if concept durations follow a Decreasing Mean Residual Life (DMRL) distribution.
We further demonstrate that intuitive allocation heuristics are sub-optimal under Increasing
Mean Residual Life (IMRL) concept duration distributions.
(3) We analyze the deployment scheduling problem under communication constraints and
establish quasi-convexity of the performance objective under mild conditions. We derive
necessary optimality conditions and introduce a randomized deployment policy that achieves
target deployment rates with theoretical performance guarantees.
Together, these contributions establish theoretical foundations and algorithmic principles for
designing resource-aware training and deployment policies in large-scale, real-world ML systems.
The remainder of the paper is organized as follows. Section 2 formalizes the system model,
including concept drift dynamics and performance metrics. Section 3 analyzes optimal training
resource allocation under budget constraints and characterizes how different aging properties of
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 4 ---
4
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
concept durations affect the resulting policies. Section 4 turns to deployment scheduling under
communication constraints and develops near-optimal, drift-aware strategies. Section 5 offers
concluding remarks.
2
A Framework for Resource Allocation under Sudden Concept Drift
We consider a system that consists of a central server, referred to as the ML service provider, that
possesses substantial computational resources and access to large-scale datasets, and multiple
client devices with limited local capabilities. While clients can perform inference using deployed
models, they cannot retrain or update them independently. The provider is therefore responsible
for monitoring model performance, retraining as necessary, and redeploying updated models in
response to distributional shifts that degrade accuracy on both the server and client sides. In this
work, we focus specifically on modeling and managing the effects of sudden concept drift, where
data distributions shift abruptly, causing immediate drops in model performance.
A modelâ€™s performance depends on several factors, including its complexity, the quality and
quantity of training data, and the computational resources allocated to training. As training pro-
gresses and model parameters are updated, performance typically improves. However, evaluation
metrics such as accuracy, F1-score, or loss fluctuate due to the stochastic nature of optimization
methods (e.g., stochastic gradient descent and its variants) and the limited size of evaluation datasets.
Consequently, these metrics offer only an approximate view of the modelâ€™s true performance on the
underlying data distribution. Under concept drift, where the distribution itself evolves, performance
assessment becomes even more challenging, motivating the need for more robust and interpretable
measures of model quality.
To address these challenges, our framework introduces an idealized performance metric, the
expected concept loss, defined as the modelâ€™s expected loss over the true data distribution correspond-
ing to the current concept. This loss is assumed to decrease monotonically during training, with
the rate and shape of improvement determined by the model architecture, optimization algorithm,
data characteristics, and allocated resources. Under sudden concept drift, the trajectory of expected
concept loss changes abruptly between concepts as the underlying distribution shifts. Let ğ‘‡ğ‘–denote
the start time of the ğ‘–th concept, and let ğ‘Œğ‘–represent its random duration, such that ğ‘‡ğ‘–+1 âˆ’ğ‘‡ğ‘–= ğ‘Œğ‘–
with E[ğ‘Œ] < âˆ. We model the evolution of the expected concept loss during concept ğ‘–by a random
function ğºğ‘–(Â·) : R+ â†’R+, drawn from a countable set G of convex, decreasing functions. Here,
ğºğ‘–(0) denotes the loss immediately following the onset of concept ğ‘–, and ğºğ‘–(ğ‘¡) describes the decay
in loss over time under unit resource allocation. The overall concept dynamics are captured by the
stochastic process ((ğ‘Œğ‘–,ğºğ‘–(Â·)) : ğ‘–âˆˆN), where the pairs (ğ‘Œğ‘–,ğºğ‘–(Â·)) are assumed to be independent
and identically distributed (i.i.d). Although this formulation primarily targets sudden drift, it also ac-
commodates mixed settings where sudden and gradual drifts coexist. In practical implementations,
a change detection algorithm would identify points of significant distributional shift, triggering
retraining when a change is detected. Our model abstracts this mechanism by capturing the timing
and magnitude of such discrete or sufficiently large gradual changes that warrant retraining.
The amount of training resources determines both how quickly a model learns and how well it
ultimately performs. In supervised learning over a fixed dataset, greater compute capacity enables
faster model parameter updates. In online learning, the training rate is limited by the data arrival
process, which can sometimes be accelerated, e.g., by increasing sensor frequency or acquiring
additional labels through crowdsourcing [2, 8, 16, 28]. Our framework unifies these settings under
the common notion of training resource allocation.
We assume that training resources can be adjusted dynamically over time. Let ğ‘’ğ‘–(ğ‘¡) : [0,ğ‘¦ğ‘–) â†’
[0, ğ‘€] denote the resource allocation during concept ğ‘–, where ğ‘€is the maximum allowable resource
level and ğ‘¦ğ‘–is a realization of the concept duration ğ‘Œğ‘–. Training speed is assumed to scale linearly
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 5 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
5
Fig. 1. Top: training resource allocation ğ‘’ğ‘–(ğ‘¡). Bottom: sample paths of the expected concept loss for the
server L(ğ‘¡) and client models Ë†L(ğ‘¡), as defined in (1) and (2). Shaded regions indicate fluctuations in observed
evaluation loss. ğ‘‡ğ‘–denotes concept change times, and ğ·ğ‘–,ğ‘—denote deployment times.
with ğ‘’ğ‘–(ğ‘¡), which we model through a horizontal scaling transformation of ğºğ‘–(Â·) controlled by
the allocation function ğ‘’ğ‘–(Â·). The resulting server-side expected concept loss at time ğ‘¡, denoted
L(ğ‘¡), reflects both the stochastic concept drift process (ğ‘Œğ‘–,ğºğ‘–(Â·)) and the corresponding resource
allocation ğ‘’ğ‘–(Â·), and is formalized as
L(ğ‘¡) = ğºğ¼(ğ‘¡)
âˆ«ğ‘¡âˆ’ğ‘‡ğ¼(ğ‘¡)
0
ğ‘’ğ¼(ğ‘¡) (ğœ)ğ‘‘ğœ

,
(1)
where ğ¼(ğ‘¡) = max{ğ‘–âˆˆN : ğ‘‡ğ‘–â‰¤ğ‘¡} denotes the index of the concept active at time ğ‘¡. Since both ğºğ¼(ğ‘¡)
and ğ¼(ğ‘¡) are random, L(ğ‘¡) is itself a stochastic process. The quantity L(ğ‘¡) captures the expected
concept loss of the server-side model under continuous training. In contrast, client devices operate
on previously deployed versions that are updated only at discrete deployment times. Consequently,
the client-side expected loss, denoted Ë†L(ğ‘¡), generally lags behind and may differ from L(ğ‘¡).
Let ğ·ğ‘–,ğ‘—denote the time of the ğ‘—th model deployment during concept ğ‘–, with the first deployment
occurring at the start of the concept, ğ·ğ‘–,0 = ğ‘‡ğ‘–. At any time ğ‘¡, client devices operate the model
version most recently deployed, i.e., the one corresponding to the latest deployment at or before ğ‘¡.
The expected concept loss experienced by clients at time ğ‘¡, denoted Ë†L(ğ‘¡), is therefore given by
Ë†L(ğ‘¡) = ğºğ¼(ğ‘¡)
âˆ«ğ·ğ¼(ğ‘¡),ğ½(ğ‘¡) âˆ’ğ‘‡ğ¼(ğ‘¡)
0
ğ‘’ğ¼(ğ‘¡) (ğœ)ğ‘‘ğœ

,
(2)
where ğ·ğ¼(ğ‘¡),ğ½(ğ‘¡) = max{ğ·ğ‘–,ğ‘—: ğ·ğ‘–,ğ‘—â‰¤ğ‘¡} is the most recent deployment time prior to ğ‘¡.
Figure 1 illustrates the evolution of the expected loss under a given training resource allocation.
The top plot shows the allocated resources ğ‘’ğ‘–(ğ‘¡), which determine the training speed. In the bottom
plot, the blue curve depicts the expected concept loss at the server, L(ğ‘¡). It rises sharply at each
concept change and then decays â€“ at a constant rate during concept ğ‘–and at a varying rate during
conceptğ‘–+1 â€“ reflecting the change in resource allocation. The orange curve shows the expected loss
of the deployed client-side model, which is updated to match the server-side model at deployment
times. The fluctuations around both curves represent variations in observed performance metrics
computed on sampled evaluation data.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 6 ---
6
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
The above framework captures how training and deployment resource allocations influence
both server-side and client-side performance under sudden concept drift. The key assumptions are
summarized as follows:
â€¢ The expected concept loss remains constant during periods when no training resources are
allocated.
â€¢ Upon a concept change, data from the new distribution becomes immediately available,
allowing training to begin without delay if resources are allocated.
â€¢ Each concept persists for an i.i.d random duration ğ‘Œğ‘–with bounded expectation, E[ğ‘Œ] < âˆ.
â€¢ The expected loss decay for concept ğ‘–is modeled by a decreasing convex function ğ‘”ğ‘–(Â·) :
R+ â†’R+, which is a realization of ğºğ‘–(Â·).
â€¢ Training resources are continuously adjustable over time.
â€¢ Allocated resources ğ‘’ğ‘–(ğ‘¡) âˆˆ[0, ğ‘€] scale the training speed linearly.
â€¢ At each concept change, the expected concept loss of both the deployed and server-side
models resets to a common initial level.
Next, we investigate two key questions: (1) Training resource allocation: Given a constraint on
total training resources, how should they be allocated over time to mitigate the effects of sudden
concept drift and optimize server-side model performance? (Section 3) (2) Deployment scheduling:
Under a constraint on deployment frequency, what constitutes the optimal deployment policy that
maximizes client-side performance under concept drift? (Section 4)
3
Optimizing Resource Allocation for Training under Concept Drift
In this section, we study resource allocation policies that optimize the time-average performance of
the ML model on the server side, subject to a time-average budget constraint and in the presence
of sudden concept drift, as defined in the framework of Section 2. We turn to model deployment
and the resulting client-side performance in Section 4.
Compute budget constraint on training resources.
As noted earlier, we assume that the ML service provider has access to elastic compute resources,
i.e., it can dynamically purchase training capacity over time. We adopt a pay-as-you-go pricing
model, where resources are billed at a fixed rate ğœğ‘’per unit of time and resource, and the provider
is subject to a fixed average cost budget constraint denoted by ğµ. Specifically, given the resource
allocation functions ğ‘’ğ‘–(ğ‘¡) : [0, âˆ) â†’[0, ğ‘€] over the duration of each concept ğ‘–, the time-average
budget constraint is expressed as
lim sup
ğ‘¡â†’âˆ
ğœğ‘’
ğ‘¡
âˆ«ğ‘¡
ğœ=0
ğ‘’ğ¼(ğœ) (ğœâˆ’ğ‘‡ğ¼(ğœ))ğ‘‘ğœâ‰¤ğµ,
(3)
where ğ¼(ğœ) denotes the index of the current concept at time ğœ, with arrival time ğ‘‡ğ¼(ğœ).
Server-side time-average expected loss. Our objective is to determine a resource allocation
policy that minimizes the time-average expected loss of the server-side model, given by
lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
âˆ«ğ‘¡
ğœ=0
L(ğœ)ğ‘‘ğœ= lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
"ğ¼(ğ‘¡)âˆ’1
âˆ‘ï¸
ğ‘–=1
âˆ«ğ‘Œğ‘–
ğœ=0
ğºğ‘–(
âˆ«ğœ
0
ğ‘’ğ‘–(ğ‘¢)ğ‘‘ğ‘¢)ğ‘‘ğœ
#
(4)
where ğºğ‘–(Â·) denotes the random expected concept loss function associated with concept ğ‘–, and
ğ‘Œğ‘–= ğ‘‡ğ‘–+1 âˆ’ğ‘‡ğ‘–is the duration of that concept. The equality follows by decomposing the time-average
loss into a sequence of cycles corresponding to concept durations and their associated losses.
As evident from the objective function in (4), the optimal training resource allocation ğ‘’ğ‘–(ğ‘¡) may
depend on both the expected loss function ğ‘”ğ‘–(Â·) and the duration ğ‘¦ğ‘–of concept ğ‘–. Determining such
an optimal policy would generally require accurate knowledge of both ğ‘”ğ‘–(Â·) and ğ‘¦ğ‘–, which is often
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 7 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
7
infeasible in real-world settings. To develop a more practical and analytically tractable formulation,
we consider stationary static training resource allocation policies, wherein the same allocation
function is applied across all concept periods, independent of concept-specific characteristics ğ‘”ğ‘–(Â·)
and ğ‘¦ğ‘–.
Definition 3.1 (Static training resource allocation policy). A collection of resource allocation
functions {ğ‘’ğ‘–(Â·) : ğ‘–âˆˆN} specifies a training resource allocation policy. Such a policy is said to be
static â€“ and therefore stationary and causal â€“ if, for all concepts ğ‘–âˆˆN and all ğ‘¡âˆˆ[0, âˆ), it holds
that ğ‘’ğ‘–(ğ‘¡) = ğ‘’(ğ‘¡).
We assume a static training resource allocation policy ğ‘’(Â·), and that the pairs ((ğ‘Œğ‘–,ğºğ‘–(Â·)) : ğ‘–âˆˆN),
where each ğ‘Œğ‘–denotes the concept duration and ğºğ‘–(Â·) the corresponding expected concept loss
function, are independent and identically distributed. Under these assumptions, the limits in (4) and
(3) exist. By the Renewal-reward Theorem [11, Section 3.4], the server-side time-average expected
loss is given by
lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
âˆ«ğ‘¡
ğœ=0
L(ğœ)ğ‘‘ğœ=
E[
âˆ«ğ‘Œ
ğœ=0 Â¯ğ‘”(
âˆ«ğœ
0 ğ‘’(ğ‘¢)ğ‘‘ğ‘¢)ğ‘‘ğœ]
E[ğ‘Œ]
,
(5)
where the equality follows from the independence of ğºğ‘–and ğ‘Œğ‘–1. We define Â¯ğ‘”(Â·) = E[ğºğ‘–(Â·)|ğ‘Œğ‘–] =
E[ğºğ‘–(Â·)] as the expected loss function, representing the average expected concept loss across
different concepts. Since each ğ‘”ğ‘–(Â·) is assumed convex and decreasing, their expectation Â¯ğ‘”(Â·)
inherits these properties2. The corresponding time-average budget constraint then becomes
lim sup
ğ‘¡â†’âˆ
ğœğ‘’
ğ‘¡
âˆ«ğ‘¡
ğœ=0
ğ‘’ğ¼(ğœ) (ğœâˆ’ğ‘‡ğ¼(ğœ))ğ‘‘ğœ= ğœğ‘’
E[
âˆ«ğ‘Œ
0 ğ‘’(ğ‘¡)ğ‘‘ğ‘¡]
E[ğ‘Œ]
â‰¤ğµ.
(6)
To determine the optimal static resource allocation policy that minimizes the time-average
expected loss in (5) while satisfying the budget constraint in (6), we rewrite both expressions using
Fubiniâ€™s Theorem and obtain the infinite-horizon continuous-time optimal control problem
min
ğ‘’(Â·)
ğ½(ğ‘’) =
âˆ«âˆ
0
Â¯ğ‘”(ğ‘¥(ğ‘¡)) Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
(7)
s.t.
ğ‘¥â€²(ğ‘¡) = ğ‘’(ğ‘¡),
ğ‘¥(0) = 0,
0 â‰¤ğ‘’(ğ‘¡) â‰¤ğ‘€,
âˆ€ğ‘¡âˆˆ[0, âˆ)
(8)
ğ¶(ğ‘’) =
âˆ«âˆ
0
ğ‘’(ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡â‰¤ğµ1,
where ğµ1 = ğµÂ· ğ¸[ğ‘Œ]
ğœğ‘’
.
(9)
Here, Â¯ğ¹ğ‘Œ(ğ‘¡) = P(ğ‘Œ> ğ‘¡) denotes the survival function (i.e., the complementary cumulative dis-
tribution function) of the concept duration ğ‘Œ. The optimization is taken over static resource
allocation functions ğ‘’(ğ‘¡) that are measurable and bounded on [0, âˆ), lying in the admissible set
E = {ğ‘’: ğ‘’(ğ‘¡) âˆˆ[0, ğ‘€], âˆ€ğ‘¡â‰¥0}. For any ğ‘’âˆˆE, we define the state variable ğ‘¥(ğ‘¡) =
âˆ«ğ‘¡
0 ğ‘’(ğœ) ğ‘‘ğœ,
which is absolutely continuous.
Lemma 3.2 (Existence of an optimal solution for Problem (7)). Let Â¯ğ‘”: [0, âˆ) â†’R+ be
continuously differentiable, convex, and non-increasing. Then the functional optimization problem (7)
1Although the time-averages in (5) are derived under the assumption that ğºğ‘–and ğ‘Œğ‘–are independent, the result extends to
more general cases in which the limit exists with a continuous, convex, decreasing function Â¯ğ‘”: [0, âˆ) â†’R+, not necessarily
equal to the mean E[ğºğ‘–(Â·)].
2The convexity assumption on individual expected concept loss functions ğ‘”ğ‘–(Â·) can be relaxed, since our analysis only
requires the convexity of the aggregated expected loss curve Â¯ğ‘”(Â·) under static training resource allocation.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 8 ---
8
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
is convex and admits at least one optimal solution. In particular, the functional ğ½(ğ‘’) defined in (7) and
the feasible set of resource allocations
Eğ¶=

ğ‘’âˆˆE

âˆ«âˆ
0
ğ‘’(ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡) ğ‘‘ğ‘¡â‰¤ğµ1

are both convex over the domain E = {ğ‘’: ğ‘’(ğœ) âˆˆ[0, ğ‘€], âˆ€ğœâ‰¥0}.
The proof of Lemma 3.2 follows from the linearity of the state function ğ‘¥(ğ‘¡) =
âˆ«ğ‘¡
0 ğ‘’(ğœ) ğ‘‘ğœ, the
convexity of Â¯ğ‘”(ğ‘¥), and the non-negativity of the survival function Â¯ğ¹ğ‘Œ(ğ‘¡). Complete proof details
are provided in Appendix A.1.
Lemma 3.2 establishes the existence of an optimal solution to Problem (7). Moreover, by Arrowâ€™s
sufficiency theorem [35, Theorem 3.1], the necessary conditions provided by Pontryaginâ€™s Maximum
Principle (PMP) are also sufficient for global optimality in this setting.
3.1
Application of Pontryaginâ€™s Maximum Principle to Problem 7
To analyze the structure of the optimal solution to the problem in (7), we apply Pontryaginâ€™s
Maximum Principle (PMP). We define the Hamiltonian function ğ»(ğ‘¡,ğ‘¥,ğ‘’, ğ‘,ğœˆ), which combines
the integrand from the objective (7), a constant Lagrange multiplier ğœˆâ‰¥0 associated with the
isoperimetric constraint (9), and the costate variable ğ‘(ğ‘¡) corresponding to the state dynamics as
ğ»(ğ‘¡,ğ‘¥,ğ‘’, ğ‘,ğœˆ) = Â¯ğ‘”(ğ‘¥(ğ‘¡)) Â¯ğ¹ğ‘Œ(ğ‘¡) + (ğ‘(ğ‘¡) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡))ğ‘’(ğ‘¡).
(10)
The necessary and sufficient conditions (by Lemma 3.2) given by Pontryaginâ€™s Maximum Principle
(PMP) for the infinite-horizon optimal control problem with free terminal state are as follows [35,
Sec. 3.6]:
ğ‘¥â€²(ğ‘¡) = ğœ•ğ»
ğœ•ğ‘= ğ‘’(ğ‘¡),
ğ‘¥(0) = 0
(11)
ğ‘â€²(ğ‘¡) = âˆ’ğœ•ğ»
ğœ•ğ‘¥= âˆ’Â¯ğ‘”â€²(ğ‘¥(ğ‘¡)) Â¯ğ¹ğ‘Œ(ğ‘¡)
(12)
ğ‘’âˆ—(ğ‘¡) = argmin
ğ‘’âˆˆE
ğ»(ğ‘¡,ğ‘¥,ğ‘’, ğ‘,ğœˆ) = argmin
ğ‘’âˆˆE
(ğ‘(ğ‘¡) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡))ğ‘’(ğ‘¡)
(13)
lim
ğ‘¡â†’âˆğ‘(ğ‘¡) = 0
(14)
ğœˆâ‰¥0,
ğœˆ
âˆ«âˆ
0
ğ‘’âˆ—(ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡âˆ’ğµ1

= 0.
(15)
The transversality condition (14) follows from the boundedness of the resource allocation function,
ğ‘’(ğ‘¡) âˆˆ[0, ğ‘€], and the finiteness of the expected concept duration, E[ğ‘Œ] < âˆ. Since the Hamiltonian
is linear in the control variable ğ‘’(ğ‘¡), the optimal resource allocation policy must take the form
of a bang-bang or singular control. We define the corresponding switching function as ğœ™(ğ‘¡) :=
ğ‘(ğ‘¡) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡). From (13), the optimal resource allocation policy is given by
ğ‘’âˆ—(ğ‘¡) =
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
0
if ğœ™(ğ‘¡) > 0
ğ‘€
if ğœ™(ğ‘¡) < 0
ğ‘’ğ‘ (ğ‘¡) âˆˆ[0, ğ‘€]
if ğœ™(ğ‘¡) = 0 over an interval (singular control).
(16)
By the transversality condition (14) and integration of the costate equation (12), we obtain ğ‘(ğ‘¡) =
âˆ«âˆ
ğ‘¡
Â¯ğ‘”â€²(ğ‘¥(ğ‘ )) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ . Substituting this expression into the switching function yields
ğœ™(ğ‘¡) =
âˆ«âˆ
ğ‘¡
Â¯ğ‘”â€²(ğ‘¥(ğ‘ )) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡).
(17)
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 9 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
9
Furthermore, using (12) and the identity Â¯ğ¹â€²
ğ‘Œ(ğ‘¡) = âˆ’â„ğ‘Œ(ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡), the derivative of the switching
function becomes
ğœ™â€²(ğ‘¡) = ğ‘â€²(ğ‘¡) + ğœˆÂ¯ğ¹â€²
ğ‘Œ(ğ‘¡) = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(ğ‘¥(ğ‘¡)) âˆ’ğœˆâ„ğ‘Œ(ğ‘¡)].
(18)
These expressions form the basis for analyzing the monotonicity of the switching function.
As we shall see, the characteristics of the concept duration distribution determine whether the
optimal policy exhibits a single switching time. We discuss these characteristics below.
Definition 3.3 (Mean Residual Life (MRL)). The mean residual life (MRL) function ğ‘šğ‘Œ(ğ‘¡) of a
nonnegative random variable ğ‘Œis defined as the expected remaining lifetime given survival up to
time ğ‘¡:
ğ‘šğ‘Œ(ğ‘¡) := ğ¸[ğ‘Œâˆ’ğ‘¡|ğ‘Œ> ğ‘¡] =
âˆ«âˆ
ğ‘¡
Â¯ğ¹ğ‘Œ(ğ‘¢)ğ‘‘ğ‘¢
Â¯ğ¹ğ‘Œ(ğ‘¡)
,
for Â¯ğ¹ğ‘Œ(ğ‘¡) > 0.
(19)
A well-known relationship (see [3]) links the derivative of the MRL function and the hazard rate
ğ‘šâ€²
ğ‘Œ(ğ‘¡) = â„ğ‘Œ(ğ‘¡)ğ‘šğ‘Œ(ğ‘¡) âˆ’1, where â„ğ‘Œ(ğ‘¡) := ğ‘“ğ‘Œ(ğ‘¡)
Â¯ğ¹ğ‘Œ(ğ‘¡) , for Â¯ğ¹ğ‘Œ(ğ‘¡) > 0. Accordingly, the random variable ğ‘Œ
is said to have
Decreasing Mean Residual Life (DMRL) if ğ‘šğ‘Œ(ğ‘¡) is non-increasing, i.e., â„ğ‘Œ(ğ‘¡)ğ‘šğ‘Œ(ğ‘¡) â‰¤1.
Increasing Mean Residual Life (IMRL) if ğ‘šğ‘Œ(ğ‘¡) is non-decreasing, i.e., â„ğ‘Œ(ğ‘¡)ğ‘šğ‘Œ(ğ‘¡) â‰¥1.
Many real-world processes naturally exhibit these aging characteristics. DMRL behavior is com-
monly observed in systems that degrade over time, such as mechanical components, electronic
devices, or biological organisms, where aging applies intuitively. In contrast, IMRL behavior charac-
terizes processes in which survival implies increased robustness or reliability, such as the lifecycles
of businesses or startups, social and political trends, and the adoption of new technologies or
products. Accordingly, depending on the application, the aging properties of concept durations can
often be assumed or empirically verified using sample-efficient statistical methods [4, 13, 20].
Next, we analyze the optimality of single-switch policies given the MRL properties of the concept
duration distribution. A single-switch resource allocation policy can take one of two forms: (i)
Front-Loading policy, where ğ‘’(ğ‘¡) = ğ‘€for ğ‘¡âˆˆ[0,ğ‘¡âˆ—] and ğ‘’(ğ‘¡) = 0 for ğ‘¡> ğ‘¡âˆ—; or (ii) Back-Loading
policy, where ğ‘’(ğ‘¡) = 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—] and ğ‘’(ğ‘¡) = ğ‘€for ğ‘¡> ğ‘¡âˆ—. Here, ğ‘¡âˆ—denotes the switching time.
For such policies to be optimal, the switch at time ğ‘¡âˆ—must satisfy ğœ™(ğ‘¡âˆ—) = 0, and the sign of the
switching function should be opposite on the intervals before and after the switch, while remaining
constant within each interval. The following theorems characterize the conditions under which the
front-loading or back-loading policy is optimal, depending on the aging characteristics (DMRL or
IMRL) of the concept duration distribution.
Theorem 3.4 (Optimality of the Front-Loading Policy). For any convex, decreasing expected
loss function Â¯ğ‘”: [0, âˆ) â†’R+ and any resource budget ğµ> 0, the optimal resource allocation is a
single-switch control of the form
ğ‘’âˆ—(ğ‘¡) =
(
ğ‘€
ğ‘¡< ğ‘¡âˆ—
ğ·ğ‘€ğ‘…ğ¿
0
ğ‘¡â‰¥ğ‘¡âˆ—
ğ·ğ‘€ğ‘…ğ¿,
(20)
if and only if the concept duration ğ‘Œhas a decreasing mean residual life (DMRL). The switching
time ğ‘¡âˆ—
ğ·ğ‘€ğ‘…ğ¿is uniquely determined by the budget constraint
âˆ«ğ‘¡âˆ—
ğ·ğ‘€ğ‘…ğ¿
0
Â¯ğ¹ğ‘Œ(ğ‘¦)ğ‘‘ğ‘¦= E[ğ‘Œ]ğµ
ğ‘€ğœğ‘’, provided that
ğµ< ğ‘€ğœğ‘’, and ğ‘¡âˆ—
ğ·ğ‘€ğ‘…ğ¿= âˆ, otherwise.
The proof of Theorem 3.4 relies on analyzing the behavior of the switching function ğœ™(ğ‘¡) under
DMRL concept duration distributions. By leveraging the convexity of the expected loss function
Â¯ğ‘”(Â·) and the monotonic aging property of ğ‘Œ, we show that ğœ™(ğ‘¡) crosses zero at most once, and
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 10 ---
10
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
that its sign pattern aligns with the optimality structure specified in (16). The complete proof is
provided in Appendix A.2.
Note that Theorem 3.4 states that ifğ‘Œis DMRL, the optimal policy is front-loading, and conversely,
if the optimal policy is front-loading for all budgets, then ğ‘Œmust be DMRL. While such policies
may appear intuitive, this result formally rules out the optimality of commonly used heuristics such
as fixed or periodic training schedules. Moreover, it implies that if ğ‘Œis not DMRL, a front-loading
policy need not be optimal for all budget levels. The next result, proved in Appendix A.3, shows
that when ğ‘Œis IMRL, the optimal policy exhibits delayed (i.e., idling) resource allocation at the
start of each concept duration.
Theorem 3.5 (Optimality of the Back-Loading Policy). For any convex, decreasing expected
loss function Â¯ğ‘”: [0, âˆ) â†’R+, if the concept duration ğ‘Œhas an increasing mean residual life (IMRL)
and the budget satisfies ğµ< ğ‘€ğœğ‘’, the optimal resource allocation exhibits an initial idling period
before training begins,
ğ‘’âˆ—(ğ‘¡) = 0,
for ğ‘¡< ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿,
(21)
where ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿> 0 denotes the time at which resource allocation starts.
Although the IMRL case may represent an edge scenario, Theorem 3.5 characterizes the condi-
tions under which the back-loading policy is optimal. The following corollary provides sufficient
conditions for the optimality of such a resource allocation policy.
Corollary 3.6 (Optimality of the Back-Loading Policy). For any convex, decreasing expected
loss function Â¯ğ‘”: [0, âˆ] â†’R+ with constant derivative Â¯ğ‘”â€²(ğ‘¡) = âˆ’ğ›½, and any concept duration ğ‘Œwith
an increasing mean residual life (IMRL), the optimal resource allocation is a single-switch control given
by
ğ‘’âˆ—(ğ‘¡) =
(
0
ğ‘¡â‰¤ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿
ğ‘€
ğ‘¡> ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿
(22)
where the switching time ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿is uniquely determined by the budget constraint
âˆ«âˆ
ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿
Â¯ğ¹ğ‘Œ(ğ‘¦)ğ‘‘ğ‘¦= E[ğ‘Œ]ğµ
ğ‘€ğœğ‘’
,
provided that ğµ< ğ‘€ğœğ‘’, and ğ‘¡âˆ—
ğ¼ğ‘€ğ‘…ğ¿= 0 otherwise.
The proof of Corollary 3.6 follows directly from Theorem 3.5 and the fact that a linearly decreasing
Â¯ğ‘”(Â·) ensures that the switching function crosses zero at most once. The complete proof is provided
in Appendix A.4.
Notably, the strength of Theorems 3.4 and 3.5 lies in their generality: their conclusions do not
depend on the specific functional form of Â¯ğ‘”(Â·), but only on its monotonicity and convexity. These
results also demonstrate that training policies that ignore the aging behavior of concept durations
are provably suboptimal. The DMRL/IMRL distinction further clarifies how drift dynamics shape the
structure of optimal resource allocation policies. Even without precise distributional information,
our analysis indicates when front-loading (DMRL) or deferral (IMRL) strategies are most effective.
As such, these results provide broadly applicable guidance for designing training resource allocation
policies based on the statistical characteristics of training cycles driven by sudden concept changes.
In Figures 2a and 2b, we present simulation results illustrating the benefits of optimal training
resource allocation and validating the relationship between the aging properties of concept durations
and the structure of optimal allocation policies. In Figure 2a, we compare the time-average expected
loss under two policies: the optimal resource allocation (specifically, the front-loading policy derived
from Theorem A.2) and a fixed resource allocation. In this experiment, concept durations follow an
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 11 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
11
(a) Comparison between constraint-binding optimal
and fixed resource allocation policies. ğ‘Œâˆ¼Exp(1),
Â¯ğ‘”(ğ‘¡) = ğ‘’âˆ’ğ‘¡, ğœğ‘’= 1, ğ‘€= 20.
(b) Constraint-binding delayed block allocation with
varying delays under different concept duration dis-
tributions. Â¯ğ‘”(ğ‘¡) = ğ‘’âˆ’ğ‘¡, ğµ= 0.1, ğœğ‘’= 1, ğ‘€= 20.
Fig. 2. Comparison of optimal, fixed and delayed resource allocation policies under sudden concept drift. All
simulations were executed on a MacBook Air M3 with 16 GB of RAM.
exponential distribution, corresponding to the DMRL case, and the expected loss function decays
exponentially. The fixed policy assigns a constant resource level such that the budget constraint is
exactly met, i.e., ğ‘’fixed(ğ‘¡) = ğµ/ğœğ‘’âˆ€ğ‘¡. The results show that, for the same resource budget, the optimal
policy reduces the time-average expected loss by up to 71.80% compared to the fixed allocation
policy. As the budget ğµapproaches the upper limit ğ‘€ğœğ‘’, the switching time ğ‘¡âˆ—
DMRL increases and the
performance gap between the two policies narrows. Additional simulation results using alternative
expected loss functions are provided in Appendix A.9.
In Figure 2b, we examine the effect of delayed block resource allocation for concept duration
distributions with different aging properties. These experiments also use an exponentially decaying
expected loss function, and the parameters are chosen such that the budget constraint is binding.
Under a delayed block allocation policy, resources are withheld until a delay ğ‘§, after which the full
capacity is applied until the budget is depleted. Formally, the allocation is defined as
ğ‘’block(ğ‘¡) =
(
ğ‘€
ğ‘§â‰¤ğ‘¡< ğ‘‡(ğ‘§)
0
otherwise
,
where
âˆ«ğ‘‡(ğ‘§)
ğ‘§
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡= ğµE[ğ‘Œ]
ğ‘€ğœğ‘’
.
(23)
Consistent with Theorem 3.4 and 3.5, the results confirm that concept duration distributions with
IMRL benefit from delayed resource allocation up to an optimal delay, whereas for DMRL distribu-
tions, introducing delay consistently degrades performance. Overall, these findings demonstrate
that the aging properties of training cycles driven by sudden concept drift have a significant impact
on the structure of optimal training resource allocation strategies. Optimizing training opera-
tions based on the characteristics of concept drift yields substantial performance improvements,
particularly under budget-constrained scenarios.
4
Deployment Optimization under Concept Drift
As discussed in Section 2, although improved versions of an ML model may exist on the server
side, the performance experienced by clients depends on the version of the model that has been
deployed to them. In this section, we study the deployment scheduling problem, aiming to optimize
the long-term performance of client-side models subject to a constraint on the deployment rate,
which may serve as a proxy for communication costs.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 12 ---
12
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
To understand the dynamics governing deployment decisions, we analyze deployment policies
in a setting where the server-side model is trained under a fixed allocation of training resources.
Following the approach in Section 3, we restrict our attention to static deployment schedulers,
defined as follows:
Definition 4.1 (Static Deployment Scheduler). Let ğ·ğ‘–,ğ‘—âˆ’ğ‘‡ğ‘–denote the deployment time offset
relative to concept arrival time ğ‘‡ğ‘–, where ğ·ğ‘–,ğ‘—is the ğ‘—-th deployment time during concept ğ‘–. A
deployment scheduler is said to be static â€“ and hence stationary and causal â€“ if, for all concepts
ğ‘–âˆˆN,
ğ·ğ‘–,ğ‘—âˆ’ğ‘‡ğ‘–= ğ›¿ğ‘—, almost surely
âˆ€ğ‘–, ğ‘—âˆˆN,
where ğ›¿0 = 0 < ğ›¿1 < Â· Â· Â· so that the set {ğ›¿ğ‘—: ğ‘—âˆˆN} defines a static deployment scheduler.
Let ğ‘ğ‘–= max{ğ‘—: ğ›¿ğ‘—< ğ‘Œğ‘–} denote the random variable representing the number of deployments
occurring during concept ğ‘–under a static deployment scheduler. Assuming a fixed unit training
resource allocation for the server-side model, i.e., ğ‘’ğ‘–(ğ‘¡) = 1, âˆ€ğ‘¡â‰¥0, the time-average expected loss
experienced by clients under a static deployment scheduler is given by
lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
âˆ«ğ‘¡
ğœ=0
Ë†L(ğœ)ğ‘‘ğœ= lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
âˆ«ğ‘¡
ğœ=0
ğºğ¼(ğ‘¡)
 ğ·ğ¼(ğ‘¡),ğ½(ğ‘¡) âˆ’ğ‘‡ğ¼(ğ‘¡)

(24)
= lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
"ğ¼(ğ‘¡)âˆ’1
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘–âˆ’1
âˆ‘ï¸
ğ‘—=0
ğºğ‘–(ğ›¿ğ‘—)(ğ›¿ğ‘—+1 âˆ’ğ›¿ğ‘—) + ğºğ‘–(ğ›¿ğ‘ğ‘–)(ğ‘Œğ‘–âˆ’ğ›¿ğ‘ğ‘–)
#
(25)
=
E[Ãğ‘ğ‘–âˆ’1
ğ‘—=0 ğºğ‘–(ğ›¿ğ‘—)(ğ›¿ğ‘—+1 âˆ’ğ›¿ğ‘—) + ğºğ‘–(ğ›¿ğ‘ğ‘–)(ğ‘Œğ‘–âˆ’ğ›¿ğ‘ğ‘–)]
E[ğ‘Œ]
(26)
=
E[Ãğ‘âˆ’1
ğ‘—=0 Â¯ğ‘”(ğ›¿ğ‘—)(ğ›¿ğ‘—+1 âˆ’ğ›¿ğ‘—) + Â¯ğ‘”(ğ›¿ğ‘)(ğ‘Œâˆ’ğ›¿ğ‘)]
E[ğ‘Œ]
(27)
=
1
E[ğ‘Œ]
âˆ
âˆ‘ï¸
ğ‘—=0
Â¯ğ‘”(ğ›¿ğ‘—)
âˆ«ğ›¿ğ‘—+1
ğ›¿ğ‘—
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡,
(28)
where Â¯ğ‘”(Â·) = E[ğºğ‘–(Â·)] denotes the expected loss function, and ğ‘ğ‘–are i.i.d. random variables
representing the number of deployments within each concept durations, with ğ‘âˆ¼ğ‘ğ‘–. Similarly,
the time-average deployment rate constraint can be expressed as
lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
ğ¼(ğ‘¡)
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘–= E[ğ‘]
E[ğ‘Œ]
(29)
=
E[Ãâˆ
ğ‘—=1 I{ğ›¿ğ‘—< ğ‘Œ}]
E[ğ‘Œ]
=
Ãâˆ
ğ‘—=1 ğ‘ƒ(ğ›¿ğ‘—< ğ‘Œ)
E[ğ‘Œ]
(30)
=
Ãâˆ
ğ‘—=1 Â¯ğ¹ğ‘Œ(ğ›¿ğ‘—)
E[ğ‘Œ]
â‰¤ğ‘Ÿğ·,
(31)
where ğ‘Ÿğ·denotes the maximum allowable deployment rate. The optimization problem minimizing
the time-average expected loss experienced by clients, subject to the time-average deployment rate
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 13 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
13
constraint, is then given by
min
{Î”ğ‘—}âˆ
ğ‘—=1
âˆ
âˆ‘ï¸
ğ‘—=0
Â¯ğ‘”(ğ›¿ğ‘—)
âˆ«ğ›¿ğ‘—+1
ğ›¿ğ‘—
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
(32)
s.t.
Î”ğ‘—= ğ›¿ğ‘—âˆ’ğ›¿ğ‘—âˆ’1 â‰¥0, âˆ€ğ‘—âˆˆN+
ğ›¿0 = 0,
(33)
âˆ
âˆ‘ï¸
ğ‘—=1
Â¯ğ¹ğ‘Œ(ğ›¿ğ‘—) â‰¤ğµ2,
where ğµ2 = ğ‘Ÿğ·Â· ğ¸[ğ‘Œ].
(34)
Here {Î”ğ‘—}âˆ
ğ‘—=1 denotes the set of inter-deployment durations. Note that the optimization problem (32)
is not necessarily convex. However, under suitable conditions, it can be shown to be quasi-convex,
as formalized in the following lemma.
Lemma 4.2 (Quasi-convexity of Problem (32)). If Â¯ğ‘”is a non-negative, convex, and decreasing
function, and Â¯ğ¹ğ‘Œis strictly decreasing, then the objective function in (32) is quasi-convex in {Î”ğ‘—â‰¥0}ğ‘—âˆˆN.
Moreover, if Â¯ğ¹ğ‘Œis also convex, then Problem (32) is quasi-convex.
A detailed proof of Lemma 4.2 is provided in Appendix A.5. As discussed there, the Karushâ€“Kuhnâ€“
Tucker (KKT) conditions are sufficient for global optimality when the feasible set defined by
the deployment rate constraint (34) is convex. However, even under this condition, a numerical
procedure is typically required to obtain the optimal deployment schedule.
Applying the KKT conditions, for a Lagrange multiplier ğœˆ> 0, the optimal deployment scheduler
{ğ›¿âˆ—
ğ‘˜}âˆ
ğ‘˜=0 satisfies the following necessary condition:
Â¯ğ‘”â€²(ğ›¿âˆ—
ğ‘˜)
âˆ«ğ›¿âˆ—
ğ‘˜+1
ğ›¿âˆ—
ğ‘˜
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡= Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘˜)[ Â¯ğ‘”(ğ›¿âˆ—
ğ‘˜) âˆ’Â¯ğ‘”(ğ›¿âˆ—
ğ‘˜âˆ’1)] + ğœˆğ‘“ğ‘Œ(ğ›¿âˆ—
ğ‘˜).
(35)
A numerical approach such as the bisection method can be employed to compute the optimal
deployment times, typically by starting from the final deployment and proceeding backward, under
an assumed upper bound on the total number of deployments. Motivated by the structure of this
solution, we next propose a randomized deployment policy derived from a modified version of
Problem (32), which remains applicable even when the concept duration distribution exhibits a
non-convex survival function.
In this modified problem, the deployment rate constraint is replaced by a fixed, deterministic
number of deployments, denoted by ğ‘ğ·. The resulting optimization problem is given by
min
{Î”ğ‘—}ğ‘ğ·
ğ‘—=1
ğ‘ğ·âˆ’1
âˆ‘ï¸
ğ‘—=0
Â¯ğ‘”(ğ›¿ğ‘—)
âˆ«ğ›¿ğ‘—+1
ğ›¿ğ‘—
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡+ Â¯ğ‘”(ğ›¿ğ‘ğ·)
âˆ«âˆ
ğ›¿ğ‘ğ·
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
(36)
s.t.
Î”ğ‘—= ğ›¿ğ‘—âˆ’ğ›¿ğ‘—âˆ’1 â‰¥0, âˆ€ğ‘—âˆˆ[1, ğ‘ğ·]
ğ›¿0 = 0.
(37)
Since the rate constraint has been removed, Lemma 4.2 guarantees that Problem (36) remains
quasi-convex.
Theorem 4.3 (Optimal Solution for Problem (36)). Let Â¯ğ‘”be a non-negative, convex, and
decreasing function, and let ğ‘Œbe a nonnegative random variable with Â¯ğ¹ğ‘Œ(ğ‘¡) > 0 for all ğ‘¡âˆˆ[0, âˆ).
Then the optimal deployment scheduler {ğ›¿âˆ—
ğ‘˜}ğ‘ğ·
ğ‘˜=0 satisfies
Â¯ğ‘”(ğ›¿âˆ—
ğ‘˜âˆ’1) âˆ’Â¯ğ‘”(ğ›¿âˆ—
ğ‘˜)
âˆ’Â¯ğ‘”â€²(ğ›¿âˆ—
ğ‘˜)
=
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
ğ‘šğ‘Œ(ğ‘‘âˆ—
ğ‘ğ·),
ğ‘˜= ğ‘ğ·
âˆ«ğ›¿âˆ—
ğ‘˜+1
ğ›¿âˆ—
ğ‘˜
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘˜)
,
ğ‘˜âˆˆ[1, ğ‘ğ·âˆ’1],
(38)
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 14 ---
14
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
where ğ‘šğ‘Œ(Â·) denotes the mean residual life (MRL) function of the concept duration ğ‘Œ. Furthermore, the
effective deployment rate achieved by the optimal scheduler, defined as ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}ğ‘ğ·
ğ‘˜=0) = Ãğ‘ğ·
ğ‘—=1 Â¯ğ¹ğ‘Œ(ğ›¿ğ‘—), is
monotonically increasing in the number of deployments ğ‘ğ·.
The proof of Theorem 4.3 follows directly from the quasi-convexity of Problem (36) and the
monotonicity of the survival function Â¯ğ¹ğ‘Œ(ğ‘¡). Full derivations are provided in Appendix A.6.
In a special case where concept durations follow an exponential distribution and the expected
loss decays exponentially, Theorem 4.3 admits a closed-form, chain-structured solution that can be
determined iteratively.
Corollary 4.4 (Exponentially distributed concept durations and Exponential Decaying
Â¯ğ‘”). Assume the expected loss function is given by Â¯ğ‘”(ğ‘¡) = ğ›¼ğ‘’âˆ’ğ›½ğ‘¡, with ğ›¼, ğ›½> 0, and that the concept
durations Y are exponentially distributed with rate parameter ğœ†, i.e., Â¯ğ¹ğ‘Œ(ğ‘¡) = ğ‘’âˆ’ğœ†ğ‘¡. Then, the optimal
inter-deployment durations Î”âˆ—
ğ‘˜= ğ›¿âˆ—
ğ‘˜âˆ’ğ›¿âˆ—
ğ‘˜âˆ’1 for the problem with a deterministic number of deployment
ğ‘ğ·are given by
Î”âˆ—
ğ‘˜=
( 1
ğ›½ln( ğ›½
ğœ†+ 1)
ğ‘˜= ğ‘ğ·,
1
ğ›½ln( ğ›½
ğœ†(1 âˆ’ğ‘’âˆ’ğœ†Î”âˆ—
ğ‘˜+1))
ğ‘˜âˆˆ[1, ğ‘ğ·âˆ’1],
(39)
with ğ›¿0 = 0 and ğ‘˜âˆˆ[1, ğ‘ğ·].
The derivation of Corollary 4.4 is provided in Appendix A.7. As shown in Equation (39), the final
inter-deployment duration is constant, while the preceding durations are determined recursively in a
backward fashion, resulting in a chain-structured solution. This structure implies that as the number
of deployments ğ‘ğ·increases, the optimal deployment times adjust to accommodate the additional
deployments, while previously determined inter-deployment intervals remain unchanged.
We now introduce a randomized deployment scheduling policy to address the deployment op-
timization problem under the rate constraint (32). The proposed approach leverages the optimal
deterministic schedulers obtained from Problem (36) for different deployment counts. The random-
ized policy operates over the convex hull of these optimal schedulers with consecutive numbers of
deployments, assigning probabilities such that the resulting expected effective deployment rate
exactly matches the allotted deployment rate limit.
Theorem 4.5 (Randomized Deployment Scheduler). Let {ğ›¿âˆ—
ğ‘˜}âˆ
ğ‘˜=0 denote the optimal deployment
scheduler for Problem (32), and let {ğ›¿âˆ—
ğ‘˜}ğ‘ğ·
ğ‘˜=0 denote the optimal deployment scheduler for Problem (36)
with a deterministic number of deployments ğ‘ğ·. Assume that the expected loss function Â¯ğ‘”(Â·) is non-
negative, convex, and decreasing, and that the concept duration ğ‘Œhas survival function Â¯ğ¹ğ‘Œ(ğ‘¡) > 0
for all ğ‘¡âˆˆ[0, âˆ). Then, for any optimal scheduler {ğ›¿âˆ—
ğ‘˜}âˆ
ğ‘˜=0, there exists a deployment count ğ‘âˆ—
ğ·âˆˆN+
and a scalar parameter ğ›¾âˆˆ[0, 1] such that a convex combination of the effective deployment rates
corresponding to the optimal schedulers with ğ‘âˆ—
ğ·and ğ‘âˆ—
ğ·+ 1 deployments exactly matches the allotted
deployment rate limit, i.e.,
ğ›¾Â· ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·
ğ‘˜=0) + (1 âˆ’ğ›¾) Â· ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·+1
ğ‘˜=0 ) = ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}âˆ
ğ‘˜=0) = ğ‘Ÿğ·E[ğ‘Œ],
(40)
where ğ‘Ÿğ·denotes the allotted deployment rate limit.
A randomized deployment policy that, at the beginning of each concept, uses the scheduler {ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·
ğ‘˜=0
with probability ğ›¾and {ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·+1
ğ‘˜=0
with probability 1 âˆ’ğ›¾achieves an expected effective deployment rate
exactly equal to the deployment rate constraint.
A detailed proof of Theorem 4.5 is presented in Appendix A.8.
The randomized deployment scheduler provides an approximate solution to the constrained
deployment optimization problem (32), which is inherently non-convex. It guarantees full utilization
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 15 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
15
of the allotted deployment budget and does not rely on the convexity of the survival function of
the concept duration distribution. Although the method requires knowledge of the expected loss
curve and the probability distribution of concept duration, it offers a computationally efficient
alternative to high-dimensional grid search or unstable non-convex optimization techniques. While
the randomized policy assumes that both the expected loss curve and concept duration distribution
can be estimated â€“ an assumption that is feasible in many practical scenarios â€“ an alternative
approach is to use a parametric scheduler that emulates the key structural property of the optimal
solution, namely, decreasing inter-deployment durations. In this context, the randomized policy
serves as a computationally efficient baseline for testing and tuning such parametric algorithms.
Our simulation results further demonstrate that the performance of the randomized scheduler
closely approximates that of the optimal (non-randomized) policy, making it an effective and
computationally efficient choice for deployment under concept drift. Figure 3 presents simulation
Fig. 3. Comparison of periodic, optimal, and randomized deployment policies under different concept duration
distributions (Exponential, Weibull(ğ‘˜= 2), and Erlang-2), with E[ğ‘Œ] = 1 and exponentially decaying expected
loss Â¯ğ‘”(ğ‘¡) = ğ‘’âˆ’ğ›½ğ‘¡. Simulations were run on a MacBook Air M3 (16 GB RAM).
results highlighting the performance gains achieved by optimal deployment policies relative to a
periodic deployment baseline and demonstrates the near-optimality of the randomized deployment
policy across different concept duration distributions. In all experiments, the expected loss follows
an exponential decay, and distribution parameters are chosen such that E[ğ‘Œ] = 1. Except for the
Exponential case, where a closed-form solution is available, the optimal deployment policies are
computed numerically. For Exponential and Weibull(ğ‘˜= 2) distributions, the optimal policy reduces
the client-side time-average expected loss by up to 43.30% and 39.25%, respectively, compared to the
periodic policy under the same effective deployment rate. In these settings, the randomized policy,
constructed as a convex combination of optimal solutions for adjacent deployment counts ğ‘ğ·,
achieves performance nearly indistinguishable from the optimal policy. In contrast, for the Erlang-2
distribution, the performance gap between optimal and periodic policies narrows, owing both to
the distributionâ€™s reduced variability (which limits the policyâ€™s leverage) and to the smaller decay
rate ğ›½of the loss function, which slows the decline of expected penalties. Under these conditions,
the randomized policy no longer matches the optimal performance, providing a concrete example
where the optimal policy strictly outperforms its randomized counterpart.
5
Conclusion
We have presented a formal framework for studying training resource allocation and model de-
ployment in distributed machine learning systems operating under persistent, discrete concept
drift. In scenarios where training resources are elastic, i.e., available on demand but incurring
computational and communication costs, a provider must optimize when to allocate resources and
when to deploy updated models. Our analysis shows that the structure of optimal policies depends
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 16 ---
16
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
fundamentally on the statistical properties of concept durations and loss functions. As machine
learning systems become increasingly ubiquitous, the monitoring-retraining-redeployment cycle
will represent a growing component of their operational costs. The results of this work lay the foun-
dation for principled, cost-efficient training and deployment strategies that adapt to non-stationary
environments.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 17 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
17
References
[1] Lucas Baier, Vincent Kellner, Niklas KÃ¼hl, and Gerhard Satzger. 2020. Switching Scheme: A Novel Approach for
Handling Incremental Concept Drift in Real-World Data Sets. arXiv:2011.02738 [cs.LG] https://arxiv.org/abs/2011.02738
[2] Jihwan Bang, Hyunseo Koh, Seulki Park, Hwanjun Song, Jung-Woo Ha, and Jonghyun Choi. 2022. Online continual
learning on a contaminated data stream with blurry task boundaries. Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (2022), 9275â€“9284.
[3] Richard E. Barlow and Frank Proschan. 1974/1975. Statistical Theory of Reliability and Life Testing: Probability Models.
In Statistical Theory of Reliability and Life Testing: Probability Models. Holt, Rinehart and Winston, New York.
[4] Bo Bergman and Bengt KlefsjÃ¶. 1989. A Family of Test Statistics for Detecting Monotone Mean Residual Life. Journal
of Statistical Planning and Inference 21, 2 (Feb. 1989), 161â€“178. doi:10.1016/0378-3758(89)90002-5
[5] Albert Bifet and Ricard GavaldÃ . 2007. Learning from Time-Changing Data with Adaptive Windowing. In Proceedings
of the 2007 SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics,
443â€“448. doi:10.1137/1.9781611972771.42
[6] M. Budka and B. Gabrys. 2018. Change-point Detection in Evolving Data Streams Using Ensembles of Classifiers. IEEE
Signal Processing Letters 25, 9 (2018), 1353â€“1357. doi:10.1109/LSP.2018.2849385
[7] D. Chen, S. Yang, and J. Li. 2024. What Role Do Small Models Play in a World of Giants?. In International Conference on
Learning Representations.
[8] Weihao Cheng, Sarah Erfani, Rui Zhang, and Ramamohanarao Kotagiri. 2018. Learning datum-wise sampling frequency
for energy-efficient human activity recognition. 32, 1 (2018).
[9] Ben Cottier, Robi Rahman, Loredana Fattorini, Nestor Maslej, Tamay Besiroglu, and David Owen. 2025. The rising
costs of training frontier AI models. arXiv:2405.21015 [cs.CY] https://arxiv.org/abs/2405.21015
[10] T. Domhan, J. Springenberg, and F. Hutter. 2015. Speeding up Automatic Hyperparameter Optimization of Deep
Neural Networks by Extrapolation of Learning Curves. In Proceedings of the 24th International Conference on Artificial
Intelligence. 3460â€“3468.
[11] Robert G. Gallager. 1996. Discrete Stochastic Processes. Springer US, Boston, MA. doi:10.1007/978-1-4615-2329-1
[12] J. Gama, I. Å½liobaitË™e, A. Bifet, M. Pechenizkiy, and A. Bouchachia. 2014. A Survey on Concept Drift Adaptation. Comput.
Surveys 46, 4 (2014), 44:1â€“44:37. doi:10.1145/2523813
[13] W. J. Hall and Jon A. Wellner. 2020. Estimation of Mean Residual Life. In Statistical Modeling for Biological Systems: In
Memory of Andrei Yakovlev, Anthony Almudevar, David Oakes, and Jack Hall (Eds.). Springer International Publishing,
Cham, 169â€“189. doi:10.1007/978-3-030-34675-1_10
[14] Meng Han, Zhiqiang Chen, Muhang Li, Hongxin Wu, and Xilong Zhang. 2022. A survey of active and passive
concept drift handling methods. IEEE Transactions on Knowledge and Data Engineering 38, 4 (2022), 1492â€“1535.
doi:10.1111/coin.12520
[15] J. Hoffmann, S. Borgeaud, A. Mensch, P. Buchlovsky, T. Cai, E. Rutherford, K. Millican, C. Jones, B. Bos, S. Gray, C.
Leahy, E. Conway, Z. Dai, A. Mirhoseini, and E. Grefenstette. 2022. Training Compute-Optimal Large Language Models.
arXiv preprint arXiv:2203.15556 (2022).
[16] Weiqiang Huang, Juecen Zhan, Yumeng Sun, Xu Han, Tai An, and Nan Jiang. 2025. Context-Aware Adaptive Sampling
for Intelligent Data Acquisition Systems Using DQN. arXiv preprint arXiv:2504.09344 (2025).
[17] R. Hussein and N. Gupta. 2025. ChatGPTâ€™s Impact Across Sectors: A Survey. International Journal of Humanâ€“Computer
Studies 152 (2025), 102761. doi:10.1016/j.ijhcs.2024.102761
[18] J. Kaplan, S. McCandlish, T. Henighan, T. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. 2020.
Scaling Laws for Neural Language Models. arXiv preprint arXiv:2001.08361 (2020).
[19] H. Kim and S. Park. 2025. Tools for Understanding How Large Language Models Work. arXiv preprint arXiv:2501.01234
(2025).
[20] Subhash C. Kochar, Hari Mukerjee, and Francisco J. Samaniego. 2000. Estimation of a Monotone Mean Residual Life.
The Annals of Statistics 28, 3 (2000), 905â€“921. jstor:2674059
[21] J. Z. Kolter and M. A. Maloof. 2007. Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts. In
Proceedings of the 2007 IEEE International Conference on Data Mining. 123â€“132.
[22] Dominik Kreuzberger, Niklas KÃ¼hl, and Sebastian Hirschl. 2022. Machine Learning Operations (MLOps): Overview,
Definition, and Architecture. arXiv:2205.02302 [cs.LG] https://arxiv.org/abs/2205.02302
[23] Mengtian Li, Ersin Yumer, and Deva Ramanan. 2019. Budgeted training: Rethinking deep neural network training
under resource constraints. arXiv preprint arXiv:1905.04753 (2019).
[24] Ahmed Ali Linkon, Mujiba Shaima, Md Shohail Uddin Sarker, Badruddowza, Norun Nabi, Md Nasir Uddin Rana,
Sandip Kumar Ghosh, Mohammad Anisur Rahman, Hammed Esa, and Faiaz Rahat Chowdhury. 2024. Advancements
and Applications of Generative Artificial Intelligence and Large Language Models on Business Management: A
Comprehensive Review. Journal of Computer Science and Technology Studies 6, 1 (March 2024), 225â€“232. doi:10.32996/
jcsts.2024.6.1.26
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 18 ---
18
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
[25] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, JoÃ£o Gama, and Guangquan Zhang. 2019. Learning under Concept Drift: A
Review. IEEE Transactions on Knowledge and Data Engineering 31, 12 (2019), 2346â€“2363. doi:10.1109/TKDE.2018.2876857
Conference Name: IEEE Transactions on Knowledge and Data Engineering.
[26] Sandeep Madireddy, Prasanna Balaprakash, Philip Carns, Robert Latham, Glenn K Lockwood, Robert Ross, Shane
Snyder, and Stefan M Wild. 2019. Adaptive learning for concept drift in application performance modeling. In
Proceedings of the 48th International Conference on Parallel Processing. 1â€“11.
[27] Ananth Mahadevan and Michael Mathioudakis. 2023. Cost-effective retraining of machine learning models. arXiv
preprint arXiv:2310.04216 (2023).
[28] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons,
Anchit Gupta, Emre Orbay, et al. 2018. Roboturk: A crowdsourcing platform for robotic skill learning through imitation.
In Conference on Robot Learning. PMLR, 879â€“893.
[29] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. 2016. Online optimization in dynamic
environments: Improved regret rates for strongly convex problems. (2016), 7195â€“7201. doi:10.1109/CDC.2016.7799379
[30] F. Moreno-GarcÃ­a, J. del Campo-Ãvila, N. GarcÃ­a-Pedrajas, and S. Ventura. 2020. Dynamic Ensemble Selection Based
on Local Accuracy for Non-Stationary Environments. Data Mining and Knowledge Discovery 34, 3 (2020), 736â€“770.
doi:10.1007/s10618-019-00658-8
[31] Maksim Muravev, Brazhenko , Dmitry, Somenkova , Anzhela, Golovkov , Alexander, and Ilia and Sergunin. [n. d.].
MLOps Architecture as a Future of Machine Learning. Journal of Computer Information Systems 0, 0 ([n. d.]), 1â€“13.
doi:10.1080/08874417.2025.2483826
[32] J. Park, K. Lee, and M. Cho. 2024. LLamaDuo: Dual-Mode Inference for On-Device LLMs. In Proceedings of EMNLP.
ACL, 1123â€“1135.
[33] Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R
Ganger, and Eric P Xing. 2021. Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning. In 15th
{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21).
[34] Florence Regol, Leo Schwinn, Kyle Sprague, Mark Coates, and Thomas Markovich. 2025. When to retrain a machine
learning model. https://openreview.net/forum?id=iGX0lwpUYj
[35] Suresh P. Sethi. 2021. Optimal Control Theory: Applications to Management Science and Economics. Springer International
Publishing, Cham. doi:10.1007/978-3-030-91745-6
[36] Ya Shen, Gang Chen, Hui Ma, and Mengjie Zhang. 2024. Cost-Aware Dynamic Cloud Workflow Scheduling Using
Self-attention and Evolutionary Reinforcement Learning. In International Conference on Service-Oriented Computing.
Springer, 3â€“18.
[37] Andrea Simonetto, Aryan Mokhtari, Alec Koppel, Geert Leus, and Alejandro Ribeiro. 2016. A Class of Prediction-
Correction Methods for Time-Varying Convex Optimization. IEEE Transactions on Signal Processing 64, 17 (2016),
4576â€“4591. doi:10.1109/TSP.2016.2568161 77 citations (Crossref) [2023-12-17].
[38] Jasper Stone, Raj Patel, Farbod Ghiasi, Sudip Mittal, and Shahram Rahimi. 2025. Navigating MLOps: Insights into
Maturity, Lifecycle, Tools, and Careers. arXiv preprint arXiv:2503.15577 (2025).
[39] Tom Viering and Marco Loog. 2023. The Shape of Learning Curves: A Review. IEEE Trans. Pattern Anal. Mach. Intell.
45, 6 (June 2023), 7799â€“7819. doi:10.1109/TPAMI.2022.3220744
[40] Shaoqi Wang, Aidi Pi, and Xiaobo Zhou. 2021. Elastic parameter server: Accelerating ML training with scalable
resource scheduling. IEEE Transactions on Parallel and Distributed Systems 33 (2021), 1128â€“1143.
[41] Menglu Yu, Ye Tian, Bo Ji, Chuan Wu, Hridesh Rajan, and Jia Liu. 2022. Gadget: Online resource optimization for
scheduling ring-all-reduce learning jobs. In IEEE INFOCOM 2022-IEEE Conference on Computer Communications. IEEE,
1569â€“1578.
[42] X. Zhou, L. Chen, and T. Wu. 2024. TinyLLaVA: A Lightweight Visionâ€“Language Assistant. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9876â€“9885.
[43] IndrË™e Å½liobaitË™e, Marcin Budka, and Frederic Stahl. 2015. Towards cost-sensitive adaptation: When is it worth updating
your predictive model? Neurocomputing 150 (2015), 240â€“249.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 19 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
19
A
Technical Appendices
A.1
Proof of Lemma 3.2
Proof. Consider any ğ‘’1,ğ‘’2 âˆˆE and any ğœ†âˆˆ[0, 1]. It holds that
ğ½(ğœ†ğ‘’1 + (1 âˆ’ğœ†)ğ‘’2) =
âˆ«âˆ
0
Â¯ğ‘”
âˆ«ğ‘¡
0
(ğœ†ğ‘’1(ğœ) + (1 âˆ’ğœ†)ğ‘’2(ğœ))ğ‘‘ğœ

Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
(41)
â‰¤
âˆ«âˆ
0

ğœ†Â¯ğ‘”
âˆ«ğ‘¡
0
ğ‘’1(ğœ)ğ‘‘ğœ

+ (1 âˆ’ğœ†) Â¯ğ‘”
âˆ«ğ‘¡
0
ğ‘’2(ğœ)ğ‘‘ğœ

Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
(42)
=ğœ†ğ½(ğ‘’1) + (1 âˆ’ğœ†)ğ½(ğ‘’2),
(43)
where the inequality (42) follows from the fact that Â¯ğ‘”(Â·) is a convex function. Therefore, ğ½(ğ‘’) is a
convex functional on the domain E. Similarly, the functional ğ¶(ğ‘’) is affine since
ğ¶(ğœ†ğ‘’1 + (1 âˆ’ğœ†)ğ‘’2) =
âˆ«âˆ
0
(ğœ†ğ‘’1(ğ‘¡) + (1 âˆ’ğœ†)ğ‘’2(ğ‘¡)) Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡
(44)
= ğœ†ğ¶(ğ‘’1) + (1 âˆ’ğœ†)ğ¶(ğ‘’2),
(45)
which completes the proof.
â–¡
A.2
Proof of Theorem 3.4
Proof. For the front-loading policy to be optimal, we need to show that the switching function
crosses zero at most once from negative to positive. Suppose 0 < ğµ< ğ‘€ğœğ‘’. Here, ğ‘¡âˆ—is uniquely
determined by the budget constraint (9), with 0 < ğ‘¡âˆ—< âˆ. Since the budget constraint is binding,
by the complementary slackness (15), we expect ğœˆ> 0. Given the policy (22), the state and costate
trajectories are
ğ‘¥âˆ—(ğ‘¡) =
(
ğ‘€ğ‘¡
ğ‘¡< ğ‘¡âˆ—
ğ‘€ğ‘¡âˆ—
ğ‘¡â‰¥ğ‘¡âˆ—
(46)
ğ‘âˆ—(ğ‘¡) =
(âˆ«ğ‘¡âˆ—
ğ‘¡
Â¯ğ‘”â€²(ğ‘€ğ‘ ) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ + Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡âˆ—) Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
ğ‘¡< ğ‘¡âˆ—
Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡)
ğ‘¡â‰¥ğ‘¡âˆ—
(47)
At the switch point ğ‘¡âˆ—, we require ğœ™(ğ‘¡âˆ—) = 0, i.e.,
ğœ™(ğ‘¡âˆ—) = ğ‘âˆ—(ğ‘¡âˆ—) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡âˆ—) = Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)( Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡âˆ—) + ğœˆ) = 0.
(48)
Since Â¯ğ¹ğ‘Œ(ğ‘¡) > 0 and Â¯ğ‘”â€²(ğ‘¡) < 0, for 0 < ğ‘¡< âˆ, we have ğœˆ> 0 and
ğœˆ= âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡âˆ—),
(49)
which is consistent with (15).
Now, we verify (13) by showing ğœ™(ğ‘¡) > 0 for ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ), and ğœ™(ğ‘¡) < 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—). For
ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ), where ğ‘’âˆ—(ğ‘¡) = 0, the switching function becomes
ğœ™(ğ‘¡) = ğ‘âˆ—(ğ‘¡) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡) = Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡) + (âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘¡)
(50)
= Â¯ğ¹ğ‘Œ(ğ‘¡) Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)[ğ‘šğ‘Œ(ğ‘¡) âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)].
(51)
Since Â¯ğ¹ğ‘Œ(ğ‘¡) â‰¥0 and Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—) < 0, for ğœ™(ğ‘¡) > 0 we need ğ‘šğ‘Œ(ğ‘¡) < ğ‘šğ‘Œ(ğ‘¡âˆ—). Since ğ‘¡> ğ‘¡âˆ—, the condition
ğ‘šğ‘Œ(ğ‘¡) < ğ‘šğ‘Œ(ğ‘¡âˆ—) holds if ğ‘šğ‘Œ(ğ‘¡) is decreasing for ğ‘¡> ğ‘¡âˆ—.
For ğ‘¡âˆˆ[0,ğ‘¡âˆ—), where ğ‘’âˆ—(ğ‘¡) = ğ‘€, the first derivative of the switching function is
ğœ™â€²(ğ‘¡) = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(ğ‘¥âˆ—(ğ‘¡)) âˆ’ğœˆâ„ğ‘Œ(ğ‘¡)] = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡) + Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡)].
(52)
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 20 ---
20
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
Since Â¯ğ‘”â€²(ğ‘€ğ‘¡) < Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—) for ğ‘¡< ğ‘¡âˆ—,
âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡) + Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡) > âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡âˆ—)(1 âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡)).
(53)
If ğ‘šğ‘Œ(ğ‘¡) > ğ‘šğ‘Œ(ğ‘¡âˆ—) for ğ‘¡< ğ‘¡âˆ—, then
1 âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡) > 1 âˆ’ğ‘šğ‘Œ(ğ‘¡)â„ğ‘Œ(ğ‘¡) > 0,
which since Â¯ğ¹ğ‘Œ(ğ‘¡) > 0 means ğœ™â€²(ğ‘¡) > 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—). Given that ğœ™(ğ‘¡âˆ—) = 0, this implies ğœ™(ğ‘¡) < 0
for ğ‘¡âˆˆ[0,ğ‘¡âˆ—).
For the case ğµâ‰¥ğ‘€ğœğ‘’, since the budget constraint is not binding, by (15), ğœˆ= 0. Since ğ‘’âˆ—(ğ‘¡) = ğ‘€
for ğ‘¡âˆˆ[0, âˆ), the state function satisfies ğ‘¥âˆ—(ğ‘¡) = ğ‘€ğ‘¡, the switching function ğœ™(ğ‘¡) = ğ‘(ğ‘¡), and
the derivative of the costate ğ‘â€²(ğ‘¡) = âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡). Since Â¯ğ‘”â€²(ğ‘¡) < 0 and Â¯ğ¹ğ‘Œ(ğ‘¡) > 0, ğ‘â€²(ğ‘¡) > 0 for
ğ‘¡âˆˆ[0, âˆ). Since by the transversality condition (14) limğ‘¡â†’âˆğ‘(ğ‘¡) = 0 and ğ‘(ğ‘¡) is increasing, we
conclude ğœ™(ğ‘¡) = ğ‘(ğ‘¡) < 0.
â–¡
A.3
Proof of Theorem 3.5
Proof. Suppose there is an optimal resource allocation policy that idles until time ğ‘¡âˆ—, i.e., ğ‘’âˆ—(ğ‘¡) =
0, âˆ€ğ‘¡âˆˆ[0,ğ‘¡âˆ—], and thus ğ‘¥(ğ‘¡) = 0, âˆ€ğ‘¡âˆˆ[0,ğ‘¡âˆ—].
At the switch point ğ‘¡âˆ—we require ğœ™(ğ‘¡âˆ—) = 0,
ğœ™(ğ‘¡âˆ—) = ğ‘âˆ—(ğ‘¡âˆ—) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡âˆ—) = 0.
(54)
Since the resource budget is binding, i.e., ğµ< ğ‘€ğœğ‘’, by (54) and Mean Value Theorem there exists
ğ‘¥0 such that 0 â‰¤ğ‘¥0 < âˆ, which yields
ğœˆ= âˆ’ğ‘âˆ—(ğ‘¡âˆ—)
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—) = âˆ’
âˆ«âˆ
ğ‘¡âˆ—Â¯ğ‘”â€²(ğ‘¥âˆ—(ğ‘ )) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
= âˆ’
Â¯ğ‘”â€²(ğ‘¥0)
âˆ«âˆ
ğ‘¡âˆ—Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
(55)
= âˆ’Â¯ğ‘”â€²(ğ‘¥0)ğ‘šğ‘Œ(ğ‘¡âˆ—) > 0,
(56)
where (15) is satisfied since Â¯ğ‘”â€²(ğ‘¡) < 0.
By (13) and (16), for ğ‘¡âˆˆ[0,ğ‘¡âˆ—) we have ğœ™(ğ‘¡) > 0. Since ğœ™(ğ‘¡âˆ—) = 0, the first derivative of the
switching function for ğ‘¡âˆˆ[0,ğ‘¡âˆ—) should be positive, i.e.,
ğœ™â€²(ğ‘¡) = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(ğ‘¥âˆ—(ğ‘¡)) âˆ’ğœˆâ„ğ‘Œ(ğ‘¡)] = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(0) + Â¯ğ‘”â€²(ğ‘¥0)ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡)] > 0.
(57)
Since Â¯ğ‘”â€²(0) â‰¤Â¯ğ‘”â€²(ğ‘¥0) for 0 â‰¤ğ‘¥1 < âˆ,
âˆ’Â¯ğ‘”â€²(0) + Â¯ğ‘”â€²(ğ‘¥0)ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡) â‰¥âˆ’Â¯ğ‘”â€²(ğ‘¥0)(1 âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡)).
(58)
If ğ‘šğ‘Œ(ğ‘¡) > ğ‘šğ‘Œ(ğ‘¡âˆ—) for ğ‘¡< ğ‘¡âˆ—, then
1 âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡) > 1 âˆ’ğ‘šğ‘Œ(ğ‘¡)â„ğ‘Œ(ğ‘¡) > 0,
where the last inequality follows from the IMRL property. Thus, since Â¯ğ¹ğ‘Œ(ğ‘¡) > 0, if ğ‘Œhas IMRL,
then ğœ™â€²(ğ‘¡) > 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—).
â–¡
A.4
Proof of Corollary 3.6
Proof. For the back-loading policy to be optimal, we need to show that the switching function
crosses zero at most once from positive to negative. Suppose 0 < ğµ< ğ‘€ğœğ‘’. Here, ğ‘¡âˆ—is uniquely
determined by the budget constraint (9), with 0 < ğ‘¡âˆ—< âˆ. Since the budget constraint is binding,
by the complementary slackness (15), we expect ğœˆ> 0. Given the policy (22), the state and costate
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 21 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
21
trajectories are
ğ‘¥âˆ—(ğ‘¡) =
(
0
ğ‘¡< ğ‘¡âˆ—
ğ‘€(ğ‘¡âˆ’ğ‘¡âˆ—)
ğ‘¡â‰¥ğ‘¡âˆ—
(59)
ğ‘âˆ—(ğ‘¡) =
(âˆ«âˆ
ğ‘¡âˆ—Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
ğ‘¡< ğ‘¡âˆ—
âˆ«âˆ
ğ‘¡
Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
ğ‘¡â‰¥ğ‘¡âˆ—.
(60)
At the switch point ğ‘¡âˆ—we require ğœ™(ğ‘¡âˆ—) = 0,
ğœ™(ğ‘¡âˆ—) = ğ‘âˆ—(ğ‘¡âˆ—) + ğœˆÂ¯ğ¹ğ‘Œ(ğ‘¡âˆ—) = 0.
(61)
By (61) and Mean Value Theorem, there exists ğ‘¥1 such that 0 â‰¤ğ‘¥1 < âˆwhich yields
ğœˆ= âˆ’ğ‘âˆ—(ğ‘¡âˆ—)
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—) = âˆ’
âˆ«âˆ
ğ‘¡âˆ—Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
= âˆ’
Â¯ğ‘”â€²(ğ‘¥1)
âˆ«âˆ
ğ‘¡âˆ—Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
(62)
= âˆ’Â¯ğ‘”â€²(ğ‘¥1)ğ‘šğ‘Œ(ğ‘¡âˆ—) > 0,
(63)
where (15) is satisfied since Â¯ğ‘”â€²(ğ‘¡) < 0.
Now, we verify (13) by showing ğœ™(ğ‘¡) > 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—), and ğœ™(ğ‘¡) < 0 for ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ). For
ğ‘¡âˆˆ[0,ğ‘¡âˆ—), where ğ‘’âˆ—(ğ‘¡) = 0, the first derivative of the switching function is
ğœ™â€²(ğ‘¡) = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(ğ‘¥âˆ—(ğ‘¡)) âˆ’ğœˆâ„ğ‘Œ(ğ‘¡)] = Â¯ğ¹ğ‘Œ(ğ‘¡)[âˆ’Â¯ğ‘”â€²(0) + Â¯ğ‘”â€²(ğ‘¥1)ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡)].
(64)
Since Â¯ğ‘”â€²(0) â‰¤Â¯ğ‘”â€²(ğ‘¥1) for 0 â‰¤ğ‘¥1 < âˆ,
âˆ’Â¯ğ‘”â€²(0) + Â¯ğ‘”â€²(ğ‘¥1)ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡) â‰¥âˆ’Â¯ğ‘”â€²(ğ‘¥1)(1 âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡)).
(65)
If ğ‘šğ‘Œ(ğ‘¡) > ğ‘šğ‘Œ(ğ‘¡âˆ—) for ğ‘¡< ğ‘¡âˆ—, then
1 âˆ’ğ‘šğ‘Œ(ğ‘¡âˆ—)â„ğ‘Œ(ğ‘¡) > 1 âˆ’ğ‘šğ‘Œ(ğ‘¡)â„ğ‘Œ(ğ‘¡) > 0,
which since Â¯ğ¹ğ‘Œ(ğ‘¡) > 0 means ğœ™â€²(ğ‘¡) > 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—), and the last inequality follows from the
IMRL property. Given that ğœ™(ğ‘¡âˆ—) = 0, this implies ğœ™(ğ‘¡) < 0 for ğ‘¡âˆˆ[0,ğ‘¡âˆ—).
For ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ), where ğ‘’âˆ—(ğ‘¡) = ğ‘€, the switching function becomes
ğœ™(ğ‘¡) =
âˆ«âˆ
ğ‘¡
Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ âˆ’
âˆ«âˆ
ğ‘¡âˆ—Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
Â¯ğ¹ğ‘Œ(ğ‘¡).
(66)
The condition ğœ™(ğ‘¡) < 0 for ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ) is satisfied if the following inequality holds for ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ):
âˆ«âˆ
ğ‘¡
Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡)
<
âˆ«âˆ
ğ‘¡âˆ—Â¯ğ‘”â€²(ğ‘€(ğ‘ âˆ’ğ‘¡âˆ—)) Â¯ğ¹ğ‘Œ(ğ‘ )ğ‘‘ğ‘ 
Â¯ğ¹ğ‘Œ(ğ‘¡âˆ—)
.
(67)
For the expected loss function with constant negative first derivative, i.e., Â¯ğ‘”â€²(ğ‘¡) = âˆ’ğ›½, ğ›½> 0,
Equation (67) becomes
ğ‘šğ‘Œ(ğ‘¡) > ğ‘šğ‘Œ(ğ‘¡âˆ—),
âˆ€ğ‘¡âˆˆ(ğ‘¡âˆ—, âˆ).
(68)
For the case ğµâ‰¥ğ‘€ğœğ‘’, since the budget constraint is not binding, by (15), ğœˆ= 0. Since ğ‘’âˆ—(ğ‘¡) = ğ‘€
for ğ‘¡âˆˆ[0, âˆ), the state function is ğ‘¥âˆ—(ğ‘¡) = ğ‘€ğ‘¡, the switching function is ğœ™(ğ‘¡) = ğ‘(ğ‘¡), and the
derivative of the costate is ğ‘â€²(ğ‘¡) = âˆ’Â¯ğ‘”â€²(ğ‘€ğ‘¡) Â¯ğ¹ğ‘Œ(ğ‘¡). Since Â¯ğ‘”â€²(ğ‘¡) < 0 and Â¯ğ¹ğ‘Œ(ğ‘¡) > 0, ğ‘â€²(ğ‘¡) > 0 for
ğ‘¡âˆˆ[0, âˆ). Since due to the transversality condition (14) limğ‘¡â†’âˆğ‘(ğ‘¡) = 0 and ğ‘(ğ‘¡) is increasing,
ğœ™(ğ‘¡) = ğ‘(ğ‘¡) < 0.
â–¡
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 22 ---
22
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
A.5
Proof of Lemma 4.2
Proof. Let us define a step function ğ‘ : [0, âˆ) â†’R associated with the deployment offsets
{ğ›¿ğ‘—}ğ¾
ğ‘—=0 as
ğ‘ (ğ‘¡) =
(
ğ›¿ğ‘—
if ğ‘¡âˆˆ[ğ›¿ğ‘—,ğ›¿ğ‘—+1) for ğ‘—= 0, . . . , ğ¾âˆ’1,
ğ›¿ğ¾
if ğ‘¡âˆˆ[ğ›¿ğ¾, âˆ),
(69)
where ğ¾= sup{ğ‘˜: ğ‘˜âˆˆN}, ğ›¿ğ¾+1 = âˆand ğ›¿0 = 0. Due to (69), the objective function (32) can be
written as a single integral,
â„({ğ›¿ğ‘—}ğ¾
ğ‘—=1) =
âˆ«âˆ
0
Â¯ğ‘”(ğ‘ (ğ‘¡)) Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡.
(70)
Let {ğ›¿(1)
ğ‘—
}ğ¾
ğ‘—=1 and {ğ›¿(2)
ğ‘—
}ğ¾
ğ‘—=1 be two different set of offsets, and ğ‘§ğ‘—= ğœ†ğ›¿(1)
ğ‘—
+ (1 âˆ’ğœ†)ğ›¿(2)
ğ‘—
, ğœ†âˆˆ[0, 1]
be the component-wise convex combination. Then {ğ‘§ğ‘—}ğ¾
ğ‘—=1 also satisfies the ordering constraint
and is feasible. Let ğ‘ (1) (ğ‘¡), ğ‘ (2) (ğ‘¡), ğ‘ (ğ‘§) (ğ‘¡) be the corresponding step functions.
Letğ‘–(1),ğ‘–(2) be such thatğ‘¡âˆˆ[ğ›¿(1)
ğ‘–(1),ğ›¿(1)
ğ‘–(1)+1) andğ‘¡âˆˆ[ğ›¿(2)
ğ‘–(2),ğ›¿(2)
ğ‘–(2)+1), respectively. Letğ‘š= min{ğ‘–(1),ğ‘–(2)}.
Since ğ›¿(1)
ğ‘š,ğ›¿(2)
ğ‘š
â‰¤ğ‘¡, their convex combination ğ‘§ğ‘šâ‰¤ğ‘¡. Hence, the left endpoint of the interval in
which ğ‘¡falls under {ğ‘§ğ‘—}ğ¾
ğ‘—=1 is at most ğ‘¡, implying that ğ‘ (ğ‘§) (ğ‘¡) â‰¥ğ‘§ğ‘šâ‰¥min{ğ›¿(1)
ğ‘š,ğ›¿(2)
ğ‘š}. Thus,
ğ‘ (ğ‘§) (ğ‘¡) â‰¥min{ğ‘ (1) (ğ‘¡),ğ‘ (2) (ğ‘¡)}.
(71)
Since Â¯ğ‘”is non-increasing, Â¯ğ¹ğ‘Œ(ğ‘¡) â‰¥0, âˆ€ğ‘¡, and by (71) it follows that for any fixed ğ‘¡
Â¯ğ‘”(ğ‘ (ğ‘§) (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡) â‰¤Â¯ğ‘”(min{ğ‘ (1) (ğ‘¡),ğ‘ (2) (ğ‘¡)})ğ¹ğ‘Œ(ğ‘¡)
(72)
â‰¤max{ Â¯ğ‘”(ğ‘ (1) (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡), Â¯ğ‘”(ğ‘ (2) (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡)}.
(73)
Thus, the function {ğ›¿ğ‘—}ğ¾
ğ‘—=0 â†¦â†’Â¯ğ‘”(ğ‘ (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡) is quasi-convex in {ğ›¿ğ‘—}ğ¾
ğ‘—=0 for any fixed ğ‘¡âˆˆ[0, âˆ).
Under the assumption that the offset sets {ğ›¿(1)
ğ‘—
}ğ¾
ğ‘—=1 and {ğ›¿(2)
ğ‘—
}ğ¾
ğ‘—=1 are component-wise greater or
equal, i.e., ğ›¿(1)
ğ‘—
â‰¥ğ›¿(2)
ğ‘—
, âˆ€ğ‘—, since the pointwise dominance of either of the offset preserved for all ğ‘¡,
it follows that
âˆ«âˆ
0
Â¯ğ‘”(ğ‘ (ğ‘§) (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡â‰¤max{
âˆ«âˆ
0
Â¯ğ‘”(ğ‘ (1) (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡,
âˆ«âˆ
0
Â¯ğ‘”(ğ‘ (2) (ğ‘¡))ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡}.
(74)
â–¡
A.6
Proof of Theorem 4.3
Proof. Given the objective function (36),
â„({ğ›¿ğ‘—}ğ‘ğ·
ğ‘—=1) :=
ğ‘ğ·âˆ’1
âˆ‘ï¸
ğ‘—=0
Â¯ğ‘”(ğ›¿ğ‘—)
âˆ«ğ›¿ğ‘—+1
ğ›¿ğ‘—
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡+ Â¯ğ‘”(ğ›¿ğ‘ğ·)
âˆ«âˆ
ğ›¿ğ‘ğ·
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡.
(75)
By the first order optimality conditions, we have
ğœ•â„
ğœ•ğ›¿ğ‘ğ·

{ğ›¿âˆ—
ğ‘—}ğ‘ğ·
ğ‘—=1
= Â¯ğ‘”(ğ›¿âˆ—
ğ‘ğ·âˆ’1) Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘ğ·) + Â¯ğ‘”â€²(ğ›¿âˆ—
ğ‘ğ·)
âˆ«âˆ
ğ›¿âˆ—
ğ‘ğ·
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡âˆ’Â¯ğ‘”(ğ›¿âˆ—
ğ‘ğ·) Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘ğ·) = 0,
(76)
(77)
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 23 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
23
and for ğ‘—= 1, . . . ğ‘ğ·âˆ’1 it holds that
ğœ•â„
ğœ•ğ›¿ğ‘—

{ğ›¿âˆ—
ğ‘—}ğ‘ğ·
ğ‘—=1
= Â¯ğ‘”(ğ›¿âˆ—
ğ‘—âˆ’1) Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—) + Â¯ğ‘”â€²(ğ›¿âˆ—
ğ‘—)
âˆ«ğ›¿âˆ—
ğ‘—
ğ›¿âˆ—
ğ‘—âˆ’1
Â¯ğ¹ğ‘Œ(ğ‘¡)ğ‘‘ğ‘¡âˆ’Â¯ğ‘”(ğ›¿âˆ—
ğ‘—) Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—) = 0.
(78)
(79)
Rearranging these terms yields (38).
To prove that the effective rate is increasing in ğ‘ğ·, let ğ›¿âˆ—
ğ‘—(ğ‘ğ·) denote the ğ‘—-th optimal deployment
offset where ğ‘ğ·is the total number of deployments. By the convexity of Â¯ğ‘”, the monotonicity of Â¯ğ¹ğ‘Œ,
and the KKT conditions, one can show that
ğ›¿âˆ—
ğ‘—(ğ‘ğ·+ 1) < ğ›¿âˆ—
ğ‘—(ğ‘ğ·),
for ğ‘—= 1, Â· Â· Â· , ğ‘ğ·.
(80)
Since Â¯ğ¹ğ‘Œis decreasing, (80) implies that
Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—(ğ‘ğ·+ 1)) > Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—(ğ‘ğ·)),
for ğ‘—= 1, Â· Â· Â· , ğ‘ğ·.
(81)
Therefore, the difference between ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘—}ğ‘ğ·+1
ğ‘—=0
) and ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘—}ğ‘ğ·
ğ‘—=0) is positive, i.e.,
ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘—}ğ‘ğ·+1
ğ‘—=0
) âˆ’ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘—}ğ‘ğ·
ğ‘—=0) =
ğ‘ğ·+1
âˆ‘ï¸
ğ‘—=1
Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—(ğ‘ğ·+ 1)) âˆ’
ğ‘ğ·
âˆ‘ï¸
ğ‘—=1
Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—(ğ‘ğ·))
(82)
=Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘ğ·+1(ğ‘ğ·+ 1)) +
ğ‘ğ·
âˆ‘ï¸
ğ‘—=1
Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—(ğ‘ğ·+ 1))
(83)
âˆ’Â¯ğ¹ğ‘Œ(ğ›¿âˆ—
ğ‘—(ğ‘ğ·)) > 0.
(84)
â–¡
A.7
Proof of Corollary 4.4
Proof. For Â¯ğ‘”(ğ‘¡) = ğ›¼ğ‘’âˆ’ğ›½ğ‘¡and Â¯ğ¹ğ‘Œ(ğ‘¡) = ğ‘’âˆ’ğœ†ğ‘¡, by Theorem 4.3 it holds that
1
ğ›½ğ‘’ğ›½Î”âˆ—
ğ‘˜(1 âˆ’ğ‘’âˆ’ğ›½Î”âˆ—
ğ‘˜) =
(
1
ğ‘˜= ğ‘ğ·
1
ğœ†(1 âˆ’ğ‘’âˆ’ğœ†Î”âˆ—
ğ‘˜+1)
ğ‘˜= 1, Â· Â· Â· , ğ‘ğ·âˆ’1.
(85)
Rearranging the terms yields (39).
â–¡
A.8
Proof of Theorem 4.5
Proof. Under the randomized policy described in Theorem 4.5, the number of deployments
within concept durations depends on whether the deployment schedule in concept ğ‘–is ğœ‹ğ‘–= {ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·
ğ‘˜=0
or ğœ‹ğ‘–= {ğ›¿âˆ—
ğ‘˜}ğ‘ğ·+1âˆ—
ğ‘˜=0
. Therefore, the time-average deployment rate under randomized deployment
policy can be written as
lim sup
ğ‘¡â†’âˆ
1
ğ‘¡
ğ¼(ğ‘¡)
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘–=
E[ğ‘Â· I{ğœ‹= {ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·
ğ‘˜=0}] + E[ğ‘Â· I{ğœ‹= {ğ›¿âˆ—
ğ‘˜}ğ‘ğ·+1âˆ—
ğ‘˜=0
}]
E[ğ‘Œ]
(86)
=
P(ğœ‹= {ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·
ğ‘˜=0) Ãğ‘âˆ—
ğ·
ğ‘—=1 Â¯ğ¹ğ‘Œ(ğ›¿ğ‘—) + P(ğœ‹= {ğ›¿âˆ—
ğ‘˜}ğ‘ğ·+1âˆ—
ğ‘˜=0
) Ãğ‘ğ·+1âˆ—
ğ‘—=1
Â¯ğ¹ğ‘Œ(ğ›¿ğ‘—)
E[ğ‘Œ]
(87)
=
ğ›¾ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}
ğ‘âˆ—
ğ·
ğ‘˜=0) + (1 âˆ’ğ›¾)ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}ğ‘ğ·+1âˆ—
ğ‘˜=0
)
E[ğ‘Œ]
,
(88)
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 24 ---
24
Hasan Burhan Beytur, Gustavo de Veciana, Haris Vikalo, and Kevin S Chan
where (87) follows from the independence of the number of deployments and the fixed deployment
policy selections in each concept.
As shown in Theorem 4.3, since the effective deployment rate achieved by the optimal scheduler
ğ‘Ÿğ‘’({ğ›¿âˆ—
ğ‘˜}ğ‘ğ·
ğ‘˜=0) = Ãğ‘ğ·
ğ‘—=1 Â¯ğ¹ğ‘Œ(ğ›¿ğ‘—) is monotonically increasing in the number of deployments ğ‘ğ·within
the range [0, âˆ), there exists ğ‘âˆ—
ğ·âˆˆN andğ›¾âˆˆ[0, 1] which ensure that the time-average deployment
rate in (88) matches the deployment rate limit ğ‘Ÿğ·.
â–¡
A.9
Additional Simulation Results with Alternative Expected Loss Curves
Note that our analysis of the optimal policies applies to any convex decreasing expected loss
curve. However, the improvements over intuitive deployment heuristics depend on the probability
distribution of concept duration, functional form of the expected loss function, and additional
system parameters. In the figures below, we provide additional simulation results for the training
resource allocation problem with different expected loss curves.
(a) Comparison between constraint-binding optimal
and fixed resource allocation policies. ğ‘Œâˆ¼Exp(1),
Â¯ğ‘”(ğ‘¡) = 1/
âˆš
1 + ğ‘¡, ğœğ‘’= 1, ğ‘€= 20.
(b) Comparison between constraint-binding optimal
and fixed resource allocation policies. ğ‘Œâˆ¼Exp(1),
Â¯ğ‘”(ğ‘¡) = 1/(1 + ğ‘¡), ğœğ‘’= 1, ğ‘€= 20.
(c) Comparison between constraint-binding optimal
and fixed resource allocation policies. ğ‘Œâˆ¼Exp(1),
Â¯ğ‘”(ğ‘¡) = ğ‘¡âˆ’0.3, ğœğ‘’= 1, ğ‘€= 20.
(d) Comparison between constraint-binding optimal
and fixed resource allocation policies. ğ‘Œâˆ¼Exp(1),
Â¯ğ‘”(ğ‘¡) = ğ‘¡âˆ’0.7, ğœğ‘’= 1, ğ‘€= 20.
Fig. 4. Comparison of optimal and fixed resource allocation policies under sudden concept drift for different
expected loss curves. All simulations were executed on a MacBook Air M3 with 16 GB of RAM.
, Vol. 1, No. 1, Article . Publication date: December 2025.


--- Page 25 ---
Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift
25
The simulation results demonstrate that the improvement over fixed resource allocation is greater
when the expected loss curve decreases more rapidly in the initial phase, as front-loading policy
effectively utilizes resources at the beginning of a concept.
We emphasize that the observed performance gains depend critically on the available resource
budget. When the budget for model updates is very limited, improvements are naturally con-
strained, whereas with an abundant budget the different deployment policies converge to similar
performance. The results shown in the figures correspond to intermediate budgets, where gains
are most pronounced. These findings â€“ both qualitative and quantitative â€“ offer valuable guidance
for selecting training and deployment strategies according to the providerâ€™s cost sensitivity and
available computational resources.
, Vol. 1, No. 1, Article . Publication date: December 2025.
