--- Page 1 ---
FINE-TUNING LARGE LANGUAGE MODEL FOR AUTO-
MATED ALGORITHM DESIGN
Fei Liu, Rui Zhang, Xi Lin, Zhichao Lu, Qingfu Zhang
Department of Computer Science, City University of Hong Kong, Hong Kong SAR
{fliu36-c,rui.zhang.cs,xi.lin}@my.cityu.edu.hk
{zhichao.lu,qingfu.zhang}@cityu.edu.hk
ABSTRACT
The integration of large language models (LLMs) into automated algorithm design
has shown promising potential. A prevalent approach embeds LLMs within search
routines to iteratively generate and refine candidate algorithms. However, most
existing methods rely on off-the-shelf LLMs trained for general coding tasks,
leaving a key question open: Do we need LLMs specifically tailored for algorithm
design? If so, how can such LLMs be effectively obtained and how well can they
generalize across different algorithm design tasks?
In this paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank-
based (DAR) sampling strategy to balance training data diversity and quality, then
we leverage direct preference optimization to efficiently align LLM outputs with
task objectives. Our experiments, conducted on Llama-3.2-1B-Instruct and Llama-
3.1-8B-Instruct, span three distinct algorithm design tasks. Results suggest that
finetuned LLMs can significantly outperform their off-the-shelf counterparts with
the smaller Llama-3.2-1B-Instruct and match the larger Llama-3.1-8B-Instruct on
the admissible set problem. Moreover, we observe promising generalization: LLMs
finetuned on specific algorithm design tasks also improve performance on related
tasks with varying settings. These findings highlight the value of task-specific
adaptation for LLMs in algorithm design and open new avenues for future research.
1
INTRODUCTION
The emerging field of automated algorithm design (AAD) with large language models (LLMs) has
attracted growing attention for its potential to automate the synthesis of expert-level algorithms (Liu
et al., 2024c;b; Romera-Paredes et al., 2024). A prevailing paradigm in this space combines LLMs
within search strategies, where the LLM focuses on generating candidate algorithms and the search
procedures controls the quality and refinement of these algorithms in an iterative manner (Zhang et al.,
2024). This framework has lead to notable advances across a spectrum of algorithmic development
tasks, including combinatorial optimization (Romera-Paredes et al., 2024; Liu et al., 2024b; Ye et al.,
2024), Bayesian optimization (Yao et al., 2024), and black-box optimization (van Stein & B¨ack,
2024), to name a few.
Despite these preliminary successes, most existing LLM-driven AAD approaches rely on off-the-
shelf LLMs, posing two limitations: ❶they require a large number of queries to LLMs, resulting
in substantial computational overhead (Romera-Paredes et al., 2024; Novikov et al., 2025), and
❷these methods exhibit marginal performance variations across different choice of LLMs (Liu
et al., 2024d; Zhang et al., 2024), suggesting that current LLMs may lack inductive biases suited to
algorithm design. These observations raise a potential need of specialized LLMs explicitly trained
for algorithm design tasks. While prior work has explored domain-specific LLMs for general coding
(e.g., programming) tasks (Jiang et al., 2024) and optimization problem formulation (Huang et al.,
2025a), and some recent efforts have fine-tuned LLMs during the search process to improve AAD
performance (Huang et al., 2025b; ˇSurina et al., 2025), the development of LLMs tailored specifically
for automated algorithm design remains largely underexplored.
1
arXiv:2507.10614v1  [cs.LG]  13 Jul 2025


--- Page 2 ---
Moreover, fine-tuning LLMs for algorithm design poses unique challenges distinct from conventional
code generation (Jiang et al., 2024) or mathematical reasoning (Ahn et al., 2024) tasks. Algorithm
design tasks rarely has clear ground-truth labels as optimal algorithms may not exist and cannot be
evaluated by a single performance metric. The algorithm design process benefits from exploring di-
verse algorithms—even suboptimal ones—as they may introduce novel ideas or structural approaches
that provide valuable insights and ultimately improve the final design. These characteristics render
existing fine-tuning techniques insufficient for addressing the inherent complexity of fine-tuning
LLMs for AAD.
In this work, we take a preliminary yet foundational step towards developing specialized LLMs for
algorithm design tasks. Our investigation is guided by the following two research questions:
• RQ1: How can we effectively obtain LLMs specialized for algorithm design?
• RQ2: How well can these LLMs generalize across different algorithm design tasks?
To address RQ1, we fine-tune general-purpose, open-source LLMs—specifically, Llama3.2-1B-
Instruct and Llama-3.1-8B-Instruct (Grattafiori et al., 2024)—on algorithm design problems. We
introduce a Diversity-Aware Rank-based (DAR) Sampling strategy to construct diverse preference
pairs, which serve as training data for fine-tuning via Direct Preference Optimization (DPO) (Rafailov
et al., 2023). We evaluate the resulting LLMs against their original counterparts in two settings: (i)
random sampling, and (ii) integration within an existing AAD framework.
To address RQ2, we assess the generalization capabilities of LLMs fine-tuned on the Capacitated
Vehicle Routing Problem (CVRP) across two scenarios: (i) generalization to variant settings of the
same problem (e.g., CVRP instances with different sizes and capacity constraints), and (ii) transfer
to a related but distinct algorithm design task—namely, the Traveling Salesman Problem (TSP).
These evaluations allow us to examine both in-distribution and out-of-distribution generalization
performance.
Our key findings are summarized as follows.
• Fine-tuning LLMs specifically for algorithm design is necessary and feasible. The proposed
Diversity-Aware Rank-based Sampling strategy enables effective and robust LLM fine-
tuning, underscoring the importance of considering diversity in algorithm preference pair
construction.
• Fine-tuned LLMs significantly improve their capabilities in (1) algorithm design with LLM-
based random sampling (Sec. 4.4), (2) algorithm design with LLM-based iterative search
(Sec 4.4), and (3) similar/related algorithm design tasks (Sec. 4.5). Notably, Llama-3.2-1B-
Instruct trained with our method matches the performance of Llama-3.1-8B-Instruct.
2
RELATED WORKS
2.1
LLM-BASED AUTOMATED ALGORITHM DESIGN
Among various approaches to integrating large language models (LLMs) into automated algorithm
design (AAD) (Liu et al., 2024c), a commonly adopted paradigm is to iteratively use LLMs to design
and refine algorithms within an evolutionary framework (Zhang et al., 2024). This paradigm has
demonstrated strong performance and robustness across diverse algorithm design tasks, including
combinatorial optimization (Liu et al., 2024b), Bayesian optimization (Yao et al., 2024), and black-
box optimization (van Stein & B¨ack, 2024). For instance, EoH (Liu et al., 2023; 2024b; Yao et al.,
2025) evolves both thoughts and code by introducing distinct prompt strategies to guide effective
LLM-based algorithm search. ReEvo (Ye et al., 2024) incorporates short- and long-term reflection
strategies into the evolutionary framework, providing additional information during the refinement
process. Llamea (van Stein & B¨ack, 2024) adopts a 1+1 evolutionary search framework with
generation and mutation operators for black-box optimization. While these methods have proven
successful in diverse algorithm design tasks, they only adopt pre-trained LLMs trained for general
tasks.
2


--- Page 3 ---
2.2
LEARNING TO OPTIMIZE
Learning to optimize seeks to automate and accelerate algorithm development by leveraging machine
learning (Tang & Yao, 2024; Chen et al., 2022; Bengio et al., 2021; Ma et al., 2025). Existing
approaches include performance prediction for algorithm selection (Jiang et al., 2021), algorithm
configuration (Halim et al., 2021), and training algorithm portfolios (Xu et al., 2010). Among these,
deep learning-based neural solvers have gained prominence due to their automation capabilities and
strong empirical performance (Ma et al., 2025). These solvers typically operate in two paradigms:
direct solution generation in an end-to-end manner (Vinyals et al., 2015; Kool et al., 2018), and
algorithm enhancement, where learned models improve existing optimization methods. However,
current neural solvers often function as black-box heuristics, suffering from limited interpretability
and generalization challenges (Berto et al., 2025; Liu et al., 2024a).
2.3
FINE-TUNING LLM FOR CODE GENERATION & OPTIMIZATION
Instruction tuning enhances the ability of large language models (LLMs) to follow diverse natural
language instructions, improving zero-shot performance on unseen tasks and facilitating effective
generalization across instruction-based scenarios. For example, ORLM (Huang et al., 2025a) intro-
duces a benchmark for optimization modeling and employs instruction tuning to fine-tune LLMs
for solving practical operations research (OR) problems. Other approaches, such as Reinforcement
Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Direct Preference Optimization
(DPO) (Rafailov et al., 2023), leverage human preference data and are efficient for fine-tuning LLMs.
Reinforcement learning (RL) techniques, such as GRPO (Guo et al., 2025), automatically learn
solutions from reward signals and have recently demonstrated strong performance in code generation
and reasoning tasks. However, these methods typically require carefully designed reward functions
and involve computationally expensive fine-tuning. Recent works, including EvoTune (ˇSurina et al.,
2025) and CALM (Huang et al., 2025b), adopt DPO and GRPO for online fine-tuning of LLMs
during algorithm search. While these approaches improve online search results, they lack sufficient
analysis and evaluation of the fine-tuned LLMs themselves.
3
FINE-TUNE LLM FOR AAD
Rather than training algorithm design LLMs from scratch, we adopt a fine-tuning approach to adapt
LLMs for automated algorithm design tasks. Among various learning methods, we employ Direct
Preference Optimization (DPO), a reward-free method that trains models to prefer high-quality
outputs over inferior ones using preference pairs.
As shown in Figure 1, our framework consists of two stages: 1) Data Generation: We use LLM-driven
iterative algorithm search (e.g., EoH (Liu et al., 2024b)) to generate diverse algorithms (Upper
section). 2) Preference Learning: The collected algorithms are sampled to compose preference pairs
(samples), enabling the LLM to learn preferred designs over less favoured ones (Lower section). The
fine-tuned LLMs can be used to generate algorithms.
3.1
DATA GENERATION
We employ LLMs in iterative search methods (e.g., EoH (Liu et al., 2024b) and FunSearch (Romera-
Paredes et al., 2024)) to generate algorithms. These search methods maintain a population of
algorithms, using LLMs to generate new algorithms or refine existing ones, thereby evolving the
population over time. Unlike repeated sampling with LLMs, this approach produces high-quality,
diverse algorithms (Zhang et al., 2024), enabling more effective learning. Throughout this process,
we record all valid algorithms generated to form the algorithm database D. These algorithms in D
are subsequently used to construct training datasets Dt using a diversity-aware rank-based sampling
strategy, as detailed in the next section.
3.2
DIVERSITY-AWARE RANK-BASED SAMPLING
A natural and intuitive method for constructing preference pairs is to define a fitness threshold and
sample positive algorithms (y+) above this threshold and negative algorithms (y−) from those below
3


--- Page 4 ---
Figure 1: Upper section: LLM-based automated algorithm design methods iteratively refine and
optimize algorithms. Through this, algorithms and their fitness are preserved in the database D. The
knowledge and experiences incorporated in the database subsequently improve the capabilities of the
LLM. Lower section: (a) Traditional sampling relies on continuous fitness values and often suffers
from unstable preference gaps. (b) Our method discretizes the fitness space into ranked subsets,
enabling structured sampling of high-quality y+ and clearly worse y−. (c) These sampled pairs are
combined with prompts and code templates to form training triples (samples) (x, y+, y−) for DPO
fine-tuning.
it (as illustrated in Figure 1(a)). A similar idea is also adopted in prior work, such as EvoTune (ˇSurina
et al., 2025), where only those preference pairs in which the y+ exhibits top-tier performance are
retained for training. While this strategy can ensure high-quality positive algorithms, discarding a
large number of potentially informative samples with mid-range performance may limit the diversity
of preference signals available to the learning model.
To address these limitations, we propose a diversity-aware rank-based sampling strategy that con-
structs preference pairs in a more structured yet flexible way. This method aims to strike a balance
between quality emphasis (favoring stronger candidates) and diversity (preserving a range of
heuristic qualities), leading to more informative and robust preference supervision.
Let the algorithm database be denoted by D = {y1, . . . , yN}, where the candidates are sorted in
descending order of fitness, i.e., y1 is the best and yN the worst. We partition D into M equally-sized
and disjoint subsets:
S1, S2, . . . , SM,
with |Sm| =
 N
M

(m = 1, . . . , M),
so that S1 contains the top-ranked, S2 the next best, and so on (see Figure 1(b)). We perform
following steps to obtain a preference sample pair (x, y+, y−):
1. Subset selection (biased toward higher quality). From the first M−2 subsets, we pick an index
i ∈{1, . . . , M−2} with probability
Pr(i) = exp((M −2 −i)/τ)
PM−2
k=1 exp(k/τ)
,
where τ > 0 is a temperature hyperparameter (we set to 3.0 by default). A smaller τ sharpens
the distribution, increasing the chance of drawing from higher-quality subsets. The temperature τ
controls exploitation vs. exploration: When τ →0 the positive sample will almost always draw from
the very best subset(s); While when τ →∞, it reduces to uniform sampling over the first M−2
subsets.
2. Positive algorithm. Sample one candidate uniformly from the chosen subset:
y+ ∼Uniform(Si).
4


--- Page 5 ---
3. Negative algorithm. To ensure a clear performance gap, we skip the nearest subset (i.e., Si+1)
and sample uniformly from the rest subsets:
y−∼Uniform


M
[
j=i+2
Sj

.
Note that we skip subset Si+1 in this step enforces a minimum gap of one quality tier, yielding clearer
supervision signals.
4. Preference sample construction. A major distinction in our implementation of standard DPO
lies in how we construct the prompt x. Different from standard DPO, where y+ and y−algorithms
are conditioned on a prompt x, we adopt a fixed prompt template consisting of two components:
(1) a description of the algorithm design task to be solved, and (2) a function template and skeleton
representing the expected format of the algorithm. This design choice has two advantages. Please
refer to Appendix for the elaborated prompt content. The resulting preference sample pair is a triplet
(x, y+, y−). Once a preference sample pair is obtained, we remove y+ and y−from the database to
eliminate duplication. This process is repeated to construct a training dataset Dt.
Empirical studies in Sec. 4.4 demonstrate that the proposed sampling strategy maintains a balance
between selecting high-quality heuristics and preserving diversity, thus producing rich and instructive
training signals for algorithm preference learning.
3.3
DPO FINE-TUNING
We employ Direct Preference Optimization (DPO) (Rafailov et al., 2023) to fine-tune LLMs using
constructed training samples. The LLM acts as a policy πθ(y|x), where x is an input prompt and y is
a generated algorithm. Our objective is to optimize the LLM’s preferences toward high-performing
algorithms while maintaining generalization.
To optimize the policy πθ, we use an objective with regularization to ensure the outputs remain close
to those of the reference model πref(y|x):
max
πθ Ex∼Dt,y∼πθ[r(x, y)] −βDKL[πref(·|x) ∥πθ(·|x)],
(1)
where DKL is the reverse KL-divergence, β controls the regularization strength, and πref is the initial
base LLM policy π0
θ. While reward maximization methods like PPO (Schulman et al., 2017) exist,
they are computationally expensive. Instead, we formulate the task as preference optimization,
allowing algorithm ranking based on r(x, y).
For a training dataset Dt = {(xi, yi
+, yi
−)}n
i=1, the loss function is:
LDPO = −E(x,y+,y−)∼Dt

log σ

β

log πθ(y+|x)
πref(y+|x) −log πθ(y−|x)
πref(y−|x)

,
(2)
where σ is the sigmoid function. This approach eliminates the need for separate reward models or
complex reward heuristics used in RL methods like GRPO (Huang et al., 2025b), enabling efficient
off-policy optimization.
We apply low-rank adapters (LoRA) (Hu et al., 2022) to fine-tune the model efficiently. LoRA
introduces a small number of trainable parameters into existing layers of the model, allowing effective
adaptation without updating the full parameter set. The adoption of LoRA is further motivated by the
inherent challenges of our task: collecting sufficient algorithm data is expensive, as each candidate
algorithm must undergo a computationally expensive evaluation phase during search. Therefore,
applying LoRA is particularly important in our case, as the size of our training data may not be
sufficient to support full fine-tuning without overfitting. Empirically, we have observed that full
fine-tuning failed to converge during training on our datasets, further motivating our adoption of
LoRA.
5


--- Page 6 ---
4
EXPERIMENTAL STUDIES
4.1
ALGORITHM DESIGN TASKS
Admissible Set Problem (ASP)
ASP aims to maximize the size of the set while fulfilling the
criteria below: (1) The elements of the set are vectors belonging to {0, 1, 2}n. (2) Each vector has the
same number w of non-zero elements but a unique support. (3) For any three distinct vectors, there is
a coordinate in which their three respective values are {0, 1, 2}, {0, 1, 2}, {0, 1, 2}. Following prior
works (Romera-Paredes et al., 2024), we set n = 15 and w = 10 in this work.
Traveling Salesman Problem (TSP)
TSP aims to find a route that minimizes the total traveling
distance for a salesman required to visit each city exactly once before returning to the starting
point. We investigate the constructive heuristic design for TSP. Specifically, we adopt an iteratively
constructive framework to start from one node and iteratively select the next node until all nodes have
been selected and back to the start node. The task is to design a heuristic for choosing the next node
to minimize the route length.
Capacitated Vehicle Routing Problem (CVRP)
CVRP aims to minimize the total traveling
distances of a fleet of vehicles given a depot and a set of customers with coordinates and demands.
The problem is constrained by: (1) The vehicles start from the depot and return to the depot; (2) Each
customer should be visited only once; (3) All the demands should be satisfied while the capacity of
the vehicle should not be exceeded. Similar to TSP, we adopt an iteratively constructive framework to
start from one node and iteratively select the next node until all nodes have been selected and return
to the depot. The task is to design a heuristic for selecting the next node to minimize the total route
length with all constraints satisfied.
4.2
EXPERIMENTAL SETTINGS
Data Generation
For each algorithm design task, we collect a diverse set of algorithmic solutions
from existing results produced by FunSearch and EoH. These results are generated by Llama-3.1-
8B-Instruct (Grattafiori et al., 2024). To standardize the code in the database, we first preprocess
the raw code implementations by unifying their function templates, including consistent function
names, docstrings, and input-output formats. Next, we discard identical algorithms by checking the
code strings. Ultimately, we obtain approximately 60,000 unique algorithms for the ASP and CVRP
problems respectively. We adopt LLM4AD (Liu et al., 2024d) implementations for both FunSearch
and EoH.
Fine-tuning
We fine-tune each LLM for five epochs with a batch size of eight. We initiate the
learning rate at 5e-6, applying a cosine decay schedule and a warmup rate of 0.05 to reduce the rate
over the training period gradually. The model processes inputs up to a maximum length of 2048
tokens. For DPO, we set the β = 0.4 and utilize LoRA with settings of r = 64 and α = 32, alongside
a dropout rate of 0.05. The LLM fine-tune and inference process is executed on NVIDIA L20 GPUs.
We use trl library (von Werra et al., 2020) for DPO implementations and vllm library (Kwon
et al., 2023) for efficient LLM inference.
Automated Algorithm Design
We test the fine-tuned LLMs in two types of settings: 1) repeated
sampling, where we use the same prompt to instruct LLMs to generate algorithms many times. This
setting can show the effectiveness of fine-tuning in a straightforward way. 2) Iterative algorithm
design, where we use the fine-tuned LLMs in EoH and FunSearch for automated algorithm design
and compare to the results using pre-trained LLMs without fine-tuning. We set the maximum number
of evaluations to be 2,000 for both EoH and FunSearch and EoH’s population size to be 20.
4.3
COMPARATIVE EVALUATION OF SAMPLING STRATEGIES
In this experiment, we evaluate the efficacy of our proposed sampling strategy against top-k-based
sampling strategies. We construct four distinct datasets using the top-k-based sampling approach
under the following configurations:
6


--- Page 7 ---
Y+ From
Top-1
Y+ From
Top-1%
Y+ From
Top-5%
Y+ From
Top-10%
Ours
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Delta of Average Gap
(a)
Base
Model
Y+ From
Top-1
Y+ From
Top-1%
Y+ From
Top-5%
Y+ From
Top-10%
Ours
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Average Gap (
)
(b)
Figure 2: Comparison on varying preference pair sampling settings. (a): Comparison of the delta
value of preference pairs sampled by different methods. The delta value is calculated by the absolute
difference of the average gap between y+ and y−data. The mean delta values of 250 preference pairs
are highlighted with circle markers, while the standard deviations are denoted by lines. (b): Violin
plot comparison on the performance of LLMs fine-tuned on various preference datasets. Each violin
reflects the performance distribution of the top 50 of 1,000 algorithms generated by each LLM. The
performance is determined by the average gap to the existing best-known algorithm, which is lower
the better.
• Top-1 Sampling: The positive samples (y+) are selected as the best-performing heuristic
(exhibiting the lowest average gap) in the database, while the negative samples (y−) are
randomly chosen from the remaining heuristics.
• Top-k% Sampling: The y+ samples are randomly drawn from the top-k% heuristics,
whereas the y−samples are selected from the rest (100 −k)%. We test different parameters
k = 1, 5, 10 for this strategy.
We evaluate the performance of the Llama-3.2-1B-Instruct model, which has been fine-tuned on ASP.
To quantify the differences between these strategies, we analyze the delta value of preference pairs,
defined as the difference in average gap values between y+ and y−samples within each pair. This
metric reflects the relative performance gap between compared heuristics.
As illustrated in Figure 2a, we visualize the delta value distributions for datasets generated by the
five sampling strategies, each comprising 250 preference pairs. The mean delta values (marked
with circles) and standard deviations (denoted by lines) reveal that the proposed sampling strategy
significantly increases the pairwise distances between preference samples compared to top-k-based
methods.
We assess the effectiveness of different sampling methods as follows: Firstly, we randomly sample
1,000 feasible algorithms that have been successfully evaluated on ASP using both the base model
and its fine-tuned variants. The prompts used for this random sampling are identical to x in the
preference pair. Subsequently, we identify and analyze the top 50 algorithms from this pool of 1,000,
plotting the distribution of their average gap values. As illustrated in Figure 2b, fine-tuning with our
proposed sampling strategy markedly enhances the model’s capability in designing algorithms. This
improvement is quantitatively demonstrated by a significantly reduced mean average gap among the
top-50 algorithms when compared to those designed by the base model and other top-k sampling
methods.
4.4
PERFORMANCE IMPROVEMENT VIA FINE-TUNING
Random Sampling
In this experiment, we investigate whether a fine-tuned smaller LLM can match
a larger LLM in terms of algorithm design capabilities. Specifically, we first fine-tune Llama-3.2-1B-
Instruct on datasets of 2,000 and 5,000 preference pairs and then compare its performance against the
Llama-3.1-8B-Instruct model on ASP and CVRP problems.
7


--- Page 8 ---
Llama1B
Llama1B-FT
2000Data
Llama1B-FT
5000Data
Llama8B
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Average Gap (
)
(a) Performance on Admissible Set
Llama1B
Llama1B-FT
2000Data
Llama1B-FT
5000Data
Llama8B
0.275
0.300
0.325
0.350
0.375
0.400
0.425
0.450
Average Gap (
)
(b) Performance on CVRP
Figure 3: Violin plot comparison on the performance of fine-tuned LLMs and base model. Each
violin reflects the performance distribution of top-50 of 1,000 algorithms generated by each LLM.
The performance is determined by the average gap to the existing best-known algorithm, which is
lower the better.
Figure 3 compares the performance of the base model and models fine-tuned on two sizes of datasets.
Similar to the previous experiment, we randomly sample 1,000 feasible algorithms and plot the
distribution of the average gap of the top 50 algorithms, with lower being better. It can be concluded
from the results that:
• The fine-tuned LLM is more capable of generating high-quality algorithms compared to the
base model, evidently on the ASP.
• Increasing the dataset size yields marginal but consistent improvements across both tasks.
• The fine-tuned 1B LLM achieves competitive performance to 8B LLM in algorithm design,
highlighting the effectiveness of our approach.
100
400
800
1200
1600
2000
#Evaluated Algorithms
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Average Gap (
)
EoH-1B-Base
EoH-1B-FT
EoH-8B-Base
EoH-8B-FT
(a) EoH
100
400
800
1200
1600
2000
#Evaluated Algorithms
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
Average Gap (
)
FunSearch-1B-Base
FunSearch-1B-FT
FunSearch-8B-Base
FunSearch-8B-FT
(b) FunSearch
Figure 4: Convergence curve comparison on the performance of top-5 algorithms generated by
various LLMs and search methods on ASP. The performance is determined by the average gap to
the best-known algorithm, which is lower the better. The mean performance averaged over three
independent runs are denoted by markers, while the standard deviations is demonstrated by the shaded
area.
Iterative Search
We couple the fine-tuned LLM with two state-of-the-art LLM-driven AAD
methods, FunSearch and EoH. We initialize all compared methods with the respective seed algorithm
on each problem. The details of the seed algorithms are provided in Appendix. We set the maximum
number of evaluated programs to 2,000. The maximum evaluation time for each heuristic is restricted
to 30 seconds to eliminate inefficient and harmful heuristics, such as infinite loops. We perform three
independent runs for each method to account for experimental biases.
8


--- Page 9 ---
Table 1: Performance comparison on the top-1 and top-10 heuristics. The performance is determined
by the average gap to the best-known optimum (%). The mean and standard deviation aggregated
over three independent runs are reported, with lower being better.
Performance on ASP
Performance on CVRP
Top-1
Top-10
Top-1
Top-10
FunSearch
Llama-1B
19.48±4.31
24.22±5.03
35.81±0.9
35.81±0.9
Llama-1B-FT
9.19±0.28
9.88±0.97
30.26±2.02
33.27±4.23
Llama-8B
8.46±0.46
9.88±0.84
30.23±1.73
31.91±2.89
Llama-8B-FT
7.43±1.22
8.09±1.5
27.38±2.06
27.98±2.38
EoH
Llama-1B
17.02±8.88
19.49±8.34
36.98±0.15
37.02±0.11
Llama-1B-FT
8.52±0.56
8.58±0.52
31.61±4.05
34.64±3.51
Llama-8B
8.99±0.62
9.83±0.85
28.45±3.21
30.22±3.29
Llama-8B-FT
8.19±0.53
8.69±0.46
27.06±2.7
27.38±2.5
We evaluate two LLMs in this study, Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct. For each
task, we generate a dataset of 2,000 preference pairs using our proposed sampling strategy and
fine-tuning both LLMs. After that, the fine-tuned LLM, as well as the base model, are utilized to
generate algorithms under the guidance of search methods.
Figures 4 (ASP problem) and 5 (CVRP problem) present the convergence curves of the performance
of top-5 algorithms, with markers indicating mean performance across three runs and shaded regions
denoting standard deviations. Complementary results for the performance of top-1 and top-10
algorithms are listed in Table 1. Our analysis reveals two key findings:
• The fine-tuned LLMs consistently outperform base models, achieving faster convergence
and smaller optimality gaps.
• The 1B LLMs can approach the performance of 8B LLMs in most cases, with the exception
of EoH on the ASP, where the 1B LLM is noticeably inferior to base 8B LLM.
These results demonstrate that preference learning enhances LLMs’ algorithm design capabilities,
which in turn elevate search performance. This underscores the importance of integrating search
strategies with fine-tuned LLMs.
200
400
800
1200
1600
2000
#Evaluated Algorithms
0.25
0.30
0.35
0.40
0.45
Average Gap (
)
EoH-1B-Base
EoH-1B-FT
EoH-8B-Base
EoH-8B-FT
(a) EoH
200
400
800
1200
1600
2000
#Evaluated Algorithms
0.250
0.275
0.300
0.325
0.350
0.375
0.400
0.425
0.450
Average Gap (
)
FunSearch-1B-Base
FunSearch-1B-FT
FunSearch-8B-Base
FunSearch-8B-FT
(b) FunSearch
Figure 5: Convergence curve comparison on the performance of top-5 algorithms generated by
various LLMs and search methods on CVRP problem. The performance is determined by the average
gap to the existing best-known algorithm, which is lower the better. The mean performance averaged
over three independent runs is denoted by markers, while the standard deviations are demonstrated
by the shaded area.
9


--- Page 10 ---
200
400
800
1200
1600
2000
#Evaluated Algorithms
0.22
0.24
0.26
0.28
0.30
0.32
0.34
Average Gap (
)
EoH-8B-Base
EoH-8B-FT
FunSearch-8B-Base
FunSearch-8B-FT
(a) CVRP-100 Problem
200
400
800
1200
1600
2000
#Evaluated Algorithms
0.10
0.12
0.14
0.16
0.18
0.20
Average Gap (
)
EoH-8B-Base
EoH-8B-FT
FunSearch-8B-Base
FunSearch-8B-FT
(b) TSP-50 Problem
Figure 6: Convergence curve comparison on the performance of top-5 algorithms generated by the
base model and LLMs fine-tuned on the CVRP-50 problem. The performance is determined by the
average gap to the existing best-known algorithm, which is lower the better. The mean performance
averaged over three independent runs is denoted by markers, while the standard deviations are
demonstrated by the shaded area.
Table 2: Performance comparison on the top-1 and top-10 heuristics. The performance is determined
by the average gap to the best-known optimum (%). The mean and standard deviation aggregated
over three independent runs are reported, with lower being better.
Performance on CVRP-100
Performance on TSP-50
Top-1
Top-10
Top-1
Top-10
FunSearch
Llama-8B
26.27±2.16
26.74±1.95
15.53±2.51
16.88±2.98
Llama-8B-FT
24.1±0.87
24.26±0.84
11.85±0.37
12.52±0.73
EoH
Llama-8B
26.88±0.42
27.47±0.69
10.87±0.84
11.76±1.03
Llama-8B-FT
23.34±2.01
23.62±1.95
10.3±0.74
11.57±0.83
4.5
GENERALIZATION OF FINE-TUNED LLMS
In this section, we investigate the capacity of online fine-tuned LLMs to enhance algorithm search
on new algorithm design tasks. We employ the Llama-3.1-8B-Instruct model. The model has been
fine-tuned on the CVRP with instance size 50, and we adopt it on two new tasks without any further
adaptation. These two settings represent different levels of generalization: 1) The same task under a
different distribution. 2) A related task with a different problem description.
Same Task with Different Distribution
We evaluated the performance of the fine-tuned Llama-
3.1-8B-Instruct model on a variant of the CVRP. Originally fine-tuned for CVRP instances with 50
nodes, the model is now tested on larger instances containing 100 nodes. The coordinates for these
nodes are randomly generated within the [0,1] interval, and the vehicle capacity has been increased
to 50. This setup introduces variations in both the number of nodes and the capacities, while the task
description remains consistent.
Results in Figure 6 and Table 2 demonstrate that the fine-tuned LLMs are effective when we change
the task settings. The average results clearly outperform base LLMs on both EoH and FunSearch.
Related Task with Different Description
The fine-tuned Llama-3.1-8B-Instruct is also tested on
the Traveling Salesman Problem (TSP), which, while related to CVRP, differs significantly in terms
of problem description and attributes.
The results in Figure 6 demonstrate that even on a different task, fine-tuned LLMs can still improve
automated algorithm search. However, the improvement is less pronounced on EoH, likely because
EoH already converges efficiently with the base model.
10


--- Page 11 ---
5
CONCLUSION
This paper presents a preliminary study on the necessity and effectiveness of fine-tuning an LLM
tailored to the algorithm design task. We adopt DPO and introduce a diverse-aware rank-based
sampling strategy, which balances training data diversity and quality for effective finetuning on
algorithm design tasks. Our experiments on three tasks demonstrate the effectiveness of the fine-tuned
LLM across different algorithm design scenarios, including: algorithm design with LLM-based
random sampling, algorithm design with LLM-based iterative search, and generalizing to related
algorithm design tasks. Notably, Llama-3.2-1B- Instruct trained with our method matches the
performance of Llama-3.1-8B-Instruct. Moreover,
REFERENCES
Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models
for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024.
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization:
a methodological tour d’horizon. European Journal of Operational Research, 290(2):405–421,
2021.
Federico Berto, Chuanbo Hua, Junyoung Park, Laurin Luttmann, Yining Ma, Fanchen Bu, Jiarui
Wang, Haoran Ye, Minsu Kim, Sanghyeok Choi, Nayeli Gast Zepeda, Andr´e Hottung, Jianan Zhou,
Jieyi Bi, Yu Hu, Fei Liu, Hyeonah Kim, Jiwoo Son, Haeyeon Kim, Davide Angioni, Wouter Kool,
Zhiguang Cao, Jie Zhang, Kijung Shin, Cathy Wu, Sungsoo Ahn, Guojie Song, Changhyun Kwon,
Lin Xie, and Jinkyoo Park. RL4CO: an Extensive Reinforcement Learning for Combinatorial
Optimization Benchmark. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, 2025. URL https://github.com/ai4co/rl4co.
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and
Wotao Yin. Learning to optimize: A primer and a benchmark. The Journal of Machine Learning
Research, 23(1):8562–8620, 2022.
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.
21783.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
A Hanif Halim, Idris Ismail, and Swagatam Das. Performance assessment of the metaheuristic
optimization algorithms: an exhaustive review. Artificial Intelligence Review, 54(3):2323–2409,
2021.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.
Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou
Wang, and Zizhuo Wang. Orlm: A customizable framework in training large models for automated
optimization modeling. Operations Research, 2025a.
Ziyao Huang, Weiwei Wu, Kui Wu, Jianping Wang, and Wei-Bin Lee. Calm: Co-evolution of
algorithms and language model for automatic heuristic design. arXiv preprint arXiv:2505.12285,
2025b.
Hao Jiang, Yuhang Wang, Ye Tian, Xingyi Zhang, and Jianhua Xiao. Feature construction for meta-
heuristic algorithm recommendation of capacitated vehicle routing problems. ACM Transactions
on Evolutionary Learning and Optimization, 1(1):1–28, 2021.
Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large language
models for code generation. arXiv preprint arXiv:2406.00515, 2024.
11


--- Page 12 ---
Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv
preprint arXiv:1803.08475, 2018.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles, 2023.
Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large
language model. arXiv preprint arXiv:2311.15249, 2023.
Fei Liu, Xi Lin, Zhenkun Wang, Qingfu Zhang, Tong Xialiang, and Mingxuan Yuan. Multi-task
learning for routing problem with cross-problem zero-shot generalization. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1898–1908,
2024a.
Fei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu
Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language
model. In International Conference on Machine Learning, pp. 32201–32223. PMLR, 2024b.
Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Zhe Zhao, Xi Lin, Xialiang Tong, Mingxuan Yuan,
Zhichao Lu, Zhenkun Wang, et al. A systematic survey on large language models for algorithm
design. arXiv preprint arXiv:2410.14716, 2024c.
Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang, Zhichao Lu, and Qingfu
Zhang. Llm4ad: A platform for algorithm design with large language model. arXiv preprint
arXiv:2412.17287, 2024d.
Zeyuan Ma, Hongshu Guo, Yue-Jiao Gong, Jun Zhang, and Kay Chen Tan. Toward automated
algorithm design: A survey and practical guide to meta-black-box-optimization. IEEE Transactions
on Evolutionary Computation, 2025.
Alexander Novikov, Ngˆan V˜u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wag-
ner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphae-
volve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131,
2025.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in neural information processing systems, 35:27730–
27744, 2022.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in Neural Information Processing Systems, 36:53728–53741, 2023.
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog,
M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang,
Omar Fawzi, et al. Mathematical discoveries from program search with large language models.
Nature, 625(7995):468–475, 2024.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Anja ˇSurina, Amin Mansouri, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, and Caglar
Gulcehre. Algorithm discovery with LLMs: Evolutionary search meets reinforcement learning. In
ICLR Workshop Scaling Self-Improving Foundation Models without Human Supervision, 2025.
URL https://openreview.net/forum?id=1kAwyBpoO1.
Ke Tang and Xin Yao. Learn to optimize-a brief overview. National Science Review, pp. nwae132,
2024.
Niki van Stein and Thomas B¨ack. Llamea: A large language model evolutionary algorithm for
automatically generating metaheuristics. IEEE Transactions on Evolutionary Computation, 2024.
12


--- Page 13 ---
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
Pointer networks.
Advances in neural
information processing systems, 28, 2015.
Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan
Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallou´edec. Trl: Transformer reinforcement
learning. https://github.com/huggingface/trl, 2020.
Lin Xu, Holger Hoos, and Kevin Leyton-Brown. Hydra: Automatically configuring algorithms
for portfolio-based selection. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 24, pp. 210–216, 2010.
Shunyu Yao, Fei Liu, Xi Lin, Zhichao Lu, Zhenkun Wang, and Qingfu Zhang. Multi-objective
evolution of heuristic using large language model. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 39, pp. 27144–27152, 2025.
Yiming Yao, Fei Liu, Ji Cheng, and Qingfu Zhang. Evolve cost-aware acquisition functions using
large language models. In International Conference on Parallel Problem Solving from Nature, pp.
374–390. Springer, 2024.
Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park,
and Guojie Song. Reevo: Large language models as hyper-heuristics with reflective evolution.
arXiv preprint arXiv:2402.01145, 2024.
Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Understanding the
importance of evolutionary search in automated heuristic design with large language models. In
International Conference on Parallel Problem Solving from Nature, pp. 185–202. Springer, 2024.
A
APPENDIX
A.1
SEED ALGORITHM
Seed Algorithm for ASP
import math
import numpy as np
def
p r i o r i t y ( e l :
tuple [ int ,
. . . ] ,
n :
int , w:
i n t ) −> f l o a t :
””” Returns
the
p r i o r i t y
with
which we want
to
add
‘ el ‘
to
the
s e t .
Args :
e l :
the
unique
v e c t o r
has
the
same number w of
non−
zero
elements .
n :
l e n g t h
of
the
v e c t o r .
w :
number
of
non−zero
elements .
”””
return
0.
Seed Algorithm for CVRP
import numpy as np
def
s e l e c t n e x t n o d e ( c u r r e n t n o d e :
int ,
depot :
int ,
u n v i s i t e d n o d e s :
np . ndarray ,
r e s t c a p a c i t y :
np . ndarray ,
demands :
np . ndarray ,
d i s t a n c e m a t r i x :
np . ndarray ) −> i n t :
”””Design a novel
algorithm
to
s e l e c t
the
next
node
in
each
s t e p .
Args :
13


--- Page 14 ---
curr ent node : ID of
the
c u r r e n t
node .
depot : ID of
the
depot .
u n v i s i t e d n o d e s :
Array
of
IDs
of
u n v i s i t e d
nodes .
r e s t c a p a c i t y :
r e s t
c a p a c i t y
of
v e h i c l e
demands :
demands
of
nodes
d i s t a n c e m a t r i x :
Distance
matrix
of
nodes .
Return :
ID of
the
next
node
to
v i s i t .
”””
b e s t s c o r e = −1
next node = −1
for
node
in
u n v i s i t e d n o d e s :
demand = demands [ node ]
d i s t a n c e = d i s t a n c e m a t r i x [ c u r r e n t n o d e ] [ node ]
i f
demand <= r e s t c a p a c i t y :
score = demand
/
d i s t a n c e
i f
d i s t a n c e > 0
e l s e
f l o a t ( ’ i n f ’ )
# Avoid
d i v i s i o n
by
zero
i f
score > b e s t s c o r e :
b e s t s c o r e = score
next node = node
return
next node
Seed Algorithm for TSP
import numpy as np
def
s e l e c t n e x t n o d e ( c u r r e n t n o d e :
int ,
d e s t i n a t i o n n o d e :
int
,
u n v i s i t e d n o d e s :
np . ndarray ,
d i s t a n c e m a t r i x :
np .
ndarray ) −> i n t :
”””Design a novel
algorithm
to
s e l e c t
the
next
node
in
each
s t e p .
Args :
curr ent node : ID of
the
c u r r e n t
node .
d e s t i n a t i o n n o d e : ID of
the
d e s t i n a t i o n
node .
u n v i s i t e d n o d e s :
Array
of
IDs
of
u n v i s i t e d
nodes .
d i s t a n c e m a t r i x :
Distance
matrix
of
nodes .
Return :
ID of
the
next
node
to
v i s i t .
”””
next node = u n v i s i t e d n o d e s [ 0 ]
return
next node
A.2
PROMPT IN DPO TRAINING
Prompt Used in Preference Pairs of ASP
Your
t a s k
i s
to
design
a
‘ p r i o r i t y ‘
f u n c t i o n
to
solve
a d m i s s i b l e
s e t
problem .
14


--- Page 15 ---
Formally ,
a d m i s s i b l e
s e t
problems ,
denoted
as A( n ,w) ,
are
c o l l e c t i o n s
of
v e c t o r s
in
{0 ,1 ,2}ˆ n
t h a t
s a t i s f y :
\
( 1 )
Each
v e c t o r
has
the
same number w of non−zero
elements
and
i s
unique .
\
( 2 )
For any
t h r e e
d i s t i n c t
v e c t o r s
t h e r e
i s
a
c o o r d i n a t e
in
which
t h e i r
t h r e e
r e s p e c t i v e
values
are
{0 ,1 ,2} ,
{0 ,0 ,1} ,
or
{0 ,0 ,2}. \
The
o b j e c t i v e
of
the
a d m i s s i b l e
s e t
problem
i s
to
maximize
the
s i z e
of
the
s e t
while
f u l f i l l i n g
a l l
the
aforementioned
c r i t e r i a .
In
t h i s
work , we s e t
n=15 and w=10.
To solve
t h i s
task , we design
a
p r i o r i t y
f u n c t i o n .
The
i n p u t
of
the
p r i o r i t y
i s
a
v a l i d
vector ,
\
and
the
p r i o r i t y
f u n c t i o n
r e t u r n s
the
score
of
the
v e c t o r .
After
s c o r i n g
a l l
v a l i d
vectors ,
\
we w i l l
append one
with
the
h i g h e s t
score
to
the
s e t .
An example
p r i o r i t y
i s
shown below :
‘ ‘ ‘ python
import
math
import numpy as np
def
p r i o r i t y ( e l :
t u p l e [ int ,
. . . ] ,
n :
i n t
= 15 , w:
i n t
= 10)
−> f l o a t :
””” Returns
the
p r i o r i t y
with
which we want
to
add
‘ el ‘
to
the
s e t .
Args :
e l :
the
unique
v e c t o r
has
the
same number w of non−
zero
elements .
n
:
l e n g t h
of
the
v e c t o r .
w :
number of non−zero
elements .
”””
r e t u r n
0.
‘ ‘ ‘
Please
design
a
novel
p r i o r i t y
f u n c t i o n
t h a t
follow
the
t em p la t e
below .
Please
only
output
the
p r i o r i t y
f u n c t i o n .
import
math
import numpy as np
def
p r i o r i t y ( e l :
t u p l e [ int ,
. . . ] ,
n :
i n t
= 15 , w:
i n t
= 10)
−> f l o a t :
””” Returns
the
p r i o r i t y
with
which we want
to
add
‘ el ‘
to
the
s e t .
Args :
e l :
the
unique
v e c t o r
has
the
same number w of non−
zero
elements .
n
:
l e n g t h
of
the
v e c t o r .
w :
number of non−zero
elements .
”””
# your
implementations
here . . .
15


--- Page 16 ---
Prompt Used in Preference Pairs of CVRP
Your
t a s k
i s
to
design
a
‘ s e l e c t n e x t n o d e ‘
f u n c t i o n
to
solve
c a p a c i t a t e d
v e h i c l e
r o u t i n g
problem
(CVRP) .
\
The
t a s k
i s
to
design
a
novel
a l g o r i t h m
to
s e l e c t
the
next
node
in
each
step ,
\
with
the
o b j e c t i v e
of
minimizing
the
t o t a l
c o s t .
‘ ‘ ‘ python
import numpy as np
def
s e l e c t n e x t n o d e ( c u r r e n t n o d e :
int ,
depot :
int ,
u n v i s i t e d n o d e s :
np . ndarray ,
r e s t c a p a c i t y :
np . ndarray ,
demands :
np . ndarray ,
d i s t a n c e m a t r i x :
np . ndarray ) −> i n t :
””” Design a
novel
a l g o r i t h m
to
s e l e c t
the
next
node
in
each
s t e p .
Args :
c u r r e n t n o d e :
ID of
the
c u r r e n t
node .
depot :
ID of
the
depot .
u n v i s i t e d n o d e s :
Array
of
IDs
of
u n v i s i t e d
nodes .
r e s t c a p a c i t y :
r e s t
c a p a c i t y
of
v e h i c l e
demands :
demands
of
nodes
d i s t a n c e m a t r i x :
Distance
matrix
of
nodes .
Return :
ID of
the
next
node
to
v i s i t .
”””
r e t u r n
u n v i s i t e d n o d e s [ 0 ]
‘ ‘ ‘
Please
design
a
novel
‘ s e l e c t n e x t n o d e ‘
f u n c t i o n
t h a t
follows
the
te m pl a t e
below .
Please
only
output
the
f u n c t i o n
implementation .
import numpy as np
def
s e l e c t n e x t n o d e ( c u r r e n t n o d e :
int ,
depot :
int ,
u n v i s i t e d n o d e s :
np . ndarray ,
r e s t c a p a c i t y :
np . ndarray ,
demands :
np . ndarray ,
d i s t a n c e m a t r i x :
np . ndarray ) −> i n t :
””” Design a
novel
a l g o r i t h m
to
s e l e c t
the
next
node
in
each
s t e p .
Args :
c u r r e n t n o d e :
ID of
the
c u r r e n t
node .
depot :
ID of
the
depot .
u n v i s i t e d n o d e s :
Array
of
IDs
of
u n v i s i t e d
nodes .
r e s t c a p a c i t y :
r e s t
c a p a c i t y
of
v e h i c l e
demands :
demands
of
nodes
d i s t a n c e m a t r i x :
Distance
matrix
of
nodes .
Return :
ID of
the
next
node
to
v i s i t .
”””
# your
implementations
here . . .
16
