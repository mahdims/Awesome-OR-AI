--- Page 1 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
Kun Ding
Institute of Automation, Chinese
Academy of Sciences
State Key Laboratory of Multimodal
Artificial Intelligence Systems
Beijing, China
kun.ding@ia.ac.cn
Ying Wangâˆ—
Institute of Automation, Chinese
Academy of Sciences
State Key Laboratory of Multimodal
Artificial Intelligence Systems
Beijing, China
ywang@nlpr.ia.ac.cn
Shiming Xiang
Institute of Automation, Chinese
Academy of Sciences
State Key Laboratory of Multimodal
Artificial Intelligence Systems
Beijing, China
smxiang@nlpr.ia.ac.cn
Abstract
Pre-trained Vision-Language Models (VLMs) have been exploited
in various Computer Vision tasks (e.g., few-shot recognition) via
model adaptation, such as prompt tuning and adapters. However,
existing adaptation methods are designed by human experts, re-
quiring significant time cost and experience. Inspired by recent
advances in Large Language Models (LLMs) based code generation,
we propose an Evolutionary Vision-Language Model Adaptation
(EvoVLMA) method to automatically search training-free efficient
adaptation algorithms for VLMs. We recognize feature selection
and logits computation as the key functions in training-free VLM
adaptation, and propose a two-stage LLM-assisted evolutionary
algorithm for optimizing these parts in a sequential manner, ef-
fectively addressing the challenge posed by the expansive search
space through a divide-and-conquer strategy. Besides, to enhance
the stability and efficiency of searching process, we propose low-
precision code conversion, web based code execution and process mon-
itoring, leading to a highly effective automatic algorithm design
system. Extensive experiments demonstrate that the algorithms
found by EvoVLMA can obtain promising results compared to pre-
vious manually-designed ones. More specifically, in the 8-shot im-
age classification setting, the classical APE algorithm can be im-
proved by 1.91 points in recognition accuracy. This research opens
new possibilities for automating the optimization of adaptation
algorithms of pre-trained multimodal models. Code is available at:
https://github.com/kding1225/EvoVLMA
CCS Concepts
â€¢ Computing methodologies â†’Computer vision problems.
Keywords
Vision-Language Model, Few-shot Recognition, Model Adaptation,
Code Generation, Evolutionary Algorithm
ACM Reference Format:
Kun Ding, Ying Wang, and Shiming Xiang. 2025. EvoVLMA: Evolutionary
Vision-Language Model Adaptation. In Proceedings of the 33rd ACM Inter-
national Conference on Multimedia (MM â€™25), October 27â€“31, 2025, Dublin,
âˆ—The corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International License.
MM â€™25, Dublin, Ireland
Â© 2025 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-2035-2/2025/10
https://doi.org/10.1145/3746027.3755520
Figure 1: Main idea of this work. (a) The manual algorithm
design. (b) The automatic algorithm design based on LLM.
Ireland. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3746027.
3755520
1
Introduction
Pre-training especially in visual-language domains has significantly
propelled the advancement of Computer Vision (CV). Thanks to
the vast amounts of paired image-text data, pre-trained Visual-
Language Models (VLMs) can learn universal representations that
exhibit strong generalization capabilities. Recently, the field of
CV has shown a keen interest in transferring pre-trained models,
e.g. CLIP [45], to various downstream tasks, including zero-shot
and few-shot recognition [49, 66, 69], open-vocabulary segmenta-
tion [7], leading to remarkable performance improvement.
To adapt pre-trained VLMs to downstream tasks - particularly
those with limited annotated data - parameter-efficient adaptation
methods have attracted extensive research attention. Methods like
prompt tuning [69] optimize learnable context vectors prepended to
class tokens, while adapter-based methods [66] introduce learnable
parameters in the later layers of encoders. Adapter-based methods
demonstrate high computational efficiency while also achieving
comparable results to prompt tuning. To further enhance the effi-
ciency and support on-device gradient-free adaptation, training-
free adaptation is drawing increased attention. For example, Zhang
et al. proposed Tip-Adapter that introduces the cache model to
supply prior information [66], Zhu et al. proposed APE that re-
moves redundant features by the criterion of minimizing inter-class
similarity [70], Wang et al. proposed a hard-to-beat training-free
method GDA [57] that exploits the classical Gaussian Discriminant
Analysis model. Despite their high effectiveness, these methods
all depend on human expertise for algorithm design (ref. Fig. 1(a)),
leading to significant time costs and the requirement for special-
ized knowledge. Consequently, the lengthy process of algorithm
development impedes the swift advancement of this field.
arXiv:2508.01558v1  [cs.CV]  3 Aug 2025


--- Page 2 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
With the rise of LLMs like ChatGPT, researchers have gradually
developed an interest in leveraging these models to accelerate the
progress of scientific research. This trend has led to diverse appli-
cations of LLMs across the research lifecycle [39]. For instance, to
streamline literature reviews, Li et al. introduced an LLM-based
agent system for automated literature generation [32]. Baek et al.
developed ResearchAgent [3] to iteratively generate research hy-
potheses. To design research plan, LLM has been integrated with
algorithm design (LLM4AD) [35]. In peer review, Lu et al. proposed
Agent Reviewers [38], a multi-agent multimodal system for acceler-
ating manuscript evaluation. Notable progress has also been made
in combinatorial optimization, where methods like FunSearch [48]
and EoH [34] leverage LLMs to generate innovative code-based so-
lutions for heuristic design. Building on this momentum, our work
explores the integration of LLMs with algorithm design for VLM
adaptation, contributing to further propel the automation trend in
the formulation of research designs within scientific workflows. As
illustrated in Fig. 1(b), our idea is to leverage the powerful code gen-
eration capability of LLM [27] to automatically produce key code
and continuously optimize it by feedback from code evaluation.
Accordingly, we propose EvoVLMA for designing efficient adap-
tation algorithms of pre-trained VLMs. As shown in the top-left
panel of Fig. 2, its main components include: initialization, crossover,
mutation and selection. The initialization step constructs an initial
population consisted of existing algorithms, which serve as the
starting point of searching. Crossover and mutation are specifically-
designed evolutionary operators based on LLMâ€™s powerful code
generation ability, which are the basic for evolving the population.
To effectively select the best individuals in the population, we de-
sign a dedicated fitness function for the adaptation task. To address
the distinct challenges of efficiency and stability while executing
and evaluating the searched algorithm on GPU, we propose web
based code execution and evaluation, while monitoring the status
of execution process. We study the effectiveness of searched algo-
rithms on several public datasets. The results demonstrate that the
found algorithms have good generalization ability across different
initialization, data distributions and backbones.
We summarize our contributions as follows: 1) We introduce
automatic algorithm design to the domain of efficient adaptation
of pre-trained VLMs and propose a two-stage evolutionary method
performing algorithm searching in code space; 2) We propose web
based code execution and evaluation method, and design a pro-
cess monitor, boosting the efficiency and stability significantly; 3)
We perform extensive experiments on several public datasets and
demonstrate the efficacy of the found algorithms.
2
Related Work
Vision-Language Models. Beyond supervised pre-training [12,
17] and self-supervised pre-training [8, 20, 43], multi-modal pre-
training is gradually becoming a new pre-training paradigm in
CV. Due to the impressive zero-shot generalization performance
provided by intensive learning on massive paired image-text data
crawled from internet, pre-trained Vision-Language Models (VLMs)
have become new kind of foundational models applicable to var-
ious vision tasks. According to the review paper by Zhang et al.,
existing VLMs have three kinds of typical architectures: two-tower
VLM [25, 45], two-leg VLM [51, 64] and one-tower VLM [23, 54]. In
terms of train objective, there are two kinds of learning objectives:
discriminative (image contrastive loss, image-text contrastive loss,
etc.) and generative (masked image modeling, masked language
modeling, etc.). Given the widespread use of the CLIP model in pre-
vious work, this paper employs CLIP as the benchmark pre-trained
model and investigates automated algorithm design methods.
Model Adaptation. Transferring knowledge from pre-trained
models to solve CV problems has become a hot topic, with meth-
ods divided into gradient-based training-required and training-free
approaches. The former often preprends learnable vectors known
as prompts [14, 16, 18, 26, 46, 61, 69] or inserts learnable feature
adapters in the middle or latter layers [9, 28]. These parameters
often account for a small percent in the overall network, making
the data efficient tuning possible. Due to the gradient-based train-
ing, these methods are usually quite slow, preventing agile model
development and deployment. Recently, researchers have gradually
developed an interest in adaptation methods that do not require
training. The key of training-free adaptation is to design a good
logits computation function that effectively exploits the informa-
tion in train imagesâ€™ features, class namesâ€™ features and test imagesâ€™
features. The related works include: introducing cache model [66],
removing redundant features [70], exploiting the classical machine
learning models [15, 57]. The training-free methods do not need
gradient-based learning and are usually very efficient. However,
the no-training approach poses higher demands for researchers on
ingenious algorithm design. This work tries to address the difficulty
of automatic algorithm design for adapting VLMs by exploiting
LLMâ€™s code generation ability.
Automatic Algorithm Design. Algorithm design for prob-
lem solving is labor-intensive as it needs in-depth domain knowl-
edge and expert experience. The advent of LLMs has propelled
the progress of automatic algorithm design remarkably, yielding a
brand-new research direction, i.e., Large Language Models for Algo-
rithm Design (LLM4AD). To design heuristics for solving combina-
torial optimization problems, ReEvo [63] and EoH [34] integrated
LLM with Evolutionary Computation, resulting in a population-
based algorithm design framework. For continuous optimization,
Niki van Stein and Thomas BÃ¤ck proposed LLaMEA [55] that vali-
dates the effectiveness of LLM in generating metaphor-based op-
timization algorithms. Hao et al. proposed LAEA [19] that intro-
duces LLM-based surrogate models to address both regression and
classification problems. Besides, LLMs were also demonstrated to
be effective in automatically designing Acquisition Functions for
Bayesian optimization [1, 62]. In the ML domain, LLMs have been
applied to various algorithm design tasks in task planning [22, 33],
reinforcement learning [50, 68], neural architecture search [6, 24],
graph learning [41], full-pipeline AutoML [53, 67], and so on.
Inspired by EoH, we propose EvoVLMA, a framework of auto-
matic algorithm design for adapting VLMs to CV tasks. The pro-
posed framework introduces an innovative two-stage decompo-
sition strategy coupled with evolutionary algorithm-based code
space exploration. This design not only overcomes current limita-
tions in LLMsâ€™ ability to generate complex code structures but also
achieves an optimal balance between exploration and exploitation
in the search process.


--- Page 3 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Figure 2: The flowchart of EvoVLMA. Top-left panel: the workflow of automatic algorithm designing; right panel: the imple-
mentation of crossover and mutation; bottom left panel: evaluating the performance of searched algorithms on downstream
tasks. The population consists of several individuals or algorithms, which could be regarded as the message passed between
two consecutive steps. The algorithm is composed of two components: thoughts, code.
3
Method
The overall workflow of EvoVLMA is displayed in Fig. 2. The core
idea is treating the algorithm design problem as a code searching
task. Starting from an initial population of algorithms, the algo-
rithms are iteratively improved following an evolution framework.
But different from classical evolution algorithm, the crossover and
mutation operators are implemented by prompting LLMs. The LLMs
generate new code of algorithm by combining and transforming
one or some randomly selected code(s) from the current population.
To simulate the algorithm design process of human experts (ref.
Fig. 1(a)), we request LLMs to also generate a natural language
description of the algorithm [34], i.e., the thoughts. To indicate
the goodness of an algorithm, we also introduce a fitness value
for each algorithm. As such, the individual or the algorithm in the
population consists of two components: thoughts and code.
Operationally, as shown in the top-left panel, EvoVLMA loops
over a sequence of operations: crossover, mutation and selection,
where crossover and mutation are implemented in the right panel
by LLM calling, code conversion, code execution, evaluation and
process monitoring. We have recognized that both the logits compu-
tation and the feature selection algorithm are important for model
adaptation [70], we therefore search both of them. Considering
the difficulty of joint searching, we propose to search the two al-
gorithms sequentially, meaning we fix one as the default while
searching for the optimal code for the other. The workflows in
each iteration shown in the right panel for searching them are
quite similar, only differing in the third step. At each iteration, the
selection operator selects top-k best algorithms according to the
fitness evaluated on a dataset, preventing the excessive growth of
the population. After several iterations, we obtain the optimal algo-
rithms and evaluate their performance on downstream tasks (ref.
bottom-left panel). The following details every parts of EvoVLMA.
3.1
Initialization
The initialization step aims to create an initial population of algo-
rithms. Considering that the searching space of code is quite large,
directly searching the algorithm without providing any prior knowl-
edge could generate sub-optimal solution [6, 37, 55, 65]. As such, we
use the public code of existing algorithms (e.g., Tip-Adapter [66],
APE [70], GDA [57]) as the initial code. Besides, we further in-
troduce extra parameters and hyper-parameters into the function
definition. The initialization with existing methods are listed in
Table 1, whose core ideas are summarized as follows: 1) For fea-
ture selection algorithm, we adopt the original code of APE as it
supports feature selection. No additional parameters and hyper-
parameters are introduced. 2) For logits computation algorithm,
different existing algorithms can be used as initialization. While
using Tip-Adapter and GDA, we add the indices of selected fea-
tures â€˜indicesâ€™ as an extra parameter. Additional hyper-parameters
are introduced, i.e., â€˜alpha1â€™ and â€˜alpha2â€™. Note that not all hyper-
parameters must be used in the generated code. The permission of
using extra hyper-parameters makes it easier for LLMs to integrate
different aspects of novel design.
An algorithm in the population should also include the thoughts
indicating the underlying idea of the algorithm. For simplicity,
we compose the thoughts of each algorithm by referring to the


--- Page 4 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
Table 1: Summary of initialization. Blue color indicates extra parameters, red color indicates extra hyper-parameters. â€˜w0â€™, â€˜w1â€™,
â€˜topkâ€™, â€˜alpha0â€™ are hyper-parameters from the original algorithms.
Type
Paper
Thoughts
Function Definition
feature selection
APE
The feature selection algorithm defines a criterion that aims to extract the feature channels that minimize the inter-class
similarity of the concatenated features of visual and category textual features, but maximize the variance of category
textual features.
feat_selection(clip_weights,
train_feats, w0, w1, topk)
logits computation
APE
The algorithm sums up two logits: the logits generated by zero-shot classifier and the logits generated by a cache model.
The first logits are computed by applying linear transformation to test features. The second logits are obtained by
first computing the similarity matrix between test and train features and then multiplying the transformed similarity
matrix to soft train label matrix. While computing image-image similarity, selected feature channels are used.
compute_logits(train_feats,
train_labels,
test_feats,
clip_weights,
indices,
alpha0,
alpha1, alpha2)
logits computation
Tip-
Adapter
The algorithm sums up two logits: the logits generated by zero-shot classifier and the logits generated by a cache model.
The first logits are computed by applying linear transformation to test features. The second logits are obtained by
first computing the similarity matrix between test and train features and then multiplying the transformed similarity
matrix to train label matrix.
compute_logits(train_feats,
train_labels,
test_feats,
clip_weights,
indices,
alpha0,
alpha1, alpha2)
logits computation
GDA
The logits consist of two parts: the logits computed by CLIPâ€™s zero-shot classifier and the logits computed by Gaussian
Discriminant Analysis (GDA) model. In each part, all feature channels are used. GDA is a probabilistic generative model
for classification that assumes all classes are generated by Gaussian distributions with a common covariance matrix
but different mean vectors. GDA first computes per-class mean vector and then estimates the inverted covariance
matrix. After that the weight and bias of the GDA classifier can be computed.
compute_logits(train_feats,
train_labels,
test_feats,
clip_weights,
indices,
alpha0,
alpha1, alpha2)
corresponding paper, which are listed in the third column of Table 1.
As for the fitness value of the initial algorithm, we execute the
corresponding code on holdout image recognition datasets, which
are different from the datasets used for the downstream validation.
3.2
Crossover and Mutation
Crossover. This operator explores new algorithms via obtaining
inspiration from multiple parent algorithms. The parent algorithms
are randomly selected from the algorithms in the existing popu-
lation. For this aim, the current algorithms in the population are
sorted according to their fitness in ascending order. Then, two al-
gorithms are randomly selected with probability ð‘ð‘–âˆ1/(ð‘Ÿð‘–+ ð‘),
where ð‘Ÿð‘–is the rank of the ð‘–-th algorithm and ð‘is the population
size. Once the parent algorithms are obtained, we instruct LLM to
generate a new algorithm, which includes the thoughts and the
corresponding Python code. We encourage LLM to explore novel
ideas by explicitly informing LLM to generate new algorithms with
different form from the parent algorithms.
The prompt template of crossover is given in appendix. The
construction logic of the prompt template follows a structured
approach: Firstly, it necessitates a clear definition of the task at
hand, ensuring that the objective is unequivocally communicated.
Secondly, it involves presenting example algorithms, from which
LLM can draw inspiration. Thirdly, the design requirements are
elaborated in detail, specifying constraints, definition of inputs and
outputs, etc. Lastly, additional pertinent information is included to
provide a comprehensive view and encourage critical thinking. By
adhering to this structured format, the prompt template ensures
that LLMs are well-equipped to tackle the problem, fostering both
comprehension and creativity in their responses.
Mutation. This operator generates new algorithms by proba-
bilistically selecting and modifying existing algorithms from the
population, where each candidate is chosen with probability ð‘ð‘–.
The prompt template for instructing LLM in this case is similar to
that for crossover operation, which is shown in appendix. Note that
we have also asked LLM to generate novel algorithm as much as
possible.
In the prompt templates for both crossover and mutation, the
content in braces should be filled with different contents for feature
selection and logits computation. For feature selection, we ask LLM
to select the best feature channels in task description, while for
logits computation we ask it to devise a good function for computing
classification logits. We encourage LLM to generate novel algorithm
by asking it to design algorithm different from the ones in literature.
The input and output information are also slightly different in these
two cases. In the description of inputs and outputs, their dimensions
are explicitly given, which would be helpful for LLM to generate
valid expression.
In the part of other information, we additionally tell LLM what
kind of code is better. First, to avoid randomness, we ask LLM not to
use random operations in the code, improving the reproducibility.
Second, to avoid generating slow code, we ask LLM to avoid deep
nested loops. Using learnable variables contradicts our intention to
design a training-free algorithm, we also ask it to avoid this. Finally,
we ask LLM to notice the readability of the generated code.
3.3
Low-precision Code Conversion
By prompting LLM, we can obtain the response text. Considering
the unstructured characteristic of text data, we need to process the
text to obtain the generated algorithm. Here, we exploit regular
expression to extract the thoughts and the code part. If either the
thoughts or the code cannot be parsed successfully, we invoke LLM
again for another try. Besides, considering fp16 is faster than fp32,
we convert the code to support fp16 computation. Here, we propose
a lookup table based code conversion method. Specifically, we pre-
define a lookup table containing the original functions and their
converted versions. Before executing the code, we convert the code
accordingly. The lookup table can be found in appendix.
3.4
Code Execution and Evaluation
Evaluation. To evaluate the quality of the generated code, we
compute the fitness value by executing it with a holdout image
classification dataset as an extra input. We define the classification
error as the fitness value. Let us denote the train and test set of the
holdout dataset as Dð‘¡ð‘Ÿð‘Žand Dð‘¡ð‘ ð‘¡, respectively. ðœƒ0 = {ð‘¤0,ð‘¤1,ð‘˜}


--- Page 5 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
collects the hyper-parameters in feature selection code, and ðœƒ1 =
{ð›¼0, ð›¼1, ð›¼2} collects the hyper-parameters in logits computation.
Note that the specific meaning of the hyper-parameters in ðœƒ0 and
ðœƒ1 could change during the searching process. The fitness value is
defined as
fitness = 1 âˆ’arg max
ðœƒâˆˆS Acc(logits(Dð‘¡ð‘ ð‘¡, I,ðœƒ1), Dð‘¡ð‘ ð‘¡),
where I = FS(Dð‘¡ð‘Ÿð‘Ž,ðœƒ0) denotes the indices of selected features,
ðœƒ= ðœƒ0âˆªðœƒ1 collects all hyper-parameters, S is the searching space of
ðœƒ. logits(Â·), Acc(Â·) and FS(Â·) are the logits computation function, the
accuracy computation function and the feature selection function,
respectively. We use the maximal test accuracy in searching space
S to prevent our method from merely tuning hyper-parameters
instead of generating creative algorithms. The proposed fitness
equation is general to support different demands. While evaluating
the feature selection function, we use the logits computation func-
tion of APE as logits(Â·). While computing classification logits by
logits(Â·), the input I could be optional.
Execution. To optimize search efficiency, the code of the func-
tions FS(Â·), logits(Â·) and Acc(Â·) are executed on GPU. However,
during the search process, certain generated code may trigger
CUDA errors that elude Pythonâ€™s â€˜try ... exceptionâ€™ exception hand-
ing. These errors disrupt subsequent code execution, ultimately
leading to a failure. To mitigate this issue, we encapsulate FS(Â·),
logits(Â·) and Acc(Â·) as independent web services, i.e., â€˜/feat_selectâ€™,
â€˜/logit_computâ€™ and â€˜/evalâ€™, respectively (see Fig. 2). Each service
operates within its own process, monitored by a dedicated process
monitor. The monitor detects unresponsive processes and initiates
timely restarts ensuring continuous operation. Additionally, lever-
aging web servers like Gunicorn, we deploy multiple instances
of each service to enable parallel code execution. This architec-
tural choice not only enhances fault tolerance but also significantly
accelerates the search process through concurrent evaluations.
Process Monitor. As shown in Fig. 2, we invoke an individual
process to monitor the worker processes of code execution. The
monitoring process regularly requests the â€˜/feat_selectâ€™, â€˜/logit_computâ€™
and â€˜/evalâ€™ services, asking them to execute test codes. Once the
response fails, we kill the process of the corresponding service,
immediately. Due to the features of Gunicorn, a new web service
will be started, keeping the total number of worker processes fixed.
3.5
Selection
At each iteration of EvoVLMA, we exploit the selection operator
to keep optimal individuals, avoiding the explosion in population
size simultaneously. In this work, we select top-ð‘individuals with
the lowest fitness value to form the new population. Here, ð‘is the
population size. After this, we will enter the next iteration.
4
Experiment
4.1
Settings
To demonstrate the generalization ability of searched algorithm, we
use two settings: few-shot recognition and domain generalization.
The first setting compares few-shot recognition accuracy on the
test set of downstream datasets. The second studies if the model
fitted on source domain can be transferred to target domain.
Datasets. Following previous works, we use 11 publicly available
datasets as the downstream datasets in the few-shot recognition set-
ting: ImageNet [13], Caltech101 [31], UCF101 [52], EuroSAT [21],
SUN397 [59], DTD [10], Food101 [5], OxfordPets [44], Stanford-
Cars [29], Flowers102 [42], FGVCAircraft [40]. Under the domain
generalization setting, we follow previous works, where ImageNet
is used as the source domain, ImageNetV2 [47] and ImageNet-
Sketch [56] are used as the target domains. For algorithm design, we
adopt CIFAR100 [30], FashionMnist [58], ObjectNet [4], UcMerced1
and UCSDBirds2 as holdout datasets.
Compared Methods. As we target at designing training-free
adaptation methods, we mainly compare the searched algorithms
with existing algorithms designed by human experts, including
Tip-Adapter [66], APE [70] and GDA [57]. It is worth noting that
GDA is currently the SOTA method, renowned for its unbeatable
performance.
Implementation Details. For automatic algorithm design, we
use multiple holdout datasets. Each holdout dataset is split to a
train set and a test set. We randomly sample 1, 2, 4, 8, 16 samples
as the few-shot train sets. We execute the code on the few-shot
train sets and evaluate the accuracy on the test sets. The final
accuracy is averaged on different holdout datasets and different
shots. The searching space S is defined as the Cartesian product
of searching space of each hyper-parameter, i.e., {0.5}2 Ã— {0.7ð‘‘} Ã—
{0.1, 1, 10}3, whereð‘‘denotes the feature dimension of CLIPâ€™s image
features. In the proposed method, we adopt a two-stage approach
to search for feature selection algorithm and logits computation
algorithm, separately. We evaluate the impact of the order in which
they are applied on the results. In each stage, the population size
and number of iterations are set to 10. For code generation, we
adopt DeepSeek [11] as the LLM due to its affordable price.
For downstream evaluation, we split each dataset to train, valida-
tion and test set following APE [70]. For each searched or manual
algorithm, we search the optimal hyper-parameters on the val-
idation set using Optuna [2]. Without further specification, we
use the TPESampler sampler and 500 trials. We find that Optuna
based hyper-parameter optimization can achieve comparable per-
formance to grid searching but with lower time cost.
Without further specification, we use ResNet-50 as the image en-
coder. We also evaluate the effectiveness using different backbones.
For performance evaluation, we report the top-1 accuracy on the
test set of the downstream datasets. We report results with differ-
ent shots (1-shot, 2-shot, 4-shot, 8-shot and 16-shot) to study the
effectiveness using different numbers of train samples. To reduce
randomness, the average results over three seeds are reported.
4.2
Main Results
Qualitative Analysis. The searching process and the found algo-
rithms are visualized in Fig. 3(a) and (b). The first column shows the
searching process. No matter for searching logits computation or
feature selection algorithm, the population has the trend of evolving
towards to the status with lower fitness. The minimal fitness curves
also reflect this point. The second column shows the thoughts and
the third column shows the code. For logits computation, the new
1http://weegee.vision.ucmerced.edu/datasets/landuse.html
2https://www.vision.caltech.edu/datasets/cub_200_2011


--- Page 6 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
Figure 3: Searching process, found algorithms and code snippets. (a) Object curve and found logits computation algorithm
using GDA based initialization (stage 1 of â€˜logitâ†’fsâ€™). (b) Object curve and found feature selection algorithm using GDA based
initialization (stage 2 of â€˜logitâ†’fsâ€™). (c) Examples of searched code snippets.
Table 2: Few-shot classification and domain generalization results. TIP: Tip-Adapter, IN: ImageNet, V2: ImageNet-V2, S: ImageNet-
S. The bold numbers represent the accuracy gap between manually-designed and automatically-generated code.
Method/Init
Strategy
Few-shot Classification
Domain Generalization (16-shot)
1-shot
2-shot
4-shot
8-shot
16-shot
Average
IN
V2
S
Average
TIP
â€“
62.49
64.25
66.71
68.98
70.76
66.64
62.75
55.35
36.10
51.40
TIP
fsâ†’logit
62.72
64.93
67.53
69.69
71.17
67.21
62.82
55.39
35.97
51.39
+0.23
+0.68
+0.82
+0.71
+0.41
+0.57
+0.07
+0.04
-0.13
-0.01
TIP
logitâ†’fs
62.91
64.76
67.25
69.33
71.00
67.05
62.93
55.23
36.35
51.50
+0.42
+0.51
+0.54
+0.35
+0.24
+0.41
+0.18
-0.12
+0.25
+0.10
APE
â€“
63.94
65.16
68.23
70.33
72.28
67.99
62.92
55.26
36.41
51.53
APE
fsâ†’logit
64.18
66.23
69.60
72.09
74.11
69.24
64.49
56.48
36.36
52.44
+0.24
+1.07
+1.37
+1.76
+1.83
+1.25
+1.57
+1.22
-0.05
+0.91
APE
logitâ†’fs
63.80
66.07
69.51
72.24
74.00
69.12
64.45
56.34
35.66
52.15
-0.14
+0.91
+1.28
+1.91
+1.72
+1.13
+1.53
+1.08
-0.75
+0.62
GDA
â€“
62.55
66.00
70.22
73.72
76.45
69.79
62.74
55.48
35.67
51.30
GDA
fsâ†’logit
63.56
66.51
70.35
73.99
76.43
70.17
64.80
56.45
35.25
52.17
+1.01
+0.51
+0.13
+0.27
-0.02
+0.38
+2.06
+0.97
-0.42
+0.87
GDA
logitâ†’fs
62.99
66.20
70.02
73.59
76.33
69.83
64.72
56.22
34.82
51.92
+0.44
+0.20
-0.20
-0.13
-0.12
+0.04
+1.98
+0.74
-0.85
+0.62
algorithm introduces important feature channels to the original
â€˜gda_logitsâ€™ term. An alignment term that multiplies per-class mean
features to test features is introduced. Besides, a new weighted log-
its introducing important features is designed. As can be seen, LLM
has the ability to exploit the provided information of important
features. As for feature selection, the searched algorithm introduces
weighted inter-class divergence and intra-class divergence. By this
way, the selected features should be good for visual and text modali-
ties, simultaneously. The algorithm also introduces a term â€˜cos_simâ€™
to promote the alignment between visual and textual modalities,
which cannot not be found in existing literature.
We further give more generated distinctive code snippets in
Fig. 3(c), from which we find: 1) The current LLMs already have the
basic abilities, such as matrix manipulation. 2) The current LLMs are
capable of generating creative code snippets. However, some may
not be practical, e.g., the sparsity. 3) LLMs can also generate some
redundant codes as well as wrong codes that cannot implement
the thoughts. The results imply that LLM-based code evolution is


--- Page 7 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Table 3: Comparison to iterative refinement.
iteration
TIP
APE
GDA
population-based (evolutionary algorithm):
accuracy
67.21
69.24
70.17
total token/K
633
676
812
individual-based (iterative refinement):
accuracy
66.77
66.41
69.22
total token/K
701
1005
884
Table 4: Two-stage searching v.s. joint searching v.s. optimiz-
ing the logits computation algorithm only.
Strategy
TIP
APE
GDA
baseline
66.64
67.99
69.79
fsâ†’logit
67.21
69.24
70.17
joint
66.58
68.46
69.43
logit
67.21
68.55
69.84
a promising technique for automatic algorithm design. However,
there is still a space for further improving LLMâ€™s code generation
ability and enhancing the usage of this ability.
Quantitative Analysis. The results of manually-designed al-
gorithms and automatically-designed algorithms under the few-
shot setting are listed in Table 2. The bolded numbers highlight
the improvements over manual algorithms, which imply that the
automatically-designed algorithms outperform manual ones in
most cases. The improvement can be as large as 1.91%. Although
GDA has already achieved SOTA results, the automatically-designed
algorithms can still achieve better results. It has been recognized
that GDA cannot achieve good results under low-shot settings
due to the inaccurate covariance estimation. The automatically-
designed algorithms improve this notably. The optimization order
of feature selection and logits computation does have effect on
downstream performance. The order â€˜fsâ†’logitâ€™ works slightly bet-
ter in most cases. However, this needs more extensive testing.
The results under the domain generalization setting are listed in
Table 2, where the performance gaps between manual algorithms
and the corresponding automatic algorithms are highlighted. As
can be seen, automatic algorithms outperform manual ones on
source domain, while can still achieve competitive performance
on target domains in most cases. In terms of average performance,
automatic algorithms can acquire better trade-off between source
and target performance.
4.3
Ablation Study
Comparison to Iterative Refinement. This work employs a pop-
ulation based evolutionary algorithm for algorithm search, which
distinguishes itself from the iterative refinement strategy widely
adopted in existing studies such as [60]. Recent research like [36]
has started to explore population-based methods, revealing that
such approaches hold a distinct advantage in balancing exploration
and exploitation. To further validate the effectiveness of population-
based methods in our specific scenario, we implemented the iter-
ative refinement method used in [60] to generate VLM adapta-
tion code. Specifically, 8 in-context learning code examples are
utilized, and the â€˜fsâ†’logitâ€™ searching strategy is employed with
100 iterations allocated to each stage. The results shown in Table 3
Table 5: Effectiveness of initializing algorithm. â€˜no initâ€™ de-
notes no initialization is used.
Strategy
TIP
APE
GDA
no init
baseline
66.64
67.99
69.79
â€“
fsâ†’logit
67.21
69.24
70.17
65.48
logitâ†’fs
67.05
69.12
69.83
66.18
Table 6: Effect of holdout datasets. C: CIFAR100, F: Fashion-
Mnist, O: ObjectNet, M: UcMerced, B: UCSDBirds
Datasets
fsâ†’logit
logitâ†’fs
TIP
APE
GDA
TIP
APE
GDA
C+F
67.71
68.39
69.98
67.24
68.60
70.05
C+F+O+M+B
67.21
69.24
70.17
67.05
69.12
69.83
demonstrate that the population-based method can outperform the
individual-based iterative refinement approach even when using
fewer tokens, which underscores the greater effectiveness of the
population-based method.
Effectiveness of Feature Selection. In this work, we optimize
both the feature selection and logits computation algorithms. Here,
we compare it with the strategy that only optimizes the logits
computation algorithm. With APE based initialization, the logits
computation uses the indices of selected feature channels by its
original feature selection algorithm. With GDA or Tip-Adapter
based initialization, no feature selection is involved. The results
are presented in Table 4. We can see that only optimizing logits
computation algorithm acquires worse results than optimizing both,
implying the importance of selecting good features.
Joint Searching. In this work, we propose a two-stage search-
ing strategy. Here, we validate its rationality by comparing it with
joint searching strategy. In this joint strategy, we create an initial
algorithm by combing the feature selection algorithm in APE and
the logits computation algorithm in one of Tip-Adapter, APE and
GDA. We create the code accordingly implementing this algorithm.
The initial algorithm with the code is given in in appendix. Be-
sides, in our method, we have also modified the prompt template
accordingly. The results of joint searching strategy are listed in
Table 4, which are worse than two-stage strategy and even the
logit-only searching, implying that joint searching cannot enable
us to find better algorithms. Through iterative analysis of popula-
tion dynamics (see appendix), we observe that contemporary LLMs
struggle to generate functionally improved code when required
to integrate multiple interdependent components. In conclusion,
the proposed two-stage strategy cuts down the complexity of the
problem effectively.
Different LLMs. We further evaluate the effectiveness of dif-
ferent LLMs in the proposed method. The compared LLMs include:
DeepSeek, qwen-coder-plus-2024-11-06 (denoted as Qwen), gpt-3.5-
turbo (denoted as GPT-3.5), and gpt-4o-mini (denoted as GPT4o-
mini). We use the â€˜fsâ†’logitâ€™ searching strategy and the ResNet-
50 backbone. The results using different existing methods (Tip-
Adapter, APE and GDA) as initialization are listed in Table 7. With
Tip-Adapter based initialization, the searched algorithms with all


--- Page 8 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
Table 7: Per-shot results of few-shot recognition using different LLMs. The searching strategy is â€˜fsâ†’logitâ€™.
Initialization
LLM
Error Rate
1-shot
2-shot
4-shot
8-shot
16-shot
Average
Tip-Adapter
â€“
â€“
62.49
64.25
66.71
68.98
70.76
66.64
DeepSeek
21.23%
62.72
64.93
67.53
69.69
71.17
67.21
Qwen
39.64%
63.50
65.23
68.44
70.92
72.67
68.15
GPT-3.5
48.42%
63.92
65.68
68.28
70.96
72.90
68.35
GPT4o-mini
52.74%
63.82
65.8
69.15
71.88
73.79
68.89
APE
â€“
63.94
65.16
68.23
70.33
72.28
67.99
DeepSeek
27.21%
64.18
66.23
69.60
72.09
74.11
69.24
Qwen
51.12%
63.47
64.97
68.38
70.76
72.52
68.02
GPT-3.5
74.96%
63.68
65.61
68.50
70.79
72.72
68.26
GPT4o-mini
84.91%
63.55
64.85
67.43
70.38
71.98
67.64
GDA
â€“
62.55
66.00
70.22
73.72
76.45
69.79
DeepSeek
17.74%
63.56
66.51
70.35
73.99
76.43
70.17
Qwen
54.48%
62.47
65.69
69.60
72.61
75.64
69.20
GPT-3.5
69.23%
62.83
65.73
69.12
73.03
76.04
69.35
GPT4o-mini
51.08%
63.22
65.79
69.56
72.19
73.91
68.93
1 2
4
8
16
shots
64
66
68
70
72
74
76
78
Acc
APE
APE(fs->logit)
GDA
GDA(fs->logit)
TIP
TIP(fs->logit)
1 2
4
8
16
shots
70
72
74
76
78
80
82
Acc
APE
APE(fs->logit)
GDA
GDA(fs->logit)
TIP
TIP(fs->logit)
Figure 4: Effect of different visual backbones. Left to right:
ResNet-101 and ViT-B/16.
Table 8: Effect of iteration number.
iteration
TIP
APE
GDA
1
67.21
69.24
70.17
2
67.65
69.31
70.37
3
67.77
69.43
70.39
compared LLMs surpass the manually designed algorithm, i.e., Tip-
Adapter. In this case, Qwen, GPT-3.5 and GPT4o-mini work better
than DeepSeek. By contrast, with APE and GDA based initialization,
codes found by DeepSeek are better than those found by the rest
LLMs. We also count the code execution error rate while searching
codes, which is defined as the percent of wrongly executed codes in
the codes being evaluated. As can be seen, DeepSeek has lower error
rate while different initialization is used, implying that DeepSeek
has strong ability in generating executable codes. In the future, it
is worth conducting a more extensive evaluation of existing LLMs
using more diverse datasets.
Initialization. To demonstrate the effectiveness of algorithm
initialization, we compare the average few-shot classification ac-
curacy across 1, 2, 4, 8, 16 shots with and without initialization in
Table 5. Clearly, while not initializing the algorithm, notable degen-
eration in accuracy w.r.t. the worst Tip-Adapter based initialization
occurs. This suggests that current LLMs still has limited capabili-
ties in designing algorithms from the ground up. Introducing more
domain knowledge to LLMs could be a promising solution.
Datasets. The effect of holdout datasets are given in Table 6.
The average few-shot classification accuracy over 1, 2, 4, 8, 16
shots is reported. Using 2 datasets and 5 datasets can obtain similar
downstream performance, which implies that code evolution should
be not very sensitive to dataset scale.
Backbone. The results with different vision backbones are shown
in Fig. 4. As can be seen, improved accuracies can also be observed
while transferring the algorithms found on ResNet-50 feature dis-
tribution to ResNet-101 and ViT-B/16 feature distributions. This
implies that the searched algorithms have good robustness to visual
feature distribution drifting.
Alternating Optimization Strategy. Our framework alter-
nately optimizes feature selection and logits computation in an
iterative manner. To investigate the impact of iteration numbers,
we conduct experiments with multiple optimization cycles, with re-
sults presented in Table 8. The empirical findings reveal that while
additional iterations yield marginal performance gains, the improve-
ment diminishes significantly after the first iteration. Therefore,
to maintain computational efficiency while ensuring competitive
performance, we adopt a single-iteration configuration for all exper-
iments. This design choice effectively balances model performance
with experimental expediency.
5
Conclusion
This work proposes an LLM-based two-stage code evolution method
EvoVLMA to address the automatic algorithm designing problem
for VLM adaptation. The core ingredients of this method include
crossover, mutation and selection, all of which are implemented by
prompting LLM. EvoVLMA is flexible to support different search-
ing strategies, including two-stage and joint searching. We have
validated the effectiveness of searched algorithms on several down-
stream datasets with different initialization, strategies, and settings.
As a preliminary exploration, we can conclude that it is possible
to utilize LLMs to design model adaptation algorithms automati-
cally. Other areas in CV that require manual algorithm design may
also be inspired by our work. However, we must also soberly recog-
nize the limitations of current LLMs in automatic algorithm design,
including their dependence on good initialization, constrained cre-
ativity, and challenges in generating complex multi-function code.
Addressing these issues will be the direction for future work.


--- Page 9 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Acknowledgments
This work was supported by the Strategic Priority Research Pro-
gram of Chinese Academy of Sciences (Grant No. XDA0480200),
the National Natural Science Foundations of China (Grant No.
62306310) and the Beijing Natural Science Foundation (Grant No.
L242093).
References
[1] Virginia Aglietti, Ira Ktena, Jessica Schrouff, Eleni Sgouritsa, Francisco J. R. Ruiz,
Alan Malek, Alexis Bellot, and Silvia Chiappa. 2024. FunBO: Discovering Acquisi-
tion Functions for Bayesian Optimization with FunSearch. CoRR abs/2406.04824
(2024).
[2] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori
Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Frame-
work. In ACM SIGKDD. 2623â€“2631.
[3] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2025.
ResearchAgent: Iterative Research Idea Generation over Scientific Literature with
Large Language Models. In NAACL-HLT. 6709â€“6738.
[4] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
Gutfreund, Josh Tenenbaum, and Boris Katz. 2019. ObjectNet: A large-scale
bias-controlled dataset for pushing the limits of object recognition models. In
NeurIPS. 9448â€“9458.
[5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101 - Mining
Discriminative Components with Random Forests. In ECCV. 446â€“461.
[6] Angelica Chen, David Dohan, and David R. So. 2023. EvoPrompting: Language
Models for Code-Level Neural Architecture Search. In NeurIPS. 7787â€“7817.
[7] Jun Chen, Deyao Zhu, Guocheng Qian, Bernard Ghanem, Zhicheng Yan,
Chenchen Zhu, Fanyi Xiao, Sean Chang Culatana, and Mohamed Elhoseiny.
2023. Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision
Encoder Distillation Only. In ICCV. 699â€“710.
[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In
ICML, Vol. 119. 1597â€“1607.
[9] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu
Qiao. 2023. Vision Transformer Adapter for Dense Predictions. In ICLR. 20 pages.
[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and An-
drea Vedaldi. 2014. Describing Textures in the Wild. In CVPR. 3606â€“3613.
[11] DeepSeek-AI, Aixin Liu, Bei Feng, and et al. 2024. DeepSeek-V3 Technical Report.
CoRR abs/2412.19437 (2024).
[12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, and et al. 2023. Scaling Vision
Transformers to 22 Billion Parameters. In ICML, Vol. 202. 7480â€“7512.
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Ima-
geNet: A large-scale hierarchical image database. In CVPR. 248â€“255.
[14] Kun Ding, Ying Wang, Pengzhang Liu, Qiang Yu, Haojian Zhang, Shiming Xiang,
and Chunhong Pan. 2024. Multi-task prompt tuning with soft context sharing
for visionâ€“language models. Neurocomputing 603 (2024), 128290.
[15] Kun Ding, Qiang Yu, Haojian Zhang, Gaofeng Meng, and Shiming Xiang. 2024.
Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation. CoRR
abs/2410.08895 (2024).
[16] Kun Ding, Haojian Zhang, Qiang Yu, Ying Wang, Shiming Xiang, and Chunhong
Pan. 2024. Weak distribution detectors lead to stronger generalizability of vision-
language prompt tuning. In AAAI, Vol. 38. 1528â€“1536.
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image
is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR.
21 pages.
[18] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi,
and Dongfang Liu. 2023. E2VPT: An Effective and Efficient Approach for Visual
Prompt Tuning. In ICCV. 17445â€“17456.
[19] Hao Hao, Xiaoqun Zhang, and Aimin Zhou. 2024. Large language models as
surrogate models in evolutionary algorithms: A preliminary study. Swarm Evol.
Comput. 91 (2024), 101741.
[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross B.
Girshick. 2022. Masked Autoencoders Are Scalable Vision Learners. In CVPR.
15979â€“15988.
[21] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019.
EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land
Cover Classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens. 12, 7 (2019),
2217â€“2226.
[22] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-
guage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-
bodied Agents. In ICML, Vol. 162. 9118â€“9147.
[23] Jiho Jang, Chaerin Kong, Donghyeon Jeon, Seonhoon Kim, and Nojun Kwak. 2023.
Unifying Vision-Language Representation Space with Single-Tower Transformer.
In AAAI. 980â€“988.
[24] Ganesh Jawahar, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, and Dujian
Ding. 2024. LLM Performance Predictors are good initializers for Architecture
Search. In ACL. 10540â€“10560.
[25] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V.
Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling Up Visual and
Vision-Language Representation Learning With Noisy Text Supervision. In ICML,
Vol. 139. 4904â€“4916.
[26] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie,
Bharath Hariharan, and Ser-Nam Lim. 2022. Visual Prompt Tuning. In ECCV,
Vol. 13693. 709â€“727.
[27] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. A
survey on large language models for code generation. CoRR abs/2406.00515
(2024).
[28] Shibo Jie, Zhi-Hong Deng, Shixuan Chen, and Zhijuan Jin. 2024. Convolutional
Bypasses Are Better Vision Transformer Adapters. In ECAI, Vol. 392. 202â€“209.
[29] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. 3D Object Repre-
sentations for Fine-Grained Categorization. In ICCVW. 554â€“561.
[30] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[31] Fei-Fei Li, Robert Fergus, and Pietro Perona. 2007. Learning generative visual
models from few training examples: An incremental Bayesian approach tested
on 101 object categories. Comput. Vis. Image Underst. 106, 1 (2007), 59â€“70.
[32] Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, and Lijie Wen. 2025. ChatCite: LLM
Agent with Human Workflow Guidance for Comparative Literature Summary.
In COLING. 3613â€“3630.
[33] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg.
2023. Text2Motion: from natural language instructions to feasible plans. Auton.
Robots 47, 8 (2023), 1345â€“1365.
[34] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao
Lu, and Qingfu Zhang. 2024. Evolution of Heuristics: Towards Efficient Automatic
Algorithm Design Using Large Language Model. In ICML. 32201â€“32223.
[35] Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Zhe Zhao, Xi Lin, Xialiang Tong,
Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, and Qingfu Zhang. 2024.
A
Systematic Survey on Large Language Models for Algorithm Design. CoRR
abs/2410.14716 (2024).
[36] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. 2024.
Large language models as evolutionary optimizers. In IEEE Congress on Evolu-
tionary Computation (CEC). 1â€“8.
[37] Chris Lu, Samuel Holt, Claudio Fanconi, Alex Chan, Jakob Foerster, Mihaela
van der Schaar, and Robert Lange. 2024. Discovering preference optimization
algorithms with and for large language models. NeurIPS 37 (2024), 86528â€“86573.
[38] Kai Lu, Shixiong Xu, Jinqiu Li, Kun Ding, and Gaofeng Meng. 2025. Agent
Reviewers: Domain-specific Multimodal Agents with Shared Memory for Paper
Review. In ICML.
[39] Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. 2025. LLM4SR: A
Survey on Large Language Models for Scientific Research. CoRR abs/2501.04306
(2025).
[40] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and Andrea
Vedaldi. 2013. Fine-Grained Visual Classification of Aircraft. CoRR abs/1306.5151
(2013).
[41] Jinzhu Mao, Dongyun Zou, Li Sheng, Siyi Liu, Chen Gao, Yue Wang, and Yong Li.
2024. Identify Critical Nodes in Complex Network with Large Language Models.
CoRR abs/2403.03962 (2024).
[42] Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated Flower Classifi-
cation over a Large Number of Classes. In ICVGIP. 722â€“729.
[43] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, and et al. 2024. DINOv2:
Learning Robust Visual Features without Supervision. Trans. Mach. Learn. Res.
2024 (2024).
[44] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. 2012.
Cats and dogs. In CVPR. 3498â€“3505.
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models
From Natural Language Supervision. In ICML, Vol. 139. 8748â€“8763.
[46] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan
Huang, Jie Zhou, and Jiwen Lu. 2022. DenseCLIP: Language-Guided Dense
Prediction with Context-Aware Prompting. In CVPR. 18061â€“18070.
[47] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019.
Do ImageNet Classifiers Generalize to ImageNet?. In ICML. 5389â€“5400.
[48] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov,
Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S.
Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi.
2024. Mathematical discoveries from program search with large language models.
Nature 625, 7995 (2024), 468â€“475.
[49] Fawaz Sammani and Nikos Deligiannis. 2024. Interpreting and Analysing CLIPâ€™s
Zero-Shot Image Classification via Mutual Knowledge. In NeurIPS, Vol. 37. 39597â€“
39631.


--- Page 10 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
[50] Dhruv Shah, Michael Robert Equi, Blazej Osinski, Fei Xia, Brian Ichter, and Sergey
Levine. 2023. Navigation with Large Language Models: Semantic Guesswork as
a Heuristic for Planning. In CoRL, Vol. 229. 2683â€“2699.
[51] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Woj-
ciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. FLAVA: A Foundational
Language And Vision Alignment Model. In CVPR. 15617â€“15629.
[52] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012.
UCF101:
A Dataset of 101 Human Actions Classes From Videos in The Wild. CoRR
abs/1212.0402 (2012).
[53] Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. 2025. AutoML-Agent: A
Multi-Agent LLM Framework for Full-Pipeline AutoML. In ICML.
[54] Michael Tschannen, Basil Mustafa, and Neil Houlsby. 2023. CLIPPO: Image-and-
Language Understanding from Pixels Only. In CVPR. 11006â€“11017.
[55] Niki van Stein and Thomas BÃ¤ck. 2024. LLaMEA: A large language model evolu-
tionary algorithm for automatically generating metaheuristics. IEEE Transactions
on Evolutionary Computation (2024).
[56] Haohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. 2019. Learning
Robust Global Representations by Penalizing Local Predictive Power. In NeurIPS.
10506â€“10518.
[57] Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, and Tieniu Tan.
2024. A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation. In ICLR.
[58] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a Novel Image
Dataset for Benchmarking Machine Learning Algorithms. CoRR abs/1708.07747
(2017).
[59] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba.
2010. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR.
3485â€“3492.
[60] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny
Zhou, and Xinyun Chen. 2023. Large language models as optimizers. CoRR
abs/2309.03409 (2023).
[61] Hantao Yao, Rui Zhang, and Changsheng Xu. 2023. Visual-Language Prompt
Tuning with Knowledge-Guided Context Optimization. In CVPR. 6757â€“6767.
[62] Yiming Yao, Fei Liu, Ji Cheng, and Qingfu Zhang. 2024. Evolve Cost-Aware
Acquisition Functions Using Large Language Models. In PPSN, Vol. 15149. 374â€“
390.
[63] Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon
Kim, Jinkyoo Park, and Guojie Song. 2024. ReEvo: Large Language Models as
Hyper-Heuristics with Reflective Evolution. NeurIPS 37 (2024), 43571â€“43608.
[64] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
Yonghui Wu. 2022. CoCa: Contrastive Captioners are Image-Text Foundation
Models. Trans. Mach. Learn. Res. 2022 (2022).
[65] Junhua Zeng, Chao Li, Zhun Sun, Qibin Zhao, and Guoxu Zhou. 2024. tnGPS: dis-
covering unknown tensor network structure search algorithms via large language
models (LLMs). In ICML. 58329â€“58347.
[66] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai,
Yu Qiao, and Hongsheng Li. 2022. Tip-Adapter: Training-Free Adaption of CLIP
for Few-Shot Classification. In ECCV, Vol. 13695. 493â€“510.
[67] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou.
2023. Automl-gpt: Automatic machine learning with gpt. CoRR abs/2305.02499
(2023).
[68] Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan,
Yingxiang Yang, Hongxia Yang, and Zhaoran Wang. 2024. How Can LLM Guide
RL? A Value-Based Approach. CoRR abs/2402.16181 (2024).
[69] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning to
Prompt for Vision-Language Models. Int. J. Comput. Vis. 130, 9 (2022), 2337â€“2348.
[70] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao,
and Peng Gao. 2023. Not All Features Matter: Enhancing Few-shot CLIP with
Adaptive Prior Refinement. In ICCV. 2605â€“2615.


--- Page 11 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Appendix
A.1
Low-precision Code Conversion
To enable fp16 inference, we post-process the searched code by
modifying some PyTorch function callings. For the functions listed
in the first column of Table 9, we replace them with the corre-
sponding functions in the right column. For example, the original
â€˜torch.linalg.pinvâ€™ does not support fp16 computation, we define a
new function calling it after converting the data type to fp32. The
results are converted back to fp16 format. In the downstream stage,
we can also use such conversion for speedup.
A.2
Hyper-parameter Optimization
For evaluating the searched code on downstream datasets, we have
adopted Optuna for hyper-parameter optimization. The searched
hyper-parameters and the searching ranges are listed in Table 10.
In this table, ð‘‘denotes the number of feature dimensions, ð‘¡ð‘œð‘ð‘˜de-
notes the number of kept feature channels. The searching strategies
â€˜fsâ†’logitâ€™, â€˜logitâ†’fsâ€™, â€˜jointâ€™ and â€˜logitâ€™ denote searching feature
selection algorithm first and then logits computation algorithm,
searching logits computation algorithm first and then feature se-
lection algorithm, searching them jointly and only searching the
logits computation algorithm, respectively. By default, the number
of trials is set to 500. However, it will be slow for GDA and the
searched algorithms using GDA as initialization, we set the number
of trials as 100 in this cases.
A.3
Initialization
The initial algorithms for two-stage searching are shown in Fig. 5.
We additionally construct the description of thoughts for each ini-
tial algorithm. While initializing the feature selection algorithm
using APE, no extra parameters and hyper-parameters are intro-
duced. While initializing the logits computation algorithm using
APE, we additionally introduce â€˜alpha1â€™ and â€˜alpha2â€™ as hyper-
parameters. While initializing the logits computation algorithm
using Tip-Adapter and GDA, we additionally introduce â€˜indicesâ€™
as parameter, â€˜alpha1â€™ and â€˜alpha2â€™ as hyper-parameters. â€˜indicesâ€™
denotes the indices of selected feature channels.
The initial algorithms for joint searching are shown in Fig. 6.
To enable simultaneous searching of feature selection and logits
computation algorithms, we construct an overall algorithm in-
tegrating them together. In the description of thoughts, we in-
dicate that the algorithm consists of two steps: 1) selecting im-
portant features; 2) computing logits. The corresponding code in-
cludes three functions: â€˜feat_selectionâ€™, â€˜compute_logits_with_fsâ€™,
and â€˜compute_logitsâ€™. The â€˜feat_selectionâ€™ function selects features
based on CLIPâ€™s text features â€˜clip_weightsâ€™ and train imagesâ€™ fea-
tures â€˜train_featsâ€™. It returns the indices of selected feature chan-
nels â€˜indicesâ€™. The â€˜compute_logits_with_fsâ€™ function computes
classification logits of test features â€˜test_featsâ€™ by exploiting the
train imagesâ€™ features â€˜train_featsâ€™, the indices of selected feature
channels â€˜indicesâ€™, and so on. The â€˜compute_logitsâ€™ calls the above
two functions to compute classification logits. Note that, the â€˜com-
pute_logits_with_fsâ€™ is constructed based on the original code of
each algorithm. As Tip-Adapter and GDA do not use selected fea-
tures, the â€˜indicesâ€™ parameter is not used in the initial code.
A.4
Prompts
The prompt template for joint searching is the same to that in Fig. 7.
However, we slightly change the definition of the keywords in the
prompt template, which is given in Fig. 8. Importantly, in the input
parameters, we have also included the hyper-parameters for feature
selection. In the â€˜other_informationâ€™ part, we instruct LLM to only
design the â€˜feat_selectionâ€™ and â€˜compute_logits_with_fsâ€™ functions.
Table 9: Lookup table for code conversion.
Before Conversion
After Conversion
torch.linalg.pinv
new_inv = lambda x: torch.linalg.pinv(x.float()).half()
torch.inverse
new_inv = lambda x: torch.inverse(x.float()).half()
torch.linalg.inv
new_inv = lambda x: torch.linalg.inv(x.float()).half()
torch.svd
new_svd = lambda x: torch.svd(x.float()).half()
torch.linalg.eig
new_eig = lambda x: torch.linalg.eig(x.float()).half()
torch.linalg.eigvals
new_eigvals = lambda x: torch.linalg.eigvals(x.float()).half()
torch.zeros
new_zeros = lambda x: torch.zeros(x, dtype=torch.half)
torch.zeros_like
new_zeros_like = lambda x: torch.zeros_like(x, dtype=torch.half)
torch.ones
new_ones = lambda x: torch.ones(x, dtype=torch.half)
torch.ones_like
new_ones_like = lambda x: torch.ones_like(x, dtype=torch.half)
torch.eye
new_eye = lambda x: torch.eye(x, dtype=torch.half)
F.one_hot
new_onehot = lambda x: F.one_hot(x).half()
Table 10: Hyper-parameters and searching ranges in the
downstream validation stage.
Method
Hyper-parameters and ranges
Manually-designed Algorithms:
Tip-Adapter
ð›¼, ð›½âˆˆ[10âˆ’9, 100]
APE
ð›¼, ð›½,ð›¾âˆˆ[10âˆ’9, 100], ð‘¡ð‘œð‘ð‘˜âˆˆ(0,ð‘‘], ð‘¤0
and ð‘¤1 are set according to original paper
GDA
ð›¼âˆˆ[10âˆ’9, 100]
Automatically-designed Algorithms:
Tip-Adapter/APE/GDA + fsâ†’logit
ð›¼0, ð›¼1, ð›¼2 âˆˆ[10âˆ’9, 100], ð‘¡ð‘œð‘ð‘˜âˆˆ(0,ð‘‘]
ð‘¤0 = ð‘¤1 = 0.5
Tip-Adapter/APE/GDA + logitâ†’fs
ð›¼0, ð›¼1, ð›¼2 âˆˆ[10âˆ’9, 100], ð‘¡ð‘œð‘ð‘˜âˆˆ(0,ð‘‘]
ð‘¤0 = ð‘¤1 = 0.5
Tip-Adapter/APE/GDA + joint
ð›¼0, ð›¼1, ð›¼2 âˆˆ[10âˆ’9, 100], ð‘¡ð‘œð‘ð‘˜âˆˆ(0,ð‘‘]
ð‘¤0 = ð‘¤1 = 0.5
Tip-Adapter/GDA + logit
ð›¼0, ð›¼1, ð›¼2 âˆˆ[10âˆ’9, 100]
APE + logit
ð›¼0, ð›¼1, ð›¼2 âˆˆ[10âˆ’9, 100], ð‘¡ð‘œð‘ð‘˜âˆˆ(0,ð‘‘]
ð‘¤0 = ð‘¤1 = 0.5
A.5
More Results
A.5.1
Joint Searching. The curves of searching process of joint
searching can be found in Fig. 9. While the logits computation al-
gorithm is initialized with Tip-Adapter or GDA, we can find that,
the minimal objective curve is a horizontal line. This implies that
no better algorithm that the initial algorithm can be found in the
searching process. We conjecture that the combination of logits
computation and feature selection makes it hard to perform search-
ing in the code space.
A.5.2
Time Cost and Token Cost. We quantify the computational
cost of our search procedure using the DeepSeek API, as detailed
in Table 11. The reported token counts encompass both input and
output tokens. Our analysis reveals that GDA-based initialization
incurs the highest computational overhead in terms of both time
and token consumption. Based on the current pricing of $0.55 per
million output tokens, each search iteration costs up to $0.44 (0.8M
tokens Ã— $0.55/M). Furthermore, our profiling indicates that the


--- Page 12 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
Table 11: Time and token cost for code searching.
Init Strategy feature selection logits computation
Total
Token/K Time/h Token/K
Time/h
Token/K Time/h
TIP
fsâ†’logit
231
0.47
402
0.76
633
1.23
TIP
logitâ†’fs
365
1.91
248
0.78
613
2.69
TIP
joint
â€“
â€“
â€“
â€“
1027
3.83
APE fsâ†’logit
256
0.98
420
1.90
676
2.88
APE logitâ†’fs
500
2.24
257
1.09
757
3.33
APE
joint
â€“
â€“
â€“
â€“
1044
3.57
GDA fsâ†’logit
279
1.03
533
3.12
812
4.16
GDA logitâ†’fs
400
2.13
298
1.50
698
3.64
GDA
joint
â€“
â€“
â€“
â€“
1428
4.40
Table 12: Comparison of time complexity of searched code.
stage
hand-designed
Deepseek-generated
TIP
APE
GDA
TIP
APE
GDA
feat extraction (fixed)
0.0122
0.0122
0.0122
0.0122
0.0122
0.0122
feat selection
â€“
0.9111
â€“
0.0006
0.0007
0.0005
logit computation
0.0004
0.0013
0.0953
0.0012
0.0142
0.0645
total time
0.0126
0.9246
0.1075
0.0140
0.0271
0.0772
majority of the time expenditure stems from code execution, which
involves intensive data processing and iterative operations across
datasets, shot configurations, and hyper-parameters. We anticipate
that scaling our evaluation infrastructure (currently comprising 2Ã—
RTX 3090 GPUs with 4 parallel services per GPU) could significantly
reduce the overall time cost.
A.5.3
Time Complexity of Searched Code. We conduct experiments
to analyze the time complexity of generated algorithms. The run-
time analysis uses the following experimental setup: ResNet-50
architecture, 2,000 training images, test batch size of 256, 100 out-
put classes, and NVIDIA RTX 3090 GPU. The results are presented
in Table 12, which reveal two key findings: 1) The generated fea-
ture selection algorithm demonstrates significant time efficiency
improvements, attributable to our explicit prompt design that effec-
tively avoids computationally expensive multi-level loop; 2) While
logit computation exhibits a marginal slowdown, this has negligible
impact on overall inference time since image feature extraction re-
mains the dominant computational bottleneck. These conclude that
the LLM-designed algorithms not only achieve improved accuracy
but also maintain computational efficiency.
A.5.4
More Per-shot Results. More per-shot results using different
strategies and different combinations of holdout datasets are given
in Table 13.
A.5.5
Per-dataset Results. The per-dataset results of few-shot clas-
sification are shown in Fig. 10. From the figure, we can see that the
searched algorithms with APE initialization surpass APE signifi-
cantly with different shots. However, the searched algorithms with
Tip-Adapter or GDA based initialization outperform the baselines
slightly. We conjecture that the logits computation algorithm of
APE has already used the selected features, which would make it
easier for subsequent code searching. By contrast, although we
have added the â€˜indicesâ€™ to the function definition of Tip-Adapterâ€™s
and GDAâ€™s logits computation code, it requires LLM learns to use
this input before further improving the usage skills.
A.5.6
Searched Algorithms. Here, we visualize more results of
searched algorithms with different initialization in Fig. 11, Fig. 12,
Fig. 13, Fig. 14 and Fig. 15.
In Fig. 11, the searched algorithms under the strategy â€˜logitâ†’fsâ€™
using Tip-Adapterâ€™s logits computation algorithm and APEâ€™s feature
selection algorithm as initialization are shown. The searched logits
computation algorithm has four terms: â€˜clip_logitsâ€™, â€˜cache_logitsâ€™,
â€˜class_similarityâ€™ and â€˜train_guided_logitsâ€™. Instead of directly se-
lecting the important features by the indices, this algorithm in-
novatively introduces a hyper-parameter as a multiplier for im-
portant features. The â€˜class_similarityâ€™ computes logits using class
aggregated train features. Besides, the â€˜train_guided_logitsâ€™ term
essentially utilizes the second-order form of the similarity matrix
to enhance non-linearity. The searched feature selection algorithm
has confused inter-class and intra-class similarity. Meantime, the
code fails to correctly compute the intra-class and inter-class simi-
larity. Therefore, to some extent, the code generation capability of
current LLMs still has flaws.
In Fig. 12, the searched algorithms under the strategy â€˜fsâ†’logitsâ€™
using Tip-Adapterâ€™s logits computation algorithm and APEâ€™s fea-
ture selection algorithm as initialization are shown. The searched
feature selection algorithm innovatively introduces the concept of
class importance and it has been correctly implemented by code. It
essentially reflects the alignment between textual and visual fea-
tures, which is of good rationality. The â€˜inter_divâ€™ and â€˜intra_varâ€™
terms are relatively conventional designs. The algorithm also comes
up with a correlation term, however, the corresponding code imple-
mentation is wrong. As for the logits computation algorithm, there
are five terms in the designed algorithm. The â€˜class_weighted_logitsâ€™
is the product of â€˜cache_logitsâ€™ and â€˜class_weightsâ€™. However, the
introduction of the class weights is useless as we have told LLM
that each class has the same number of train samples.
In Fig. 13, the searched algorithms under the strategy â€˜logitâ†’fsâ€™
using APEâ€™s algorithm as initialization are shown. The searched log-
its computation algorithm has combined six terms. In the cache log-
its, the selected feature channels are adopted. The â€˜train_guied_logitsâ€™
is a productive composite of two logits: logits using class mean as
projection matrix and the same term with only selected features
used. This form could be useful for improving the nonlinearity of
the classifier. The â€˜class_interaction_logitsâ€™ term is slightly com-
plex, which is a composite of two kinds of logits: a cache-like
logits without exp function, a â€˜clip_logitsâ€™ like logits with only se-
lected features used. The â€˜cross_attention_logitsâ€™ has similar form
to â€˜class_interaction_logitsâ€™, but introduces the softmax function
to the train-test similarity matrix. The â€˜divergence_logitsâ€™ consid-
ers the information of per-class feature divergence. Besides, the
logits also undergoes a transform of normalization and re-scaling.
However, this line of code is redundant as it does not change the
index of maximal items. As for the feature selection algorithm, the
automatically-designed algorithm has a concise form, which avoids
the nested loops. The designed criterion has three terms: â€˜align-
mentâ€™, â€˜sparsityâ€™ and â€˜uniquenessâ€™. The alignment term requires
the selected features should be aligned well between visual and
textual modalities. The â€˜uniquenessâ€™ term is also a variance term,
which selects the features of high variance. The â€˜sparsityâ€™ term
may have little effect as the features may not be sparse. Meantime,


--- Page 13 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Thoughts:
The feature selection algorithm defines a criterion that aims to extract the feature channels that minimize
the inter-class similarity of the concatenated features of visual and category textual features, but
maximize the variance of category textual features.
Code:
import torch
def feat_selection(clip_weights, train_feats, w0, w1, topk):
feats = torch.cat([clip_weights.unsqueeze(1), train_feats], dim=1)
cate_num, samp_num, feat_dim = feats.shape
sim_sum = torch.zeros((feat_dim)).cuda()
count = 0
for i in range(cate_num):
for j in range(cate_num):
if i != j:
sim_sum += (feats[i].unsqueeze(1) * \
feats[j].unsqueeze(0)).mean(dim=0).mean(dim=0)
count += samp_num*samp_num
sim = sim_sum / count
criterion = (-1) * w0 * sim + w1 * torch.var(clip_weights, dim=0)
_, indices = torch.topk(criterion, k=topk)
return indices
APEâ€™s feature selection algorithm
Thoughts:
The algorithm sums up two logits: the logits generated by zero-shot classifier and the logits generated
by a cache model. The first logits are computed by applying linear transformation to test features. The
second logits are obtained by first computing the similarity matrix between test and train features and
then multiplying the transformed similarity matrix to soft train label matrix. While computing image-
image similarity, selected feature channels are used.
Code:
import torch
import torch.nn.functional as F
def compute_logits(train_feats, train_labels, test_feats,
clip_weights,indices, alpha0, alpha1, alpha2):
# feature selection
train_feats = train_feats.view(-1, train_feats.shape[-1])
new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)
new_train_feats = F.normalize(train_feats[:, indices], dim=-1)
new_test_feats = F.normalize(test_feats[:, indices], dim=-1)
# compute cache logits
train_labels = F.one_hot(train_labels.view(-1))
key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)
cache_div = torch.sum(train_labels * torch.log2((train_labels
+ 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]
soft_train_labels = train_labels * (cache_div * alpha2).exp()
R_fF = new_test_feats @ new_train_feats.t()
cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp()
@ soft_train_labels
# zero-shot logits
clip_logits = 100*test_feats @ clip_weights.t()
# fused logits
logits = clip_logits + alpha0 * cache_logits
return logits
APEâ€™s logits computation algorithm
Thoughts:
The algorithm sums up two logits: the logits generated by zero-shot classifier and the logits generated by a cache model.
The first logits are computed by applying linear transformation to test features. The second logits are obtained by first
computing the similarity matrix between test and train features and then multiplying the transformed similarity matrix to
soft train label matrix. While computing image-image similarity, selected feature channels are used.
Code:
import torch
import torch.nn.functional as F
def compute_logits(train_feats, train_labels, test_feats,
clip_weights,indices, alpha0, alpha1, alpha2):
c, k, d = train_feats.shape
train_feats = train_feats.view(-1, d)
train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)
clip_logits = 100*test_feats @ clip_weights.t()
similarity_matrix = test_feats@train_feats.t()
cache_logits = ((-1)*(alpha1-alpha1*similarity_matrix)).exp()@train_labels
logits = clip_logits + alpha0*cache_logits
return logits
Tip-Adapterâ€™s logits computation algorithm
Thoughts:
The logits consist of two parts: the logits computed by CLIP's zero-shot classifier and the logits computed by Gaussian
Discriminant Analysis (GDA) model. In each part, all feature channels are used. GDA is a probabilistic generative model for
classification that assumes all classes are generated by Gaussian distributions with a common covariance matrix but
different mean vectors. GDA first computes per-class mean vector and then estimates the inverted covariance matrix. After
that the weight and bias of the GDA classifier can be computed.
Code:
import torch
def compute_logits(train_feats, train_labels, test_feats, clip_weights,
indices, alpha0, alpha1, alpha2):
num_classes, k, d = train_feats.shape
train_feats = train_feats.view(-1, d)
train_labels = train_labels.view(-1)
# compute per-class mean features
mus = torch.cat([train_feats[train_labels == i].mean(dim=0, \
keepdim=True) for i in range(num_classes)])
# use KS Estimator to estimate inverted covariance matrix
center_vecs = torch.cat([train_feats[train_labels == i] - \
mus[i].unsqueeze(0) for i in range(num_classes)])
cov_inv = center_vecs.shape[1] * torch.linalg.pinv((center_vecs.shape[0] \
- 1) * center_vecs.T.cov() + center_vecs.T.cov().trace() * \
torch.eye(center_vecs.shape[1]).cuda())
ps = torch.ones(num_classes).cuda() * 1. / num_classes
W = mus @ cov_inv
b = ps.log() - (W*mus).sum(dim=1) / 2
gda_logits = (test_feats @ W.t() + b)
clip_logits = 100*test_feats @ clip_weights.t()
logits = clip_logits + alpha0 * gda_logits
return logits
GDAâ€™s logits computation algorithm
Figure 5: Initial algorithms for two-stage searching.
Table 13: Results of few-shot recognition (ResNet-50). C: CIFAR100, F: FashionMnist, O: ObjectNet, M: UcMerced, B: UCSDBirds
Datasets
Initialization
Strategy
1-shot
2-shot
4-shot
8-shot
16-shot
Avg.
C+F+O+M+B
Tip-Adapter
fsâ†’logit
62.72
64.93
67.53
69.69
71.17
67.21
logitâ†’fs
62.91
64.76
67.25
69.33
71.00
67.05
joint
62.51
64.16
66.52
69.01
70.71
66.58
logit
62.51
64.77
67.49
69.73
71.54
67.21
C+F+O+M+B
APE
fsâ†’logit
64.18
66.23
69.60
72.09
74.11
69.24
logitâ†’fs
63.80
66.07
69.51
72.24
74.00
69.12
joint
64.18
65.75
68.45
71.03
72.89
68.46
logit
63.57
66.14
69.22
71.11
72.71
68.55
C+F+O+M+B
GDA
fsâ†’logit
63.56
66.51
70.35
73.99
76.43
70.17
logitâ†’fs
62.99
66.20
70.02
73.59
76.33
69.83
joint
62.97
65.79
69.85
72.97
75.57
69.43
logit
62.96
66.06
70.24
73.69
76.23
69.84
C+F+O+M+B
â€“
fsâ†’logit
62.25
63.60
65.79
67.48
68.28
65.48
logitâ†’fs
61.53
64.12
66.56
68.73
69.94
66.18
C+F
Tip-Adapter
fsâ†’logit
63.73
65.14
68.02
70.07
71.58
67.71
logitâ†’fs
63.00
64.53
67.53
69.88
71.26
67.24
C+F
APE
fsâ†’logit
64.01
65.64
68.47
70.84
72.97
68.39
logitâ†’fs
63.66
65.94
68.74
71.41
73.27
68.60
C+F
GDA
fsâ†’logit
63.56
66.51
70.35
73.99
76.43
70.17
logitâ†’fs
62.99
66.20
70.02
73.59
76.33
69.83


--- Page 14 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
Thoughts:
The algorithm has two steps. First, select important feature channels according to a devised criterion. The criterion gives high scores
for channels that minimize the inter-class similarity of the concatenated features of visual and category textual features, but
maximize the variance of category textual features. Second, compute logits by combining the logits generated by CLIP's zero-shot
classifier and the logits generated by a cache model. The first logits are computed by applying linear transformation to test features.
The second logits are obtained by first computing the similarity matrix between test and train features and then multiplying the
transformed similarity matrix to soft train label matrix. While computing image-image similarity, selected feature channels are used.
Code:
import torch
import torch.nn.functional as F
def feat_selection(clip_weights, train_feats, w0, w1, topk):
feats = torch.cat([clip_weights.unsqueeze(1), train_feats], dim=1)
cate_num, samp_num, feat_dim = feats.shape
sim_sum = torch.zeros((feat_dim)).cuda()
count = 0
for i in range(cate_num):
for j in range(cate_num):
if i != j:
sim_sum += (feats[i].unsqueeze(1) * \
feats[j].unsqueeze(0)).mean(dim=0).mean(dim=0)
count += samp_num*samp_num
sim = sim_sum / count
criterion = (-1) * w0 * sim + w1 * torch.var(clip_weights, dim=0)
_, indices = torch.topk(criterion, k=topk)
return indices
def compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices,
alpha0, alpha1, alpha2):
train_feats = train_feats.view(-1, train_feats.shape[-1])
new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)
new_train_feats = F.normalize(train_feats[:, indices], dim=-1)
new_test_feats = F.normalize(test_feats[:, indices], dim=-1)
# compute cache logits
train_labels = F.one_hot(train_labels.view(-1))
key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)
cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6)
/ (key_logits + 1e-6)), dim=1)[:, None]
soft_train_labels = train_labels * (cache_div * alpha2).exp()
R_fF = new_test_feats @ new_train_feats.t()
cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels
# zero-shot logits
clip_logits = 100*test_feats @ clip_weights.t()
# fused logits
logits = clip_logits + alpha0 * cache_logits
return logits
def compute_logits(train_feats, train_labels, test_feats, clip_weights, w0, w1, topk,
alpha0, alpha1, alpha2):
# feature selection
indices = feat_selection(clip_weights, train_feats, w0, w1, topk)
# compute logits
logits = compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights,
indices, alpha0, alpha1, alpha2)
return logits
Thoughts:
The algorithm has two steps. First, select important feature channels according to a devised criterion. The criterion gives high scores for channels that minimize
the inter-class similarity of the concatenated features of visual and category textual features, but maximize the variance of category textual features. Second,
compute logits by combining the logits generated by CLIP's zero-shot classifier and the logits computed by Gaussian Discriminant Analysis (GDA) model. In
each part, all feature channels instead of selected channels are used. GDA is a probabilistic generative model for classification that assumes all classes are
generated by Gaussian distributions with a common covariance matrix but different mean vectors. GDA first computes per-class mean vector and then estimates
the inverted covariance matrix. After that the weight and bias of the GDA classifier can be computed.
Code:
import torch
import torch.nn.functional as F
def feat_selection(clip_weights, train_feats, w0, w1, topk):
feats = torch.cat([clip_weights.unsqueeze(1), train_feats], dim=1)
cate_num, samp_num, feat_dim = feats.shape
sim_sum = torch.zeros((feat_dim)).cuda()
count = 0
for i in range(cate_num):
for j in range(cate_num):
if i != j:
sim_sum += (feats[i].unsqueeze(1) * feats[j].unsqueeze(0)).mean(dim=0).mean(dim=0)
count += samp_num*samp_num
sim = sim_sum / count
criterion = (-1) * w0 * sim + w1 * torch.var(clip_weights, dim=0)
_, indices = torch.topk(criterion, k=topk)
return indices
def compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices,
alpha0, alpha1, alpha2):
cate_num, samp_num, feat_dim = train_feats.shape
train_feats = train_feats.view(-1, feat_dim)
train_labels = train_labels.view(-1)
# compute per-class mean features
mus = torch.cat([train_feats[train_labels == i].mean(dim=0, keepdim=True) for i in range(cate_num)])
# use KS Estimator to estimate inverted covariance matrix
center_vecs = torch.cat([train_feats[train_labels == i] - mus[i].unsqueeze(0) for i in range(cate_num)])
cov_inv = center_vecs.shape[1] * torch.linalg.pinv((center_vecs.shape[0] - 1) * center_vecs.T.cov() +
center_vecs.T.cov().trace() * torch.eye(center_vecs.shape[1]).cuda())
ps = torch.ones(cate_num).cuda() * 1. / cate_num
W = mus @ cov_inv
b = ps.log() - (W*mus).sum(dim=1) / 2
gda_logits = (test_feats @ W.t() + b)
clip_logits = 100*test_feats @ clip_weights.t()
logits = clip_logits + alpha0 * gda_logits
return logits
def compute_logits(train_feats, train_labels, test_feats, clip_weights, w0, w1, topk, alpha0,
alpha1, alpha2):
# feature selection
indices = feat_selection(clip_weights, train_feats, w0, w1, topk)
# compute logits
logits = compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices,
alpha0, alpha1, alpha2)
return logits
Thoughts:
The algorithm has two steps. First, select important feature channels according to a devised criterion. The criterion gives high scores for channels that minimize the inter-class similarity of the concatenated features of visual and category textual features, but maximize the variance of category textual
features. Second, compute logits by combining the logits generated by CLIP's zero-shot classifier and the logits generated by a cache model. The first logits are computed by applying linear transformation to test features. The second logits are obtained by first computing the similarity matrix between test
and train features and then multiplying the transformed similarity matrix to soft train label matrix. In each part, all feature channels instead of selected channels are used.
Code:
import torch
import torch.nn.functional as F
def feat_selection(clip_weights, train_feats, w0, w1, topk):
feats = torch.cat([clip_weights.unsqueeze(1), train_feats], dim=1)
cate_num, samp_num, feat_dim = feats.shape
sim_sum = torch.zeros((feat_dim)).cuda()
count = 0
for i in range(cate_num):
for j in range(cate_num):
if i != j:
sim_sum += (feats[i].unsqueeze(1) * feats[j].unsqueeze(0)).mean(dim=0).mean(dim=0)
count += samp_num*samp_num
sim = sim_sum / count
criterion = (-1) * w0 * sim + w1 * torch.var(clip_weights, dim=0)
_, indices = torch.topk(criterion, k=topk)
return indices
def compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):
cate_num, samp_num, feat_dim = train_feats.shape
train_feats = train_feats.view(-1, feat_dim)
train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)
clip_logits = 100*test_feats @ clip_weights.t()
similarity_matrix = test_feats@train_feats.t()
cache_logits = ((-1)*(alpha1-alpha1*similarity_matrix)).exp()@train_labels
logits = clip_logits + alpha0*cache_logits
return logits
def compute_logits(train_feats, train_labels, test_feats, clip_weights, w0, w1, topk, alpha0, alpha1, alpha2):
# feature selection
indices = feat_selection(clip_weights, train_feats, w0, w1, topk)
# compute logits
logits = compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2)
return logits
APEâ€™s feature selection + logit computation algorithm
APEâ€™s feature selection algorithm + GDAâ€™s logit computation algorithm
APEâ€™s feature selection algorithm + Tip-Adapterâ€™s logit computation algorithm
Figure 6: Initial algorithms for joint searching.


--- Page 15 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Prompt Template of Crossover:
{task_description}
I have 2 existing algorithms with their codes as follows:
No.1 algorithm and the corresponding code are:
{thoughts_of_algorithm1}
{code_of_algorithm1}
No.2 algorithm and the corresponding code are:
{thoughts_of_algorithm2}
{code_of_algorithm2}
Please help me create a new algorithm that has different form
from the given ones but can be motivatedby them.
First, describe your new algorithm and main steps in one
sentence. The descriptionmust be inside a brace.
Next,
implement
it
in
Python
as
a
function
named
{function_name}.
This
function
should accept
5
input(s):
{inputs}. The function should return 1 output(s): {outputs}.
{inout_information} {other_information}
Do not give additional explanations.
Prompt Template of Mutation:
{task_description}
I have one algorithm with its code as follows. Algorithm 
description: {thoughts_of_algorithm1}
Code:
{code_of_algorithm1}
Please assist me in creating a new algorithm that is a modified 
version of the algorithm provided. 
First, describe your new algorithm and main steps in one 
sentence. The description must be inside a brace. 
Next, implement it in Python as a function named 
{function_name}. This function should accept 5 input(s): 
{inputs}. The function should return 1 output(s): {outputs}. 
{inout_information} {other_information}
Do not give additional explanations.
Feature Selection:
task_description: Given CLIP's encoded features of train images and class names, you need to select the best feature
channels. Your selected channels should improve classification accuracy on test images. Help me design a novel feature
selection algorithm that is different from the ones in literature.
function_name: 'feat_selection'
inputs: 'clip_weights', 'train_feats', 'w0', 'w1', 'topk'
outputs: 'indices'
inout_information: 'clip_weights' denotes the zero-shot classifier weights computed by encoding all the class names by
CLIP's text encoder, which has a shape of (c,d) with c the number of classes and d the number of feature dimensions. Each
row of 'clip_weights' is L2-normalized. 'train_feats' denotes the train images' features, which is a matrix of shape (c,k,d) with
k the number of train images per class. The third dimension of 'train_features' is L2-normalized. 'w0' and 'w1' are preset
hyper-parameters, which are in the range [0,1]. 'topk' denotes the number of selected features. 'indices' denotes the indices of
selectedfeature channels, which should have a length of 'topk'.
other_information: All except 'w0', 'w1', 'topk' are PyTorch Tensors. Do not introduce randomness in the code. Do not
introduce any learnable parameters. Please optimize runtime efficiency while maintaining code readability, avoiding the use
of deep nested loops. You can use any mathematical operation on the inputs, please try to be creative and make full use of the
input information.
Logits Computation:
task_description: You are given a task for adapting CLIP for few-shot image classification. You need to devise a good
function for computing the classification logits of test images. Actually, you need to combine the information in text features,
test features, train features and train labels. Also, the set of important feature channels is provided. You should consider to use
this information in the devised function. By finding the index corresponing to the maximal item in the logits outputted by this
function, we can obtain the predictive labels. The devised logits function should be different from those in the existing
literature.
function_name: 'compute_logits'
inputs: 'train_feats', 'train_labels', 'test_feats', 'clip_weights', 'indices', 'alpha0', 'alpha1', 'alpha2'
outputs: 'logits'
inout_information: 'train_feats' denotes the train images' features, which is a matrix of shape (c,k,d) with c the number of
classes, k the number of train images per class, d the number of feature dimensions. The third dimension of 'train_feats' is L2-
normalized. 'train_labels' denotes the train labels with a shape of (c,k). The values of the i-th row in 'train_labels' are i.
'test_feats' denotes the test images' features of shape (n,d), where n is the number of test samples. 'clip_weights' denotes the
zero-shot classifier weights computed by encoding all the class names by CLIP's text encoder, which has a shape of (c,d).
Each row of 'clip_weights' is L2-normalized. 'indices' denotes the indices of important features. 'alpha0', 'alpha1', 'alpha2' are
optional hyper-parameters,they are in the range [0, 20]. You could use some or all of these hyper-parameters.
other_information: All except 'alpha0', 'alpha1' and 'alpha2' are PyTorch Tensors. Do not introduce randomness in the code.
Do not introduce any learnable parameters. Please optimize runtime efficiency while maintaining code readability, avoiding
the use of deep nested loops. You can use any mathematical operation on the inputs, please try to be creative and make full
use of the input information.
Figure 7: Prompts of crossover and mutation for searching feature selection and logits computation algorithms.
task_description: You are given a task for adapting CLIP for few-shot image classification. You need to devise a good algorithm for computing the
classification logits of test images. By finding the index corresponing to the maximal item in the logits, we can obtain the predictive labels. The
devised algorithm should be different from those in the existing literature.
function_name: 'compute_logits'
inputs: 'train_feats', 'train_labels', 'test_feats', 'clip_weights', 'w0', 'w1', 'topk', 'alpha0', 'alpha1', 'alpha2'
outputs: 'logits'
inout_information: 'train_feats' denotes the train images' features, which is a matrix of shape (c,k,d) with c the number of classes, k the number of
train images per class, d the number of feature dimensions. The third dimension of 'train_feats' is L2-normalized. 'train_labels' denotes the train labels
with a shape of (c,k). The values of the i-th row in 'train_labels' are i. 'test_feats' denotes the test images' features of shape (n,d), where n is the number
of test samples. 'clip_weights' denotes the zero-shot classifier weights computed by encoding all the class names by CLIP's text encoder, which has a
shape of (c,d). Each row of 'clip_weights' is L2-normalized. 'w0', 'w1' and 'topk' are preset hyper-parameters for selecting important feature channels.
'w0' and 'w1' are used for defining criterion for feature selection, which are in the range [0,1], 'topk' denotes the number of selected channels. 'alpha0',
'alpha1', 'alpha2' are optional hyper-parameters in logits function, they are in the range [0, 20]. You could use some or all of these hyper-parameters.
other_information:
You
only
need
to
design
the
feature
selection
function
'feat_selection'
and
the
logits
computation
function
'compute_logits_with_fs' using selected features. Do not modify the input parameter names of these two functions. All except 'w0', 'w1', 'topk',
'alpha0', 'alpha1' and 'alpha2' are PyTorch Tensors. Do not introduce randomness in the code. Do not introduce any learnable parameters. Please
optimize runtime efficiency while maintaining code readability, avoiding the use of deep nested loops. You can use any mathematical operation on the
inputs, please try to be creative and make full use of the input information.
Figure 8: Prompts of crossover and mutation for joint searching. The prompt template is the same to that in Fig. 7.
the reversal of the sign of the sparsity term does not achieve the
original intention of the algorithm.
In Fig. 14, the searched algorithms under the strategy â€˜fsâ†’logitâ€™
using APEâ€™s algorithm as initialization are shown. In the searched
feature selection algorithm, the first term emphasizes the correla-
tion between visual and textual features, as well the variance of
feature channels. The second, third and fourth terms actually repre-
sent the feature variance. As such, there are discrepancies between
the code and the comments. As for the logits computation algorithm,
modulating the logits by the test featuresâ€™ norm could be a relatively
novel design, which are reflected in the â€˜weighted_class_simâ€™ and
â€˜novel_class_simâ€™ terms.


--- Page 16 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
Figure 9: Visualization of searching process of joint searching. The logits computation algorithm is initialized with Tip-Adapter
(left), APE (middle) and GDA (right).
1 2
4
8
16
Shots Number
62
64
66
68
70
72
74
76
Accuracy (%)
Average
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
88
89
90
91
92
93
Accuracy (%)
Caltech-101
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
86
87
88
89
90
Accuracy (%)
OxfordPets
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
57.5
60.0
62.5
65.0
67.5
70.0
72.5
75.0
Accuracy (%)
StanfordCars
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
70
75
80
85
90
95
Accuracy (%)
Flowers102
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
77.25
77.50
77.75
78.00
78.25
78.50
78.75
79.00
Accuracy (%)
Food101
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
20
25
30
35
40
Accuracy (%)
FGVC
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
60
62
64
66
68
70
Accuracy (%)
SUN397
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
45
50
55
60
65
Accuracy (%)
DTD
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
55
60
65
70
75
80
85
Accuracy (%)
EuroSAT
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
64
66
68
70
72
74
76
78
Accuracy (%)
UCF101
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
1 2
4
8
16
Shots Number
60.5
61.0
61.5
62.0
62.5
63.0
63.5
64.0
64.5
Accuracy (%)
ImageNet
Tip
GDA
APE
Tip(fs->logit)
GDA(fs->logit)
APE(fs->logit)
Tip(logit->fs)
GDA(logit->fs)
APE(logit->fs)
Figure 10: Comparison of per-dataset few-shot classification results.


--- Page 17 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
import torch
import torch.nn.functional as F
def compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):
c, k, d = train_feats.shape
train_feats = train_feats.view(-1, d)
train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)
# Adjust test and train features using important feature channels
test_feats_adjusted = test_feats.clone()
train_feats_adjusted = train_feats.clone()
test_feats_adjusted[:, indices] *= alpha2
train_feats_adjusted[:, indices] *= alpha2
# Zero-shot logits with adjusted test features
clip_logits = 100 * test_feats_adjusted @ clip_weights.t()
# Compute similarity matrix using adjusted train features
similarity_matrix = torch.exp(-alpha1 * (1 - test_feats_adjusted @ train_feats_adjusted.t()))
# Cache model logits with weighted similarity matrix
cache_logits = similarity_matrix @ train_labels
# Class-specific feature aggregation term with feature channel importance weighting
class_aggregated_feats = train_feats.view(c, k, d).mean(dim=1)
class_aggregated_feats[:, indices] *= alpha2
class_similarity = test_feats @ class_aggregated_feats.t()
# Train-feature-guided logits weighted by similarity
train_guided_logits = (test_feats @ train_feats.t()) * (test_feats_adjusted @ train_feats_adjusted.t())
@ train_labels
# Combine logits
logits = clip_logits + alpha0 * cache_logits + alpha1 * class_similarity + train_guided_logits
return logits
import torch
def feat_selection(clip_weights, train_feats, w0, w1, topk):
feats = torch.cat([clip_weights.unsqueeze(1), train_feats], dim=1)
cate_num, samp_num, feat_dim = feats.shape
# Compute intra-class cosine similarity
intra_sim = torch.zeros(feat_dim).cuda()
for i in range(cate_num):
class_feats = feats[i]
mean_feats = class_feats.mean(dim=0, keepdim=True)
intra_sim += torch.sum(class_feats * mean_feats, dim=0) / samp_num
intra_sim /= cate_num
# Compute inter-class cosine similarity of textual features
mean_clip_weights = clip_weights.mean(dim=0, keepdim=True)
inter_sim = torch.sum(clip_weights * mean_clip_weights, dim=0) / cate_num
# Combine criteria
criterion = w0 * inter_sim - w1 * intra_sim
_, indices = torch.topk(criterion, k=topk)
return indices
The
new
algorithm
computes
classification
logits
by
combining
zero-
shot classification logits, a
feature-channel importance-
weighted
similarity-based
cache logits, a class-specific
feature
aggregation
term
with
feature-channel
importance weighting, and a
train-feature-guided
logits
where
the
feature-channel
importance is used to refine
both the test features and
the
train
features
before
computing
the
similarity
matrix, and the train-feature-
guided
logits
are
further
weighted by the similarity
between test features and
train features.
The new algorithm selects
feature
channels
by
maximizing
the
discriminative power of the
features, which is achieved
by minimizing the intra-class
cosine
similarity
of
the
concatenated
visual
and
textual
features
while
maximizing
the
inter-class
cosine
similarity
of
the
textual features.
Figure 11: Visualization of searching process and results (with Tip-Adapter based initialization). Top row: logits computation
(first stage), bottom row: feature selection (second stage). From left to right: population at each iteration, thoughts, code.
import torch
def feat_selection(clip_weights, train_feats, w0, w1, topk):
cate_num, samp_num, feat_dim = train_feats.shape
# Compute class-specific feature importance
class_importance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)
# Compute inter-class divergence
class_means = train_feats.mean(dim=1) # Shape: (cate_num, feat_dim)
inter_div = torch.var(class_means, dim=0) # Shape: (feat_dim,)
# Compute intra-class variance
intra_var = torch.var(train_feats, dim=(0, 1)) # Shape: (feat_dim,)
# Compute correlation between feature channels
correlation = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)
# Combine criteria
criterion = w0 * (class_importance * inter_div) - w1 * (intra_var + correlation)
# Select topk features
_, indices = torch.topk(criterion, k=topk)
return indices
import torch
import torch.nn.functional as F
def compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):
c, k, d = train_feats.shape
train_feats = train_feats.view(-1, d)
train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)
# Zero-shot classifier logits
clip_logits = 100 * test_feats @ clip_weights.t()
# Feature channel importance-based logits with adaptive reweighting
important_feats = test_feats[:, indices]
important_weights = clip_weights[:, indices]
channel_importance = torch.norm(important_weights, dim=1, keepdim=True)
adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)
channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)
# Class prototype-based logits with dynamic scaling
class_prototypes = train_feats.view(c, k, d).mean(dim=1)
prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))
prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()
# Cache model logits with adaptive kernel
similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))
cache_logits = similarity_matrix @ train_labels
# Feature channel importance-based regularization term
regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) *
torch.norm(clip_weights[:, indices], p=2, dim=1)
# Class-specific weighting based on train labels
class_weights = train_labels.sum(dim=0).unsqueeze(0)
class_weighted_logits = cache_logits * class_weights
# Combine all logits
logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits +
alpha2 * prototype_logits + regularization_term
return logits
The new algorithm selects
feature
channels
by
maximizing
the
class-
specific feature importance
weighted by the inter-class
divergence,
while
minimizing
the
intra-class
variance and the correlation
between selected channels,
weighted
by
hyper-
parameters.
The
new
algorithm
computes
classification
logits
by
combining
zero-
shot
classifier
logits,
a
feature channel importance-
based logits with adaptive
reweighting,
a
class
prototype-based logits with
dynamic
scaling,
and
a
weighted
similarity-based
cache
model
logits
with
adaptive
kernel,
while
introducing a novel feature
channel
importance-based
regularization
term
and
a
class-specific
weighting
scheme
based
on
train
labels.
Figure 12: Visualization of searching process and results (with Tip-Adapter based initialization). Top row: feature selection
(first stage), bottom row: logits computation (second stage). From left to right: population at each iteration, thoughts, code.


--- Page 18 ---
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Kun Ding, Ying Wang, Shiming Xiang
import torch
import torch.nn.functional as F
def compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):
# feature selection
train_feats = train_feats.view(-1, train_feats.shape[-1])
new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)
new_train_feats = F.normalize(train_feats[:, indices], dim=-1)
new_test_feats = F.normalize(test_feats[:, indices], dim=-1)
# compute zero-shot logits
clip_logits = 100 * test_feats @ clip_weights.t()
# compute feature-channel-weighted similarity-based cache logits
train_labels = F.one_hot(train_labels.view(-1))
key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)
cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]
soft_train_labels = train_labels * (cache_div * alpha2).exp()
R_fF = new_test_feats @ new_train_feats.t()
cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels
# compute train-feature-guided logits with class-wise aggregation
class_means = torch.stack([train_feats[train_labels[:, i].bool()].mean(dim=0) for i in range(train_labels.shape[1])])
class_means_selected = F.normalize(class_means[:, indices], dim=-1)
train_guided_logits = (test_feats @ class_means.t()) * (new_test_feats @ class_means_selected.t())
# compute feature-channel-weighted class-wise interaction term
class_interaction = new_test_feats @ new_train_feats.t()
class_interaction_logits = class_interaction @ train_labels * (1 + alpha0 * (new_test_feats @ new_clip_weights.t()).mean(dim=1, keepdim=True))
# compute feature-channel-weighted cross-attention term with adaptive weighting
attention_weights = F.softmax(new_test_feats @ new_train_feats.t(), dim=-1)
cross_attention_logits = attention_weights @ train_labels * (1 + alpha0 * (new_test_feats @ new_clip_weights.t()).mean(dim=1, keepdim=True))
# compute feature-channel-weighted class-wise divergence term
class_divergence = torch.stack([torch.mean((new_train_feats[train_labels[:, i].bool()] - class_means_selected[i])**2, dim=0)
for i in range(train_labels.shape[1])])
divergence_logits = (new_test_feats @ class_means_selected.t()) * (1 - alpha1 * (new_test_feats @ class_divergence.t()))
# fused logits with additional normalization and weighting
logits = clip_logits + alpha0 * cache_logits + alpha1 * train_guided_logits + alpha2 * class_interaction_logits
+ alpha0 * cross_attention_logits + alpha1 * divergence_logits
logits = F.normalize(logits, dim=-1) * (alpha0 + alpha1 + alpha2)
return logits
import torch
def feat_selection(clip_weights, train_feats, w0, w1, topk):
cate_num, samp_num, feat_dim = train_feats.shape
# Compute class-wise mean of visual features
class_mean = train_feats.mean(dim=1) # Shape: (cate_num, feat_dim)
# Compute alignment between class-wise mean and textual features
alignment = torch.sum(class_mean * clip_weights, dim=0) # Shape: (feat_dim,)
# Compute sparsity of features
sparsity = torch.sum(torch.abs(class_mean), dim=0) / (torch.sum(class_mean ** 2, dim=0) + 1e-8) # Shape: (feat_dim,)
# Compute class-wise feature uniqueness
uniqueness = torch.sum((class_mean - class_mean.mean(dim=0)) ** 2, dim=0) # Shape: (feat_dim,)
# Combine criteria with weighted combination
criterion = w0 * alignment + w1 * sparsity + uniqueness
# Select topk features
_, indices = torch.topk(criterion, k=topk)
return indices
The
new
algorithm
computes
classification
logits
by
combining
zero-
shot
logits,
a
feature-
channel-weighted similarity-
based cache logits, a train-
feature-guided
logits
with
class-wise
aggregation,
a
feature-channel-weighted
class-wise interaction term,
and
a
novel
feature-
channel-weighted
cross-
attention term with adaptive
weighting based on feature-
channel
importance,
while
introducing
additional
normalization and weighting
schemes, and incorporating
a
feature-channel-weighted
class-wise divergence term.
The new algorithm selects
feature
channels
by
maximizing
the
alignment
between
class-wise
mean
visual features and textual
features, while incorporating
feature sparsity and class-
wise
feature
uniqueness,
using
a
weighted
combination
of
alignment,
sparsity,
and
uniqueness
scores.
Figure 13: Visualization of searching process and results (with APE based initialization). Top row: logits computation (first
stage), bottom row: feature selection (second stage). From left to right: population at each iteration, thoughts, code.
import torch
def feat_selection(clip_weights, train_feats, w0, w1, topk):
cate_num, samp_num, feat_dim = train_feats.shape
# Compute class-specific feature relevance based on CLIP weights
class_relevance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)
# Compute global feature consistency
global_consistency = torch.mean(train_feats, dim=(0, 1))
# Compute class-specific discriminative power
class_means = train_feats.mean(dim=1)
discriminative_power = torch.sum((class_means - global_consistency) ** 2, dim=0)
# Compute cross-class feature overlap
cross_overlap = torch.sum(class_means ** 2, dim=0)
# Compute feature variance across samples
feature_variance = torch.var(train_feats, dim=(0, 1))
# Compute class-specific feature interaction score
class_feats = train_feats.permute(1, 0, 2) # (k, c, d)
pairwise_corr = torch.einsum('kcd,kcd->d', class_feats, class_feats) / (cate_num * samp_num)
# Combine criteria with dynamic feature saliency
criterion = w0 * class_relevance * global_consistency * discriminative_power
- w1 * cross_overlap + feature_variance + pairwise_corr
# Select topk features
_, indices = torch.topk(criterion, k=topk)
return indices
import torch
import torch.nn.functional as F
def compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):
# feature selection
train_feats = train_feats.view(-1, train_feats.shape[-1])
new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)
new_train_feats = F.normalize(train_feats[:, indices], dim=-1)
new_test_feats = F.normalize(test_feats[:, indices], dim=-1)
# compute zero-shot logits
clip_logits = 100 * test_feats @ clip_weights.t()
# compute class-specific feature aggregation with feature channel importance
class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])
class_feats = F.normalize(class_feats, dim=-1)
class_sim = new_test_feats @ class_feats.t()
weighted_class_sim = class_sim * (1 + alpha1 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))
# compute cache logits with feature channel importance
train_labels = F.one_hot(train_labels.view(-1))
key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)
cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]
soft_train_labels = train_labels * (cache_div * alpha2).exp()
R_fF = new_test_feats @ new_train_feats.t()
cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels
# compute feature-channel importance-weighted regularization term
importance_weights = torch.zeros_like(test_feats)
importance_weights[:, indices] = 1.0
weighted_test_feats = test_feats * importance_weights
weighted_logits = 100 * weighted_test_feats @ clip_weights.t()
# compute feature-channel importance-weighted similarity term
similarity_term = new_test_feats @ new_clip_weights.t()
# compute novel feature-channel importance-weighted class-specific feature aggregation term
novel_class_sim = new_test_feats @ class_feats.t() * (1 + alpha0 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))
# fused logits with regularization and similarity term
logits = clip_logits + alpha0 * weighted_class_sim + alpha1 * cache_logits + alpha2 * weighted_logits
+ alpha0 * similarity_term + alpha1 * novel_class_sim
return logits
The new algorithm selects
feature
channels
by
maximizing
class-specific
discriminative
power,
minimizing
cross-class
feature
overlap,
incorporating
feature
variance
across
samples,
and
introducing
a
class-
specific
feature
interaction
score
based
on
pairwise
feature correlations.
The
new
algorithm
computes
classification
logits
by
combining
zero-
shot
logits,
a
feature-
channel
importance-
weighted
class-specific
feature
aggregation,
a
feature-channel importance-
weighted cache model, and
a
feature-channel
importance-weighted
regularization term, with an
additional
feature-channel
importance-weighted
similarity term to enhance
discriminative power, while
introducing a novel feature-
channel
importance-
weighted
class-specific
feature aggregation term.
Figure 14: Visualization of searching process and results (with APE based initialization). Top row: feature selection (first stage),
bottom row: logits computation (second stage). From left to right: population at each iteration, thoughts, code.


--- Page 19 ---
EvoVLMA: Evolutionary Vision-Language Model Adaptation
MM â€™25, October 27â€“31, 2025, Dublin, Ireland
import torch
def feat_selection(clip_weights, train_feats, w0, w1, topk):
cate_num, samp_num, feat_dim = train_feats.shape
# Compute mutual information between visual and textual features
visual_mean = train_feats.mean(dim=1) # (c, d)
mi = torch.sum(visual_mean * clip_weights, dim=0) # (d,)
# Compute discriminative power of visual features across classes
inter_class_div = torch.var(visual_mean, dim=0) # (d,)
# Compute sparsity of the selected channels
sparsity = torch.sum(torch.abs(visual_mean), dim=0) # (d,)
# Compute correlation between visual and textual features
correlation = torch.sum(visual_mean * clip_weights, dim=0) # (d,)
# Compute variance of category textual features
text_var = torch.var(clip_weights, dim=0) # (d,)
# Compute entropy of visual features
visual_prob = torch.softmax(visual_mean, dim=0)
entropy = -torch.sum(visual_prob * torch.log(visual_prob + 1e-10), dim=0) # (d,)
# Combine criteria
criterion = w0 * mi + (1 - w0) * inter_class_div - w1 * sparsity + (1 - w1) * text_var + correlation + entropy
# Select topk features
_, indices = torch.topk(criterion, k=topk)
return indices
import torch
def compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):
num_classes, k, d = train_feats.shape
train_feats = train_feats.view(-1, d)
train_labels = train_labels.view(-1)
# Normalize train and test features using only important channels
train_feats_norm = train_feats[:, indices] / torch.norm(train_feats[:, indices], dim=1, keepdim=True)
test_feats_norm = test_feats[:, indices] / torch.norm(test_feats[:, indices], dim=1, keepdim=True)
# Compute per-class mean features using only important channels
mus = torch.cat([train_feats_norm[train_labels == i].mean(dim=0, keepdim=True) for i in range(num_classes)])
# Estimate inverted covariance matrix using only important channels
center_vecs = torch.cat([train_feats_norm[train_labels == i] - mus[i].unsqueeze(0) for i in range(num_classes)])
cov_inv = center_vecs.shape[1] * torch.linalg.pinv((center_vecs.shape[0] - 1) * center_vecs.T.cov()
+ center_vecs.T.cov().trace() * torch.eye(center_vecs.shape[1]).cuda())
# Compute GDA logits using only important channels
ps = torch.ones(num_classes).cuda() * 1. / num_classes
W = mus @ cov_inv
b = ps.log() - (W * mus).sum(dim=1) / 2
gda_logits = (test_feats_norm @ W.t() + b)
# Compute CLIP logits
clip_logits = 100 * test_feats @ clip_weights.t()
# Compute feature importance-weighted similarity logits between test features and class prototypes
prototype_logits = torch.cat([(test_feats_norm @ mus[i].unsqueeze(1)).mean(dim=1, keepdim=True)
for i in range(num_classes)], dim=1)
# Compute feature importance-weighted attention logits
channel_weights = torch.zeros(d).cuda()
channel_weights[indices] = alpha2
attention_logits = (test_feats * channel_weights) @ clip_weights.t()
# Compute feature importance-weighted cross-attention logits
cross_attention_logits = torch.cat([(test_feats_norm @ train_feats_norm[train_labels == i].t()).mean(dim=1, keepdim=True)
for i in range(num_classes)], dim=1)
# Compute feature importance-weighted similarity logits between test features and train features
sim_logits = torch.cat([(test_feats_norm @ train_feats_norm[train_labels == i].t()).mean(dim=1, keepdim=True)
for i in range(num_classes)], dim=1) * alpha1
# Combine all logits with hyper-parameters
logits = clip_logits + alpha0 * gda_logits + alpha1 * prototype_logits + alpha2 * attention_logits
+ cross_attention_logits + sim_logits
return logits
The new algorithm selects
feature
channels
by
maximizing
a
weighted
combination of the mutual
information between visual
and
textual
features,
the
discriminative
power
of
visual
features
across
classes, the sparsity of the
selected
channels,
the
correlation
between
visual
and textual features, and the
entropy
of
the
visual
features, while incorporating
the
variance
of
category
textual
features
as
a
weighting factor.
The
new
algorithm
combines CLIP's zero-shot
classifier
logits,
a
feature
importance-weighted
GDA
logits, a feature importance-
weighted
similarity
logits
between test features and
class prototypes, a feature
importance-weighted
attention mechanism, and a
feature importance-weighted
cross-attention
mechanism
between test features and
train features, with hyper-
parameters
controlling
the
contributions
of
each
component,
while
introducing
a
feature
importance-weighted
normalization step for the
train and test features, and
additionally
incorporates
a
feature importance-weighted
similarity logits between test
features and train features.
Figure 15: Visualization of searching process and results (with GDA based initialization). Top row: feature selection (first stage),
bottom row: logits computation (second stage). From left to right: population at each iteration, thoughts, code.
