--- Page 1 ---
Preprint. Under review.
Throughput-Optimal Scheduling Algorithms for LLM Infer-
ence and AI Agents
Yueying Li, J.G. Dai, & Tianyi Peng ∗
Department of Computer Science, ORIE and Decision, Risk and Operations, CBS
Cornell University and Columbia University
{yl3469,jd694}@cornell.edu, tianyi.peng@columbia.edu
Abstract
As demand for Large Language Models (LLMs) and AI agents rapidly
grows, optimizing systems for efficient LLM inference becomes critical.
While significant efforts have targeted system-level engineering, little is
explored through a mathematical modeling and queueing perspective.
In this paper, we aim to develop the queueing fundamentals for LLM infer-
ence, bridging the gap between queueing and LLM system communities.
In particular, we study the throughput aspect of LLM inference systems.
We prove that a large class of ’work-conserving’ scheduling algorithms can
achieve maximum throughput for both individual requests and AI-agent
workloads, highlighting ’work-conserving’ as a key design principle in prac-
tice. Evaluations of real-world systems show that Orca and Sarathi-serve
are throughput-optimal, reassuring practitioners, while FasterTransformer
and vanilla vLLM are not maximally stable and should be used with cau-
tion. Our results highlight the substantial benefits queueing community
can offer in analyzing and improving LLM inference systems and call for
more interdisciplinary developments.
1
Introduction
Large Language Models (LLMs) have become the backbone of many AI-driven applications,
requiring efficient inference serving engines to meet latency and throughput requirements.
LLM inference or serving servers process each user request in two phases: prefill (prompt
processing) phase followed by decode (token generation) phase, each with distinct com-
putational and memory bandwidth characteristics. Since LLM engines generate tokens
autoregressively1, processing a single request requires running the model multiple times,
with each iteration generating one output token. To optimize GPU utilization, it is impor-
tant to batch processing multiple requests during their decode phase. This paper studies
performance guarantees for various scheduling algorithms in forming batches.
To maximize throughput while meeting latency requirements, several inference systems
with different scheduling algorithms have been developed. FasterTransformer (NVIDIA,
2024) was introduced to perform batching at the request level, thereby increasing throughput
during the decoding stage. FasterTransformer gives priority to decode phase. When there
are not enough requests to be batched in decode phase, the throughtput in these iterations
suffers.
Subsequently, Orca (Yu et al., 2022) and vLLM (Kwon et al., 2023) were developed to en-
able batching at the token level, with prioritization of prefill tokens for newly arriving
requests—allowing these requests to enter the decoding stage sooner. Compared to Faster-
Transformer, Orca and vLLM are introduced to reduce latency, although they differ in
whether mixed batching (i.e., batching prefill and decode tokens together) is supported.
∗equal contribution
1A token in an autoregressive transformer is the basic input or output unit—typically a word,
subword, or character—that the model sequentially predicts based on previously generated tokens.
1
arXiv:2504.07347v2  [stat.ML]  24 Apr 2025


--- Page 2 ---
Preprint. Under review.
More recently, the Sarathi-Serve (Agrawal et al., 2023) introduces chunk-prefill to limit the
number of tokens in a single batch and the length of prefill tokens, which prevents requests
with long prefills from blocking decoding. In practice, we observe that Sarathi-Serve is
gaining popularity, likely due to its competitive empirical performance (Agrawal et al.,
2023).
While significant advances have been made from the system community, the field lacks
a cohesive theoretical framework for evaluating and comparing inference systems with
different scheduling algorithms. Consequently, practitioners are often left to decide whether
to switch from one system to another based on case studies on comprehensive workload
benchmarking (Databricks, 2023), sometimes using profiling tools (Shen et al., 2022; NVIDIA
Corporation, 2023) to understand which algorithm falls short. This is often costly to evaluate
across various models, hardware setups, workload generation patterns, datasets, etc.
Meanwhile, the operations research and queueing theory communities have established
robust theoretical foundations and practical methodologies for scheduling and resource
allocation problems, successfully implementing these across diverse domains including
cloud computing (Mitzenmacher, 2001; Grosof et al., 2023; Tirmazi et al., 2020; Patke et al.,
2024; Ghodsi et al., 2011; Zhang et al., 2023), operating systems and networking (Yu et al.,
2021; Stoica et al., 1998; Iyer et al., 2023; Li et al., 2024), telecommunications (Whitt, 1993;
Maglaris et al., 1988; Canetti & Irani, 1995), manufacturing and healthcare operations (Hu
et al., 2022; Chen et al., 2025; Zychlinski et al., 2023), etc.
Motivated by this gap, we analyze LLM inference systems through queueing theory to
address the fundamental question
What is the fundamental limit of the maximal throughput rate for an LLM inference
system, and what classes of scheduling algorithms can achieve this limit?
We develop a novel queueing model of the LLM engine, identify a broad class of work-
conserving scheduling algorithms that are proved to be throughput optimal in our queueing
model, which is based on realistic system profiling and performance modeling. Our analysis
confirms that scheduling algorithms like those in Sarathi-Serve and Orca are throughput
optimal, thereby offering practical assurance to system designers. In contrast, non-mixed
batching algorithms—such as those in FasterTransformer and vanilla vLLM—are inherently
suboptimal in throughput and can become unstable under moderate load; see Figure 1.
These findings contribute to a deeper theoretical foundation for LLM system design: to
optimize latency performance, one should optimize among throughput-optimal scheduling
algorithms.
Our work supports a vision stated recently in Mitzenmacher & Shahout (2025),
“LLM systems present a wealth of new queueing and scheduling chal-
lenges that expand on problems from traditional queueing systems. While
modeling LLM systems remains largely unexplored in the queueing com-
munity, we believe queueing theory insights may lead to better scheduling
systems.”
Specifically, our work extends batch queueing theory by addressing LLM-specific chal-
lenges: dual-phase processing with distinct resource profiles, dynamic batch formation,
and request interdependencies. We further analyze AI-agent workloads, in which a net-
work of LLM instances collaboratively processes agent-level tasks. As AI agents become
increasingly prevalent and embedded in human workflows, understanding the perfor-
mance characteristics of such distributed inference systems becomes critical. Our analysis
identifies several regimes in which optimal throughput can still be achieved and character-
izes the corresponding scheduling strategies. However, we also uncover scenarios where
work-conserving policies fail to attain optimality—an outcome that may be unintuitive to
practitioners—thereby revealing the added complexity and subtle trade-offs involved in
scheduling for multi-agent LLM systems.
Our contributions include:
2


--- Page 3 ---
Preprint. Under review.
0
200
400
600
800
Time (s)
0
2000
4000
6000
8000
Requests in System
faster_transformer
sarathi (token budget 512)
vllm
Figure 1: Experimental results demonstrating the instability of FasterTransformer and vLLM.
The setup is using a CodeLlama-34B with A100, requests are homogeneous with an average
of 129 prefill and 112 decode tokens, arriving with 14.3 queries per second.
• A formal queueing-theoretic framework for LLM inference scheduling that captures
the unique characteristics of the prefill and decode phases, explicitly modeling the
batch processing time. This modeling effort lowers the barrier for the queueing theory
communities to contribute to improving LLM serving systems.
• A rigorous theoretical analysis establishing that work-conserving scheduling algo-
rithms achieve maximum throughput in single-instance LLM inference, highlighting
this as a core design principle for practitioners.
• An examination of widely used scheduling algorithms reveals that Orca and Sarathi-
Serve are throughput-optimal, while FasterTransformer and non-mixed batching in
vanilla vLLM can become unstable under moderate load. These findings support
the adoption of Sarathi-Serve–type algorithms and offer practical guidance for
system selection.
• An extended analysis on throughput-optimality for AI-agent workloads. Our results
reveal key challenges in optimizing throughput in such systems and highlight the
need for deeper theoretical and practical investigation in this emerging domain.
Our work emphasizes the practical implications of scheduling algorithm design through
both theoretical analysis and empirical validation on real-world workloads. Figure 1 illus-
trates how different scheduling algorithms (FasterTransformer, vanilla vLLM, and Sarathi)
handle request dynamics differently, leading to queue blowing up in real system.
We hope this research contributes toward building a foundation for more efficient scheduling
algorithm design in LLM serving systems and encourages cross-disciplinary collaboration
between the queueing theory and machine learning systems communities to address the
growing demand for large-scale LLM inference.
1.1
Background
LLM generative inference. When processing an input request, LLM inference consists of
two phases: prefill and decoding. The prefill phase processes the entire request to compute
the KV cache (Key-Value cache, which stores the attention keys and values for each layer to
avoid redundant computation) and generates the initial response token in a single step. The
decoding phase then utilizes the prior contexts or KV cache to generate subsequent tokens
one at a time. These phases have distinct resource utilization patterns: prefill is compute-
bound, while decoding is memory I/O-bound. Conventional LLM inference engines place
both phases on the same GPU group despite their different computational characteristics
to maximize resource utilization. While we acknowledge that some recent works propose
to disaggregate prefill and decode stages on separate GPUs (Patel et al., 2023; Zhong et al.,
2024), the modeling of the p/d-disaggregation is considered analogous to our work.
Inference and serving goal. Two critical metrics evaluate LLM serving performance:
throughput and inference latency. Throughput measures the token generation rate within
a given timeframe. Inference latency represents the time required to complete a request
3


--- Page 4 ---
Preprint. Under review.
from start to finish. There are various latency metrics, such as TTFT (time to first token,
measuring prefill latency) and TBT (time between token, measuring decoding latency).
We evaluate latency performance using service level objective (SLO) attainment, which
measures the percentage (e.g., 99%) of requests fulfilled within a predefined timeframe. For
online serving, the objective is to optimize for throughput under latency SLO constraints.
For offline inference or batch inference, the objective is to optimize batch throughput. In a
production cloud system, we observe that from a major cloud service provider that batch
or offline inference dominates a major capacity in all workloads, making optimizing for
throughput a first order concern.
Inference optimization. To address the challenges of meeting performance requirements
while minimizing resource wastes, past work leverage GPU hybrid parallelization strategies,
combining data, tensor, and pipeline parallelism to optimize computation across GPUs.
Tensor model parallelism (TP) (Narayanan et al., 2021) distributes computations across
GPUs by partitioning transformer weight matrices row-wise and column-wise, with layer
outputs aggregated through two AllReduce operations. Pipeline parallelism (PP) (Huang
et al., 2019) segments the model into multiple stages assigned to specific GPUs or GPU
groups, with inter-layer activations communicated between stages. Furthermore, advanced
memory management techniques, such as paged attention (Kwon et al., 2023), help alleviate
memory pressure and improve resource utilization. Scheduler optimizations, including
dynamic batching and chunked prefill processing (Agrawal et al., 2023), and disaggregating
prompting and decoding phases (Zhong et al., 2024; Patel et al., 2024), ensure high through-
put while meeting latency constraints, enabling these systems to efficiently handle diverse
LLM inference workloads.
Batching. The computational differences between prefill and decoding phases result in
varying performance outcomes when applying batching strategies. The state-of-the-art
LLM serving systems currently leverage an optimization technique known as continuous
batching (Yu et al., 2022), which combines new request prefill operations with ongoing
request decoding operations in the same batch to improve GPU utilization, making batching
decisions at the token level. However, this approach creates significant interference between
the prefill and decoding phases. When even a single prefill job is introduced into a batch
of decoding tokens, it substantially degrades performance for both operations, with the
performance penalty becoming more severe as the length of the prefill input increases. There
are some works from queueing theory community that investigate the scheduling policy
with ML-guided prediction of token length (Mitzenmacher & Shahout, 2025). Our work is
complementary and aims to develop a foundation for modeling and scheduling algorithm
for LLM and LLM agents.
Queueing theory for batch services. Our work extends the rich tradition of batch queueing
theory in several directions. While prior research has explored stochastic service capacity
(Chang & Choi, 2005; Janssen & van Leeuwaarden, 2005) and multi-class batch service
(Baetens et al., 2018; Reddy et al., 1993), LLM inference presents unique challenges and
opportunities: the dual-phase nature of processing with distinct resource profiles, dynamic
batch formation across operation types, and interdependencies between request types.
Recent work such as (Ao et al., 2025) focuses on new policies to improve TTFT and latency
under heavy-traffic and memory constraints, which assumes knowledge about future
decoding token lengths, and is complementary to our work. By developing a queueing-
theoretic framework specifically for LLM inference, we bridge theoretical concepts with
practical system design. Our findings on work-conserving policies not only validate certain
industry practices but also reveal fundamental limitations in others. Furthermore, our
analysis of AI agent workflows extends batch queueing theory into the emerging domain of
distributed, collaborative AI systems where traditional assumptions may no longer hold.
2
A stochastic processing model for LLM inference
In this section, we introduce a stochastic processing model that we believe is stingy enough
while capturing the realistic scheduling and batching problems for LLM inference. In partic-
ular, our model is anchored in Sarathi-serve, a state-of-the-art LLM serving system widely
4


--- Page 5 ---
Preprint. Under review.
deployed (Agrawal et al., 2023). Yet, it is designed to accommodate general scheduling
algorithms, including existing implementations such as those in FasterTransformer, vLLM,
and Orca (NVIDIA, 2024; Kwon et al., 2023; Yu et al., 2022).
A key insight in our modeling is the formulation of batch processing time: we find empirically
that the processing time of a batch depends primarily on the total number of tokens, rather
than the specific contents of the batch. Moreover, the relationship is piecewise linear. This
observation significantly simplifies the analysis and enables us to derive actionable insights
for designing optimal-throughput scheduling algorithms in practice.
2.1
Model Setup
Arrival. In our model, time is assumed to be discrete, indexed by n ∈N ≡{1, 2, . . . , }. For
time slot n, we use an to denote the number of requests arriving at the system in the time
slot: multiple requests arriving at the same time slot are allowed. We envision all requests
are ordered. For i = 1, 2, . . . , request i is assumed to have vp(i) ∈N (ordered) prefill-tokens,
vd(i) ∈N (ordered) decode-tokens, and feature z(i) ∈K where K is a finite set. Here
the feature z(i) can capture the types/contents of a request. (vp(i), vd(i), z(i)) is used to
describe the system while they may not be observed for the scheduling algorithm (e.g., vd(i)
is often not observed). We assume that
{an : n ∈N}
and
{(vp(i), vd(i), z(i)), i ∈N}
(2.1)
are two iid-sequences, and these two sequences are independent, while (vp(i), vd(i), z(i)) is
allowed to be correlated. We assume each quantity in (2.1) has the first moment
λ = E(a1),
mp = E(vp(1)),
and
md = E(vd(1)).
(2.2)
Physically, time slots can correspond to the clock frequency unit or other time units that
capture the precision of decision-making.2
Iteration-level batch serving. We assume the system has one LLM engine that processes
both the prefill and decode tokens. The scheduler of LLM engines can select a batch of
tokens from multiple requests to process simultaneously for the benefits of computational
efficiency. In particular, a certain number of unprocessed tokens from each request can be
batched together with the following feasible constraints: (defined mathematically in Eq.(2.5))
(a) All prefill tokens in a request must be processed before the first decode token can
be loaded for processing. Within both the prefill and decode phases of a request, a
token can be loaded into a batch only after the previous token has been processed
or has also been included in the batch.
(b) No two decode-tokens from the same request can be loaded into a single batch.
Request 1
Request 2
Request 3
Iteration 1
Batch  
Processing
Prefill
Decode
Iteration 2
Iteration 3
exit
token budget=6, token load=6
token load=4 
token load=2 
Processed
Processing
Figure 2: Visualization of key scheduling terminologies in LLM engine. In this example,
before Iteration 1, Request 1 has progressed to the decoding stage, while Request 2 and
Request 3 have just arrived at the serving engine.
2The discrete-time model is introduced for simplicity. Extending it to continuous time is straight-
forward.
5


--- Page 6 ---
Preprint. Under review.
We refer to the total number of loaded tokens b′ in a batch as token load, and we require b′ ≤b,
a predefined token budget. Completing one batch of processing is known as an iteration.
The terms iteration, batch, and token budget follow the same definitions as in Sarathi-serve
(Agrawal et al., 2023)3. In particular, the authors introduced token budget to ensure iteration
times will not have a huge variation from long input prompts to negatively affect TBT of
requests in the decode phase.
A scheduling algorithm can select b′ and determine the token constituents of a batch at each
iteration. In fact, most existing LLM serving systems fit within this model. For example,
Sarathi-Serve prioritizes decoding requests when loading a batch while allowing prefill
tokens to be appended in a first-come, first-served manner up to the token budget b (b = 256
or b = 512 is often selected). An example of such a scheduling algorithm is shown in Fig. 2.
Other systems (NVIDIA, 2024; Kwon et al., 2023; Yu et al., 2022) differ in their approaches
for the batch constituents, such as whether they prioritize prefill or decoding, whether
preemption is allowed, and whether mixing batches across decoding and prefill tokens is
permitted, among other factors (see Section 3 for more details).
128
256
512
1024
2048
Token Budget
10
20
50
100
200
Batch Processing Time (ms)
Variation in Batch Processing Time
1B
8B
34B
Figure 3: Batch processing time remains relatively constant for a given token budget
(when the LLM is at full token load), and the CoV (coefficient of variation) becomes even
smaller given larger models. The time is measured with SGLang under a high load with
various token budgets (maximum batch size and token numbers) with different request
prefill/decode compositions driven by ShareGPT dataset (Contributors, 2024).
100 200 300 400 500 600 700 800 900 1000
Token load
0
50
100
150
200
250
300
Batch Processing Time (ms)
Piecewise Linear Fit (CodeLlama-34B)
TP-1 data
y = 46.71 + 0.289 ⋅[x −128] +
TP-2 data
y = 23.47 + 0.145 ⋅[x −96] +
TP-4 data
y = 11.76 + 0.068 ⋅[x −64] +
100 200 300 400 500 600 700 800 900 1000
Token load
50
100
150
200
250
300
350
Batch Processing Time (ms)
Piecewise Linear Fit (Llama-70B)
TP-2 data
y = 45.50 + 0.302 ⋅[x −64] +
TP-4 data
y = 24.10 + 0.148 ⋅[x −64] +
Figure 4: Piecewise linear fit for CodeLlama-34B and Llama-70B models for batch processing
time under various token budgets and Tensor-parallel sizes. R2 is above 0.985 for all cases.
Batch processing time. A key gap in the current literature is the modeling of batch pro-
cessing time. While machine learning models exist for processing time prediction (Agrawal
et al., 2024), no closed-form approximations are known. It is important to establish closed-form
or analytical batch time prediction for downstream scheduling analysis, end-to-end differentiability,
etc.
3In Sarathi-serve, the terms chunk size and token budget are used interchangeably. Furthermore, the
number of requests in a batch is referred to as the batch size.
6


--- Page 7 ---
Preprint. Under review.
We observe that the processing time of a batch in fact can be well approximated by a
piecewise linear function that only depends on the token load b′
tb′ = c + a · max(0, b′ −b0).
(2.3)
Equation (2.3) serves as an empirical formula. Fig. 3 illustrates that tb′ exhibits minimal
variation when batch constituents change while keeping the token load b′ fixed. Fig. 4
demonstrates how well a linear fit approximates tb′ as b′ varies. The parameters (c, a, b0)
depend on the LLM model and configuration. For example, in Llama-3-70B running on two
A100 80GB GPUs with NVLink under tensor parallel, we observe c = 45.5 ms, a = 0.30, and
b0 = 64 ms/token.
The strong agreement between equation (2.3) and real processing times is due to the dom-
inance of linear-layer computations (i.e., feed-forward neural layers) in LLM inference
(Kamath et al., 2024; Zhu et al., 2024; Ye et al., 2025). In Section A, we extend this empirical
formula to account for attention layer computation and KV-cache memory overhead, to
make it even more widely applicable.
We assume batching decisions are made at the start of each time slot, with batch processing
spanning multiple slots and completing at the end of a slot. Since batching occurs at the
token level, processing is uninterrupted. When the last decode-token of a request finishes at
the end of a time slot, the request departs the system.
2.2
A Markov chain
With the model setup complete, we now introduce discrete-time Markov chains (DTMCs)
to describe the system’s dynamics. At each time slot n, let Qn denote the set of requests that
have not yet departed. We assume that {(Pi(n), Di(n), Zi(n)) : i ∈Qn} is ordered (e.g., by
request arrival times)4. Here, Pi(n) and Di(n) represent the number of unprocessed tokens
in the prefill and decoding stages, respectively, excluding tokens currently being processed
in a batch.
Let R(n) denote the remaining time of the current batch processing. If R(n) = 0, a new
batch can be formed; otherwise, no decision is required. The system state at the beginning
of slot n is then defined as
X(n) =

R(n), {(Pi(n), Di(n), Zi(n)) : i ∈Qn}

.
(2.4)
Denote X as the set of all possible states X(n) can take for each n ∈N.
Scheduling algorithms. Let x = (r, {(pi, di, zi) : i ∈Q}) ∈X be an arbitrary system state.
Given x with r = 0, a scheduling algorithm selects a batch configuration: π(x) = (δp
i , δd
i )i∈Q,
where δp
i and δd
i denote the number of prefill-tokens and decode-tokens, respectively, from
request i to be included in the batch. For π(x) to be feasible, it must satisfy:
pi > 0 implies δd
i = 0
(2.5a)
δp
i ≤pi
(2.5b)
δd
i ≤1
(2.5c)
∑
i∈Q

δd
i + δp
i

= b′ ≤b.
(2.5d)
Eq. (2.5a) requires prefill tokens to be completed before decoding, while Eq. (2.5b) bounds
batched tokens within unprocessed ones. Eq. (2.5c) enforces sequential decoding, and
Eq. (2.5d) sets the token budget constraint. Furthermore, when r > 0, we enforce b′ = 0
since batch processing is ongoing. However, even when r = 0, an idle time slot (i.e., b′ = 0)
is allowed. Any π satisfying all above conditions is referred to as a scheduling algorithm.
4This assumption is not necessary but allows consideration of scheduling algorithms that utilize
order information.
7


--- Page 8 ---
Preprint. Under review.
System dynamics. When the system is in state X(n) = (R(n), {Pi(n), Di(n), Zi(n), i ∈Qn})
at time slot n, we use X′(n) to denote the post-decision state in time slot n before taking into
account of the new request-arrivals in time slot n. Specially, let π(X(n)) = (δp
i , δd
i )i∈Q, we
have
P′
i (n) = Pi(n) −δp
i , i ∈Qn,
D′
i(n) = Di(n) −δd
i , i ∈Qn,
Q′
n = Qn \ {i ∈Qn : D′
i(n) = 0}.
Then, the state X(n + 1) in time slot n + 1 after taking into account of the new request-
arrivals in time slot n is X(n + 1) = (R(n + 1), {Pi(n + 1), Di(n + 1), Zi(n + 1), i ∈Qn+1}).
Denoting An the set of new request-arrivals in time slot n, we have Qn+1 = Q′
n ∪{An} and
∀i ∈Q′
n :
Pi(n + 1) = P′
i (n),
Di(n + 1) = D′
i(n),
Zi(n + 1) = Zi(n)
∀i ∈An :
Pi(n + 1) = vp(i),
Di(n + 1) = vd(i),
Zi(n + 1) = z′(i)
For the remaining processing time R(n + 1), we have
R(n + 1) =
R(n) −1
R(n) > 0
tb′ −1
R(n) = 0
where tb′ is the number of slots required for processing the new batch b′ := ∑i∈Q(n)(δp
i + δd
i ).
Assume the iid assumption (2.1), under any scheduling algorithm, {Xn : n ∈N} is a DTMC.
2.3
Throughput-optimal algorithms
After formalizing the scheduling problem in Secs. 2.1 and 2.2, we are now in a position
to analyze the system rigorously. The central question we address is: What is the maximal
throughput rate of the system, and which scheduling algorithms can achieve it?
A scheduling algorithm is said to achieve a certain throughput λ if it stabilizes the queueing
system under that arrival rate in (2.2) is λ; in other words, the associated discrete-time
Markov chain (DTMC) {Xn, n ∈N} is irreducible and positive recurrent when the arrival
rate in (2.2) is λ; see Section 5.7 of (Dai & Harrison, 2020) for a discussion of maximally stable
scheduling algorithms. Our main result is that a class of ‘work-conserving’ scheduling
algorithms can (almost) achieve the system’s maximal throughput rate.
Specifically, a scheduling algorithm π is said to be work-conserving if π(x) forms a batch
with b′ = b whenever it is able to. Namely, π(x) satisfies
∑
i∈Q

δd
i + δp
i

= b
(2.6)
whenever ∑i∈Q

pi + 1(pi = 0)

≥b.
Under a work-conserving algorithm, whenever possible, π will not waste the bandwidth: it
will serve up to the token budget. This is achieved by allowing batches that can mix prefill-
and decode-tokens.
In proving the stability results for Markov models of a network of LLM engines in Section 4,
it is convenient to adopt the fluid limit technique that will be introduced in Section C. Antic-
ipating of using the fluid limit technique, we introduce the following family of scheduling
algorithms.
(Kp, Kd)-FCFS algorithms. Let Kp, Kd ≥1 be integers. When loading tokens in phase
f ∈{p, d}, only tokens from (at most) K f oldest requests are loaded, f ∈{p, d}. Among
these K f requests, tokens can be selected in any rule including the smallest-remaining-
token-first. Having an upper limit of K f requests in phase f is realistic because of the
memory constraint in the LLM engine, particularly during the prefill phase. (Our model has
completely ignored the memory constraint.) The exact values of (Kp, Kd) are not important
in our fluid limit analysis.
8


--- Page 9 ---
Preprint. Under review.
We now state the theorem formally. In the theorem, we assume each DTMC is irreducible.
This condition is easily satisfied by assuming P{an = 0} > 0 because the work-conserving
policy will empty the queue eventually if there is no incoming requests.
Theorem 1. Assume the iid assumption (2.1) and the first moment assumption (2.2). Assume
further the second moment assumption E[D2
1] < ∞. (a) Assume the following system load condition
λ(mp + md) < b/tb.
(2.7)
Then the DTMC {Xn, n ∈N} is positive recurrent under any work-conserving (Kp, Kd)-FCFS
algorithm. (b) Assume
λ(mp + md) > b/tb.
(2.8)
Then
P{ lim
n→∞|X(n)| = ∞} = 1,
(2.9)
where |X(n)| = ∑i∈Qn(Pi(n) + Di(n)) is the total of unprocessed tokens at time slot n.
Note that b/tb represents the maximal token processing rate. Therefore, when the system is
overloaded—i.e., when condition (2.8) is satisfied—it is evident that no scheduling algorithm
can stabilize the system. In practice, under such overload conditions, system memory may
become exhausted, leading to requests being blocked or dropped.
While the result in Thm. 1 may appear unsurprising, it highlights two important consid-
erations: (i) In real-world deployments, some systems use scheduling algorithms that are
not ‘work-conserving’, and these can be unstable even when the load condition (2.7) is
satisfied. We will delve into this further in appendix C.4, where the theorem offers practical
guidance on selecting or modifying scheduling policies to ensure stability. (ii) In Sec. 4,
we demonstrate that when LLM instances are connected to a network to handle AI-agent
workloads, even work-conserving algorithms may fail to achieve maximal throughput.
These observations underscore that analyzing queue stability is far from trivial—it has
long been a central focus in the queueing theory community (Dai & Harrison, 2020; Dai,
1995). Thus, the stability guarantees provided by our theorem not only offer reassurance to handle
typical workloads, but also allow practitioners to shift their focus toward optimizing more complex
performance metrics such as latency, or tail latency for prefill and decode phases, and addressing
challenging system scenarios.
Proof Scketch. We will provide two proofs for Part a. One proof uses the fluid limit technique
that has been standard in the literature (Dai, 1995; Stolyar, 1995; Dai & Harrison, 2020). The
fluid limit technique will also be used to prove Part b. The fluid model and fluid limits will
be developed in Appendix B.
The second proof works with DTMC directly under the additional assumption that all
random variables in (2.1) are bounded. For this second proof of Part a, we construct a
Lyapunov function on the system state X(n):
f (X(n)) = ∑
i∈Qn
(Pi(n) + Di(n)) + R(n) · b
tb
.
We will show that this Lyapunov function f exhibits a negative drift whenever X(n) /∈C,
for some finite set C. By the Foster-Lyapunov criterion, this implies the stability of the
system.
A key advantage of this proof technique is that it accommodates a wide class of scheduling al-
gorithms π, including those that make decisions based on flexible prioritization rules—such
as prioritizing requests with the fewest remaining tokens (SJF), or using arbitrary functions
of request features and token lengths. Notably, proving stability for shortest-remaining-
time-style algorithms has historically been challenging. However, due to the structure of
our Lyapunov function and the characteristics of our model, we are able to incorporate such
policies into our analysis. Full details are provided in appendix B.
9


--- Page 10 ---
Preprint. Under review.
Ad, Bd, Cd
Ad,Bd,Cd, Dp
D,E arrives
Dp, Ep
Ad, Bd, Cd, Dd
Ad, Bd, Cd
Dp, Ep,
Ad, Bd, Cd
Ad, Bd, Cd
Ad, Bd, Cd
Bd, Cd
Bd
Dp, Ep
Ad, Bd, Cd
Bd,Cd,Dd, Ep
Bd, Cd, Ed
Bd, Cd, Dd, Ed
Bd
                         Request length: Ad = 2, Bd = 4, Cd = 3, Dd = 1, Ed = 1
Bd
Dd, Ed
Bd, Ed
Orca
FasterTrasnformer
Sarathi-Serve
Ad, Bd, Cd waiting 
Dp, Ep waiting 
Prefill Only
Decode Only
Mixed (Pre Prioritized)
Mixed (De Prioritized)
Batch Composition
vLLM
Figure 5: Example workload and where work-conserving criteria are broken. For vLLM, the
second batch is not work-conserving because of limited prefill, and decoding tokens from
earlier requests are still waiting. For FasterTransformer, the second to fourth batches are not
work-conserving, because the prefills are blocked with earlier decodes.
3
Stability issues of incumbent scheduling algorithms
In this section, we formally describe several scheduling algorithms that have been widely
deployed in the last several years. Both Sarathi-serve and Orca are work-conserving, and,
therefore, are throughput-optimal, guaranteed by Theorem 1. In contrast, FasterTransformer
and vanilla vLLM are not guaranteed to be throughput optimal and should be used with
caution. Indeed, Figure 1 shows that, when system memory is not a constraint, both
FasterTransformer and vanilla vLLM are not able to process all workloads, while Sarathi-
serve can. The latest version of vLLM with chunk-prefill enabled is work-conserving and,
thus, is throughput optimal (vLLM Project, 2025). Our throughput guarantee for Sarathi-
Serve and the latest version of vLLM provides reassurance to practitioners, so that they can
focus on configuration tuning to meet latency requirements.
Decode-prioritized schedule (without mixed batching): An example scheduler is Faster-
Transformer (NVIDIA, 2024). It fills in all the decode tokens from any request i with pi = 0
until batch size limit with a predefined order (e.g. FCFS, SJF, etc.). This policy should form
instability due to the coarse-grained batching.
Prefill-prioritized schedule (without mixed batching): An example scheduler is vLLM
(vanilla) (Kwon et al., 2023). It fills in prefill tokens from a request i with pi > 0 until batch
size limit with a predefined order (e.g. FCFS, SJF).
Prefill-prioritized schedule (with mixed batching): An example scheduler is Orca (Yu et al.,
2022). It fills in i with pi > 0 first. Only after pi = 0 for all i = 1, . . . , q, pack as many decode
tokens as possible till batch size limit or the token budget limit.
Decode-prioritized chunk schedule (Sarathi-Serve): When forming a batch to the pro-
cessed, fill in as many as decode tokens (at most one decode token for each request) as
possible (Agrawal et al., 2023). When there is additional space, fill in as many prompt
tokens as possible (from a minimal number of requests). Note that how decode-tokens and
prompt-tokens are selected is not important for our results below.
Note: Orca and vLLM both utilize FCFS iteration-level batching with eager admission of
prefill requests but differ in how they compose batches. Orca allows hybrid batches that
mix prefill and decode requests, whereas vLLM restricts batches to either entirely prefill
or entirely decode requests. Despite this distinction, both systems enhance throughput by
maximizing batch size until memory capacity bound.
Instability of vanilla vLLM and FasterTransformer:
From the above formulation, it is
evident that Orca and Sarathi both belong to the ‘work-conserving’ class so that they are
throughput-optimal in theory (though the practical performance on latency can still vary).
Figure 1 shows that FasterTransformer and vLLM are unstable even when the load condition
(2.7) is satisfied.
10


--- Page 11 ---
Preprint. Under review.
Load Balancer
LLM Serving
Scheduler
LLM Engine
LLM
KV 
Cache 
Scheduler
LLM Engine
LLM
KV 
Cache 
Scheduler
LLM Engine
LLM
KV 
Cache 
Orchestrator
Agents
     Humans
Tools
Global History
LLM Agent #1
LLM Agent #2
Function Calls
Agentic Program
Environment
API Calls
   Robots
…
     Web
Terminal
Database
LLM Agent #3
Figure 6: AI-agent workload (Luo et al., 2025)
Figure 5 shows the reason why some scheduling policies are not stable or not throughput-
optimal. The instability in vLLM arises because it prioritizes prefill over decode. If incoming
prefill requests are consistently short, the scheduler becomes non-work-conserving by not
batching decodes together, thus unable to reach compute-bound phase due to short prefills,
leading to decode queue buildup.
In contrast, FasterTransformer always prioritizes decode and lacks continuous batching.
When decode requests with token loads below b0 (as defined in Equation 2.3) continue to
generate tokens sequentially, they fail to utilize the GPU’s parallel processing capabilities
efficiently. Meanwhile, incoming prefill requests remain blocked, creating a processing
bottleneck.
4
Generalization to the AI-agent workload
LLMs are increasingly seen as autonomous agents capable of handling complex tasks and
interacting with other agents. In this context, a request may consist of a sequence of LLM
calls, possibly involving interactions across different LLM engines. The execution flow of
such a request can dynamically change based on LLM outputs. We refer to this type of
workload as an AI-agent workload (see Figure 6 for an example). Efficient scheduling and
resource optimization for AI-agent workloads are crucial to maximizing throughput and
minimizing latency in a world where humans and AI agents coexist and collaborate.
In Section 2, we define the basic stochastic processing model of the LLM engine. In this
section, We show that work-conserving scheduling algorithms can still achieve maximal throughput
in several network scenarios. However, inspired by Rybko-Stolyar-type (Rybko & Stolyar, 1992)
examples, we also identify settings where work-conserving algorithms may fail, highlighting key
challenges in designing AI-agent workflows. We hope this modeling effort encourages
operations researchers to contribute to this emerging field.
4.1
Homogeneous, parallel LLM engines
Suppose there are K identical LLM engines. These engines can be configured to work in
parallel. Requests can be routed to any one of these LLM engines through a load balancing
algorithm; see Figure 6. In this case, the load condition is
λ(mp + md) < Kb/tb.
(4.1)
One load-balancing algorithm is to assign a request randomly (uniformly) among K engines.
Another one is to assign an arriving request to the engine that has the least number of
requests. When the information of the numbers of outstanding tokens among all engines is
available to the load balance algorithm, one can use the engine that has the least number of
tokens.
11


--- Page 12 ---
Preprint. Under review.
Proposition 1. Fix a load balancing algorithm and a work-conserving (Kp, Kd)-FCFS
scheduling algorithm that is used in all engines. The DTMC describing the system is
stable (i.e., positive recurrent) under load condition (4.1).
The proof is in Appendix B.5.
4.2
An LLM engine serving two types of requests
Suppose type j ∈{1, 2} requests has arrival rate λj and mean token sizes (mj
p, mj
d). Assume
that
∑
j∈{1,2}
λj(mj
p + mj
d) < b/tp.
(4.2)
The fluid model of the basic LLM engine is fully developed in Appendix B. The fluid model
for the current LLM engine serving two types of requests can be developed similarly. We
claim that the fluid model is stable under any (Kp, Kd)-FCFS work-conserving scheduling
algorithm. Here, the Lyapunov function is still the total workload
f (t) = ∑
j

mj
pZj
p(t) + (mj
p + mj
d)Zj
d(t)

.
Under work-conserving scheduling algorithms, f (t) > 0 implies that
∑
j
˙Bj
p(t) + ˙Bj
d(t) = b/tb.
when t is a regular point. Therefore, f (t) > 0 implies ˙f (t) = −δ, where
δ = b/tb −∑
j∈{1,2}
λj(mj
p + mj
d) > 0,
proving f (t) = 0 for t ≥f (0)/δ. Therefore, the fluid model is stable as defined in Section
B.2. Following the development in Section B.4, the DTMC describing the system is positive
recurrent under load condition (4.2).
4.3
Two networks of LLM engines
In this section, we introduce two networks of LLM engines that serve AI-agent workload: a
fork-join network and a Rybko-Stolyar type network. The latter is adapted from a queueing
network in Rybko & Stolyar (1992) to a network of LLM engines.
A fork-join network.
Suppose that there are a network of four LLM engines, now called
four agents indexed by j ∈{0, 1, 2, 3}. Requests arrive at agent 0 with rate λ. Upon leaving
agent 0, a request forks into two new sub-requests (tasks). Task 1 will be processed by
agent 1 and task 2 will be processed by agent 2. Upon both tasks from the same request are
completed, the merged two sub-requests arrives at agent 3. Assume token sizes at agent j is
(mj
p, mj
d).
Conjecture 1. Assume
λ(mj
p + mj
d) < bj/tj
b
for each j ∈{0, 1, 2, 3}.
(4.3)
Under any (Kj
p, Kj
d)-FCFS work-conserving scheduling algorithm at agent j for j ∈{0, 1, 2, 3}, the
DTMC describing the system is positive recurrent.
If we remove agent 3 is the request workflow, which can be “justified” when agent 3 is
lightly loaded, it is easy to see the conjecture is true for the resulting 2-agent system. Indeed,
the result for the 2-agent system can easily be proved using the fluid limit technique in
Appendix B.
12


--- Page 13 ---
Preprint. Under review.
Type A (s,s)
(l,s)
(l,s)
Type B (s,s)
Agent 1
Agent 2
Figure 7: Illustration of the structure of RS LLM network of agents in the instable example.
(s,s) denotes short prefill and short decode, (l,s) denotes long prefill and short decode.
Rybko-Stolyar (RS) type network.
Assume there is a network of two LLM engines indexed
by j ∈{1, 2}. These two engines are called ”agents” in Figure 7. A type A request has
two sub-tasks, first a short task by agent 1, followed by a long task by agent 2. After
agent 2 completes the second sub-task, the type A request leaves the system. A type B
request has two sub-tasks, first a short task by agent 2, followed by a long task by agent
1. After agent 1 completes the second sub-task, the type B request leaves the system.
Figure 7 shows the logical overview of the two types. Such workflows are increasingly
plausible as multi-agent systems gain traction in real-world applications. For instance, in
an e-commerce setting, Agent 1 handles product and knowledge retrieval—covering query
parsing, decomposition, RAG, and verification—while Agent 2 focuses on emotional tone,
compliance, and empathetic communication. A type A request, like a product search, flows
from Agent 1 to Agent 2: Agent 1 retrieves the information, and Agent 2 delivers it in a
customer-friendly way. A type B request, such as a customer complaint, flows in the reverse
direction: Agent 2 crafts an empathetic response, then directs Agent 1 to retrieve relevant
documents or policies.
102
103
104
105
Time
100
101
102
103
104
105
Number
Agent 1 Requests
Agent 2 Requests
102
103
104
105
Time
100
101
102
103
104
105
106
107
Number
Agent 1 Tokens
Agent 2 Tokens
96500
97000
97500
98000
98500
99000
99500 100000
Time
100
101
102
Number
Agent 1 Requests
Agent 2 Requests
96500
97000
97500
98000
98500
99000
99500 100000
Time
100
101
102
103
104
Number
Agent 1 Tokens
Agent 2 Tokens
Figure 8: [Up] Unstable scheduling policy’s total number of request (left) and token (right)
in two agents; [Down] Stable scheduling policy’s total number of request (left) and token
(right) in two agents at ρ = 0.9.
Requests arrive according to a Poisson process, with type A and type B requests occurring
at equal rates. The system operates under a total load of ρ = 0.9. Each agent has a token
budget of b = 768, and each batch—whether full or partial—requires exactly one time slot
to process. Assume that the token sizes have Poisson distribution with mean
(m(A,1)
p
, m(A,1)
d
) = (32, 32)
and
(m(B,1)
p
, m(B,1)
d
) = (512, 32)
(4.4)
for agent 1,
(m(A,2)
p
, m(A,2)
d
) = (512, 32)
and (m(B,2)
p
, m(B,2)
d
) = (32, 32)
(4.5)
13


--- Page 14 ---
Preprint. Under review.
for agent 2. All agents use work-conserving (mixed batch) scheduling algorithms. For
concreteness, agent 1 gives non-preemptive priority to type B sub-tasks, Agent 2 gives
non-preemptive priority to type A sub-tasks. Within a type, uses decode-priority FCFS
algorithm. For agent 1, type A and B tokens can be mixed only when type B tokens cannot
form a full batch. Similarly, agent 2 follows a similar rule.
Although ρ < 1, suggesting that a work-conserving algorithm should eventually stabilize
the system, our analysis shows that this is not necessarily the case.
In particular, in Figure 8 [Up], we show that under the parameters b = 768, tb = 1, the
token sizes i.i.d. with means in (4.4) and (4.5), the total number of tokens in this system
grows without bound to infinity as time goes to ∞. It illustrates that not all work-conserving
scheduling policies are stable for networks of LLM agents.
On the other hand, suppose one reverses the priority of two types at agents 1 and 2 (Figure 8
[Down]). One can prove that the corresponding Markov chain is positive recurrent using
the fluid limit approach in Appendix B. As a consequence, the total number of tokens will
not grow without bound. This example highlights the importance of designing throughput-
optimal algorithms. This instability phenomenon was well known in the queueing network
setting; see (Rybko & Stolyar, 1992) or a more recent account in (Dai & Harrison, 2020).
5
Latency Optimization
40
100
6 × 101
2 × 102
E2E (ms)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
CCDF
94.23
113.37
75.97
115.01
101.30
178.65
20
100
Prefill time (ms)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
CCDF
sarathi (token budget 1024)
sarathi (token budget 512)
sarathi (token budget 128)
Figure 9: Trade-off between TTFT (Right) and end-to-end latency (Left) with different token
budget of the Sarathi scheduling policy, running on CodeLlama-34B with high arrival rate
scaled with real production conversation traces.
We have developed a class of algorithms that achieve optimal throughput. However, in
practice, the algorithm with the lowest latency can vary depending on several factors.
While a comprehensive study on latency optimization is left for future work, we present
preliminary analyses exploring how the token budget in Sarathi-Serve affects latency
performance. These results reveal both challenges and opportunities for further research.
All experiments were conducted using the CodeLlama-34B model. As shown in Fig. 9,
both end-to-end (E2E) latency and prefill time are influenced by the choice of token budget
in Sarathi-Serve. We find that a moderate token budget (e.g., 512 tokens) yields lower
median E2E latency, while a larger token budget (e.g., 1024 tokens) improves prefill latency.
However, when the budget is too small (e.g., 128 tokens), overhead increases significantly
due to repeated cross-attention across iterations, as indicated by the red line.
The key takeaway is that balancing the token budget is essential: larger budgets benefit
prefill latency, moderate budgets improve E2E latency, and excessively small budgets lead
to significant overhead. In designing scheduling algorithms, it is important to consider
not only the token budget but also the distribution of prefill lengths and the service level
objectives (SLOs) for the target workloads.
6
Future Directions and Conclusion
We have focused on throughput and stability of LLM and LLM agents but haven’t compre-
hensively looked into the optimal scheduling policy under performance measures like tail of
14


--- Page 15 ---
Preprint. Under review.
TBT or TTFT under various load scenarios (Yu & Scully, 2024). The KV cache memory man-
agement for longer context or test-time workloads is also considered future work. Moreover,
future work may examine scheduling policies under multi-tenancy with best-effort and
latency-critical requests collocated together on the same set of agent models or multi-tenancy
models collocated on the same set of physical servers.
Further research into joint optimization techniques that concurrently address model au-
toscaling, resource allocation with multiple models, KV cache policies, and load-balancing
policies may yield substantial improvements in overall system responsiveness and efficiency,
particularly in environments characterized by bursty or skewed workload distributions.
Collaborative efforts between queueing theorists and system practitioners will be essen-
tial to develop models that accurately capture these dynamics and inform the design of
next-generation scheduling algorithms.
References
Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and
Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with
chunked prefills. arXiv preprint arXiv:2308.16369, 2023.
Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav
Gulavani, Ramachandran Ramjee, and Alexey Tumanov. Vidur: A large-scale simulation
framework for llm inference. Proceedings of Machine Learning and Systems, 6:351–366,
2024.
Ruicheng Ao, Gan Luo, David Simchi-Levi, and Xinshang Wang. Optimizing llm inference:
Fluid-guided online scheduling with memory constraints, 2025.
Jens Baetens, Bart Steyaert, Dieter Claeys, and Herwig Bruneel. Delay analysis of a two-
class batch-service queue with class-dependent variable server capacity. Mathematical
Methods of Operations Research, 88(1):37–57, 2018.
Ran Canetti and Sandy Irani. Bounding the power of preemption in randomized scheduling.
In Proceedings of the twenty-seventh annual ACM symposium on Theory of computing,
pp. 606–615, 1995.
Sung-Hee Chang and Doo-Il Choi. Performance analysis of a finite-buffer discrete-time
queue with bulk arrival, bulk service and vacations. Computers & Operations Research,
32(4):795–812, 2005.
Jinsheng Chen, Jing Dong, and Pengyi Shi. Optimal routing under demand surges: The
value of future arrival rates. Operations Research, 73(1):510–542, 2025. URL https:
//pubsonline.informs.org/doi/full/10.1287/opre.2022.0282.
ShareGPT Contributors. Sharegpt: A dataset of multi-turn chat interactions with large
language models. https://sharegpt.com/, 2024. Accessed: 2024-11-18.
J. G. Dai. A fluid limit model criterion for instability of multiclass queueing networks. Ann.
Appl. Probab., 6(3):751–757, 1996. ISSN 1050-5164. doi: 10.1214/aoap/1034968225. URL
https://doi.org/10.1214/aoap/1034968225.
JG Dai. On positive harris recurrence of multiclass queueing networks: a unified approach
via fluid limit models. The Annals of Applied Probability, 5(1):49–77, 1995.
JG Dai and J Michael Harrison. Processing networks: fluid models and stability. Cambridge
University Press, 2020.
Databricks.
Llm
inference
performance
engineering:
Best
prac-
tices,
October
2023.
URL
https://www.databricks.com/blog/
llm-inference-performance-engineering-best-practices.
15


--- Page 16 ---
Preprint. Under review.
Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Scott Shenker, and Ion Sto-
ica. Dominant resource fairness: Fair allocation of multiple resource types. Proceedings of
the 8th USENIX Symposium on Networked Systems Design and Implementation, 2011.
URL https://dl.acm.org/doi/10.5555/1972457.1972470.
Isaac Grosof, Mor Harchol-Balter, and Alan Scheller-Wolf.
New stability results for
multiserver-job models via product-form saturated systems. SIGMETRICS Performance
Evaluation Review, 51(2):6–8, October 2023.
doi: 10.1145/3626570.3626574.
URL
https://dl.acm.org/doi/10.1145/3626570.3626574.
Yue Hu, Carri W. Chan, and Jing Dong. Optimal scheduling of proactive service with
customer deterioration and improvement. Management Science, 68(4):2533–2578, April
2022. doi: 10.1287/mnsc.2021.3992. URL https://pubsonline.informs.org/doi/abs/10.
1287/mnsc.2021.3992.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training
of giant neural networks using pipeline parallelism. Advances in neural information
processing systems, 32, 2019.
Rishabh Iyer, Musa Unal, Marios Kogias, and George Candea. Achieving microsecond-scale
tail latency efficiently with approximate optimal scheduling. In Proceedings of the 29th
Symposium on Operating Systems Principles, pp. 466–481, 2023.
A.J.E.M. Janssen and J.S.H. van Leeuwaarden. Analytic computation schemes for the
discrete-time bulk service queue. Queueing Systems, 50(2):141–163, 2005.
Aditya K Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee,
and Ashish Panwar. Pod-attention: Unlocking full prefill-decode overlap for faster llm
inference. arXiv preprint arXiv:2410.18038, 2024.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large
language model serving with pagedattention. In Proceedings of the 29th Symposium on
Operating Systems Principles, pp. 611–626, 2023.
Yueying Li, Nikita Lazarev, David Koufaty, Tenny Yin, Andy Anderson, Zhiru Zhang,
G Edward Suh, Kostis Kaffes, and Christina Delimitrou. Libpreemptible: Enabling
fast, adaptive, and hardware-assisted user-space scheduling. In 2024 IEEE International
Symposium on High-Performance Computer Architecture (HPCA), pp. 922–936. IEEE,
2024.
Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi
Wang, Yanping Huang, Zhifeng Chen, Joseph E Gonzalez, et al. Autellix: An efficient
serving engine for llm agents as general programs. arXiv preprint arXiv:2502.13965, 2025.
Basil Maglaris, Dimitris Anastassiou, Prodip Sen, Gunnar Karlsson, and John D Robbins.
Performance models of statistical multiplexing in packet video communications. IEEE
transactions on communications, 36(7):834–844, 1988.
Michael Mitzenmacher. The power of two choices in randomized load balancing. IEEE
Transactions on Parallel and Distributed Systems, 12(10):1094–1104, 2001.
Michael Mitzenmacher and Rana Shahout. Queueing, predictions, and llms: Challenges
and open problems. arXiv preprint arXiv:2503.07545, 2025.
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary,
Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catan-
zaro, et al. Efficient large-scale language model training on gpu clusters using megatron-
lm. In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, pp. 1–15, 2021.
NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer, 2024. Ac-
cessed: 2022-02-17.
16


--- Page 17 ---
Preprint. Under review.
NVIDIA Corporation. NVIDIA Nsight Systems, 2023. URL https://developer.nvidia.
com/nsight-systems. Version 2023.5.
Pratyush Patel, Esha Choukse, Chaojie Zhang, ´I˜nigo Goiri, Aashaka Shah, Saeed Maleki,
and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting.
arXiv preprint arXiv:2311.18677, 2023.
Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, ´I˜nigo Goiri, Saeed Maleki,
and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting.
In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture
(ISCA), pp. 118–132. IEEE, 2024.
Archit
Patke,
Dhemath
Reddy,
Saurabh
Jha,
Haoran
Qiu,
Christian
Pinto,
Chandrasekhar
Narayanaswami,
Zbigniew
T.
Kalbarczyk,
and
Ravi
Iyer.
Queue
management
for
slo-oriented
large
language
model
serving.
ACM
Symposium on Cloud Computing,
2024.
URL https://www.semanticscholar.
org/paper/Queue-Management-for-SLO-Oriented-Large-Language-Patke-Reddy/
7c229d35c0befee40be4a3b01cba90deb19c5e9b.
G Venkata Reddy, R Nadarajan, and P Kandasamy. Scheduling in a multi-class single-server
batch-service queueing system. Computers & operations research, 20(2):211–218, 1993.
Aleksandr Nikolaevich Rybko and Alexander L Stolyar. On the ergodicity of stochas-
tic processes describing functioning of open queueing networks. Problemy Peredachi
Informatsii, (3):3–26, 1992.
Li Shen, Jisoo Kim, Xiaodong Zhang, et al. Pytorch profiler: Design and applications.
In Proceedings of the Machine Learning and Systems Conference (MLSys), pp. 1–15.
PyTorch Foundation, 2022. URL https://pytorch.org/docs/stable/profiler.html.
Ion Stoica, Scott Shenker, and Hui Zhang. Core-stateless fair queueing: Achieving approxi-
mately fair bandwidth allocations in high speed networks. In Proceedings of the ACM
SIGCOMM’98 conference on Applications, technologies, architectures, and protocols for
computer communication, pp. 118–130, 1998.
Alexander L Stolyar. On the stability of multiclass queueing networks: a relaxed sufficient
condition via limiting fluid processes. Markov Processes and Related Fields, 1(4):491–512,
1995.
Muhammad Tirmazi, Adam Barker, Nan Deng, Md E. Haque, Zhijing Gene Qin, Steven
Hand, Mor Harchol-Balter, and John Wilkes. Borg: The next generation. In Proceedings
of the Fifteenth European Conference on Computer Systems, EuroSys ’20, pp. 1–14, New
York, NY, USA, April 2020. Association for Computing Machinery. ISBN 978-1-4503-
6882-7. doi: 10.1145/3342195.3387517. URL https://dl.acm.org/doi/10.1145/3342195.
3387517.
vLLM Project. Chunked prefill — optimization and tuning, 2025. URL https://docs.
vllm.ai/en/latest/performance/optimization.html#chunked-prefill. Accessed: 2025-
04-23.
Ward Whitt. Tail probabilities with statistical multiplexing and effective bandwidths in
multi-class queues. Telecommunication Systems, 2:71–107, 1993.
Kuang Xu. Drift method: from stochastic networks to machine learning. URL: https://web.
stanford. edu/˜ kuangxu/papers/driftmethod. pdf. Last visited on, 3(09), 2023.
Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi
Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer:
Efficient and customizable attention engine for llm inference serving, 2025.
George Yu and Ziv Scully. Strongly tail-optimal scheduling in the light-tailed m/g/1.
Proceedings of the ACM on Measurement and Analysis of Computing Systems, 8(2):
1–33, 2024.
17


--- Page 18 ---
Preprint. Under review.
Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.
Orca: A distributed serving system for transformer-based generative models. In 16th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp.
521–538, 2022.
Zhuolong Yu, Jingfeng Wu, Vladimir Braverman, Ion Stoica, and Xin Jin.
Twenty
years after: Hierarchical Core-Stateless fair queueing. In 18th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 21), pp. 29–45. USENIX Associ-
ation, April 2021. ISBN 978-1-939133-21-2. URL https://www.usenix.org/conference/
nsdi21/presentation/yu.
Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. {SHEPHERD}: Serving
{DNNs} in the wild. In 20th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 23), pp. 787–808, 2023.
Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and
Hao Zhang. {DistServe}: Disaggregating prefill and decoding for goodput-optimized
large language model serving. In 18th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 24), pp. 193–210, 2024.
Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu
Xu, Tian Tang, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Stephanie Wang, Arvind
Krishnamurthy, and Baris Kasikci. Nanoflow: Towards optimal large language model
serving throughput, 2024.
Noa Zychlinski, Carri W. Chan, and Jing Dong. Managing queues with different resource
requirements. Operations Research, 71(4):1387–1413, 2023. URL https://pubsonline.
informs.org/doi/10.1287/opre.2022.2284.
A
Discussion
A.1
Long Context Processing
Current inference systems are typically formulated for general-sized prompts. However,
for applications with longer contexts, both the memory backend and prefix caching system
become increasingly critical for system throughput and latency performance. Additionally,
attention mechanisms grow computationally expensive with longer sequences, creating
non-linear relationships between batch processing time and token sizes (Equation 2.3 no
longer holds).
Recall that in Section C, we assumed short context scenarios where batch processing time is
primarily dominated by MLP layers. For long contexts, we must consider a more precise
processing time model for batch n that accounts for the non-negligible attention computa-
tion:
tattn(π, n) = α∑
i
(δp
i )2 + β∑
i

( ¯Di(n −1) + ¯Pi(n −1))δd
i

(A.1)
t = tattn(π, n) + tmlp(b′)
(A.2)
where ¯Pi(n) and ¯Di(n) denote the cumulative number of prompt and decoded tokens for
request i, respectively, up to and including batch n. α, β are model architecture and hardware
dependent constants.
A.2
Memory Subsystem Constraints
To quantify the computational overhead o(xi) associated with limited KV cache capacity,
we introduce a piecewise model that captures two distinct optimization strategies according
to different KV caching/recomputation policies:
18


--- Page 19 ---
Preprint. Under review.
o(xi) =
a1xi + a2
for swapping operations
b1x2
i + b2
for recomputation operations
(A.3)
where xi represents the number of KV tokens exceeding memory limits for request i. The
coefficients (a1, a2, b1, b2) depend on system-specific parameters including CPU-GPU band-
width, chunk size, hidden dimension, and other model configurations.
B
Proof of Theorem 1 Part a
Proof. Assume b −λ(p + d) = ϵ′. Let ϵ = min(ϵ′, Lλ). We construct the following Lyapunov
function
f (X) =
∑
i∈[Q(X)]
 
p′
i + d′
i + ϵ(1(p′
i > 0) + d′
i)2
4Lλ
!
.
Let π(X) = (δp
i , δd
i )i∈Q(X). Note that 1(p′
i > 0) + d′
i will decrease by 1 only when δp
i = p′
i or
δd
i = 1. Thus the decrease of the Lyapunov function is
∆(X) =
∑
i∈[Q(X)]
 
(δp
i + δd
i ) + ϵ(2(d
′
i + 1(p′
i > 0)) −1)
4Lλ
1(δp
i = p′
i or δd
i = 1)
!
Note that for the new arrivals, we have E[pl + dl] = p + d and E[(dl + 1)2] ≤2E[d2
l ] ≤2L.
Hence, the one-step change of the Lyapunov function is bounded by
E[ f (X(n + 1))] −f (X(n)) ≤λ(p + d) + 2ϵLλ
4Lλ −∆(X(n)).
We aim to show that once f (X(n)) ≥B for a large enough B, we have
E[ f (X(n + 1))] −f (X(n)) ≤λ(p + d) + ϵ
2 −∆(X(n)) ≤−ϵ
2.
(B.1)
If so, we can invoke the classical Foster-Lyapunov Criterion (Theorem 4.3.1 in (Xu, 2023)) to
conclude that X(n) is positive recurrent.
To show (B.1) holds, note that if we have ∑i∈[Q(X)] p′
i + ∑i∈[Q(X)] 1(p′
i = 0) ≥b, then
∆(X(n)) ≥
∑
i∈[Q(X)]
(δp
i + δd
i ) = b
by the work-conserving policy. Thus (B.1) holds:
E[ f (X(n + 1))] −f (X(n)) ≤λ(p + d) + ϵ
2 −b ≤ϵ
2 −ϵ′ ≤−ϵ
2.
Now let’s consider ∑i∈[Q(X)] p′
i + ∑i∈[Q(X)] 1(p′
i = 0) < b, and f (X(n)) ≥B > b. Note that
in this setting, every request i in the queue will have either δp
i = p′
i (serving the pre-fill part
until finishing) or δd
i = 1 (serving the decoding part by 1). Otherwise the work-conserving
assumption is violated.
We establish the proof by the following intuition: when f (X(n)) is large enough, there exists
a request with sufficiently large decoding length ˜d. This request will be served and the
decrease of the Lyapunov function ∆(X) will be lower bounded by ϵ(2 ˜d−1)
4Lλ
≥b, which will
be sufficient for showing (B.1).
19


--- Page 20 ---
Preprint. Under review.
In particular, let B := 2b

4Lλ
ϵ b
2
+ b and ˜d := maxi∈[Q(X)] d′
i. We have
f (X(n)) ≥B =⇒
∑
i∈[Q(X)]
 
p′
i + d′
i + ϵ(d′
i + 1)2
4Lλ
!
≥B
(i)
=⇒
∑
i∈[Q(X)]
d′
i + ϵ(d′
i)2
Lλ
≥B −b
(ii)
=⇒
˜d + ϵ ˜d2
Lλ ≥(B −b)
b
(iii)
=⇒2 ˜d2 ≥(B −b)
b
=⇒
˜d ≥4Lλ
ϵ b
where (i) is due to that ∑i∈[Q(X)] p′
i < b and d′
i + 1 ≤2d′
i for d′
i ≥1; (ii) is due to that
∑i∈[Q(X)] p′
i + ∑i∈[Q(X)] 1(p′
i = 0) < b implies Q(X) < b (the number of requests is bounded
by b since each request can be served by at least one token) and the pigeonhole principle;
(iii) is due to that d′
i ≤d′2
i and ϵ ≤Lλ by the definition of ϵ. This provides a lower bound
for ˜d.
Next, note that ˜d also provide a lower bound for ∆(X) because all requests in the queue
satisfy δp
i = p′
i or δd
i = 1:
∆(X(n)) ≥ϵ(2 ˜d −1)
4Lλ
≥ϵ ˜d
4Lλ.
Then when f (X(n)) ≥B, we have
∆(X(n)) ≥ϵ ˜d
4Lλ ≥
ϵ
4Lλ
4Lλ
ϵ b = b.
Thus (B.1) holds and we complete the proof.
C
The fluid model of the LLM engine and Proof of Theorem 1
Recall the DTMC model of the LLM engine introduced in Section 2. It is convenient to
repeat the load condition (2.7) here, namely
λ(mp + mb) < b/tp,
(C.1)
where b is token budget, mp and mb are mean prefill and decode token sizes of a request, tp
is the processing time of a full batch, and λ is the request arrival rate. In this appendix, we
will first introduce the dynamics of the processing model of the LLM engine in Section C.1.
We will next define the fluid model of the LLM engine and prove the fluid model operating
under any work-conserving scheduling algorithm is stable under load condition (C.1) in
Section C.2. In Section C.3, we introduce fluid limits that justify the fluid model equations
in the fluid model. In Section C.4, we give a proof of Theorem 1 in Section 2.
Although Theorem 1 has been proved in Appendix A without invoking fluid limit technique
when random variables in (2.1) are bounded. However, in the setting of AI agents served
by a network of LLM engines, fluid limit technique will be useful, even critical, to study
maximal stability of many scheduling algorithms. Readers are referred to (Dai & Harrison,
2020) and references there for a detailed coverage of fluid limit technique for studying
stochastic processing network stability.
C.1
System dynamics and scheduling algorithms
To study the dynamics of the processing model of the LLM engine, we keep track of the
following quantities. Let Zpn) and Zd(n) be the number of requests in phase p and phase d,
20


--- Page 21 ---
Preprint. Under review.
respectively, at the end of time slot n, after accounting for the new arrivals and the service
completions in time slot n. Let Wp(n) and Wd(n) be the number of prefill tokens in phase p
and decode tokens in phase d at time n, including those in a batch being processed.
Assume a batch is completed at n and is ready to load the next batch to be started at time
n + 1. The LLM engine needs a scheduling algorithm to decide which tokens to go into the
batch.
The following scheduling algorithms can be used to load the next batch. Using newly
introduced notation, we recap the definitions of two families of scheduling algorithms.
Work-conserving scheduling algorithms. When
Wp(n) + Zd(n) ≥b,
(C.2)
the next batch is a full batch.
To describe the dynamics, we further introduce the quantities Bf (n), E(n), Ff (n), and Vf (n).
For each phase f ∈{p, d}, define Bf (n) to be the cumulative number of tokens that have
been completed phase f processing by time n; define Ff (n) to be the cumulative number of
requests that have completed phase f processing by time n; define E(n) = ∑n
ℓ=1 aℓto be the
cumulative number of requests that have arrived by n, and
Vf (N) =
N
∑
i=1
v f (i)
(C.3)
to be the total number of phase f tokens brought in by the first N requests. It follows that
Zp(n) = E(n) −Fp(n) ≥0,
(C.4)
Zd(n) = Fp(n) −Fd(n) ≥0,
(C.5)
Wp(n) = Vp(E(n)) −Bp(n) ≥0,
(C.6)
Wd(n) = Vd(Fp(n)) −Bd(n) ≥0,
(C.7)
Bf
 n2tb
 −Bf
 n1tb
 ≤(n2 −n1)b,
for each f ∈{p, d},
n1, n2 ∈N with n1 < n2.
(C.8)
Under any (Kp, Kd)-FCFS scheduling algorithm,
Vf
 Ff (n) −K f + 1
 −(K f −1)
max
1≤i≤E(n) v f (i)
≤Bf (n) ≤Vf
 Ff (n) + K f

,
f ∈{p, e}.
(C.9)
Relationship (C.9) is identical to the Key Relationship (6.51) in (Dai & Harrison, 2020).,
which plays an important role in defining fluid limits.
C.2
The LLM fluid model & fluid model calculus
In this section, we introduce the fluid model of the LLM engine. The fluid model is defined
through a set of fluid model equations including some inequalities. These equations will
need to be justified through a fluid limit procedure to be explained in Section C.3. Fix any
(Kp, Kd)-FCFS scheduling algorithm. These fluid model equations include: for any time
t ∈R+ ≡[0, ∞),
Zp(t) = Zp(0) + λt −Fp(t),
(C.10)
Zd(t) = Zd(0) + Fp(t) −Fd(t),
(C.11)
Bf (0) = 0,
0 ≤Bf (t) −Bf (s) ≤(t −s)b/tb,
0 ≤s ≤t,
f ∈{p, d}.
(C.12)
Ff (t) =
1
m f
Bf ( f ),
f ∈{p, d},
(C.13)
Wf (t) = m f Zf (t),
f ∈{p, d}.
(C.14)
21


--- Page 22 ---
Preprint. Under review.
Here, we intentionally overload the notational system in Section C.1 to emphasize the
similarity of the fluid analog. Each overloaded function is differentiated by its argument
n ∈N and t ∈R+. Fluid model equation (C.12) says that the function Bf (·) is Lipschitz
continuous with Lipschitz constant b/tb. Fluid model equation (C.13) implies that function
Ff (·) is Lipschitz continuous. Fluid model equations (C.10)-(C.11) and (C.14) imply that
Zf (·) and Wf (·) are Lipschitz continuous as well. In conclusion, the multidimensional
function
 Bf (·), Ff (·), Wf (·), Zf (·), f ∈{p, d}

(C.15)
is Lipschitz continuous. Therefore, from real analysis (c.f. Lemma A.2 of (Dai & Harrison,
2020). and its commentary), this function is absolutely continuous. As a consequence, it is
differential almost surely everywhere, and for any component x(·) in (C.15),
x(t) −x(s) =
Z t
s
˙x(u)du,
s < t,
(C.16)
where ˙x(u) denotes the derivative of the function x(·) at time u. Equation (C.16) is simply
the fundamental theorem of calculus when x(·) is continuously differentiable. In general, the
integral on the right of (C.16) is interpreted as the Lebesgue integral, and it is sufficient that
˙x(u) is well defined almost everywhere. See, for example, Lemma A.3 of (Dai & Harrison,
2020). and the references there.
Definition 1. A point t > 0 is said to be a regular point for a fluid model solution (C.15) if
the solution is differentiable at time t.
When the fluid model solution is clear in a context, we simply say a time t is a regular point
without reference to the fluid model solution. When the (Kp, Fd)-FCFS scheduling algorithm
is also work-conserving, the fluid model equations include: for any regular time t > 0,
Wp(t) + Wd(t) > 0 implies ˙Bp(t) + ˙Bd(t) = b/tb.
(C.17)
Definition 2. A function in (C.15) is said to be a fluid model solution if it satisfies fluid model
equation (C.10)-(C.14), a work-conserving fluid model solution if, in addition, it satisfies (C.17).
Definition 3. The fluid model is said to be stable if there exists a time δ > 0 such that
Z(t) = 0 for t ≥δ for any fluid model solution with |Z(0)| ≡Zp(0) + Zd(0) ≤1.
Definition 4. The fluid model is said to be weakly unstable if there exists a time a > 0 such
that for each fluid model solution starting from 0, Zp(a) + Zd(a) ̸= 0.
Proposition 2. (a) Under any work-serving, (Kp, Kd) FCFS algorithm, the fluid model is
stable under load condition (C.1). (b) Assume
λ(mp + md) > b/tp.
(C.18)
Under any scheduling algorithm, the fluid model is weakly unstable.
Proof. Given any fluid model solution (B(·), F(·), W(·), Z(·)), we use the Lyapunov function
f (t) = mpZp(t) + (mp + md)Zd(t)
(C.19)
as the total workload in the system at time t. One interpretation of the total workload is the
time needed for the LLM engine to clear off all existing “fluid tokens” in the system when
the request arrival is turned off. It follows from fluid model equations (C.10)-(C.14) that
f (t) = f (0) + λ(mp + md)t −
 Bp(t) + Bd(t)

.
Proof of (a). For each regular point t > 0, f (t) > 0 implies that ˙f (t) = λ(mp + md) −b/tp,
which is negative by load condition (C.1). It follows from Lemma 8.5 of (Dai & Harrison,
2020). that f (t) = 0 for t ≥f (0)/δ, where
δ = b/tp −λ(mp + md) > 0.
Proof of (b). Let t > 0 be a regular point. Because
˙Bp(t) + ˙Bd(t) ≤b/tb,
˙f (t) ≥λ(mp + md) −b/tp > 0. Since f (0) is assumed to zero, it follows that
f (t) =
Z t
0
˙f (u)du ≥t(λ(mp + md) −b/tp) > 0.
Thus, the fluid model is weakly unstable by choosing any a > 0.
22


--- Page 23 ---
Preprint. Under review.
C.3
Fluid limit
Fix a work-conserving (Kp, Kp)-FCFS scheduling algorithm. In this section, we define fluid
limits and prove that each fluid limit is a fluid model solution satisfying (C.10)-(C.17). For
each state x in the space X in Section 2.2, we use zp and zd to denote the corresponding
token counts in prefill phase and decode phase, respectively. Define |x| = zp + zd to be the
total number of tokens in state x. Assume that two iid sequences in (2.1) are defined on
some probability space (Ω, F, P). We make a slightly stronger moment assumption on the
token size distribution. There exists an ϵ > 0 such that
E
h
(v f (1))1+ϵi
< ∞
f ∈{p, d}.
(C.20)
Clearly, both light- and heavy-tailed token sizes are allowed. Under condition (C.20),
Proposition B.8 implies the following
P
n
lim
N→∞
1
N max
1≤i≤N
 vp(i) + vd(i)
 = 0
o
= 1.
(C.21)
Recall the definitions in Section C of Bf (n), E(n), Ff (n), Wf (n), and Zf (n) for each n ∈N
and each phase f ∈{p, d}. For each sample path ω ∈Ω, one has realization of two
sequences {an(ω), n ∈N} and {(vp(i, ω), vd(i, ω)), i ∈N}. With these two sequences, the
given initial state x ∈X, and any fixed K-FCFS scheduling algorithm, one can construct
the corresponding realization Bx
f (n, ω), E(n, ω), Fx
f (n, ω), Wx
f (n, ω), and Zx
f (n, ω) for each
n ∈N and each f ∈{p, d}. For each ω ∈Ω, each time t ≥0, and each state x ∈X with
|x| > 0, define fluid scaled quantities
ˆEx(t, ω) = 1
|x| E(|x|t, ω),
ˆBx
f (t, ω) = 1
|x| Bx
f (|x|t, ω),
ˆFx
f (t, ω) = 1
|x| Fx
f (|x|t, ω),
ˆWx
f (t, ω) = 1
|x|Wx
f (|x|t, ω),
ˆZx
f (t, ω) = 1
|x| Zx
f (|x|t, ω),
where, whenever |x|t is not an integer, it is applied the floor operation to make it an integer.
Following the SLLN theorem,
P
n
ω ∈Ω: lim
n→∞
1
n E(n, ω) = λ,
lim
N→∞
1
N Vp(N, ω) = mp,
lim
N→∞
1
N Vd(N, ω) = md
o
= 1.
(C.22)
Denote the set Ω0 ⊂Ωin which the limits in (C.22) and (C.21) exist. Clearly P{Ω0} = 1.
We now define fluid limits for each sample path ω ∈Ω0.
Lemma 1. For each ω ∈Ω0 and each unbounded set A ⊆X, there exists a sequence {xk} ⊂A
with |xk| →∞as k →∞, and a function ( ˆBp(·), ˆBd(·)) such that
lim
k→∞sup
0≤t≤T
|B(xk)
f
(t, ω) −ˆBf (t)| = 0
for each T > 0 and f ∈{p, d}.
(C.23)
The proof follows from the Lipschitz property (C.8) and is identical to the proof of (12.37)
on page 285 of (Dai & Harrison, 2020). The convergence mode in (C.23) is known as the
uniform convergence on compact sets or u.o.c. convergence. Since | ˆZ(xk)(0, ω)| = 1, the
sequence { ˆZ(xk)(0, ω), k ≥1} is in a compact set of R2+. By taking a subsequence if needed,
we will assume
Z(xk)(0, ω) →z = (zp, zd)
(C.24)
for some z ∈R2+ for the sequence of xk in (C.23).
Lemma 2. Under any (Kp, Kd)-FCFS scheduling algorithm, fix a ω ∈Ω0 and a sequence {xk} ⊂
X such that (C.23) and (C.24) hold. For each f ∈{p, d},

ˆFxk
f (·, ω), ˆWxk
f (·, ω), ˆZxk
f (·, ω)

, →

ˆFf (·), ˆWf (·), ˆZf (·)

u.o.c.
as k →∞,
(C.25)
23


--- Page 24 ---
Preprint. Under review.
where
ˆFf (t) =
1
m f
ˆBf (t),
(C.26)
ˆZp(t) = zp + λt −ˆFp(t),
(C.27)
ˆZd(t) = zd + ˆFp(t) −ˆFd(t),
(C.28)
ˆWf (t) = m f ˆZf (t).
(C.29)
Proof. To prove (C.25), it is sufficient to prove the u.o.c. convergence for each component.
For ˆFxk
f (·, ω), we utilize relationship (C.9). The proof is given by Lemma 6.8 of (Dai &
Harrison, 2020). and fluid model equation (C.26) is identical to (6.43) of (Dai & Harrison,
2020). The rest of the proof is easy to complete, following (C.4)-(C.7) and analogous to the
proof of Theorem 12.13 in (Dai & Harrison, 2020).
Definition 5. Functions ( ˆBf (·), ˆFf (·), Wf (·), Zd(·)) is called a fluid limit if there exists ω ∈Ω0
and a sequence of initial states {xn} ⊂X such that |x|k →∞and limits (C.23) and (C.25)
hold.
Lemma 3. Under any (Kp, Kd)-FCFS work-conserving scheduling algorithm, each fluid limit
satisfies fluid model equations (C.10)-(C.17).
Proof. The fluid limit satisfies fluid model equations (C.10)-(C.15) is covered in (C.26)-(C.29).
It suffices to prove that each work-conserving fluid limit satisfies (C.17). The proof of the
latter is analogous to the proof of Theorem 7.2 of (Dai & Harrison, 2020).
C.4
Proof of Theorem 1
Definition 6. Under a fixed scheduling algorithm, the fluid limits are said to be stable if
there exists a constant δ > 0 such that
ˆZf (t) = 0
t ≥δ
(C.30)
for each fluid limit ( ˆB, ˆF, ˆW, ˆZ) defined in Definition 5.
Proposition 3. Fix a (Kp, Kd)-FCFS work-conserving scheduling algorithm. If the fluid
limits are stable, then the corresponding DTMC in Section 2.2 is positive recurrent.
Proof. The proof is analogous to the proof of Theorem 6.2 of (Dai & Harrison, 2020). The
latter theorem is stated in the setting of continuous time Markov chains. Assume the fluid
limits are stable. The Lyapunov function used in Lemma 3.7 of (Dai & Harrison, 2020). in the
continuous setting can be copied verbatim in the current setting; see the proof of Theorem
12.27 in (Dai & Harrison, 2020)., which is for the slotted discrete time model. In the proof of
Theorem 6.2 in (Dai & Harrison, 2020)., one needs to verify that the sequence of random
variables
 E(n)
n
, n ≥1

is uniformly integerable.
(C.31)
In (Dai & Harrison, 2020)., Proposition B.5 is cited to prove (C.31). The proof of Proposition
B.5 there utilized the finiteness of the second moment of an in (2.1). Examining the proof of
Lemma 4.5 of (Dai, 1995), one concludes that Proposition B.5 continues to hold under the
first moment assumption on an as in (2.2).
Proof of Theorem 1. Part (a). By Lemma 3, each fluid limit ( ˆB, ˆF, ˆW, ˆZ) satisfies |Z(0)| = 1
and fluid model equations (C.10)-(C.17). The fluid model has been shown stable under the
load condition (C.1) in Part (a) of Proposition 2. Therefore the fluid limits are stable. By
Proposition 3, the DTMC is positive recurrent.
Part (b). Assume condition (2.8). Part (b) of Proposition 2 shows the the fluid model is
weakly unstable. Lemma 3 shows that each fluid limit is a fluid solution. Therefore, the
proof of Theorem 3.2 of (Dai, 1996) can be adapted to conclude Part (b) of the theorem.
24


--- Page 25 ---
Preprint. Under review.
C.5
A Sketch of the Proof of Proposition 1
Proof. The fluid limit technique for proving Theorem 1 continues to apply here.
Let
(Zk
p(t), Zk
d(t)) be the fluid request level at server k. Let Ak(t) be the cumulative amount of
fluid requested to server k by time t. Define
f k(t) = mpZk
p(t) + (mp + md)Zk
d(t).
Under the random assignment load-balancing algorithm, at each regular point, ˙Ak(t) =
λ/K. Assume f k(t) > 0,
˙f k(t) = (mp + md)λ/K −b/tp < 0.
Thus f k(t) = 0 for t ≥f k(0)/δ and k = 1, . . . , K, where b/tb −(mp + md)λ/K > 0. It
follows that Z(t) = 0 for t > max( f k(0))/δ, proving the fluid model is stable.
Under the join-lowest-request load balancing algorithm, define
f (t) = max
k
f k(t).
Suppose that f ℓ(t) > maxk̸=ℓf k(t). At each regular point of t that is also a differential point
of f,
˙f (t) = ˙f ℓ(t)
Since ˙Aℓ(t) = 0,
˙f ℓ(t) = −b/tp.
Therefore, ˙f (t) = −b/tp.
Suppose that f ℓ1(t) = f ℓ2(t) > maxk̸∈{ℓ1,ℓ2} f k(t). One can also prove ˙f (t) = ˙f ℓ1(t) =
−b/tp.
Suppose that f 1(t) = . . . = f K(t). In this case,
˙f (t) = ˙f k(t) and f k(t) > 0,
k = 1, . . . , K
Thus,
˙f (t) = 1
K ∑
k
˙f k(t) = 1
K ∑
k

˙Ak(t)(mp + md) −b/tp

= 1
K (mp + md)∑
k
˙Ak(t) −b/tp.
Because ∑k ˙Ak(t) = λ, one has ˙f (t) = −δ < 0.
25
