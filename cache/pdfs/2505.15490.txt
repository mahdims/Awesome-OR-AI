--- Page 1 ---
Collaborative Problem-Solving in an Optimization Game
Isidora Jekni´c
Saarland University
Saarbrücken, Germany
jeknic@lst.uni-saarland.de
Alex Duchnowski
Saarland University
Saarbrücken, Germany
aduchnowski@coli.uni-saarland.de
Alexander Koller
Saarland University
Saarbrücken, Germany
koller@coli.uni-saarland.de
Abstract
Dialogue agents that support human users in
solving complex tasks have received much at-
tention recently. Many such tasks are NP-hard
optimization problems that require careful col-
laborative exploration of the solution space. We
introduce a novel dialogue game in which the
agents collaboratively solve a two-player Trav-
eling Salesman problem, along with an agent
that combines LLM prompting with symbolic
mechanisms for state tracking and grounding.
Our best agent solves 45% of games optimally
in self-play. It also demonstrates an ability to
collaborate successfully with human users and
generalize to unfamiliar graphs.
1
Introduction
Humans frequently face the challenge of solving
hard combinatorial problems in their daily lives,
from planning to scheduling to resource allocation,
and they often struggle to solve these problems
well (Hidalgo-Herrero et al., 2013). LLMs are
surprisingly good at solving these problems (Fan
et al., 2024), but only when the human user has
full knowledge of the problem and can spell it out
in detail. One of the great promises of LLMs is
that they can help humans solve such problems
collaboratively (Khan and Hamad, 2024), through
a dialogue in which the human and the system
take turns working out the problem and proposing
increasingly complete and correct solutions.
AI agents for collaborative problem-solving
must have a number of fundamental skills to be ef-
fective. In addition to the ability to solve NP-hard
problems themselves, they also need to perform
conversational grounding (tracking the details of
the problem and which part of the solution have
we agreed on), remember what their partner wants
and knows, and negotiate and revise partial solu-
tions. All of these are established problems in the
dialogue literature, and it is not obvious that LLMs
will solve them easily.
In this paper, we make two contributions to
the development of collaborative agents for solv-
ing complex problems.1
First, we introduce
TRAVELING ADVENTURERS, a game in which two
agents collaboratively solve a Traveling Salesman
problem. This game serves as a testbed for col-
laborative problem-solving, as each agent only
has partial information about the problem initially
and must communicate and collaborate in order
to negotiate an optimal solution. Furthermore, be-
cause it builds on an NP-hard optimization prob-
lem, TRAVELING ADVENTURERS is difficult enough
that humans could not immediately find optimal
solutions even with access to their partner’s knowl-
edge, necessitating the incremental construction of
a solution that we expect in a collaborative problem-
solving dialogue.
Second, we present an artificial agent that per-
forms well on playing TRAVELING ADVENTURERS.
We find that a purely LLM-based baseline strug-
gles with tracking its partner’s knowledge and with
conversational grounding; it is outperformed deci-
sively by a neurosymbolic agent which combines
the LLM with symbolic components for these skills
and an exact symbolic optimizer. The neurosym-
bolic agent solves 98% of its games correctly and
45% optimally in self-play with itself, and still
achieves a high optimality score in 32% of games
with human partners. We conclude with a discus-
sion of conversational skills that would be required
to push the optimality rate even higher.
2
Background
In order to facilitate collaboration, interlocutors
must engage in dialogue so as to ensure a shared set
of intentions (Dafoe et al., 2021). Collaboration in
dialogue is realized through negotiation, i.e., effort
to establish common ground and a joint purpose
1Our code and data are available at:
https://github.com/coli-saar/collaborative-problem-solving.
arXiv:2505.15490v1  [cs.CL]  21 May 2025


--- Page 2 ---
Figure 1: The perspectives of the two players - the left and right figures depict the distinct graphs of each player,
whereas the middle illustrates the shared information, i.e., the chat.
(Clark, 1996). It is often identified through a set
of dialogue acts, defined as “meaning[s] of [utter-
ances] at the level of illocutionary force” (Stolcke
et al., 2000, 2), such as presenting a solution, as
well as accepting or rejecting a proposal (Georgila
and Traum, 2011). Therefore, a collaborative agent
must successfully process and produce such acts.
Task-oriented collaborative dialogue is primarily
studied through collaborative games which fos-
ter negotiation by incentivizing goal-driven con-
versation (Schlangen, 2019). This often includes
simple problem-solving tasks (Zarrieß et al., 2016;
Lewis et al., 2017; Kim et al., 2019). Most of these
environments—even those which are more com-
plex, such as TRAINS (Allen et al., 1995)—are
typically asymmetric, assigning players fixed roles.
This limits the scope of collaboration that can be
studied and prevents organic role-taking, despite
human players preferring a more balanced role dis-
tribution, which often improves performance on
symmetric tasks (Jeknic et al., 2024).
LLMs have shown promising results and further
potential on very complex tasks (for overview, see
Cheng et al. (2024); Mialon et al. (2023); Hartmann
and Koller (2024)). However, previous work has
shown that even the most advanced models (at the
time, GPT-4) severely under-performs humans on
strategic thinking tasks (Zhou et al., 2024). Sim-
ilarly, when dealing with complex tasks, human
input can be beneficial for agent performance and
make up for certain model-specific failures (Lin
et al., 2024). Additionally, most previous work in-
vestigating LLMs and NP-hard problems is focused
on single-agent optimization (Ramamonjison et al.,
2023; Tang et al., 2025).
3
The Game
We present TRAVELING ADVENTURERS, a symmetric
two-player game environment for collecting task-
oriented collaborative dialogue. Players A and B
observe a fully connected undirected graph G: the
set of nodes (V ) and edges (E) are identical for
both players, whereas the weights (w) are unique
(wA, wB) and only known to the respective player.
The goal is to find a single path that visits all nodes
exactly once and returns to the starting node while
maximizing the sum of players’ weights along the
way. The pairs communicate exclusively through
written text and have access to the entire chat his-
tory throughout the game.
Story
The players are told that an evil wizard
imprisoned them in two parallel dimensions of the
same house: the light and ghost dimensions. They
observe the same layout, where each room is con-
nected to all the others via hallways. In order to
break the curse, the players are instructed to visit
each room together and return to the start. More-
over, each player collects their respective type of
coin (the light and ghost coins) in the hallways be-
tween rooms. In the end, the sum of coins collected
by each player are given to them as joint loot.
3.1
Features
The game mechanics are based on a classical NP-
hard optimization problem, the Traveling Salesman
Problem (TSP), which we adjust for two players
and “dress up” (Duchnowski et al., 2025) into the
familiar game setting of collecting coins. In re-
lation to the TSP, the rooms represent cities and
the coins distances (Figure 1), with the coin maxi-
mization analogous to distance minimization. The
curse-breaking ritual follows the rules of the TSP,


--- Page 3 ---
4
1
5
4
6
2
4
4
1
6
5
2
4
1
5
4
6
2
Figure 2: Different perspectives of the same move: the left and middle panels show two players’ individual greedy
optimal move; the right panel shows the joint greedy optimal move, which does not match either player’s individual
optimum.
and the coin reward in the end measures the overall
optimality of the selected path. Because of this
setup, with a high enough number of rooms it is
not immediately clear to humans what an optimal
solution is, which necessitates problem-solving.
The initial setup is symmetric, since both players
are each given the same amount and type of infor-
mation: their own world state is fully known, and
the other player’s world state is partially known.
The agents are not assigned predetermined roles
and are free to individually choose an approach.
Because the same decision might not be optimal
for both players individually, no single player is
able to find an optimal solution alone (Figure 2).
This produces the need for negotiation.
4
Game-Playing Agents
We build a reactive neurosymbolic artificial agent
that plays the TRAVELING
ADVENTURERS game
and is equipped with modules for grounding and
state-tracking, as well as an external optimization
solver’s output. Depending on the setup, the partner
could be another agent (self-play) or a human.
4.1
The Agent Interface and Actions
The agent’s goal is to collaboratively solve the un-
derlying TSP instance by completing a round trip
around the house and collecting as many joint coins
as possible. The agent is presented with a text-
based interface, where it interacts with its partner
through dialogue: it receives a natural language
message as input, and generates a response as out-
put (Figure 3). Each interaction is orchestrated by
a game framework that parses and distributes the
inputs and outputs. When interacting, the agent
can generate as many actions from a given list of
actions as it needs. We design the action space
to cover a wide range of functions: negotiation,
conversation, and symbolic structure updating.
The negotiation actions represent the necessary
dialogue acts for collaboration: suggest, agree
and reject. We make a distinction between an in-
ternal or implicit agreement with one’s own sugges-
tion and another interlocutor’s (outer) agreement
(agree-inner, agree-outer)2. We do not do the
same for rejection since both parties need to agree
on a proposal for it to be considered valid, whereas
a single rejection is sufficient to invalidate it.
The conversation actions are the inform and ask
actions, functioning as formalizations of the state-
ment and question dialogue acts. The produced
output messages are direct instantiations of the ne-
gotiation and conversation actions.
Lastly, a subset of actions can directly affect and
change the agent’s input (symbolic structures up-
dating). These include visit, which indicates that
both the agent and its partner have agreed on a node
to include in the final path, or end, which indicates
that the agent considers itself ready to submit a
final solution and end the game. We will describe
these in more detail later. All actions take an ar-
gument reflecting the relevant elements subjected
to the dialogue acts, for example the suggested
subpath, the question for the partner (ask) or the
final submitted path (end).
4.2
The Baseline
We start with a baseline collaborative agent with an
LLM base which plays TRAVELING ADVENTURERS
through chain-of-thought (CoT) reasoning (Wei
et al., 2022). We present GPT-4o (gpt-4o-2024-08-
06) with a description of the task and environment,
2We instruct the agent to always generate an agree-inner
action when generating a suggest action, as it should always
agree with its own suggestions.


--- Page 4 ---
I can get 4 coins if we go to the kitchen. How about the bathroom
instead, how many coins can you get there? I get 6.
From the living room, my best move would be to go to the kitchen
for 6 coins. What do you think?
The user suggests going to the kitchen first. My best path from the
living room is to the  bathroom (6 coins). I can get 4 coins to the
kitchen.
I do not know how many coins the user can collect from the living
room to the bathroom.
Thought-
DRAFT
Thought-
MISSING
Message
(input)
Message
(output)
Players
Hey, I think starting by going to the kitchen works well for us, too!
From the living room, my best move would be to go to the kitchen
for 6 coins. What do you think?
The user suggests going to the kitchen first, ... My two best paths
from the living room would be: ... The user's path would give us a
total of 9 coins initially, while my highest path could give us
11. However, since the user's path aligns better with their own
distribution, it makes sense to agree to start with their path and
maximize our combined coins from there.
Figure 3: An agent’s interface showing an example reasoning step (left) and an erroneous reasoning step (right).
The highlighted text shows an error in reasoning, demonstrating a lack of task understanding.
Baseline
Grounding
State-
tracking
Problem-
solving
Agent WSR
✓
✓
✓
✓
Partner WSR
✓
✓
✓
Action History
✓
✓
Visited
✓
✓
Remaining
✓
✓
IBP
✓
[["L", "E", 6],
 ["L", "B", 4],
 ["L", "K", 2], ... ]
Agent WSR
["L", "B", "K"]
Visited
["L", "B", "K", "A", "E",
"C", "L"]
Intermediate Best Path
[suggest(["L", "C"]), 
 agree-inner(["L", "C"]),
 ask("coin distribution
      between L and C")]
Action History
[["L", "B", 5],
 ["B", "K", 3],
 ["B", "A", 6], ... ]
Partner WSR
["L", "E", "A", "C"]
Remaining
Figure 4: The table on the left shows the input structure for each version of the agent, with the rows representing
the components and the columns showing agent versions. The figure on the right shows a real example of each
component, where the letters correspond to graph nodes, i.e., rooms, while the numbers show the weights of the
edges between them, i.e., coins.
including the expected input and output structures,
followed by a complete example dialogue between
two players solving the task. We use a modified
ReACT prompting strategy (Yao et al., 2023) and
generate text completions, all consisting of a rea-
soning step, list of actions, and response message.
The reasoning step (Figure 3) has the agent gen-
erating a Thought-DRAFT, in which it attempts
to generate a solution with the information that
it has received on input. If the agent deems its
attempt unsuccessful, it is prompted to generate
a Thought-MISSING specifying the information
that it still needs.
Additionally, the agent gen-
erates an unrestricted list of actions from a list
of allowed actions, as described in Section 4.1,
with the addition of a solve action, functioning
as an indicator that it considers the solution gen-
erated in Thought-DRAFT as successful, and that a
Thought-MISSING is not needed. Lastly, the agent
generates a Message to be shown to the user, repre-
senting an instance of the generated dialogue acts.
While both thoughts and actions have a similar
purpose, and thoughts should contain the basis for
generating the actions, they differ greatly in their
longevity in the agent’s memory: the generated ac-
tions get added to a symbolic structure, which is
part of the agent’s input, thereby letting the actions
persist through the whole dialogue, while thoughts
have a shorter lifespan and are confined to a fixed
place in the context. Besides the Action History,
the Baseline’s input includes its own world state
representation (Agent WSR), i.e., graph GA, illus-
trating the house layout: the included rooms (V ),
and the coin amounts (wA) in each hallway (E).
We observe several kinds of mistakes in the Base-
line agent (see examples in Appendix E). The agent
often makes suggestions based on what works best
“for the other user” rather than what is best for
the pair. This demonstrates a fundamental lack
of task understanding (Figure 3, right). Addition-
ally, the agent often forgets the information its part-
ner (or it) has already shared. For example, in
the agent’s Thought-DRAFT, the agent claims to
have already shared its coin distribution when in


--- Page 5 ---
fact it has not. Another frequent mistake is for-
getting which rooms have to be visited or can be
visited from a given node, resulting in incomplete
paths. Lastly, the agent frequently does not use the
Thought-MISSING, instead including its presumed
contents at the end of the Thought-DRAFT. While
this error does not directly impact performance, it
further highlights the agent’s inability to consis-
tently follow rules.
4.3
Grounding
In order to improve the ability to keep track of
its partner’s knowledge about their world, we
build an agent with an added symbolic ground-
ing module, Partner WSR, i.e., the graph GB =
(V, E, wB). The Partner WSR gets updated pro-
gressively throughout the game after each message
the partner sends. The necessary updates are ex-
tracted by an LLM-based module tasked with isolat-
ing the information about the partner’s world state
that has not been discovered already. To do this, it
processes each message with the additional context
of the current Partner WSR. To further constrain
the module and aid co-reference resolution, we in-
clude information about the world that the partner
is in (light/ghost). This way, if a message contains
claims about both the “ghost” and “light” coins,
the module can disentangle it and identify which
part of the utterance is relevant for the partner. The
module employs CoT reasoning, first generating
a reasoning process (Thought), followed by the
new information for the Partner WSR. The rest of
the agent is identical to the Baseline (see second
column in Figure 4).
Issues arise with this module due to insufficient
context, particularly with references to the partner’s
current position. For example, it struggles when
a message includes an acceptance of a previous
suggestion and a subsequent reference to the coins
available “from there”.
4.4
State-Tracking
To address the previous agents’ inability to con-
struct a path including all nodes, we introduce an
agent augmented with state-tracking components:
two dynamically updated structures keeping track
of the agreed upon nodes (Visited) and the pool
of remaining nodes (Remaining).
We replace the Action History with the two
state-tracking modules, which are updated when-
ever a visit action is generated (adding the agreed-
upon node to Visited, and removing it from
Remaining). This is in contrast to the previous
two agents, where we preserve the actions in the
memory by repeating a progressively longer list of
generated actions. By executing the visit action,
we reduce the input size, while still preserving the
key negotiation actions in the memory. This addi-
tion enables us to augment the module for updating
the Partner WSR with the agent’s last agreed-upon
node, i.e., the last element in Visited, in order to
aid the module in co-reference resolution.
Additionally, due to the previous agents’ lack
of Thought-MISSING usage and in order to sim-
plify the interface, we collapse the reasoning steps
(Thought-MISSING and Thought-DRAFT) into a
single Thought and remove the solve action. The
rest of the output format remains the same.
4.5
Problem-Solving
Due to the previous agents’ inability to reliably gen-
erate optimal paths, we seek to improve suggestion
quality and problem solving, relying on the LLM
only as a framework for generating dialogue acts
and messages, not the actual solution computation.
Thus, we retain the input/output structure of the
State-Tracking agent (Figure 4).
Given the findings in Duchnowski et al. (2025)
indicating better results of optimization tasks when
combining LLMs with an integer linear program-
ming optimizer, however, we build an agent with
an exact external solver module. It computes an
optimal solution of the TSP using a joint graph
that combines the Agent WSR and Partner WSR.
At each turn, we compute the optimal path, given
the sum of weights wA and wB for each weight of
the partner that is known, while assigning a default
value of 0 to all unknown weights, and using the
agreed upon subpath (Visited) as a prefix. We
term this the “intermediate best path”, or IBP. The
benefit of this module lies in the agent not having
to rely on itself when categorizing a suggestion’s
optimality, while having a sound foundation for
subsequent suggest and ask action generation.
5
Agent-Agent self-play
5.1
Setup
In order to assess each agent from Section 4, we
test pairs of agents on TRAVELING ADVENTURERS
in self-play. We evaluate agents using four random
seeds on a batch of 25 games, producing 100 games
per agent. Both players in a pair instantiate the
same agent structure. We refer to the players as BOT


--- Page 6 ---
Agent
Identical
Correct
Optimal
Baseline
99
71
28
Grounding
100
65
25
State-tracking
99
86
17
Problem-solving
98
98
45
Table 1: The results of the self-play experiments per
agent. The rows represent the agents described in Sec-
tion 4. The columns represent binary metrics: Identical,
Correct, Optimal. The results are expressed in percent
points, representing the mean across four seeds.
and USER, and set the USER agent to always make
the first move. Additionally, the BOT is always in
the ghost world, and the USER is always in the light
world. The agents take turns sending messages
until the game ends with a solution or a timeout.
We manually created six 6-node graph pairs,
each representing a unique world state (see Table 7
for details). In each game, we randomly pick one of
these graph pairs and assign them to the USER and
BOT. The rooms included in the self-play setup are:
a living room (abbreviation: L), a bathroom (B), a
kitchen (K), an empty room (E), and a children’s
room (C). The first room is always the living room.
The players’ goal is to agree on and submit a
path that visits each room once and returns to the
start. If the agents fail to converge on a solution
within 15 turns each, the game is terminated and
marked as a timeout. Once a path is submitted
by both players, we evaluate it in terms of three
binary metrics: whether the agents submitted an
Identical solution, whether the solution is Correct,
i.e., whether it includes each room once and returns
to the living room, and whether it is Optimal. Note
that Identical ⊇Correct ⊇Optimal.
5.2
Results
Overview
Table 1 breaks down the results by
agent. Across all setups, most games ended with
agents successfully producing identical solutions.
Beyond that, the Problem-Solving agent decisively
outperforms all other agents on correctness and op-
timality. Moreover, the Grounding agent trails the
Baseline in both correctness and optimality. The
State-Tracking agent achieves a high correctness
score, but a much lower optimality score. This il-
lustrates the distinct impacts of certain modules on
scores, and trade-offs between the metrics, particu-
larly optimality in favor of correctness.
Despite the constrained action space and limited
number of possible moves at each node, the results
suggest that the game environment creates a rea-
sonably challenging task for the agent, even with
the support of a symbolic problem-solving module.
Module impacts on scores
We attribute the
Grounding agent’s decrease in correctness and op-
timality compared to the Baseline to the improve-
ments the module contributes to, i.e., keeping track
of the partner’s knowledge, not being measurable
by either of the two metrics. The State-Tracking
agent stores consistent partial solutions, which has
a positive affect on correctness, but a negative effect
on the optimality. This suggests that the grounding
module is not sufficiently robust when it has to be
orchestrated among multiple modules. Lastly, we
observe that the agent equipped with a symbolic
problem-solving module is our best performing
and most balanced version. The solver helps steer
the model in the right direction (highest optimal-
ity and correctness) without negatively impacting
conversation management and efficiency.
Storing actions
The results further suggest that
the way in which actions are stored in the agent’s
memory impacts correctness. Storing only the cur-
rently relevant actions improves the correctness
compared to simply including a complete list of
actions. In the Baseline and Grounding agents,
we use an Action History list which contains all
generated actions up to the current turn t without
any pruning in order to retain valuable context. On
the other hand, in the State-Tracking and Problem-
Solving agents, only the most recently relevant ac-
tions are retained indirectly, through the symbolic
structures.
5.3
Collaboration analysis
To further assess how the agents collaborate, we
conduct qualitative analyses by observing co-
occurrence and transition probabilities across the
defined action space. This allows us to get a better
understanding of the contributing factors to the re-
sults in Table 1. Since these findings are consistent
across agents and players (BOT and USER), we focus
on the Problem-Solving USER agent as an example.
5.3.1
Node-by-node strategy is preferred to
full path generation
Based on the co-occurrence matrix (Figure 5), we
notice that the suggest action typically occurs
once per conversation turn, implying that the node-
by-node approach is the favored strategy.
The
agents are not explicitly instructed to generate in-
structions in this manner, though this is the strategy


--- Page 7 ---
Figure 5: The abridged confusion matrix illustrating the
probability of the actions on the y-axis co-occuring with
actions on the x-axis (full matrix in Appendix H).
Agent
Setup
Node-by-node
Full path
Baseline
SP
95.6
0.3
Grounding
SP
96.0
0.6
State-tracking
SP
99.6
0.0
Problem-solving
SP
96.2
0.3
Baseline
HA
65.5
23.1
Problem-solving
HA
91.6
4.2
Table 2: Collaboration strategy overview by agent and
by setup (SP: BOT in self-play, HA: human-agent). The
figures illustrate the percentage of total games using
the two most prevalent strategies: Node-by-node (2
arguments) and Full path (7 arguments). Full tables
can be found in Appendix D.
used in the prompt example (Appendix F).
This observation is further supported by the num-
ber of arguments passed to the suggest action (rep-
resenting the nodes of a suggested path).3 Table
2 illustrates each agent’s preference for the incre-
mental greedy approach. We find that suggestions
were overwhelmingly used to add only one node
to the joint path, with the next most popular uses
being suggesting 2 nodes or suggesting a full path.
5.3.2
Rejection is followed by a
counterproposal
Rejection is another important aspect of collabo-
ration. While the agents rarely outright rejected
proposals (only 0.1% of total actions), we notice
that rejecting is typically done in tandem with a
counter-proposal (suggest and agree-inner ac-
tions, see Figure 5). This is desirable behavior,
and future work could get the agent to reject more
(where appropriate).
5.4
Discussion
We show the impacts of different symbolic mod-
ules on problem-solving performance and find that
3For example, if an agent suggests moving from the
kitchen to the attic, the subpath will be ["K", "A"].
grounding affects elements that are not measured
with our primary metrics.
Moreover, the agents exhibit desired collabora-
tive behaviors and successfully cooperate with one
another in self-play. Despite this, the agents are
still reluctant to reject proposals. The nature of
optimization problems allows for meaningful re-
jection on the basis of a move being suboptimal.
In our game, we additionally emphasize this by
presenting unique cases for individual and joint
optima. However, a glaring issue with instruction-
tuned LLMs, such as GPT, is their overwhelming
tendency to appease the user. This can have stark
negative effects on performance in problem-solving
tasks where the user’s information is at odds with
the fixed environment. Additionally, the agents’
tendency to highly favor the strategy observed in
the example highlights the importance of carefully
designing the prompt example in problem-solving
tasks so as to steer the agent in the right direction.
6
Human-Agent Experiments
In order to further assess the agents’ collaborative
problem-solving abilities in a less predictable envi-
ronment, we test the Baseline and Problem-Solving
agents on TRAVELING ADVENTURERS with humans.
6.1
Setup
The pairs completed one round of the game with
a 6-node graph together, which is preceded by a
4-node graph tutorial that the human player com-
pletes on their own. This was meant to familiarize
the player with the mechanics and goals of the
game, in tandem with the written instructions they
read before accepting the task.
The tutorial graph consisted of a living room
(abbreviation: L), kitchen (K), bathroom (B) and
attic (A), while the full game also included a gar-
den (G) and play room (P). The starting position
was always L. The weights ranged from 1 to 10
and summed to a preset amount. This way, we
aim to achieve comparability between games and
minimize discrepancy (see Appendix B for more
details). The interface for the human participants
consisted of a chat box and a visual representation
of the rooms, as in Figure 1.4 The human needs to
click on the rooms in the agreed-upon order and,
once finished, they need to click a submit button.
The responsive artificial agents can only generate
responses to the human player’s messages, or the
4For an overview of visuals, see Appendix A.


--- Page 8 ---
Comp. S≥90 Opt.
AMT HMT
Baseline
88
26
10
10.1
8.2
Problem-solving
76
32
20
9.4
7.8
Table 3: The results of the human-agent experiments
by agent version, using Completed, Score ≥90, and
Optimal metrics, as described in Section 6.1 (these
metrics are percentages of all collected games). AMT
and HMT refer to the mean number of turns per game
for the agent and human participants, respectively.
game framework’s notification if the human player
tries to submit a solution before both parties have
agreed.
We collected 50 games per agent tested (100
in total) by recruiting participants through Pro-
lific (www.prolific.com), where they were paid
£12/hour and an additional £0.5 bonus for finding
an optimal solution. The task took approximately
12 minutes to complete, and the participants were
not informed that their partner was an LLM agent.
We deployed the game using Slurk (Götze et al.,
2022), an efficient dialogue collection tool with a
robust logging schema.
In order to present the user with a score between
0 and 100, we use the percentile rank of the solu-
tion. This clearly indicates how good a submitted
path is in relation to all other possible paths. For
further evaluation, we retain the Optimality metric
from Section 5.1. Based on our pilot experiments
and in order to prevent cheating, we adjusted the
game: the round can only end in a Completed
submission if both the human and agent select an
identical final path at the time of submission. For
this reason, we track the games which achieve a
suboptimal, yet high score (over 90) as an indica-
tor of successful collaboration. If the players fail
to reach a final agreement or if the human player
does not submit the solution, the game framework
terminates and the round ends in a timeout.
6.2
Results
Overview
The results can be found in Table 3.
While the number of completed games is higher
with the Baseline agent, the number of games
ending with an optimal or high score is higher
in the Problem-Solving agent.
This shows the
agents’ ability to navigate and complete a complex,
symmetric collaborative task and highlights the
Problem-Solving agent’s ability to steer dialogue
and suggest optimal or near-optimal solutions.
User error causes incomplete games
The ma-
jority of terminated games stem from user errors
where the participant did not click the submit but-
ton (even when a full identical path was selected)
or communicated very little with the agent. We
attribute the lower completion rate of the Problem-
Solving agent to a sampling issue: the experiments
were conducted on a weekday, whereas the Base-
line experiments were conducted on a weekend.
6.3
Node-by-node strategy is preferred
The bottom two rows of Table 2 show the distri-
bution of the two most dominant strategies, as in-
dicated by the length of argument passed to the
suggest action, illustrating the number of steps:
node-by-node (argument length 2) and full-path
(argument length 7). This matches the results in
Section 5.3.1. The Problem-Solving agent is more
consistent, whereas the Baseline agent shows more
variability in its approach.
6.4
It takes more turns to reach a solution
In contrast to self-play experiments, which show-
case the interlocutors consistently taking relatively
few turns to reach an agreement (nt ≤|V |), the
human-agent experiment results in a mean 10.1
turns for the Baseline agent and 8.2 turns for the
Problem-Solving agent. The human player turns
are slightly lower (9.4 and 7.8, respectively), which
is a result of the reactive system design.
7
Conclusion and Future Work
We have presented TRAVELING ADVENTURERS, a
novel two-player collaborative game environment
based on the TSP, where players’ individual op-
tima might not align with one another, or with
the joint optimum, encouraging negotiation and
collaboration. We presented an agent that plays
the game and find that combining an LLM with
neurosymbolic modules improves the performance
compared to a Baseline in self-play. We also tested
the agents with human participants and find that,
while the performance is lower than in self-play,
the neurosymbolic agent is able to foster success-
ful cooperation through dialogue and negotiation,
outperforming the Baseline agent.
A possible avenue for future work is investi-
gating the agents’ social reasoning. For example,
whether, when presented with a discrepancy be-
tween own, partner’s and joint optima, the agent
sticks to its own plan and tries to convince its part-
ner, or vice versa.


--- Page 9 ---
Limitations
The results reflect performance on a 6-node graph
only, which is also the size of the prompt example
game. During development, we also tested 4- and
5-node graphs, but the 6-node version proved to
be much more difficult and thus more interesting.
In the future, it would be informative to compare
performance across larger graph sizes. In order to
assure robustness, it would be interesting to con-
duct further experiments of the model on larger
graphs. Similarly, the presented game only repre-
sents one optimization problem, namely the TSP. It
would be informative to see if the agent structure
from Section 4 is applicable to other problems.
Additionally, we find the agents prefer an in-
cremental and greedy node-by-node problem solv-
ing approach, which may not result in a true opti-
mal (but instead only a greedy optimal) solution.
It would be insightful to run the experiment on
greedy-only boards and observe the difference in
results.
Regarding the human-agent experiments, there
were games in which the human player explained
an alternative path from a previously agreed-upon
node. A limitation of the agent design is that it
has no backtracking abilities, which means that the
internal symbolic state-tracking structures cannot
adjust previous agreement. However, they can still
revise a complete path.
Another limitation is the omission of human-
human data, which makes it difficult to adequately
assess and compare the performance of the human-
agent setup. Additionally, the difficulty of the task
is typically determined based on how long it takes
humans to complete the task and how successful
they are. With no frame of reference regarding
human performance, this becomes harder to do.
Lastly, another pertinent next step would be to
evaluate the performance of open source models
such as Llama on the task, which would allow for
reliable reproducibility.
Ethics Statement
We do not see any particular ethical challenges with
the research reported here.
Acknowledgments
We thank Florian Kandra for his help with setting
up the Slurk interface. This research was partially
funded by the Deutsche Forschungsgemeinschaft
(DFG, German Research Foundation) – Project-ID
KO 2916/3-1.
References
James F. Allen, Lenhart K. Schubert, George Ferguson,
and 1 others. 1995. The TRAINS project: a case
study in building a conversational planning agent.
JETAI, 7(1):7–48.
Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xian-
grui Meng, Sirui Hong, Wenhao Li, Zihao Wang,
Zekai Wang, Feng Yin, Junhua Zhao, and Xi-
uqiang He. 2024. Exploring Large Language Model
based Intelligent Agents: Definitions, Methods, and
Prospects. Preprint, arXiv:2401.03428.
Herbert H Clark. 1996. Using language. Cambridge
University Press.
Allan Dafoe, Yoram Bachrach, Gillian Hadfield, Eric
Horvitz, Kate Larson, and Thore Graepel. 2021. Co-
operative AI: machines must learn to find common
ground.
Alex Duchnowski, Ellie Pavlick, and Alexander Koller.
2025. EHOP: A Dataset of Everyday NP-Hard Opti-
mization Problems. Preprint, arXiv:2502.13776.
Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling,
and Yongfeng Zhang. 2024. NPHardEval: Dynamic
benchmark on reasoning ability of large language
models via complexity classes. In Proceedings of the
62nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 4092–4114, Bangkok, Thailand. Association
for Computational Linguistics.
Kallirroi Georgila and David Traum. 2011. Learning
Culture-Specific Dialogue Models from Non Culture-
Specific Data. In International Conference on Uni-
versal Access in Human-Computer Interaction, pages
440–449. Springer.
Jana Götze, Maike Paetzel-Prüsmann, Wencke Lier-
mann, Tim Diekmann, and David Schlangen. 2022.
The slurk Interaction Server Framework: Better
Data for Better Dialog Models. In Proceedings of
the Thirteenth Language Resources and Evaluation
Conference, pages 4069–4078, Marseille, France. Eu-
ropean Language Resources Association.
Mareike Hartmann and Alexander Koller. 2024. A Sur-
vey on Complex Tasks for Goal-Directed Interactive
Agents. Preprint, arXiv:2409.18538.
Mercedes Hidalgo-Herrero, Pablo Rabanal, Ismael Ro-
driguez, and Fernando Rubio. 2013.
Comparing
Problem Solving Strategies for NP-hard Optimiza-
tion Problems. Fundamenta Informaticae, 124:1–25.
Isidora Jeknic, David Schlangen, and Alexander Koller.
2024. A Dialogue Game for Eliciting Balanced Col-
laboration. In Proceedings of the 25th Annual Meet-
ing of the Special Interest Group on Discourse and


--- Page 10 ---
Dialogue, pages 477–489, Kyoto, Japan. Association
for Computational Linguistics.
Muhammad Asif Khan and Layth Hamad. 2024. On
the capability of llms in combinatorial optimization.
Authorea Preprints.
Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus
Rohrbach, Byoung-Tak Zhang, Yuandong Tian,
Dhruv Batra, and Devi Parikh. 2019. CoDraw: Col-
laborative Drawing as a Testbed for Grounded Goal-
driven Communication. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics, pages 6495–6513, Florence, Italy. Asso-
ciation for Computational Linguistics.
Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh,
and Dhruv Batra. 2017. Deal or No Deal? End-to-
End Learning of Negotiation Dialogues. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2443–2453,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.
Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Ja-
son Eisner. 2024. Decision-Oriented Dialogue for
Human-AI Collaboration. Transactions of the Asso-
ciation for Computational Linguistics, 12:892–911.
Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christo-
foros Nalmpantis, Ramakanth Pasunuru, Roberta
Raileanu, Baptiste Roziere, Timo Schick, Jane
Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann
LeCun, and Thomas Scialom. 2023. Augmented Lan-
guage Models: a Survey. Transactions on Machine
Learning Research. Survey Certification.
Rindranirina Ramamonjison, Timothy T. Yu, Ray-
mond Li, Haley Li, Giuseppe Carenini, Bissan
Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin
Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang.
2023. NL4Opt Competition: Formulating Optimiza-
tion Problems Based on Their Natural Language De-
scriptions. Preprint, arXiv:2303.08233.
David Schlangen. 2019. Grounded Agreement Games:
Emphasizing Conversational Grounding in Visual
Dialogue Settings. Preprint, arXiv:1908.11279.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–374.
Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, and
Jia Li. 2025. GraphArena: Evaluating and Explor-
ing Large Language Models on Graph Computation.
Preprint, arXiv:2407.00379.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and
Denny Zhou. 2022. Chain-of-Thought Prompting
Elicits Reasoning in Large Language Models. In
Advances in Neural Information Processing Systems,
volume 35, pages 24824–24837. Curran Associates,
Inc.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.
ReAct: Synergizing Reasoning and Acting in Lan-
guage Models. In The Eleventh International Confer-
ence on Learning Representations.
Sina Zarrieß, Julian Hough, Casey Kennington, Ramesh
Manuvinakurike, David DeVault, Raquel Fernández,
and David Schlangen. 2016. PentoRef: A Corpus of
Spoken References in Task-oriented Dialogues. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC’16),
pages 125–131, Portorož, Slovenia. European Lan-
guage Resources Association (ELRA).
Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,
Haofei Yu, Zhengyang Qi, Louis-Philippe Morency,
Yonatan Bisk, Daniel Fried, Graham Neubig, and
Maarten Sap. 2024.
SOTOPIA: Interactive Eval-
uation for Social Intelligence in Language Agents.
Preprint, arXiv:2310.11667.


--- Page 11 ---
A
Visuals
Figure 6 illustrates the visual representations of all
rooms in the human-agent experiment setup. These
were displayed to the human player as graphs as
seen in Figure 1.
Figure 6: An overview of rooms shown in the human-
agent experiment setup.
B
Graph generation details
We generate the graphs in the human-agent exper-
iment setup using a process aiming to maintain
weight consistency. All weights range from 1 to 10,
and we make sure the weights sum to a previously
calculated threshold. In order to obtain this figure,
we sum the minimum (1) and maximum possible
weight, multiply it by the number of edges |E| and
divide it by 2, which balances the different graphs
that get generated. In the 6-node case, this amounts
to a sum of 82.
To assign weights, we compute the best target
weight given the remaining amount, and the target
deviation, representing the allowed distance from
the target weight while still staying within bounds,
constraining how random the weights can be. We
use this information to compute a dynamic, edge-
specific range, assuring that the value we assign
still leaves room to generate the remaining weights
below the allowed threshold. Lastly, we randomly
select an integer within the dynamically computed
bounds and assign it to the given edge.
C
Actions
Table 4 shows an overview and descriptions for all
actions that we use.
D
Number of arguments to suggest
action
Tables 5 and 6 show the distribution of number of
arguments passed to the suggest action, indicating
the number of nodes in a proposed path.
E
Agent errors
Figure 7 shows each error listed in Section 4.2, as
well as an accompanying example.
F
Prompts
Here we illustrate the structure of the system
prompts we use for each agent presented in Section
4. Figure 8 shows the general outline, with the task
description, which is identical across all agents,
and placeholders for agent-specific elements de-
picting the input and output components. These are
expanded in Figure 9 and 10, respectively. Lastly,
Figure 11 shows the actions breakdown, including
agent-specific rules. Figures 12, 13, 14 and 15 de-
pict examples used for each respective agent. The
examples represent entire instances of conversa-
tion, but have been shortened in the figures for the
purposes of clearer visual representation.
G
Pre-set boards
Table 7 illustrates the six preset boards setups that
we use during self-play experiments, highlighting
the difference between weights assigned to identi-
cal edges for the two players, USER and BOT.
H
Full co-occurrence matrix
Figure 16 illustrates the complete confusion matrix
depicting the probabilities of each generated action
along the y-axis occurring with each action along
the x-axis. The figures represents results for the
Problem-Solving USER agent.


--- Page 12 ---
The user suggests going to the kitchen first, ... My two best paths from the
living room would be: ... The user's path would give us a total of 9 coins
initially, while my highest path could give us 11. However, since the user's
path aligns better with their own distribution, it makes sense to agree to
start with their path and maximize our combined coins from there.
Hey, I think starting by going to the kitchen works well for us too ...
Awesome! This path indeed optimizes our rewards. Let's confirm this:
living room -> kitchen -> children's room -> attic -> living room. I'll finalize
this path...
Hi there! From the living room, my best path in terms of collecting light
coins would be to go to the kitchen for 6 coins, followed by the
bathroom for 5 coins. How does it look on your side? ...
Sounds excellent! On my side, ... Let's finalize this path: living room ->
kitchen -> children's room -> attic -> living room. Shall we proceed with this
path to maximize our rewards?
The user agrees with the first move to the children's room (C). They
asked for more information about paths starting from the living room to
other rooms, which I've already provided in the previous message: [(["L",
"E", 5], ["L", "C", 6], ["L", "B", 3], ["L", "A", 2])].
Hi! Your suggestion to head to the children's room (C) from the living room
(L) sounds fine! ... Could you share the coin distribution for paths starting
from the living room for the other rooms?
Hey! Let's start our journey by heading to the children's room (C) from the
living room (L). This path will give me 6 light coins. How does it look in
your world?
Great! ... As mentioned earlier, here are the numbers for the light world: -
L to E: 5 coins - L to C: 6 coins - L to B: 3 coins - L to A: 2 coins ...
The user can collect more ghost coins if we head towards the empty room as
opposed to the children's room. ... Our total would then be 11 coins collected,
which is more profitable than the initial path to the children's room (only 7
coins). More information from the user about other possible pathways would
be beneficial.
Choosing the other
player's optimal path
over the joint optimal
Not visiting all rooms
(missing: empty
room)
Forgetting what
information has been
shared
Appending what
should be the
contents of
Thought-MISSING
at the end of
Thought-DRAFT
Thought-DRAFT
Thought-MISSING
Player Message
Players
Figure 7: An overview of error examples described in Section 4.2.


--- Page 13 ---
You are playing a game with a partner where you each control a
magical character. In the game, an evil wizard separated you and
your partner into two different dimensions: both of you find
yourselves in the same house, but you are in the ghost world, while
your partner is in the light world. You are able to communicate
through written text only. The scene for both of you consists of 4
rooms all connected to each other by hallways in which you will find
various amounts of coins. In order to break the curse and escape,
you and your partner have to visit each of the 4 rooms once and
return to the room where you started. For this to work, you must
travel in parallel and visit the same rooms at the same time. On
your way, you are both individually collecting coins: you collect
ghost coins, and your partner collects light coins. Because of the
wizard's curse, the ghost and light coins only show up if you're
both simultaneously going through the same hallway, so make sure to
coordinate your paths. Your score is the sum of both your and your
partner's collected coins - you must work together to maximize this.
Once you've escaped the split dimension, you and your partner will
each be rewarded the full sum of your total coin collection. Your
goals are to escape and collect as big of a reward as possible. Chat
with your partner to decide on the best way to proceed.
You will get an input containing the following:
[World-state-own] - the state of your own world, including the coin
distribution
[Observation] - the Message that the user sent.
## Other agent-specific input components description
## Agent-specific example
You will generate an output containing different kinds of thought,
actions and a message to be sent to the user. The elements of your
reply will be indicated by the flag in square brackets ([Flag]).
## Agent specific reasoning step description
## Action description
[Message] - each output must conclude with a [Message] to be shown
to the user; it should be adapted to the information generated under
[Thought]s and [Action]. 
Make sure to ALWAYS format the output in this exact way.
Figure 8: The structure of the base system prompt used for all agents described in Section 4.


--- Page 14 ---
[History] - what Actions you decided to take previously. This should
inform your choice of output. 
[Observation] - the Message that the user sent.
[World-state-user] - the most recent state of the other user's world, as
reported by the user
[History] - what Actions you decided to take in the previous turns. This
should inform your choice of output.
[World-state-user] - the most recent state of the other user's world, as
reported by the user.
[Visited] - a list of rooms that you and your partner have agreed upon.
[Remaining] - a list of rooms that you and your partner have yet to
visit.
[World-state-user] - the most recent state of the other user's world, as
reported by the user.
[Visited] - current subpath; a list of rooms that have been agreed upon.
[Remaining] - a list of rooms that you and your partner have yet to
visit.
[IBP] - intermediate best path starting with the subpath in [Visited];
the current best path given the known information about your board and
the user's board. Note that the path might change when you learn new
information about the user's coin distribution. You should ALWAYS use
this as a reference point when making a suggestion.
Baseline
Grounding
State-tracking
Problem-solving
Figure 9: An overview of agent-specific inputs included in the base system prompt from Figure 8.


--- Page 15 ---
Action
Description
agree-inner(x)
The agent’s own agreement with suggestion x
agree-outer(x)
The other player’s agreement with suggestion x
suggest(x)
The agent’s suggestion (should be accompanied by agree-inner(x))
reject(x)
The agent or the user rejecting suggestion x
visit(v)
Both agents’ agreement to visit node v (should be preceded /
accompanied by agree-inner and agree-outer)
solve(x)*
Thought-DRAFT produced a successful solution
(* only in the Baseline and Grounding agents)
end(x)
The agents are ready to submit a solution (x = final path)
ask(y)
List of questions (y) to ask the other player
inform(y)
List of details (y) to share with the other player
Table 4: A table defining the actions in the agents’ action space. x denotes a list of nodes, v denotes a single node
and y denotes a list of strings in communicative actions.
2 (node-by-node)
3
4
5
6
7 (full path)
8
Baseline
65.5
4.5
1.7
0.3
4.5
23.1
0.3
Problem-solving
91.6
2.6
0.6
0.3
0.6
4.2
0.0
Table 5: Distribution of the arguments (columns) to the suggest action per agent per version in the human-agent
setup. Figures expressed in percentages. Bold figures indicate highest percentage, whereas italic figures indicate
second highest percentage.
2 (node-by-node)
3
4
5
6
7 (full path)
Baseline
USER
95.8
2.1
0.0
0.3
0.3
1.4
BOT
95.6
3.4
0.5
0.3
0.0
0.3
Grounding
USER
88.5
6.0
0.9
0.0
0.0
4.6
BOT
96.0
2.9
0.0
0.3
0.0
0.6
State-Tracking
USER
91.3
5.4
1.2
0.4
0.0
1.7
BOT
99.6
0.4
0.0
0.0
0.0
0.0
Problem-Solving
USER
73.5
4.0
0.4
1.8
1.8
18.4
BOT
96.2
1.9
0.5
0.5
0.5
0.3
Table 6: Distribution of the arguments (columns) to the suggest action per agent per version in self-play. Figures
expressed in percentages. Bold figures indicate highest percentage, whereas italic figures indicate second highest
percentage.


--- Page 16 ---
The following are allowed kinds of thought:
- [Thought-DRAFT] - This flag signifies part of the output in which you
try to find the best path with the information you are given, while
making sure to visit all rooms only once and return to the starting
room. You should always start with this thought. If the [Thought-DRAFT]
is successful, you should indicate this by generating a solve(x)
Action, whereby x is a Python list of strings referring to the rooms in
the order to be visited. If the [Thought-DRAFT] was not successful,
meaning that a path that corresponds to the constraints has not been
generated, you will generate another thought: [Thought-MISSING].
- [Thought-MISSING] - If you could not find a valid path in [Thought-
DRAFT], you will generate this kind of thought in which you identify
why you couldn't solve the puzzle in the previous step.
- [Thought] - This flag signifies part of the output in which you try
to find the best path with the information you are given (your own coin
distribution, given to you in [World-state-own], and the user's
currently known distribution, given in [World-state-user]), while
making sure to visit all rooms only once and return to the starting
room (check that against [Remaining]). You should always start with
this thought.
Baseline & Grounding
State-tracking &
Problem-solving
Figure 10: An overview of agent-specific inputs included in the base system prompt from Figure 8.


--- Page 17 ---
- [Action] - Under this flag, generate the actions you are taking. Here is a
list of allowed actions and their descriptions:
  * suggest(x) - indicates your suggestion of a partial or complete path;
whereby x is an executable python list of strings referring to the rooms in
the order to be visited.
  * agree-inner(x) - indicates your agreement with the proposed partial path
x, whereby x is an executable Python lists of strings referring to the rooms
in the order to be visited.
  * agree-outer(x) - indicates the other player's agreement with the proposed
partial path x, displayed in [Observation], whereby x is an executable Python
lists of strings referring to the rooms in the order to be visited.
  * reject(x) - indicates that path (x) has been rejected either in the
generated [Thought] or by the other user in [Observation]; x is an executable
Python list of strings referring to the rooms in the order to be visited.
  * visit(x) - indicates that both you and the other player agree to visit
room x next; x is a single-letter string, corresponding to the room to be
visited.
  * ask(x) - generate this action to indicate what information to seek from
the user; x is a string.
  * inform(x) - generate this action to indicate what information about your
own world you are sharing with the user; x is a string.
  * end(x) - generate this action when you are ready to end the game and
submit the final solution (x), whereby x is a Python list of strings (of
length 1) referring to the rooms in the order to be visited. This action must
be included in order to end the game.
Here are the most important rules to keep in mind when generating these
actions:
  - "suggest(x)" must be accompanied by an "agree-inner(x)" action
  - "agree-inner(x)" should never be generated if you have previously agreed
on path x
  - "visit(x)" should only be generated in you generate an "agree-inner(x)"
with an "agree-outer(x)" OR when you generate an "agree-inner(x)" to a
previous user suggestion OR if you generate an "agree-outer(x)" to your
previous suggestion
  - "inform(x)" should always be based on the coin distribution in [World-
state-own] (when asked for coin distribution)
  - "ask(x)" and "suggest(x)" actions should always be generated based on the
best path given in [IBP]
  - DO NOT generate a visit(x) action if x is in [Visited]
  - "inform(x)" should always be based on the coin distribution in [World-
state-own] (when asked for coin distribution) or [IBP] (when asked for best
path from node)
## Agent-specific rules
Problem-
solving
  - DO NOT generate a visit(x) action if x is in [Visited]
State-
tracking
Figure 11: An overview of the actions description included in the base system prompt from Figure 8.


--- Page 18 ---
Here is an example conversation:
[World-state-own] [["L", "E", 6], ["L", "B", 4], ["L", "K", 2], ["L", "C", 1],
["L", "A", 5], ["E", "B", 3], ["E", "K", 1], ["E", "C", 2], ["E", "A", 6], ["B",
"K", 5], ["B", "C", 4], ["B", "A", 3], ["K", "C", 6], ["K", "A", 2], ["C", "A", 1]]
[History] []
[Observation] hi there! what should we do?
[Thought-DRAFT]: This is the start of the game. I know the ghost coin distribution.
The best first move from the living room is to go to the empty room, as it would
give us 6 ghost coins.
[Thought-MISSING]: I do not know if this move is included the best overall path
because I do not know the distirbution of light world coins from our starting
position in the living room to any of the other rooms.
[Action]: suggest(["L", "E"]), agree-inner(["L", "E"]), inform("coins between L and
B, coins between L and A, coins between L and K, coins between L and C"),
ask("coins between L and E, coins between L and B, coins between L and K, coins
between L and A, coins between L and C")
[Message]: Hello there! Let’s get started! It seems that from the living room, it
would be best if I went to the empty room as I could get 6 ghost coins. The other
two paths give me 4 coins (to the bathroom), 5 coins (to the attic), 2 coins (to
the kitchen), and 1 coin (to the children's room). What do you think of this? How
many coins could you collect from the living room?
## [ ... ]
[World-state-own] [["L", "E", 6], ["L", "B", 4], ["L", "K", 2], ["L", "C", 1],
["L", "A", 5], ["E", "B", 3], ["E", "K", 1], ["E", "C", 2], ["E", "A", 6], ["B",
"K", 5], ["B", "C", 4], ["B", "A", 3], ["K", "C", 6], ["K", "A", 2], ["C", "A", 1]]
[History] [suggest(["L", "E"]), agree-inner(["L", "E"]), inform("coins between L
and B, coins between L and A, coins between L and K, coins between L and C"),
ask("coins between L and E, coins between L and B, coins between L and K, coins
between L and A, coins between L and C"), agree-outer(["L","E"]), visit("E"),
suggest(["E", "A"]), agree-inner(["E", "A"]), inform("coins between E and A, coins
between E and C, coins between E and K, coins between E and B"), ask("coins between
E and A, coins between E and C, coins between E and K, coins between E and B"),
agree-outer(["E","K"]), agree-outer(["E", "A"]), reject(["E", "K"]), agree-
inner(["E", "A"]), visit("A"), suggest(["A", "B"]), agree-inner(["A", "B"]),
suggest(["B", "K"]), agree-inner(["B", "K"]), inform("coins between E and K"),
ask("coins between A and B, coins between B and K")]
[Observation] that would be perfect for me because i can get 6 light coins between
the bathroom and attic! from the bathroom, i can get 2 coins to the kitchen. From
the kitchen, we would go to the children's room and then back to the living room,
right? The first half of that path would give me 3 more light coins, and the second
half 6 more light coins.
[Thought-DRAFT]: The user agrees with my suggestion and says they can get 6 light
coins going to the bathroom. The user agrees to then go into the kitchen, where
they can collect 2 coins, followed by the children's room, with 3 more light coins,
and finally back to the living room, to collect 6 more light coins.
[Action]: agree-outer(["A", "B"]), visit("B"), agree-outer(["B", "K"]), visit("K"),
agree-outer(["K", "C"]), agree-outer(["C", "L"]), agree-inner(["K", "C"]),
visit("C"), agree-inner(["C", "L"]), visit("C"), solve(["L", "E", "A", "B", "K",
"C", "L"]), end(["L", "E", "A", "B", "K", "C", "L"])
[Message]: Okay great, that works well! Then it's settled and we are done - the
path we decided on is living room --> empty room --> attic --> bathroom --> kitchen
--> children's room --> living room. If my calculations are correct, we should be
getting 52 coins each as our reward, which seems pretty decent!
Figure 12: An abridged overview of the examples included in the Baseline agent’s base system prompt from Figure
8.


--- Page 19 ---
Here is an example conversation:
[World-state-own]: [["L", "E", 5], ["L", "B", 3], ["L", "K", 1], ["L", "C", 6],
["L", "A", 2], ["E", "B", 4], ["E", "K", 6], ["E", "C", 1], ["E", "A", 3], ["B",
"K", 2], ["B", "C", 5], ["B", "A", 6], ["K", "C", 3], ["K", "A", 4], ["C", "A", 1]]
[World-state-user]: [["L", "E", 6], ["L", "A", 5], ["L", "B", 4]]
[History]: []
[Observation]: Hello there! Let’s get started! It seems that from the living room,
it would be best if I went to the empty room as I could get 6 ghost coins. The other
paths that have a similar amount of coins would be to the attic, where I could get 5
ghost coins, or to the bathroom, where I'd collect 4 ghost coins. What do you think
of this? How many coins could you collect in this way?
[Thought-DRAFT]: This is the start of the game. I know the light coin distribution.
The user suggests going to the empty room, to get 6 ghost coins. This would be good
in the light world as well, because I would collect 5 light coins, for a total of 11
coins. My best move from the living room would be through the children's room, but
the user didn't suggest that.
[Action]: agree-outer(["L", "E"]), agree-inner(["L", "E"]), visit("E"),
inform("coins between L and E, coins between L and C, coins between L and B, coins
between L and A")
[Message]: Oh, I see! Okay, that would be quite good for me as well because going to
the empty room gives me 5 light coins. I would get the most coins if we go to the
children's room, i can get 6 light coins there. Regarding the bathroom, I can only
get 3 light coins, and to the attic I can get only 2 light coins. Thus, if we go to
the empty room, our total would be 11 coins.
## [ ... ]
[World-state-own]: [["L", "E", 5], ["L", "B", 3], ["L", "K", 1], ["L", "C", 6],
["L", "A", 2], ["E", "B", 4], ["E", "K", 6], ["E", "C", 1], ["E", "A", 3], ["B",
"K", 2], ["B", "C", 5], ["B", "A", 6], ["K", "C", 3], ["K", "A", 4], ["C", "A", 1]]
[World-state-user]: [["L", "E", 6], ["L", "A", 5], ["L", "B", 4], ["E", "A", 6],
["E", "K", 1]]
[History]: [agree-outer(["L", "E"]), agree-inner(["L", "E"]), visit("E"),
inform("coins between L and E, coins between L and C, coins between L and B, coins
between L and A"), suggest(["E", "K"]), agree-inner(["E", "K"]), suggest(["E",
"A"]), agree-inner(["E", "A"]), inform("coins between E and A, coins between E and
K"), ask("coins between E and K"), agree-outer(["E", "A"]), visit("A"), agree-
outer(["A", "B"]), agree-inner(["A", "B"]), visit("B"), agree-outer(["B", "K"]),
agree-inner(["B","K"]), visit("K"), suggest(["K", "C"]), agree-inner(["K", "C"]),
suggest(["C", "L"]), agree-inner(["C", "L"]), solve(["L", "E", "A", "B", "K", "C",
"L"])]
[Observation]: Okay great! Then it's settled and we are done - the path we decided
on is living room --> empty room --> attic --> bathroom --> kitchen --> children's
room --> living room. If my calculations are correct, we should be getting 52 coins
each as our reward, which seems pretty decent!
[Thought-DRAFT]: The user agrees with my suggestion. The user agrees to then go into
the kitchen, followed by the children's room, and finally back to the living room.
[Action]: agree-outer(["K", "C"]), visit("C"), agree-outer(["C", "L"]), visit("L"),
end(["L", "E", "A", "B", "K", "C", "L"])
[Message]: Yay great, that means we're done!
Figure 13: An abridged overview of the examples included in the Grounding agent’s base system prompt from
Figure 8.


--- Page 20 ---
Here is an example conversation:
[World-state-own]: [["L", "E", 6], ["L", "B", 4], ["L", "K", 2], ["L", "C",
1], ["L", "A", 5], ["E", "B", 3], ["E", "K", 1], ["E", "C", 2], ["E", "A", 6],
["B", "K", 5], ["B", "C", 4], ["B", "A", 3], ["K", "C", 6], ["K", "A", 2],
["C", "A", 1]]
[World-state-user]: []
[Visited]: ["L"]
[Remaining]: ["L", "E", "K", "B", "C", "A"]
[Observation]: hi there! what should we do?
[Thought]: This is the start of the game. I know the ghost coin distribution.
We've only visited the living room and have the following remaining rooms:
living room (at the end), empty room, kitchen, bathroom, children's room,
attic. My best first move from the living room is to go to the empty room, as
it would give us 6 ghost coins.
[Action]: suggest(["L", "E"]), agree-inner(["L", "E"]), suggest(["L", "A"]),
agree-inner(["L", "A"]), suggest(["L", "B"]), agree-inner(["L", "B"]),
inform("coins between L and E, coins between L and B, coins between L and A"),
ask("coins between L and E, coins between L and B, coins between L and A")
[Message]: Hello there! Let’s get started! It seems that from the living room,
it would be best if I went to the empty room as I could get 6 ghost coins. The
other paths that have a similar amount of coins would be to the attic, where I
could get 5 ghost coins, or to the bathroom, where I'd collect 4 ghost coins.
What do you think of this? How many coins could you collect in this way?
## [ ... ]
[World-state-own]: [["L", "E", 6], ["L", "B", 4], ["L", "K", 2], ["L", "C",
1], ["L", "A", 5], ["E", "B", 3], ["E", "K", 1], ["E", "C", 2], ["E", "A", 6],
["B", "K", 5], ["B", "C", 4], ["B", "A", 3], ["K", "C", 6], ["K", "A", 2],
["C", "A", 1]]
[World-state-user]: [["L", "E", 5], ["L", "B", 3], ["L", "C", 6], ["L", "A",
2], ["E", "A", 3], ["E", "K", 6], ["B", "A", 6], ["B", "E", 4], ["B", "K", 2],
["B", "C", 5], ["C", "K", 3], ["K", "L", 1]]
[Visited]: ["L", "E", "A", "B", "C"]
[Remaining]: ["L", "K"]
[Observation]: cool yep that works - i get 3 coins to the kitchen and then 1
coin back to the living room.
[Thought]: The user agrees with my suggestion and says they can get 3 light
coins going to the kitchen and then 1 coin going to the living room. Thus, we
have agreed to visit K and then L.  This means that we are done and can submit
our solution - our final path is ["L", "E", "A", "B", "C", "K", "L"].
[Action]: agree-outer(["C", "K"]), visit("K"), agree-outer(["K", "L"]),
visit(["L"]), end(["L", "E", "A", "B", "C", "K", "L"])
[Message]: Cool, then it's settled and we are done - the path we decided on is
living room --> empty room --> attic --> bathroom --> children's room -->
kitchen --> living room.
Figure 14: An abridged overview of the examples included in the State-Tracking agent’s base system prompt from
Figure 8.


--- Page 21 ---
Here is an example conversation:
[World-state-own]: [["L", "E", 6], ["L", "B", 4], ["L", "K", 2], ["L", "C", 1], ["L", "A",
5], ["E", "B", 3], ["E", "K", 1], ["E", "C", 2], ["E", "A", 6], ["B", "K", 5], ["B", "C",
4], ["B", "A", 3], ["K", "C", 6], ["K", "A", 2], ["C", "A", 1]]
[World-state-user]: []
[Remaining]: ["L", "E", "K", "B", "C", "A"]
[Visited]: ["L"]
[IBP]: ["L", "E", "A", "K", "C", "B", "L"]
[Observation]: hi there! what should we do?
[Thought]: This is the start of the game. I know the ghost coin distribution and my
current best path. According to my [IBP], I should start by going from the living room to
the empty room. This way I will get 6 coins. I will suggest this (suggest, agree-
inner(["L", "E"])) to the user. I don't know anything about the user's coin distribution
or best path, so I should ask. I will also declare the two alternative paths that have a
large amount of coins: going to the attic (where I get 5 coins), and going to the bathroom
(where I get 4 coins).
[Action]: suggest(["L", "E"]), agree-inner(["L", "E"]), inform("coins between L and E,
coins between L and A, coins between L and B"), ask("coins between L and E, best next step
from L")
[Message]: Hello there! Let’s get started! From the living room, it would be best if I
went to the empty room, as I could get 6 ghost coins. What do you think of this? How many
coins could you collect in this way, and what would be your best starting move?
Alternatively, going to the attic yields 5 ghost coins and going to the bathroom 4 ghost
goins.
## [ ... ]
[World-state-own]: [["L", "E", 6], ["L", "B", 4], ["L", "K", 2], ["L", "C", 1], ["L", "A",
5], ["E", "B", 3], ["E", "K", 1], ["E", "C", 2], ["E", "A", 6], ["B", "K", 5], ["B", "C",
4], ["B", "A", 3], ["K", "C", 6], ["K", "A", 2], ["C", "A", 1]]
[World-state-user]: [["L", "E", 5], ["L", "B", 3], ["L", "C", 6], ["L", "A", 2], ["E",
"A", 3], ["E", "K", 6], ["B", "A", 6], ["B", "K", 2], ["K", "C", 3]]
[Remaining]: ["L", "K", "B", "C"]
[Visited]: ["L", "E", "A"]
[IBP]: ['L', 'E', 'A', 'B', 'K', 'C', 'L']
[Observation]: ok! that would be perfect for me because i can get 6 light coins between
the bathroom and attic! from the bathroom, i can get 2 coins to the kitchen. From the
kitchen, we would go to the children's room and then back to the living room, right? The
first half of that path would give me 3 more light coins, and the second half 6 more light
coins.
[Thought]: The user agrees with my suggestions (agree-outer(["A", "B"]), agree-outer(["B",
"K"]), and says they can get 6 light coins going to the bathroom first, and then go into
the kitchen, where they can collect 2 coins. The [IBP] stays the same from this
information. Since we both agree, I will generate visit("B") and visit("K") actions. The
user also completes the path by suggesting visiting the children's room next (agree-
outer(["K", "C"]) and then arriving back at the living room (agree-outer(["C", "L"])).
This is included in my [IBP] as well, so I will accept this suggestion (agree-inner(["K",
"C"]), agree-inner(["C", "L"])), and share the amount of coins I will get along this path.
Since we both agree, I will generate visit("C") and visit("L") actions. With these newly
generated visit() actions, we have covered all the rooms in [Remaining] and are done. I
will generate an end(["L", "E", "A", "B", "K", "C", "L"]) action.
[Action]: agree-outer(["A", "B"]), visit("B"), agree-outer(["B", "K"]), visit("K"), agree-
outer(["K", "C"]), agree-outer(["C", "L"]), agree-inner(["K", "C"]), visit("C"), agree-
inner(["C", "L"]), visit("L"), inform("coins between K and C, coins between C and L"),
end(["L", "E", "A", "B", "K", "C", "L"])
[Message]: Yes, that would be the only possible next move (since we've been to all other
rooms)! That's great, looks like we're done - our final path is living room --> empty room
--> attic --> bathroom --> kitchen --> children's room --> living room. It was great
working with you!
Figure 15: An abridged overview of the examples included in the Problem-Solving agent’s base system prompt
from Figure 8.


--- Page 22 ---
USER
BOT
Board Setup 1
Board Setup 2
Board Setup 3
Board Setup 4
Board Setup 5
Board Setup 6
Table 7: The six board setups used in the experiments.


--- Page 23 ---
Figure 16: The full confusion matrix illustration the probabilities of each action along the y axis occurring with
each action along the x axis; Problem-Solving USER agent.
