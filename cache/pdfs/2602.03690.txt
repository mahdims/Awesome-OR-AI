--- Page 1 ---
LLM-Inspired Pretrain-Then-Finetune
for Small-Data, Large-Scale Optimization
Zishi Zhang
Guanghua School of Management, Peking University, Beijing 100871, China, zishizhang@stu.pku.edu.cn
Jinhui Han
Guanghua School of Management, Peking University, Beijing 100871, China, jinhui.han@gsm.pku.edu.cn
Ming Hu
Rotman School of Management, University of Toronto, Toronto, Ontario, Canada M5S 3E6, ming.hu@rotman.utoronto.ca
Yijie Peng
Guanghua School of Management, Peking University, Beijing 100871, China, pengyijie@pku.edu.cn
We consider small-data, large-scale decision problems in which a firm must make many operational deci-
sions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy,
data points per instance. Inspired by the success of large language models (LLMs), we propose a pretrain-
then-finetune approach built on a designed Transformer model to address this challenge. The model is first
pretrained on large-scale, domain-informed synthetic data that encode managerial knowledge and structural
features of the decision environment, and is then fine-tuned on real observations. This new pipeline offers two
complementary advantages: pretraining injects domain knowledge into the learning process and enables the
training of high-capacity models using abundant synthetic data, while finetuning adapts the pretrained model
to the operational environment and improves alignment with the true data-generating regime. While we
have leveraged the Transformer’s state-of-the-art representational capacity, particularly its attention mech-
anism, to efficiently extract cross-task structure, our approach is not an off-the-shelf application. Instead,
it relies on problem-specific architectural design and a tailored training procedure to match the decision
setting. Theoretically, we develop the first comprehensive error analysis regarding Transformer learning in
relevant contexts, establishing nonasymptotic guarantees that validate the method’s effectiveness. Critically,
our analysis reveals how pretraining and fine-tuning jointly determine performance, with the dominant con-
tribution governed by whichever is more favorable. In particular, finetuning exhibits an economies-of-scale
effect, whereby transfer learning becomes increasingly effective as the number of instances grows. The results
also highlight the important role of domain-guided pretraining, which provides an informative initialization
that improves performance, especially when domain knowledge is accurate and the real data are extremely
limited. These insights are echoed in numerical experiments and, more broadly, shed light on the effectiveness
of Transformer-based LLM training.
1.
Introduction
Transformer-based models (Vaswani et al. 2017) have achieved remarkable success across a broad
range of scientific domains, including natural language processing (Wolf et al. 2020), protein struc-
ture prediction (Jumper et al. 2021), and even mathematical discovery (Romera-Paredes et al.
1
arXiv:2602.03690v1  [cs.LG]  3 Feb 2026


--- Page 2 ---
2
2024). In particular, large language models (LLMs) stand out as perhaps the most visible and widely
adopted instantiation of this model, with prominent examples including the GPT family (Floridi
and Chiriatti 2020), BERT (Devlin et al. 2019), and Deepseek (Liu et al. 2024). From a method-
ological perspective, the pretrain-then-finetune paradigm built on Transformer architectures has
emerged as a central principle behind the success of LLMs. Under this framework, a model is first
pretrained via (self-)supervised learning on large-scale raw-text corpora, enabling it to acquire rich,
transferable representations. The pretrained model is then finetuned using task-specific data to
adapt these representations to downstream objectives.
LLMs are, as the name implies, essentially very large; for example, GPT-3 contains roughly
175 billion parameters, and Deepseek-V3 activates about 37 billion parameters per token during
inference. Such a scale is enabled by access to massive text corpora, and the model serves a general-
purpose set of objectives, spanning information retrieval and question answering to increasingly
sophisticated forms of reasoning. Business decision-making settings differ in several key aspects.
The available data are typically far more limited; in many cases, they are also private and highly
sensitive. Moreover, the decision objectives are well specified (e.g., minimizing inventory costs or
maximizing revenue in pricing) rather than open-ended. These features make a smaller, special-
ized model both practical and, often, necessary: it can be trained feasibly on limited data while
directing modeling capacity toward the decision-relevant structure. Motivated by these considera-
tions and by the success of LLM training pipelines, we study the pretrain-then-finetune approach
for small-data, large-scale stochastic optimization. Our method builds on a designed Transformer
architecture, pretrained using domain-informed (synthetic) data and subsequently finetuned on
scarce real observations. We further establish quantitative theoretical guarantees that characterize
its effectiveness for the target optimization problem.
Specifically, we focus on operations management (OM) problems in which firms must manage
vast product catalogs, yet most items lie in the long tail (Anderson et al. 2006) and generate only
sparse sales histories. This phenomenon, referred to as a small-data large-scale problem in Miˇsi´c
and Perakis (2020), poses a central challenge for deploying data-driven methods in relevant OM
scenarios. For instance, using data from Tmall, Miao et al. (2022) report that among more than
75,000 products listed between May and July 2018, over 21.6% attracted fewer than 10 unique
visitors per day on average, and more than 14.3% received at most one visitor per day. A similar
pattern is observed in fast fashion: Shein reportedly launches as many as 10,000 new SKUs per
day, typically with small initial batches of roughly 100 units, and many products have life cycles
of approximately 40 days (Kale 2023). In such environments, decisions must be made before suffi-
cient sales data have accumulated, even though the decision space is enormous. More broadly, data


--- Page 3 ---
3
limitations also arise in moderate-scale problems due to privacy constraints, short histories, or infre-
quent observations. These unique characteristics limit the applicability of traditional approaches
based on direct sample average approximation (SAA) and, typically, render them operationally
infeasible.
The pretrain-then-finetune idea, originally developed in LLM training, is well-suited to our target
problem, which is formulated as solving N data-driven stochastic optimization problems simultane-
ously, each governed by a parameterized distribution with unknown parameters. It proceeds in two
stages. First, pretraining leverages abundant raw or synthetic data and, as evidenced in LLM appli-
cations, can yield strong performance even when task-specific data are limited. In OM contexts,
it offers two particular advantages. On the one hand, it makes training feasible in settings where
proprietary business data are scarce, thereby enabling the use of expressive modern AI models. On
the other hand, it provides a disciplined way to inject domain knowledge into learning. For exam-
ple, one can construct synthetic data from managerial judgment, stylized models in the literature,
or empirical regularities from historical experience; more recently, generative models have further
expanded the ability to produce high-quality pretraining data tailored to problem structure (see,
e.g., Hollmann et al. 2025). Second, fine-tuning adapts the pretrained model to the specific opera-
tional environment using available real-world observations. Consistent with the small-data regime,
fine-tuning updates only a subset of parameters while keeping others fixed, enabling more pre-
cise adaptation while leveraging useful knowledge learned during pretraining. Taken together, the
two-stage procedure yields an approach that is both domain-informed and empirically grounded.
Operationally, we implement this pipeline using a tailored Transformer architecture that handles
variable-length problem instances and infers the latent parameters underlying each instance. These
parameter estimates are then mapped to the corresponding optimal decisions. To the best of our
knowledge, both the solution approach and the associated problem-solving tool are new to the OM
literature.
Although our pretrain-then-finetune approach is inspired by LLM training, it is not a direct
application of off-the-shelf LLM tools; Section 4 provides the details of our implementation and
highlights the key methodological design. Instead, we develop a compact, task-specific Transformer
tailored to the problem structure. In our framework, the Transformer primarily serves as a param-
eter estimator. During pretraining, it is trained in a supervised manner on synthetic datasets with
known labels that encode domain knowledge. In finetuning, however, supervision is unavailable
because the ground-truth parameters are unobserved. To address this challenge, we construct a
mean squared error (MSE)-equivalent objective using a generalized Stein identity, which yields a
tractable loss whose minimization aligns with minimizing the parameter estimation error. To fur-
ther accommodate the small-data regime, we adopt low-rank adaptation (LoRA; Hu et al. 2022) to


--- Page 4 ---
4
update only a selected subset of Transformer parameters, thereby reducing data requirements and
improving computational efficiency. Specifically, we emphasize that the Transformer is not intended
as a generic function approximator that could be readily replaced by standard deep neural networks
(DNNs). Its attention mechanism is particularly well suited to our setting because it enables the
model to aggregate information across heterogeneous instances and transfer learned structure to
new problems—capabilities that are central to the effectiveness of the pretrain–finetune paradigm
in our target problem.
What about theoretical guarantees, i.e., can we formally validate efficiency, build confidence for
practical deployment, and provide insights into understanding its performance? We provide a com-
prehensive error analysis, from the estimation error of the pretrained-then-finetuned Transformer
to the resulting decision quality, which compares the induced decisions with an oracle benchmark
that has prior access to the shared meta-information across tasks. This is, to our knowledge, the
first analysis to comprehensively characterize the error mechanisms of Transformer-based models in
the learning-theoretic literature. More broadly, despite extensive empirical evidence on how LLMs
perform well (see, e.g., Kaplan et al. 2020, Brown et al. 2020), theoretical understanding remains
very limited, especially given that these systems are substantially larger and more complex “black
boxes” than standard AI models such as DNNs. To this end, while our analysis does not aim to
explain the full behavior of modern LLMs, it provides principled insights into LLMs by focusing on
their core architectural primitive, i.e., the Transformer, and by establishing guarantees that link
representation learning to end-to-end decision performance.
The theoretical analysis in Section 5 begins with a general principle that decomposes the estima-
tion error into three different sources: (i) a pretraining domain gap, which captures the mismatch
between pretraining knowledge and the operational environment; (ii) a finetuning generalization
error, which quantifies how well rules learned from finite samples generalize to new instances; and
(iii) an approximation error, which reflects representational limitations of the model class. A key
insight is that either the domain gap or the generalization error can be the dominant contribu-
tor, highlighting the substantive interaction between pretraining and finetuning. Specifically, the
domain gap decreases as the distributional discrepancy between the pretraining data and the true
stochastic environment narrows. Importantly, the finetuning generalization error decreases with
problem scale, at a rate no slower than O(1/
√
N). This characterization yields a natural interpre-
tation: when domain knowledge is reliable but real data are scarce, the model’s output is primarily
guided by pretraining; as additional observations become available, finetuning increasingly aligns
the learned representations with practice. Even in the latter regime, effective pretraining remains
valuable by improving the efficiency and stability of finetuning and, consequently, the final decision
performance. Two additional implications are worth emphasizing. First, the generalization error


--- Page 5 ---
5
scales with the number of tasks N, which formally supports the ability of our approach to transfer
information across tasks in an automated manner. Second, the approximation error decreases with
model capacity, providing a theoretical rationale for the effectiveness of larger architectures when
abundant pretraining data are available. Finally, we balance these error components to guide net-
work design and to derive an excess-risk bound for the induced decisions. In summary, this new
comprehensive analysis provides theoretical guarantees for our approach and contributes to the
learning-theoretic understanding of Transformer models in data-driven decision problems.
Numerically, we evaluate the proposed approach on a multi-product newsvendor problem, using
it as a testbed to assess performance and to verify the theoretical insights on pretraining–finetuning
interaction developed in Section 5. We find: (1) Synthetic-data pretraining is essential for train-
ing large Transformer models when task-specific observations are scarce, and the quality of the
pretrained estimator depends critically on how well the decision maker’s domain knowledge aligns
with the true environment; (2) When this knowledge is sufficiently accurate, the pretrained model
already performs strongly, so additional finetuning on real data provides limited incremental bene-
fit; and (3) When domain knowledge is misspecified, finetuning becomes indispensable: by leverag-
ing cross-task transfer, the finetuned model gradually mitigates the bias inherited from pretraining.
Moreover, as the problem scale grows, finetuning becomes increasingly effective, and the result-
ing decisions approach the oracle benchmark. These qualitative patterns remain robust in various
data-generating settings.
2.
Literature Review
This paper contributes to the development of data-driven AI methodologies for operations manage-
ment, with a particular focus on state-of-the-art Transformer-based techniques inspired by recent
advances in LLMs. Our work relates to two primary streams of the literature.
The first stream studies how machine learning and data can be leveraged to support OM deci-
sions. This literature has expanded rapidly in recent years, driven by the growing availability of data
and advances in algorithms and computational resources. A central theme is to extract decision-
relevant information from historical observations and translate it into prescriptive policies for the
underlying operational problem. Two broad regimes are commonly distinguished: “big-data” and
“small-data” settings. In the big-data regime, one can approximate the system’s stochasticity using
the empirical distribution, so the sample size for each decision problem must be sufficiently large
to ensure statistical accuracy. For example, Levi et al. (2015) analyze SAA for the data-driven
newsvendor problem and establish finite-sample performance guarantees; Lin et al. (2022) extend
this line of analysis in a more general framework. Beyond SAA, several papers incorporate addi-
tional information structures. Ban and Rudin (2019) show that contextual information can improve


--- Page 6 ---
6
inventory decisions, and Han et al. (2025) demonstrate that DNNs can further enhance such perfor-
mance. Chen et al. (2023) learn customer preferences from transaction data to design assortment
and pricing decisions for new customers. More broadly, Bertsimas and Kallus (2020), Chen et al.
(2025), and Elmachtoub et al. (2023) study general stochastic optimization problems with predic-
tive components. A complementary set of papers investigates the consequences of misalignment
between estimation and optimization: Siegel and Wagner (2021, 2023) investigate potential bias
when learning objectives are not well aligned with optimization objectives, and Iyengar et al.
(2023) propose a statistical criterion that can be used to detect and correct such bias in general
data-driven optimization frameworks.
In the small-data regime, each problem instance is associated with only a limited number of obser-
vations, even though the total data volume aggregated across instances can be substantial (Miˇsi´c
and Perakis 2020). This setting is common and important in practice, particularly in platforms
and long-tail retail environments (Miao et al. 2022). The key challenge is that the per-instance
sample size is too small to support a reliable empirical approximation of the underlying uncer-
tainty for each individual optimization problem. Gupta and Rusmevichientong (2021) study linear
optimization under small-data constraints and propose a Bayes-regularized policy with theoretical
guarantees. Gupta and Kallus (2022) show that effective data pooling via Shrunk-SAA can improve
general performance. Bastani et al. (2022) examine pricing with sequential product offerings and
demonstrate how meta-learning can be used to transfer knowledge across products. In the context
of low-sale items on Alibaba, Miao et al. (2022) propose a clustering-based pricing algorithm that
pools data within clusters to improve learning and decision quality. Feng et al. (2023) leverage
structural relationships between data and optimal solutions to facilitate transfer learning across
problems, and Feng et al. (2025) further illustrate how operational data analytics can enhance
performance in contextual newsvendor problems.
Our work is motivated largely by the small-data regime, but the proposed pretrain–finetune
framework can also accommodate settings with richer per-instance data in the big-data regime.
Pretraining enables the use of expressive models when real observations are limited by providing
a strong representation learned from abundant domain-informed data, whereas finetuning adapts
that representation to the operational environment using the available real samples. The extent of
fine-tuning (i.e., the subset of parameters that are updated) can be chosen to reflect data avail-
ability, thereby keeping the approach effective even when proprietary business data are scarce.
Consistent with this logic, we show both theoretically and numerically that pretraining can help
improve performance, particularly in data-limited settings. On the other hand, when each problem
instance is supported by sufficiently many observations (with the number of instances fixed), the


--- Page 7 ---
7
setting returns to the big-data regime in which decisions can be learned accurately from the empir-
ical information available for each instance. Overall, our framework provides a unified approach
that performs well across both regimes.
Conceptually, our work introduces a new LLM-inspired pretrain–then-finetune approach for data-
driven stochastic optimization. Rather than deploying off-the-shelf LLMs, we distill the key training
principle behind their success and tailor it to our decision-making setting, with accompanying
theoretical guarantees. This perspective differs from Huang et al. (2025), which focuses on building
open-source LLMs for automated optimization modeling and solution. Several recent studies also
draw on Transformer architectures in OM. Wang et al. (2023) develop a Transformer model to
learn customers’ multiple-choice behavior. Cao and Bayati (2024) emphasize the role of human
interaction in AI-enabled systems. Lu and Kannan (2025) introduce a transformer-based framework
to capture customer interactions at both population and individual levels and demonstrate its
value for targeting. Hu and Simchi-Levi (2025) show that pretrained AI models can be used to
impute missing covariates and improve decision-making, and Aouad et al. (2025) study alignment
in LLM-based systems in the presence of heterogeneous human preferences. Our problem setting
differs substantially from the studies above, and our focus is on how domain-knowledge-guided
pretraining and practice-driven fine-tuning interact to jointly contribute to effective learning and
optimization. From this perspective, we develop a new methodology grounded in a comprehensive
theoretical analysis that clarifies when and why the approach performs well, yielding insights that
extend to Transformer-based learning paradigms, including LLMs.
Our work also contributes to another stream of literature on learning theory, focusing on devel-
oping theoretical foundations for modern AI models. Despite the striking empirical success of LLMs
across domains, rigorous theoretical understanding of these systems, and even of their core archi-
tectural primitive, i.e., the Transformer, remains limited. A related body of work studies neural-
network learning through the lens of statistical learning theory, characterizing generalization using
tools from empirical process theory (Anthony and Bartlett 1999, Mohri et al. 2018). More recently,
Farrell et al. (2021) establish nonasymptotic bounds for DNNs and analyze second-stage inference
based on the first-step estimation via DNNs. Along this line, Ye et al. (2025) combine deep learning
with doubly robust estimation for causal inference in large-scale experiments and show statistical
efficiency, consistency, and asymptotic normality. Jiao et al. (2023) derive minimax-optimal rates
for least-squares regression with DNNs and show that intrinsic low-dimensional structure can mit-
igate the curse of dimensionality. Han et al. (2025) provide explicit performance guarantees for an
end-to-end data-driven newsvendor policy based on deep quantile estimation. Wang et al. (2024)
show that overparameterized neural representations can effectively learn choice models. Cao (2024)


--- Page 8 ---
8
develops conformal methods with finite-sample guarantees to address model misspecification in
data-driven decision-making.
Within this stream, to the best of our knowledge, we provide the first comprehensive theoreti-
cal analysis of Transformer-based pretrain-then-finetune learning in relevant settings. The analysis
and resulting insights differ significantly from existing theory for standard DNNs. Much of the
DNN literature treats the network primarily as a generic function approximator. In contrast, in
our framework, the Transformer’s role extends beyond approximation: it is embedded in an LLM-
inspired pretrain–then-finetune pipeline tailored to small-data, large-scale optimization. Specifi-
cally, pretraining incorporates domain knowledge and supports feasible training even when real
data are scarce, while finetuning adapts the model to the operational environment by updating
only a targeted subset of parameters, which is both data-efficient and well aligned with the small-
data regime. From this standpoint, the pretrain–then-finetune approach itself is central, and the
Transformer provides an effective computational vehicle for implementing it. Moreover, our theory
explicitly characterizes the interaction between pretraining and fine-tuning, a feature absent from
prior analyses and that informs the broader understanding of error behavior in Transformer-based
models, such as LLMs. Finally, we quantify how architectural features unique to Transformers,
notably the attention mechanism, enter the error decomposition and shape performance through
the domain-gap, generalization, and approximation terms. These results, and the accompanying
proof techniques, are new to the literature.
3.
Problem Formulation
In this paper, we study a small-data, large-scale setting, in which the number of unknown parameter
sets is large while the data available to estimate each individual set is limited (Miˇsi´c and Perakis
2020). Specifically, we consider a firm that must handle a collection of N stochastic optimization
problems simultaneously. For instance, the firm may need to make inventory and pricing decisions
simultaneously for a large set of products facing random demand (Mukherjee et al. 2015, Miao et al.
2022). For each problem i ∈{1,...,N}, the firm chooses a decision x and receives a random payoff
ci(x,ξi), where ξi denotes the random outcome. We assume that ξi follows a parametric distribution
governed by an unknown parameter vector θi ∈Rp. Therefore, the resulting optimization problems
are
arg max
xi∈Xi vi(xi,θi) := Eξi∼Pθi[ci(xi,ξi)],
for i = 1,2,··· ,N,
(1)
where Pθi denotes the probability measure governing the random variable ξi, and its functional
form can be flexible; and Xi denotes the feasible decision set for task i. While our method and
analysis allow for heterogeneity across different problems, we henceforth adopt a common objective
c, feasible set X, and value function v for notational convenience.


--- Page 9 ---
9
Although the aggregate amount of data can be substantial when the problem scale N is large, the
number of unknown parameters also grows linearly with N. As a result, traditional approaches that
estimate and optimize each problem in isolation can perform poorly in the small-data regime. At the
same time, the task-specific parameters may exhibit shared structure at the population level, which
can be leveraged to pool information across tasks and improve both estimation and decision quality
(Gupta and Kallus 2022). To capture and exploit such cross-task connections, we adopt an empirical
Bayes (EB) framework (Efron 2019) and assume that the parameters θ1,··· ,θN are independently
and identically distributed (i.i.d.) draws from an unknown distribution G, i.e., θi
i.i.d.
∼G for i =
1,··· ,N. This assumption is mild and flexible. Practically, it reflects the notion that demand
patterns across a large product catalog can be viewed as realizations of an underlying market-
level distribution that summarizes common structure, even though that distribution is unobserved.
Technically, the EB formulation imposes minimal structure while providing a principled basis for
effective transfer learning across tasks.
To implement the optimization in (1), the firm needs to infer the task-specific parameters from
the available observations. To emphasize the small-data nature of the problem, we assume that the
firm can access only a single data point for each task when estimating its unknown parameter. This
assumption is made without loss of generality and can readily be extended to the case of finitely
many observations per task. Specifically, for each i = 1,...,N, the firm observes di ∈Rp, which we
model as a noisy measurement of the unknown parameter θi, capturing measurement error and
other sources of deviation. In other words, the observations follow the hierarchical model
di|θi
i.i.d.
∼N(θi,σ2Ip),
θi ∼G,
for i = 1,··· ,N,
(2)
where N denotes the normal distribution, σ (assumed known) captures the observation noise level
(and thus precision), and Ip is the p × p identity matrix. As before, we impose a common σ to
streamline notation; accommodating task-specific observation quality is straightforward. We want
to highlight that this nonparametric formulation is flexible as it allows the shared prior G to be
arbitrary rather than restricting it to a parametric family, so the induced marginal distribution fG
for the observations remains sufficiently rich to capture a broad class of data-generating processes.
Effective data-driven decision-making in this setting requires pooling information across tasks
and extracting shared structure from the set of observations. Under the EB formulation, this
transfer learning process is naturally interpreted as learning the unknown prior distribution G. To
remain broadly applicable, it is desirable to estimate G without imposing restrictive parametric
assumptions, thereby allowing the procedure to adapt flexibly and automatically across differ-
ent application contexts. A classical approach is nonparametric maximum likelihood estimation


--- Page 10 ---
10
(NPMLE); see, e.g., Jiang and Zhang (2009), Soloff et al. (2025). However, NPMLE can be com-
putationally burdensome in high dimensions. More importantly, in small-data regimes where each
task contributes only a few and potentially noisy observations, purely data-driven transfer may be
statistically fragile, in part because the learning process begins with no informative structure at
initialization. Motivated by the success of Transformer-based LLMs, such as the GPT series, which
are pretrained on web-scale corpora, we propose to incorporate domain knowledge into the learning
and optimization pipeline via a pretraining stage. Crucially, pretraining can be conducted using
synthetic data, thereby avoiding reliance on scarce proprietary observations while encoding struc-
tural information derived from managerial judgment, stylized models, or prior experience. Starting
from this domain-informed initialization, we then finetune the model on real data to adapt it to
the operational environment, balancing prior structure with empirical alignment. In the limiting
case where domain knowledge perfectly matches practice, finetuning makes little or no adjustment.
Conversely, when domain knowledge is imperfect, performance can still improve as additional real
data accumulate and finetuning becomes more informative. We describe the proposed LLM-inspired
methodology in Section 4, develop theoretical guarantees in Section 5, and validate the approach
numerically in Section 6.
4.
A New Pretrain-Then-Finetune Paradigm Inspired by LLM
To solve the target problem, we adopt a two-step estimate-then-optimize (ETO) procedure. In
large-scale settings, ETO offers greater flexibility than integrated estimate-and-optimize methods
for applying estimation to various downstream tasks. Once the unknown parameters are esti-
mated, each stochastic optimization problem reduces to a deterministic one and can then be solved
independently. Importantly, although the optimization step is task-specific, the estimation stage
incorporates cross-task information sharing; as a result, transfer learning effects induced during
estimation naturally carry over to downstream decisions. Accordingly, we focus this section on
the estimation component of the procedure. We adopt an LLM-inspired pretrain-then-finetune
paradigm to obtain efficient and scalable parameter estimates; see Section 4.2 for details.
Our method builds on a designed Transformer model, the core architectural primitive underlying
modern LLMs. Compared with standard architectures such as DNNs and recurrent neural networks
(RNNs), Transformers offer advantages that extend beyond generic function approximation and
are particularly well aligned with the pretrain–finetune paradigm. On one hand, they have demon-
strated strong generalization and transferability in practice: after pretraining on massive raw data
(e.g., web-scale text corpora), an LLM can often perform well on previously unseen tasks through
prompting alone, even without updating model parameters (Von Oswald et al. 2023). On the other
hand, Transformers can be adapted effectively to new tasks using comparatively small amounts


--- Page 11 ---
11
of task-specific data and computation (Ding et al. 2023), yielding substantial improvements in
downstream performance. This capability is further enhanced by parameter-efficient finetuning
methods such as low-rank adaptation (LoRA; Hu et al. 2022), which update only a selected subset
of parameters while keeping the remainder fixed, thereby reducing both data requirements and
computational cost. These properties are particularly attractive in the small-data, large-scale opti-
mization setting we study. We next introduce the Transformer architecture used in our framework
in Section 4.1.
4.1.
Preliminary: Transformer Architecture
At a high level, the Transformer is a foundational neural architecture that maps input sequences to
output representations by learning attention-based dependencies across elements. In our context,
we design a specific encoder-only Transformer, illustrated in Figure 1.1 The model takes as input a
sequence of data whose length may vary and outputs corresponding parameter estimates. Formally,
we can view the Transformer as a mapping from observations D = (d1,··· ,dN) ∈(Rp)N to esti-
mates ˆθ = (ˆθ1,..., ˆθN) ∈(Rp)N, denoted by TTF(·;W) : (Rp)N →(Rp)N, where W ∈Φ represents the
model parameters and Φ denotes the parameter space. As depicted in Figure 1, our Transformer
architecture consists of three main modules, arranged in data-flow order: a pre-processing block,
an encoder block, and an output head. Drawing an analogy to natural language processing (NLP),
each observation di can be viewed as a token in the input sequence, analogous to a word token in
text. We describe each component of the architecture in detail below.
Pre-Processing Block. This block prepares the raw input sequence for the encoder, and it con-
sists of three major steps; see Figure 1. The first is an embedding layer. In NLP, embedding layers
map discrete tokens to continuous vectors, enabling their processing within a unified framework.
Analogously, in our setting, each observation di is passed through an embedding network, a mul-
tilayer perceptron (MLP) with ReLU activations, to produce an embedding ˜di ∈Rpemb. We denote
the depth (number of hidden layers) and width (maximum number of neurons per hidden layer) of
this MLP by Demb and Wemb, respectively. Conceptually, this step extracts lower-dimensional latent
representations that facilitate subsequent representation learning in the encoder. The resulting
embeddings are then processed sequentially by a normalization layer (referred to as CenterNorm
in Figure 1) and a radical clipping layer that enforces uniform bounds. These operations improve
numerical stability during training and serve an additional role in the theoretical analysis by reg-
ularizing the embedding space; in particular, they help ensure that the Transformer’s output is
Lipschitz continuous with respect to its input (see the online appendix for details).
1 Many Transformer-based variants, such as the GPT family, employ decoder-only or encoder–decoder architectures,
where the decoder is primarily designed for autoregressive content generation. By contrast, the encoder is designed
to process input data and produce representations that capture latent structure. Because our objective is parameter
estimation rather than sequence generation, an encoder-only architecture is particularly well suited to our application.


--- Page 12 ---
12
Figure 1
The architecture of our adopted Transformer model.
Encoder Block. The encoder block implements the self-attention mechanism, which is the defin-
ing architectural feature of Transformers. Its role is to learn representations from input embeddings
that capture shared structure and cross-token interactions, including interactions across tasks. In
NLP, the term “attention” refers to relevance weights assigned to other words in a sentence when
handling a given word. Analogously, in our setting, attention weights quantify how information from
other tasks contributes to estimating the parameter of the currently focused task. This mechanism
thus provides an explicit and quantitative channel for transfer learning: embeddings deemed more
relevant receive higher attention weights and exert greater influence on the resulting representation
and parameter estimate.
Definition 1 (Self-Attention Mechanism). For matrices WQ,WK ∈Rpemb×pK and WV ∈
Rpemb×pV and an embedding sequence ˜D := ( ˜d1,..., ˜dN)⊤∈RN×pemb, the single-head self-attention
mapping f is defined as
f( ˜D) = softmax
QK⊤
√pK

V ∈RN×pV ,
(3)
where Q := ˜DWQ,K := ˜DWK ∈RN×pK, and V := ˜DWV ∈RN×pV ; the softmax function softmax(·)
is applied to each row of QK⊤/√pK, i.e., the (i,j)-th entry of softmax
 QK⊤/√pK

is
ωij :=
exp
 QiK⊤
j /√pK

PN
j′=1 exp

QiK⊤
j′/√pK
,
(4)


--- Page 13 ---
13
where Qi, Kj, and Vj denote the i-th, j-th, and j-th rows of Q, K, and V , respectively. We further
define the i-th row of f( ˜D) as fi( ˜D) = PN
j=1 ωijVj, and refer to the matrices WQ,WK, and WV as
the parameters of the attention mechanism.
From Definition 1 and the right panel of Figure 1, each embedding element ˜di enters the attention
mechanism in three distinct roles:
• Query. When task i is the focal instance, ˜di is mapped to a query vector used to compare
against all other embeddings. Specifically, in (3), the i-th row of the query matrix Q := ˜DWQ
represents the query associated with task i, and the parameter matrix WQ governs how each
embedding is transformed into a query representation.
• Key. Each embedding ˜di also serves as a comparison object when processing other embeddings
˜dj for j ̸= i. In (3), the j-th row of the key matrix K := ˜DWK represents the key associated with
task j, and the parameter matrix WK aggregates the corresponding effect via keys. The attention
mechanism then evaluates the similarity between the focal task i and every task j ∈{1,...,N} via
the dot product, i.e., QiK⊤
j , and applies a row-wise softmax normalization to obtain the attention
weights ωi = (ωi1,ωi2,...,ωiN)⊤in (4).
• Value. Values carry information aggregated according to attention weights. Specifically, the j-
th row of the value matrix V := ˜DWV represents the value representation for task j, parameterized
by WV . The resulting representation for task i in the encoder block is the weighted sum fi( ˜D) =
PN
j=1 ωijVj, where ωij, computed from the query-key similarities, quantifies the contribution of the
task j’s value Vj to the representation of task i.
Compared with alternative AI models such as DNNs and RNNs, attention mechanisms have been
shown to deliver strong performance, particularly in capturing long-range dependencies in LLM
applications. This capability is especially valuable in large-scale settings, where each instance may
benefit from leveraging information dispersed across a large collection of related tasks.
Beyond single-head attention, an extended multi-head attention mechanism can be employed,
in which multiple attention heads operate in parallel. Each head can be viewed as a separate
self-attention module that attends to the input sequence through a distinct projection, and the
collection of heads enables the model to capture heterogeneous interaction patterns among input
instances (Lu and Kannan 2025). The head-specific representations are then concatenated to form
the final encoder representation, thereby enhancing the ability to model diverse dependencies. We
defer the formal definition of multi-head attention to the online appendix. Finally, at the end of the
encoder block in Figure 1, we apply an additional normalization layer to further stabilize training.
Output Head. The last component in Figure 1 is an output head implemented as an MLP
with ReLU activations, of depth Doh and width Woh. It produces the final point estimates ˆθ =
(ˆθ1,..., ˆθN) ∈Rp×N. The output head serves two roles. First, the ReLU nonlinearities increase the


--- Page 14 ---
14
model’s expressive capacity. Second, the MLP maps the encoder’s latent representations into the
parameter space, ensuring dimensional consistency and yielding interpretable parameter estimates.
4.2.
Our Pretrain-Then-Finetune Method
The central difficulty in the considered small-data, large-scale problem (1) is estimating a large
set of task-specific parameters simultaneously from only sparse observations per task. This chal-
lenge is particularly acute for machine learning tools, which typically require substantial data to
train reliably from scratch. To address this issue, we adopt an LLM-inspired pretrain-then-finetune
strategy, illustrated in Figure 2. We first pretrain the model on a large corpus of relevant historical
data (if available) and on domain-informed synthetic data designed to encode structural knowledge
about the application. During pretraining, the Transformer learns generic latent representations
that embed domain expertise and provide an informative initialization for subsequent estimation.
In the fine-tuning stage, we adapt the pretrained model to the target environment using limited
real-world observations. To remain effective in the small-data regime, we fix most parameters and
update only a subset, thereby reducing both data requirements and computational cost while
improving alignment with the task distribution. Importantly, neither pretraining nor finetuning is
a direct application of standard routines. We introduce several problem-specific design choices to
tailor the procedure to our setting, as detailed below.
Figure 2
Illustration of our pretrain-then-finetune framework.
4.2.1.
Supervised Pretraining. As described above, the Transformer is pretrained on a
collection of synthetic datasets of size K, denoted by {Dpre
k }K
k=1. Each pretraining instance Dpre
k
:=
(dk,1,...,dk,Nk) is a sequence that may vary in length Nk. Without loss of generality, we assume
that the pretraining data are generated according to
dk,i ∼f(· | θpre
k,i ),
θpre
k,i ∼Gpre,
for k = 1,...,K and i = 1,··· ,Nk.


--- Page 15 ---
15
In other words, the pretraining data follow the same hierarchical structure as in (2). There are,
however, two important distinctions. First, the latent parameters θpre
k
:= (θpre
k,1,··· ,θpre
k,Nk) are known
for the pretraining datasets, which enables supervised learning. Second, the prior distribution
Gpre is specified by the firm and reflects domain knowledge about the problem context. This
prior may incorporate managerial experience, insights from theoretical models in the literature, or
empirical regularities drawn from related historical settings. As we show theoretically in Section 5.1,
informative domain knowledge can substantially improve performance.
During pretraining, each sequence Dpre
k
is fed into the Transformer, which outputs the corre-
sponding parameter estimates
TTF(Dpre
k ;W) = ˆθpre
k
= (ˆθpre
k,1,··· , ˆθpre
k,Nk),
where W denotes the model parameters. The pretraining objective minimizes the mean squared
error (MSE) between the model’s outputs and the corresponding true labels θpre
k , i.e.,
W0 := arg min
W ∈Φ
Lpre(TTF) :=
K
X
k=1
∥TTF(Dpre
k ;W) −θpre
k ∥
2 ,
(5)
where ∥· ∥denotes the ℓ2 norm and Φ is the parameter space. We denote the resulting pretrained
model by TTF(·;W0).
4.2.2.
Label-Free Finetuning. Although pretraining can leverage abundant data, it serves
only as an initialization for the estimation tasks of interest. We therefore finetune the pretrained
model using the observed task-specific dataset D = (d1,...,dN). Unlike pretraining, however, fine-
tuning faces a fundamental challenge: ground-truth labels (i.e., the true parameter vectors) are
unavailable in real-world applications, so a direct supervised loss is also unavailable. Concretely,
when D is fed into TTF and the model outputs the estimates TTF(D) = (ˆθ1,..., ˆθN), the corre-
sponding true parameters θ1,...,θN are unavailable. Consequently, standard comparison-based
objectives, such as MSE, cannot be computed to guide fine-tuning.
To address the absence of labels and obtain a tractable finetuning objective, we invoke a gen-
eralized Stein identity to derive a loss function that eliminates dependence on the unobserved
parameters. Specifically, the MSE of any candidate finetuning model TTF can be rewritten as
L(TTF) := Eθ∼G,d∼f(·|θ)

∥TTF(d) −θ∥2 Stein’s
identity
=
Ed∼fG

∥TTF(d)∥2 −2d⊤TTF(d) + 2σ2∇· TTF(d)

|
{z
}
Lfinetune(TTF)
,
(6)
where ∇· TTF denotes the divergence of the mapping TTF. The proof of the equality is provided in
the online appendix. We denote the right-hand side by Lfinetune(TTF). This objective depends only


--- Page 16 ---
16
on the observed data and the model outputs, is directly computable, and is exactly equivalent to
the MSE. To this end, the finetuning stage proceeds by empirical risk minimization (ERM) based
on the expected loss in (6). Specifically, we solve
arg min
∆W :W0+∆W ∈Φ
ˆLfinetune
N
(TTF) := 1
N
N
X
i=1

∥TTF(di)∥2 −2d⊤
i TTF(di) + 2σ2∇· TTF(di)

.
(7)
It is worth emphasizing that, given the pretrained parameters W0, finetuning retrains only a small
subset of the Transformer parameters through ∆W. Specifically, we adopt low-rank adaptation
(LoRA), a widely used parameter-efficient finetuning approach in LLMs. As we briefly describe
below, LoRA incorporates two key ideas that make finetuning feasible in the small-data regime.
First, adapting a pretrained Transformer does not require updating all components. Instead,
one can finetune only selected modules, such as the embedding layer, the output head, or a sub-
set of weights within the attention block, while keeping the majority of the model fixed. In our
experiments, for example, updating only the embedding layers and the output head is sufficient,
with the attention block left frozen. This finding suggests that attention patterns learned during
pretraining on synthetic data already capture transferable structure that remains valuable when
adapting to real-world tasks. Second, even for modules that are fine-tuned, LoRA restricts updates
to a low-rank modification of the pretrained weights. To illustrate, with a slight abuse of notation,
let W0 ∈Rdout×din denote the pretrained weight matrix of a module selected for finetuning (e.g., a
weight matrix in the output head), where din and dout are its input and output dimensions. LoRA
writes the adapted weight matrix as
W =
W0
|{z}
pretrained model
+
∆W
|{z}
finetuning adaptation
,
(8)
with
∆W = BA,
where B ∈Rdout×r and A ∈Rr×din. Importantly, here r ≪min{dout,din} is a prescribed rank, typ-
ically chosen to be small in practice. Finetuning updates only A and B, while the pretrained
backbone W0 remains fixed. This parameterization constrains the update to a low-dimensional
subspace: the number of trainable parameters is 2r(dout + din), rather than doutdin. Consequently,
the model learns an effective adaptation direction without re-optimizing the entire weight matrix,
substantially reducing both computational burden and the amount of real data required.


--- Page 17 ---
17
5.
Theoretical Guarantees for Our Transformer-Based Learning and
Optimization
In this section, we establish theoretical guarantees for our pretrain-then-finetune approach intro-
duced in Section 4. To the best of our knowledge, our results provide the first comprehensive the-
oretical analysis of Transformer learning in relevant settings and offer a principled account of why
the pretrain–finetune paradigm can be effective in LLM-style pipelines. More broadly, the analysis
clarifies when and why Transformer-based models perform well, with implications for LLMs built
on the same architectural backbone. Specifically, Section 5.1 presents a general decomposition of
the estimation error, yielding high-level insights into when pretraining is critical and when finetun-
ing alone can be sufficient. Section 5.2 then bounds each error component and characterizes how
they depend on key problem and model primitives, including the number of tasks, architectural
features of the Transformer, and the quality of domain knowledge encoded in pretraining. Finally,
Section 5.3 connects estimation error to decision performance under the Transformer-based ETO
framework for (1).
5.1.
High-Level Insights: When Pretraining is Critical and When Finetuning
Suffices
Estimation accuracy is central to downstream decision quality: under the ETO framework, esti-
mation error propagates directly into the eventual decision error. To evaluate the LLM-inspired
estimation method introduced in the previous section, we first present a high-level error decom-
position that yields clear insights into the respective roles of pretraining and finetuning. We first
introduce several common performance metrics. For a generic parameter estimator T (·) : Rp →Rp
that maps an observation d ∈Rp to an estimate of the corresponding task parameter, we measure
estimation performance by the MSE:
L(T ) := Ed|θ∼f(·|θ), θ∼G

∥T (d) −θ∥2
.
(9)
That is, the MSE is evaluated on a new, independent draw from the hierarchical model in (2). In
what follows, when the context is clear, we will omit subscripts in the expectation operator E for
brevity. If the prior G were known, the oracle benchmark that minimizes (9) would be the posterior
mean, i.e., T ∗(d) := E[θ|d]. Accordingly, for any data-driven estimator T constructed from the
available data D, we define its estimation error as the excess MSE relative to the oracle:
Eest(T ) := ED[L(T ) −L(T ∗)] = EDEd∼fG

∥T (d) −T ∗(d)∥2
,
(10)
where the second equality follows from the orthogonality principle for the posterior mean T ∗.
The outer expectation over D accounts for sampling variability: since D is random, the resulting
estimator is random as well.


--- Page 18 ---
18
Figure 3
Overview of our pretraining–finetuning error analysis framework.
Let ˆTTF denote the Transformer estimator obtained by pretraining on the synthetic datasets and
subsequently finetuning on the real sample D. Intuitively, its estimation error may stem from (i)
the finite expressive power of the chosen Transformer architecture, (ii) statistical error induced by
finetuning with limited and noisy observations, and (iii) potential mismatch between the pretraining
distribution and the true data-generating environment. To formalize these effects and highlight
the interaction between pretraining and finetuning, we decompose the estimation error into three
components: an approximation error, a finetuning generalization error, and a domain gap; see
Figure 3 for an illustrative overview.
The first source of error arises from the model’s finite representational capacity. For a given
architecture, the Transformer ranges only over a restricted hypothesis class mapping data to esti-
mates (the shaded ellipse in Figure 3), defined as H := {TTF(·;W) : W ∈Φ}. The oracle mapping
T ∗needs not belong to H, and this misspecification induces an approximation error defined as2
Eapprox := L(T ∗
H) −L(T ∗),
(11)
where T ∗
H := arg minT ∈H L(T ) is the best-in-class estimator within H. As illustrated in Figure 3,
T ∗
H can be interpreted as the projection of the target mapping T ∗onto the hypothesis space H, and
Eapprox quantifies the resulting discrepancy. As we discuss in Section 5.2, the approximation error
is governed by the Transformer’s representational capacity, including the depth and width of the
2 For complete rigor, there is also an approximation error in the pretraining stage relative to the pretraining target
T ∗
pre (introduced in (12)), as shown in Figure 3. However, T ∗and T ∗
pre differ only through the prior distribution
used in the corresponding Bayes estimators and thus share the same functional form. As a result, the approximation
analysis developed below applies to either target with only notational changes. For clarity, we only state definitions
and results for Eapprox with respect to T ∗.


--- Page 19 ---
19
MLP components and the dimensions of the attention mechanism (e.g., pK and pV ). All else equal,
larger models typically yield greater expressiveness and therefore smaller approximation error.
The second source of error is the finetuning generalization error. It is incurred because the
finetuned estimator ˆTTF is obtained by minimizing the empirical objective ˆLfinetune
N
(TTF) on the
finite sample D (see (7)), rather than the population loss Lfinetune(TTF) in (6). Sampling noise in D
can therefore lead ˆTTF to deviate from the population-optimal solution within the model class. We
quantify this effect by the gap between the expected and empirical finetuning objectives evaluated
at the learned estimator:
Egene := ED[Lfinetune( ˆTTF) −ˆLfinetune
N
( ˆTTF)].
Here, the expectation is taken over the randomness in D since both ˆLfinetune
N
and ˆTTF depend on
the realized sample. As N increases, the empirical objective concentrates around its population
counterpart, and Egene is expected to decrease. We provide a detailed analysis in Section 5.2.
The third component, central and new to our LLM-inspired approach, captures the quality of
pretraining. Recall that in Section 4.2, we pretrain the Transformer on synthetic datasets to obtain
an informative initialization for the real tasks. Because domain knowledge is imperfect, the pre-
training marginal distribution fGpre used in this stage may differ from the true data-generating
distribution fG. Consequently, pretraining can steer the model toward a different target mapping
(illustrated in Figure 3), namely the Bayes oracle associated with the pretraining distribution rather
than the oracle for the true environment. This phenomenon mirrors a well-known limitation of
general-purpose LLMs: when the pretraining distribution is poorly aligned with a specialized task
domain, e.g., short-horizon stock-price prediction, performance can be unreliable because the pre-
trained representations may be systematically miscalibrated for the target setting. Mathematically,
the target mapping induced by the pretraining stage is
T ∗
pre := arg min
T
Ed′|θ′∼f(·|θ′), θ′∼Gpre
∥T (d′) −θ′∥2
.
(12)
This population objective corresponds to the supervised pretraining loss in (5) in Section 4.2.1,
where we know the latent parameters θ′. In general, T ∗
pre may not coincide with the target oracle
T ∗, unless Gpre = G. We refer to the resulting discrepancy as the domain gap, defined by
Edomain := L(T ∗
pre) −L(T ∗) = Ed∼fG

∥T ∗
pre(d) −T ∗(d)∥2
,
where the final equality again follows from the orthogonality principle for T ∗.
Taken together, these three components jointly determine estimation accuracy and interact in
nontrivial ways, as formalized in the following Proposition 1. Before that, we introduce two mild
technical conditions that ensure that the estimation problem is well-defined.


--- Page 20 ---
20
Assumption 1. Let L(TTF(·;W)) denote the MSE of TTF with parameter set W. We assume that
L is differentiable with respect to W at the pretrained parameter W0 defined in (5).
Assumption 2. Let g(x) and gpre(x) be the probability densities of G and Gpre, respectively.
There exist constants c1,c2,c3 > 0 such that, for all d ∈Rp satisfying ∥d∥> c1, the densities satisfy
g(d),gpre(d) ≤c2e−c3∥d∥2.
Assumption 1 is mild. Training Transformer and other AI models is typically carried out via
gradient-based optimization, for which differentiability of the loss with respect to the model param-
eters is a standard regularity condition. Assumption 2 imposes mild tail regularity on both G and
Gpre. It covers common cases in theory and practice, including priors with compact support and,
more generally, distributions with tails no heavier than sub-Gaussian (Fu et al. 2024).
Proposition 1 (Pretraining-Finetuning Interaction). Suppose Assumption 1 holds. The
estimation error satisfies
Eest ≤min

Egene
|{z}
Finetuning
,
Edomain
| {z }
Pretraining
	
+ Eapprox.
(13)
Proposition 1 provides the high-level insights into how pretraining and finetuning jointly shape
estimation accuracy. The approximation error constrained by the chosen Transformer architecture
may persist regardless of the training pipeline. Conditional on this architectural limit, however,
the pretrain-then-finetune approach can effectively leverage whichever signal is stronger: a small
finetuning generalization error or a small domain gap. In other words, the bound in (13) is driven
by the better of the two. This mechanism highlights an important advantage over purely empirical
approaches that rely exclusively on task-specific real data. Such methods are governed solely by the
finetuning component in (13) and cannot exploit additional information available through relevant
historical data or structured domain knowledge. By contrast, the pretrain–finetune paradigm can
incorporate these sources through pretraining, a practice central to the success of modern LLMs.
In small-data decision environments, where real observations are scarce and costly, Proposition 1
implies that informative pretraining can even dominate finetuning and materially improve esti-
mation accuracy. Our numerical results in Section 6 further suggest that pretraining can improve
performance even when the finetuning protocol is held fixed, indicating benefits beyond the con-
servative “either-or” structure captured by (13).
Proposition 1 also yields a transparent regime interpretation. When real data are scarce—much
as when one queries an LLM for a specialized task with limited supporting information—the
estimator must rely primarily on the pretrained model and thus on the domain knowledge encoded
during pretraining. In this regime, the finetuning generalization error can exceed the domain gap,


--- Page 21 ---
21
so the estimation error is driven mainly by the latter. The approximation error can be made
small with sufficient model capacity; we formalize this in Theorem 3. As shown in Section 5.2, the
fine-tuning generalization error decreases as N increases. Intuitively, transfer learning exhibits an
economy of scale in the number of tasks: as the task portfolio grows, the model can pool more cross-
task information, improving adaptation and reducing finetuning error. Because the domain gap is
fixed for a given pretraining distribution, sufficiently informative finetuning data will eventually
drive the finetuning error below the domain gap, at which point the overall estimation error is
primarily governed by finetuning. Consequently, even when pretraining is poorly aligned with the
target environment (i.e., domain knowledge is limited), the proposed method becomes increasingly
effective as more real data accrue and can still deliver strong performance guarantees by learning
directly from operational observations. More broadly, because our approach is directly inspired
by LLM training pipelines, the quantitative theory developed here, though tailored to a small-
data optimization setting, offers a principled lens on pretrain–finetune error mechanisms and helps
understand when and why such paradigms perform well, including in Transformer-based LLMs.
5.2.
Pretraining, Finetuning, and Approximation Error Analysis
We begin by analyzing the domain gap, a component that is unique to the pretrain–finetune
pipeline. The next theorem quantifies this gap in terms of the distributional mismatch between the
true marginal fG and the pretraining marginal fGpre, which encodes the decision maker’s domain
knowledge. We measure this mismatch using the Hellinger distance, H2(fG,fGpre) = 1
2
R
Rp
 p
fG(y)−
p
fGpre(y)
2dy, which satisfies 0 ≤H(fG,fGpre) ≤1 by definition.
Theorem 1 (Domain Gap). Suppose Assumption 2 holds. Then there exists a constant C > 0
such that
Edomain ≤CH2 fG,fGpreh
1 −log

H(fG,fGpre)
imax{3,p}
.
(14)
Theorem 1 formalizes the intuition that more accurate domain knowledge, reflected in a smaller
discrepancy between fGpre and fG, leads to a smaller domain gap. In the idealized case where
the pretraining and target distributions coincide, the domain gap vanishes; by Proposition 1, fine-
tuning becomes unnecessary, and the pretrained estimator is already oracle-optimal. Even when
pretraining is imperfect, the resulting initialization can still provide a useful starting point and
yield informative estimates, particularly in regimes where finetuning is constrained by limited effec-
tive real data. Importantly, misaligned domain knowledge does not preclude strong performance:
as task-specific data accumulate, the finetuning generalization error decreases, and the overall esti-
mator improves accordingly. This perspective also underscores a practical limitation of a single
universal pretrained model for all optimization domains: without adequate alignment, pretraining
alone cannot guarantee optimality across heterogeneous settings. Nevertheless, pretraining remains


--- Page 22 ---
22
valuable because it enables the decision maker to leverage large, domain-informed synthetic or his-
torical corpora and substantial computational resources to build representations that meaningfully
enhance performance in the small-data, large-scale regime.
Building on the decomposition in Proposition 1, we next study the finetuning generalization
error Egene for the pretrained Transformer estimator ˆTTF. To quantify how finetuning performance
depends on the key ingredients of our pipeline, including the LoRA settings, the randomness in the
real dataset, and the structure of the label-free finetuning objective (see Section 4.2.2 for details),
we adopt an information-theoretic perspective inspired by Xu and Raginsky (2017) and Russo and
Zou (2019). Our results are novel in that they provide quantitative generalization guarantees for
Transformer-based fine-tuning, which is the core architectural component of LLMs; to the best of
our knowledge, this is the first comprehensive treatment along these lines. Generally speaking, the
information-theoretic approach relates generalization error to the mutual information between the
model’s inputs and outputs. When this mutual information is small, the model’s outputs depend
less on the idiosyncratic features of a particular dataset realization, thereby implying stronger
out-of-sample generalization.
To establish our fine-tuning generalization bound, we require the following assumption (Assump-
tion 3). In particular, a sub-Gaussian tail condition for the marginal fG follows from Assumption 2;
however, the resulting sub-Gaussian parameter can be cumbersome to track explicitly, so we state
it directly for convenience. We make a similar simplification by assuming that the coordinates of
d ∈Rp are independent. This independence condition is not essential: the analysis readily extends
to dependent coordinates with only minor modifications; see the online appendix.
Assumption 3. The marginal distribution fG is σ2
d-sub-Gaussian. In addition, the coordinates
of d ∈Rp are independent of each other.
Theorem 2 (Finetuning Generalization Error). Suppose Assumption 3 holds. Consider a
finetuning procedure in which a collection of weight matrices {W (ℓ) ∈Rd(ℓ)
out×d(ℓ)
in }ℓ∈S is updated via
the LoRA scheme introduced in Section 4, where S denotes the set of layer indices selected for
finetuning. Let {rℓ}ℓ∈S be the corresponding retained ranks (cf. (8)), and define the total number
of finetuned parameters as dfinetuned := P
ℓ∈S rℓ
 d(ℓ)
in + d(ℓ)
out

. Then, there exists an absolute constant
C > 0 such that the finetuning generalization error satisfies
Egene ≤CLTF

σ2 + σ2
d[LTF + eC/(L4
TFσ4
d)]

min
(r
dfinetuned
N
, dfinetuned + 1
N
)
,
(15)
where the Transformer mapping is Lipschitz continuous with respect to the input, with Lipschitz
constant denoted by LTF (an explicit expression is provided in the online appendix.


--- Page 23 ---
23
The proof is provided in the online appendix. Theorem 2 advances the existing literature by
explicitly characterizing how key design choices in LoRA finetuning shape the generalization behav-
ior of Transformer models. In particular, the retained low ranks {rℓ}ℓ∈S and the selection of finetun-
ing layers S serve as central hyperparameters: reducing either the ranks or the size of S decreases
the number of trainable parameters and, consequently, tightens the generalization bound. At the
same time, if the retained rank is chosen too small or the finetuning set S is overly restricted, the
LoRA adaptation space may fail to contain descent directions that deliver nontrivial improvements,
limiting the effectiveness of finetuning.
A further technical contribution underlying Theorem 2 concerns the Lipschitz continuity of the
Transformer mapping. Standard Transformer architectures are not Lipschitz by default, due in
part to the instability of self-attention blocks and the use of normalization layers (Kim et al.
2021). Yet Lipschitz continuity plays a critical role in both training stability (Qi et al. 2023)
and theoretical analysis. To address this, we introduce two strategic architectural modifications:
a Radical Clipping layer placed before the self-attention block and the replacement of standard
normalization with CenterNorm; see Section 4.1 and the online appendix for details. Importantly,
Theorem 2 shows that the finetuning generalization error essentially decays at a rate no slower
than O(1/
√
N). Thus, even in the small-data, large-scale regime, increasing the number of task
instances N yields more accurate estimates, bringing the fine-tuned model closer to the best-in-
class solution within the Transformer hypothesis space; see Figure 3. This result highlights a key
advantage of our approach: although each task instance may provide only limited information, the
method effectively aggregates shared structure across a large collection of instances to improve
individual decision-making.
We finally analyze the remaining component in Proposition 1, namely the approximation error
Eapprox. Unlike canonical AI models that use only fully connected feedforward networks, Transform-
ers incorporate structural modules, most notably the attention mechanism (see Section 4.1), which
are essential to their empirical success. These design features also make approximation analysis
substantially more delicate, and existing results for standard DNNs (e.g., Han et al. 2025) do not
directly apply. Recall that our object of interest is the oracle mapping T ∗that takes an input
sequence of observations and returns an output sequence of parameter estimates, for arbitrary
sequence length τ ∈N+. To study its approximability, we decompose this sequence-to-sequence
mapping into interpretable segments that can be approximated separately. The following lemma
motivates this decomposition and, importantly, mirrors the Transformer’s modular structure.
Lemma 1 (Kolmogorov-Arnold Representation Theorem Kolmogorov 1957). On
a
compact Ω⊂Rp, let T ∗: Ωτ →(Rp)τ map the input sequence d = (d(1),...,d(τ)) to T ∗(d) =


--- Page 24 ---
24
(T ∗
1 (d),··· ,T ∗
τ (d)), and denote the k-th coordinate of T ∗
i
as T ∗(k)
i
for k = 1,...,p. Then, for
each k ∈{1,··· ,p}, there exist continuous functions F ∗∈C([0,1]2τp+1,R), f ∗∈C(Ω,[0,1]2τp+1),
and ρ∗∈C(Ω× Ω,R) (depending on the chosen k, with the dependence suppressed for notational
simplicity) such that, for every d ∈Ωτ and each i ∈{1,··· ,τ},
T ∗(k)
i
(d) = F ∗
 
τ
X
j=1
softmax

ρ∗ d(i),d(·)

(j)f ∗ d(j)

!
.
(16)
We now explain how the representation in (16) can be interpreted through the lens of the
Transformer architecture introduced in Section 4.1. For each entry i = 1,··· ,τ, the data point d(i)
plays the role of the query, and each d(j) serves as a key against which the query is compared.
The resulting attention weight assigned to token j is given by softmax

ρ∗ d(i),d(·)

(j) in (16).
The corresponding value is f ∗(d(j)), and the weighted aggregation over j forms an attention-style
representation for position i. Finally, F ∗acts as an output head that maps this representation to the
k-th coordinate of the estimate. Lemma 1 therefore motivates a modular approximation strategy:
approximate ρ∗, f ∗, and F ∗separately, in a manner that mirrors the Transformer’s query–key–value
structure. That said, the lemma is not directly applicable to our setting because it is stated on
a compact domain, whereas our Transformer mapping is defined on an unbounded support. To
proceed, we introduce two auxiliary definitions that characterize the function class to which the
target mapping belongs.
Definition 2 (Spectral Complexity). For any square integrable function ρ∗∈C(Ω×Ω,R),
its proper orthogonal decomposition (see the precise definition in Berkooz et al. 1993) admits the
expansion ρ∗(u,v) = P
i≥1 λiφ∗
i (u)ψ∗
i (v), where λ1 ≥λ2 ≥··· ≥0 are the coefficients and {φ∗
i } and
{ψ∗
i } are orthonormal bases in L2(Ω). Fix α > 0 and for any T ∗having representations of the form
(16), we define its spectral complexity by CS(T ∗,α) := infF ∗,f∗,ρ∗inf{c > 0 : λs(ρ∗) ≤cs−α,∀s ≥1},
where the outer infimum ranges over all admissible triples (F ∗,f ∗,ρ∗) that represent T ∗in (16).
Definition 3. For β,B > 0 and a domain Ω⊂Rp, the H¨older function class Hβ(Ω,B) is defined
as
Hβ(Ω,B) :=
n
f : Ω→R

max
∥γ∥1≤⌊β⌋∥∂γf∥∞≤B,
max
∥γ∥1=⌊β⌋sup
x̸=y
|∂γf(x) −∂γf(y)|
∥x −y∥β−⌊β⌋
≤B
o
,
where γ = (γ1,...,γp)⊤denotes a multi-index of nonnegative integers with ∥γ∥1 := Pp
i=1 γi, and
∂γ := ∂γ1
1 ···∂
γp
p
denotes the corresponding mixed partial derivative.
Assumption 4. (i) For some α > 1, the target mapping T ∗has finite α-spectral complexity,
i.e., CS(T ∗,α) < ∞; (ii) In the representation (16) of T ∗, ρ∗is square integrable. Moreover, the
functions F ∗and f ∗in (16), as well as the basis functions {φ∗
s,ψ∗
s}s≥1 appearing in the proper
orthogonal decomposition of ρ∗in Definition 2, belong to the H¨older function class Hβ(Ω,B) on
their respective domains for some β,B > 0 (with Ωallowed to vary across functions). The F ∗is
Lipschitz continuous on its domain.


--- Page 25 ---
25
Assumption 4 imposes standard regularity conditions on the target mapping T ∗, in the sense
that it specifies a broad smoothness property rather than enforcing a restrictive parametric form.
In particular, for a wide class of regular mappings, one can identify constants (e.g., the spectral
complexity α) under which Assumption 4 holds, making it a mild requirement. At the same time,
such assumptions are necessary because any approximation guarantee must depend on baseline
properties of the target mapping in order to quantify how well it can be approximated and to control
the resulting approximation error. Such regularity conditions are standard in the approximation
analysis of state-of-the-art AI models (see, e.g., Jiang and Li 2024). Under Assumption 4, we next
establish an approximation error bound. We use the notation a ≍b to indicate equality up to
universal constant factors, i.e., a ≍b if there exist constants 0 < c′ ≤c′′ < ∞such that c′b ≤a ≤c′′b.
Theorem 3 (Approximation Error). Suppose Assumptions 2 and 4 hold. Consider a Trans-
former architecture as described in Section 4.1 with the following components: (i) an embedding
block implemented as an MLP with width Wemb ≍(⌊β⌋+ 1)23pp⌊β⌋+2Nemb⌈log2(8Nemb)⌉and depth
Demb ≍(⌊β⌋+ 1)2Memb⌈log2(8Memb)⌉+ p, for any Memb,Nemb ∈N+; (ii) attention heads with key
and value dimensions pK and pV ≥2p + 1 (see Definition 1); and (iii) an output head imple-
mented as an MLP with width Woh ≍(⌊β⌋+1)23pV p⌊β⌋+1
V
pNoh⌈log2(8Noh)⌉and depth Doh ≍(⌊β⌋+
1)2Moh⌈log2(8Moh)⌉+pV , for any Moh,Noh ∈N+, together with the radical clipping (RC) and Cen-
terNorm layers in the pre-processing block (see the definitions in the online appendix). Then the
approximation error satisfies

Eapprox
1/2 ≤C
h
p⌊β⌋+(β∨1)/2
V
(NohMoh)
−2β
pV + p−(α−1)
K
+ (2pK + pV )
1
2 (NembMemb)−2β
p
i
,
(17)
where C is a constant depending only on the regularity of the target mapping (e.g., its spectral
complexity).
Theorem 3 shows that increasing model capacity through, for example, deeper or wider embed-
ding and output-head networks, or larger attention dimensions pK and pV , typically reduces the
approximation error Eapprox. This pattern is consistent with observations in other AI tools, such
as DNNs. At the same time, obtaining an explicit, architecture-aware approximation bound for
Transformers is technically nontrivial and, to our knowledge, has not previously been available in
this form. Because Transformers constitute the core building block of modern LLMs, Theorem 3
also helps clarify why large models are empirically effective: greater capacity reduces approxi-
mation error, but exploiting that capacity requires sufficient training data. Without pretraining,
enlarging the model entails a sharper trade-off: while approximation error decreases, the finetuning
generalization error can deteriorate as model scale grows (see Theorem 2). Pretraining mitigates
this tension by leveraging abundant synthetic or historical data to fit rich representations before


--- Page 26 ---
26
exposure to scarce task-specific observations. Our framework inherits the same advantage: pre-
training enables domain-knowledge-guided representation learning without being constrained by
limited real data, and finetuning subsequently adapts the pretrained estimator to the operational
environment while exploiting the transferable structure acquired during pretraining.
5.3.
From Estimation Error to Optimization Performance: Excess-Risk Bounds
With the parameter estimates produced by our Transformer model, we can then solve the optimiza-
tion tasks in (1) simultaneously. In particular, cross-task information sharing is already encoded
in these estimates, so each task reduces to a deterministic problem conditional on its estimated
parameter. Specifically, for any θ, let x(θ) := arg maxx∈X v(x,θ) denote the optimal decision asso-
ciated with parameter value θ. Under the ETO framework, we focus on the decision quality of the
resulting data-driven solutions. For this purpose, we use an excess risk criterion that benchmarks
the induced policy against an oracle with access to the true prior. Given the observed data D and
the pretrained-and-finetuned Transformer estimator ˆTTF, consider a fresh test instance d generated
according to (2). The resulting decision is x( ˆTTF(d)). In contrast, the oracle estimator is T ∗, induc-
ing the oracle decision x∗(d) := x(T ∗(d)). We define the excess risk as the expected performance
loss relative to the oracle:
Eexcess := EDEd,θ

v
 x∗(d),θ

−v
 x( ˆTTF(d)),θ

,
(18)
where the inner expectation Eθ,d is taken with respect to the hierarchical model in (2), and the
outer expectation ED averages over the randomness in ˆTTF, which depends on D.
Assumption 5. For any θ, the optimal decision x(θ) uniquely exists and is Lx-Lipschitz con-
tinuous in θ; meanwhile, the value function v(x,θ) is Lv-Lipschitz continuous in x ∈X.
Assumption 5 is mild and is commonly satisfied in OM tasks; see two illustrative examples
below. The existing ETO literature usually assumes that the objective is twice continuously dif-
ferentiable in the model parameters, together with other regularity conditions (e.g., Iyengar et al.
2023, Elmachtoub et al. 2023), which is technically comparable to Assumption 5.
Example 1 (Large-Scale Newsvendor). Consider a firm that manages inventories for a
large collection of products indexed i = 1,...,N. Let the decision vector be x = (x1,··· ,xN) ∈X,
where xi denotes the order quantity for product i. The random demand vector is d = (d1,...,dN),
with each component distributed as di ∼N(θi,σ2). The expected newsvendor loss for product i is
given by v(xi,θi) = E[bi(di −xi)+ + hi(xi −di)+], where bi and hi represent the unit shortage and
holding costs, respectively. This formulation is similar to the setting in Mukherjee et al. (2015). The
optimal order quantity for each product admits a closed-form solution, xi(θi) = θi + σΦ−1(
bi
bi+hi ),
where Φ−1(·) is the inverse cumulative distribution function of the standard normal distribution.
It follows immediately that xi(θi) is Lipschitz continuous in θi.


--- Page 27 ---
27
Example 2 (Large-Scale Pricing). Consider a firm that sells a large range of products and
must simultaneously determine pricing decisions. For illustrative purposes, suppose the random
demand for product i follows di ∼N(θi −νixi,σ2), where θi represents the market size and νi > 0 is
a known price-sensitivity coefficient. The individual expected revenue is then v(xi,θi) = E[xidi] =
 xiθi −νix2
i

. Maximizing this objective yields the optimal price for each product, xi(θi) = θi/(2νi),
which is Lipschitz continuous in θi.
We conclude by analyzing the excess risk in the following theorem. The key trade-off is between
the finetuning generalization error in Theorem 2, which typically increases with model complexity,
and the approximation error in Theorem 3, which decreases as the model becomes richer. By
selecting a network configuration that balances these two effects, we obtain the best achievable
convergence rate as a function of the number of simultaneous tasks, N, thereby quantifying the
efficiency of cross-task transfer. This small-data, large-scale perspective differs from the classical
big-data regime, in which accuracy improves primarily by increasing the number of observations per
task. The optimal model size also depends on which Transformer modules are adapted via LoRA. A
recent highly influential empirical study by Schulman and Lab (2025) reports that applying LoRA
to attention layers delivers little incremental benefit beyond finetuning the MLP components (i.e.,
the embedding layer and output head). Our experiments in the online appendix are consistent with
this finding. Accordingly, the theorem below focuses on the setting in which LoRA finetuning is
applied only to the embedding layer and output head, while the attention block remains frozen.
Theorem 4 (Excess Risk). Suppose that Assumptions 1-5 hold, and that LoRA with rank r
is applied to all layers in the embedding block and the output head. A rate-optimal choice of model
size is given by
NembMemb ≍N
p
p+8β (pK)
2p
p+8β ,
NohMoh ≍N
pV
pV +8β ,
pK ≍N
2β
(pV +8β)(α−1) ,
with pV = max{2p+1, α−1/2
α−1 p}. Under this design, up to logarithmic factors, the excess risk satisfies
Eexcess ≤CLvLx

min

Cgene
√rN
−
4β
pV +8β ,Edomain
	
+ p2⌊β⌋+(β∨1)
V
N
−
4β
pV +8β

.
(19)
where Cgene = LTF

σ2 + σ2
d[LTF + eC/(L4
TFσ4
d)]

, and the constant C > 0 depends on the complexity of
the target mapping. The domain gap Edomain is bounded as in (14).
This result demonstrates that our approach effectively addresses the large-scale, small-data opti-
mization setting and exhibits an economy-of-scale effect: performance improves as the number
of tasks N increases. Under the rate-optimal model configuration, it attains a convergence rate
of O(N
−
4β
pV +8β ) convergence rate, with pV = max{2τp + 1,
α
α−1p}. The rate is faster for smoother
targets (larger β) and for targets of lower spectral complexity (larger α).


--- Page 28 ---
28
6.
Simulation Studies
In this section, we conduct simulation experiments to examine the effectiveness of the proposed
pretrain-finetune paradigm. Beyond documenting numerical performance, our primary objective is
to verify the theoretical insights on the interaction between pretraining and finetuning developed
in Section 5. We focus on three implications. First, when real observations are scarce, pretraining
is critical for enabling the effective training of large Transformer models, and the quality of the
resulting initialization depends on the distributional proximity between the pretraining distribution
and the true data-generating distribution, i.e., on the accuracy of the decision maker’s domain
knowledge. Second, when this domain knowledge is well aligned with practice, the pretrained model
alone may be sufficient, and additional fine-tuning offers little incremental value. Third, even under
misspecified domain knowledge, the proposed framework can still address the small-data, large-
scale regime by transferring information across many instances, thereby reducing excess risk as
the number of tasks N increases. Because our focus is the pretrain–finetune mechanism and given
space constraints, we leave empirical evaluations on real-world datasets to future work.
For illustration, we consider the large-scale multi-product newsvendor setting in Example 1.
There are N products, and for each product i the decision maker observes a single demand real-
ization di ∼N(θi,1), where θi ∈R. We set bi = hi = 2. Based on these observations, the decision
maker selects an order quantity for each product. In Section 6.1, to make the domain gap easier to
quantify, we first study a simple setting in which the oracle prior for θi follows a Gaussian mixture
model, while the decision maker’s domain knowledge is represented by a misspecified Gaussian mix-
ture with possibly shifted component locations. In Section 6.2, we extend to richer data-generating
processes for θi, spanning both parametric families and flexible nonparametric specifications, to
assess the robustness of the numerical patterns.
Transformer Model Architecture. Our Transformer uses an embedding dimension of pemb = 64,
and the multi-head self-attention module uses 8 attention heads. The embedding block contains two
layers, and the output head comprises 24 layers, yielding approximately 5×104 trainable parameters
in total. All experiments were run on a workstation equipped with four NVIDIA GeForce RTX
4090 GPUs (24GB each).
6.1.
Pretrain-Finetune Interaction
In what follows, we first quantify how the pretrained model’s performance varies with the accuracy
of the decision maker’s domain knowledge. We then examine how finetuning on real observations
improves pretrained models across different levels of domain-knowledge quality. Finally, we study
the role of the problem scale N, which in our setting is the effective finetuning sample size.
Target and pretraining distributions. We take the task parameters to follow a three-component
Gaussian mixture, i.e., θi ∼G, where G = 1
3N(1,1) + 1
3N(3,1) + 1
3N(4,1). The decision maker’s


--- Page 29 ---
29
domain knowledge could be imperfect and posits that demand is concentrated around three loca-
tions ˜θ1, ˜θ2, ˜θ3. Accordingly, the pretraining prior is specified as Gpre = P3
j=1
1
3N(˜θj,1). To emulate
heterogeneous levels of domain-knowledge accuracy, we construct 15 pretrained models by varying
(˜θ1, ˜θ2, ˜θ3), drawing each ˜θj independently from Unif[0,5]. Closed-form expressions for the Hellinger
distance or KL divergence between Gaussian mixtures are generally unavailable. Nevertheless,
as noted by Goldberger et al. (2003), when mixture weights and component variances coincide,
the discrepancy between two mixtures can be upper bounded by the ℓ2 distance between their
component-mean vectors. We therefore use
(˜θ1, ˜θ2, ˜θ3) −(1,3,4)
 as a tractable surrogate for the
distributional distance between the pretraining distribution and the target distribution.
Pretraining and finetuning setup. In pretraining, each synthetic data sequence Dpre
k
:=
(dk,1,...,dk,512) has fixed length 512, with k indexing different sequences. We pretrain all models
with Adam (batch size 32) for 1000 iterations. This yields approximately 512×32×1000 ≈1.6×107
synthetic observations with orders of magnitude more than the real sample, thereby providing a
stable and meaningful initialization for the Transformer training. We then finetune each pretrained
model on N = 500 real observations generated from the target distribution. For LoRA, we set the
retained rank to r = 8, freeze all attention-module parameters, and update only the embedding and
output-head components, amounting to roughly 6 × 103 trainable parameters. Finetuning is run
for 10 epochs. To motivate these design choices and offer practical guidance, we conduct extensive
ablation studies in the online appendix. The results indicate that r = 8, and often even smaller
ranks, suffices for effective adaptation in our setting. Moreover, finetuning only the embedding and
output-head modules delivers essentially the full performance gains, with no clear benefit from
finetuning all components, consistent with Schulman and Lab (2025).
Figure 4
Excess risk of pretrained-only, pretrained-finetuned, and finetuned-from-scratch models versus the
distributional distance between the pretraining and target distributions.


--- Page 30 ---
30
Effect of domain knowledge accuracy. Figure 4 reports the excess risk for the 15 pretrained models
and their finetuned counterparts. We estimate excess risk by averaging realized performance over
500 independent test instances drawn from the target distribution. The horizontal axis measures
the distributional distance between the target and pretraining distributions; moving from left to
right corresponds to less accurate domain knowledge. Here, “Pretrained + Finetuned” refer to
models that are pretrained on synthetic datasets of varying quality and then finetuned using the
real observations, whereas “Pretrained” evaluates the same models immediately after pretraining,
without any finetuning. We also include a “Finetuned from scratch” baseline, which trains an
unpretrained Transformer (initialized with PyTorch defaults) using only the real data. Since this
baseline does not depend on the pretraining distribution, it appears as a horizontal line in Figure 4.
Several interesting patterns emerge in Figure 4. First, as predicted by Theorem 1 and Proposi-
tion 1, the pretrained-only estimator degrades as the pretraining distribution moves farther from
the target distribution, implying that weaker domain knowledge translates into higher excess risk.
Second, once fine-tuned on real observations, all models achieve uniformly low excess risk, largely
independent of pretraining quality. This insensitivity suggests that, after finetuning, the gener-
alization component dominates the decomposition in Proposition 1, so residual variation in the
domain gap plays a limited role in realized performance. Interestingly, when the distance between
the pretraining and target distributions is below 2, pretraining alone is often sufficient as the
pretrained-only model performs comparably to, and even better than, the finetuned version. This
regime is consistent with Proposition 1, indicating that sufficiently accurate domain knowledge
can render additional adaptation unnecessary. Lastly, the “finetuned-from-scratch” baseline per-
forms markedly worse than the pretrained-and-finetuned models. The gap underscores the practical
importance of pretraining even when domain knowledge is imperfect: the available real data are
too limited to reliably train a large Transformer from random initialization, whereas pretrain-
ing supplies a well-structured starting point that makes subsequent finetuning both feasible and
effective.
Effect of the number of problem instances N. We next examine how performance varies with the
finetuning sample size N. We select four pretrained models whose pretraining distributions have
different distances to the target distribution (specifically, 1.41, 3.60, 4.12, and 4.58). For each model,
we vary the finetuning sample size from N = 10 to N = 700 and finetune accordingly. Figure 5
reports the excess risk of the pretrained-only models and their pretrained–finetuned counterparts.
Across all four configurations, the pretrained-only models exhibit consistently higher excess risk,
whereas the pretrained–finetuned models improve steadily and their excess risk declines as N
increases. This pattern aligns with Theorem 4 and demonstrates the value of the proposed approach
in large-scale regimes, where a larger pool of instances strengthens the transfer-learning effect.


--- Page 31 ---
31
Figure 5
Excess risk versus finetuning data size N under four pretraining configurations.
Meanwhile, Theorem 3 implies an irreducible approximation component determined by model
capacity; accordingly, in our experiments, the excess risk does not fully disappear even at the largest
N, plausibly reflecting residual approximation error from the finite Transformer architecture, which
may not represent the oracle estimator exactly.
6.2.
Robustness to More Complex Target Distributions
We further assess the robustness of the preceding findings in more challenging environments. To
better reflect practical demand heterogeneity, we consider a range of data-generating processes for
θi, including both simple parametric families and flexible nonparametric specifications. In partic-
ular, we include:
• Exponential distribution: θi follows an exponential distribution with mean 5, which exhibits
a heavier right tail than the Gaussian-mixture case.
• Dirichlet process (DP) with Uniform base: We consider a DP prior with concentration param-
eter αDP = 1 and base distribution Unif[0,5]. Equivalently, θ1,...,θN are generated according to:
θj =





θi,
with probability
j −1
αDP + j −1,
i ∈{1,...,j −1},
x ∼Unif[0,5],
with probability
αDP
αDP + j −1.


--- Page 32 ---
32
This construction induces flexible distributions for θi that can depart substantially from standard
parametric families.
• Dirichlet process with Gaussian base: A DP prior with αDP = 1 and a Gaussian base distribu-
tion N(2.5,1).
• Neural distribution: Following Teh et al. (2025), we also consider a highly flexible, nonparamet-
ric data-generating mechanism constructed from randomly initialized neural networks. Specifically,
we sample four two-layer networks, each mapping R4 to R, with weights initialized using PyTorch’s
default scheme. For each hidden layer, the activation function is drawn uniformly from GELU, ReLU,
SELU, CELU, SiLU, and TanhShrink, and the final layer uses a sigmoid activation. To generate θi, we
draw an input vector uniformly from [0,1]4, select one of the four networks uniformly at random,
and take its scalar output as the realization of θi.
Figure 6
Excess risk of the pretrained-only and pretrained-finetuned models versus the finetuning data size N
across different target distributions.
Across all four target distributions, we endow the decision maker with domain knowledge of the
same form as in Section 6.1, namely a three-component Gaussian-mixture pretraining distribution,
1
3N(1,1) + 1
3N(3,1) + 1
3N(4,1). We pretrain the Transformer on this distribution using the same


--- Page 33 ---
33
pretraining protocol as in Section 6.1, and then finetune the resulting model on task-specific obser-
vations drawn from the target environment. We again vary the number of instances from N = 50
to N = 1000 and track the resulting excess risk. Figure 6 reports the performance across target
distributions; for reference, we also report Monte Carlo estimates of the Hellinger distance between
each target distribution and the pretraining distribution.
Consistent with Theorem 4 and the evidence in Section 6.1, the proposed approach performs
robustly across all distributional settings: the excess risk of the pretrained–finetuned estimator
declines as the number of instances N increases, demonstrating effective learning in the large-scale,
small-data regime. Moreover, for the exponential and Dirichlet-process targets with a Uniform
base measure, the pretrained-only estimator can outperform its fine-tuned counterpart when N
is extremely small. This phenomenon suggests that, with only a handful of task-specific observa-
tions, the structure transferred via pretraining may be more informative than adapting the model
using highly scarce data. As N grows, however, finetuning rapidly becomes beneficial and the
pretrained–finetuned model overtakes the pretrained-only baseline.
The advantage of pretraining followed by finetuning is most pronounced under the Neural distri-
bution, which embodies a highly complex, nonparametric data-generating process. In this setting,
the pretrained–finetuned estimator improves rapidly with only a modest amount of real data and
substantially outperforms the pretrained-only baseline. The reason is that the three-component
Gaussian-mixture prior used for pretraining, intended to encode the decision maker’s domain
knowledge, does not capture the salient structure of the Neural target distribution. Consequently,
strong performance must be driven primarily by information learned from task-specific observations
through finetuning. This finding highlights the value of the proposed pretrain–finetune framework
when domain knowledge is misspecified.
7.
Conclusion
In this paper, we propose to solve the small-data, large-scale stochastic optimization problems
using an LLM-inspired pretrain-then-finetune approach. We implement the idea in a Transformer
model designed for this purpose, which is naturally compatible with the pretrain-finetune pipeline
and leverages the attention mechanism to deliver state-of-the-art representational capacity. On the
theory side, we develop, to the best of our knowledge, the first comprehensive error analysis for
Transformer learning in this decision-centric setting. The results quantify how performance depends
on (i) the domain gap induced by mismatch between the pretraining distribution and the opera-
tional environment, (ii) the finetuning sample size, which in our formulation scales with the number
of tasks and captures an economics-of-scale effect in transfer learning, and (iii) architectural choices
that govern approximation capacity. The analysis also clarifies the interaction between pretraining


--- Page 34 ---
34
and finetuning: when effective real data are scarce, domain-knowledge-guided pretraining can be the
primary driver of accuracy, whereas with sufficiently many task instances, finetuning progressively
dominates and corrects pretraining bias. Beyond their implications for small-data optimization,
these insights contribute to a principled understanding of why pretrain-finetune pipelines can be
effective more broadly. Looking ahead, we view the proposed framework as a step toward small,
specialized models for operational decision problems that can leverage both domain knowledge and
limited proprietary data. Promising directions for future research include evaluating the approach
on large-scale field datasets and developing principled, operationally grounded procedures for con-
structing domain-informed pretraining distributions in practice.
References
Anderson C, Nissley C, Anderson C (2006) The Long Tail (Hyperion Audiobooks).
Anthony M, Bartlett P (1999) Neural Network Learning: Theoretical Foundations (Cambridge).
Aouad A, El Gadarri A, Farias VF (2025) The sign estimator: LLM alignment in the face of choice hetero-
geneity. arXiv e-prints arXiv–2510.
Ban GY, Rudin C (2019) The big data newsvendor: Practical insights from machine learning. Oper. Res.
67(1):90–108.
Bastani H, Simchi-Levi D, Zhu R (2022) Meta dynamic pricing: Transfer learning across experiments. Man-
agement Sci. 68(3):1865–1881.
Berkooz G, Holmes P, Lumley JL (1993) The proper orthogonal decomposition in the analysis of turbulent
flows. Annu. Rev. Fluid Mech. 25(1):539–575.
Bertsimas D, Kallus N (2020) From predictive to prescriptive analytics. Management Sci. 66(3):1025–1044.
Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G,
Askell A, et al. (2020) Language models are few-shot learners. Adv. Neural Inform. Processing Systems
33:1877–1901.
Cao
J
(2024)
A
conformal
approach
to
feature-based
newsvendor
under
model
misspecification.
arXiv:2412.13159 .
Cao J, Bayati M (2024) A probabilistic approach for model alignment with human comparisons.
arXiv:2403.10771 .
Chen N, Cire AA, Hu M, Lagzi S (2023) Model-free assortment pricing with transaction data. Management
Sci. 69(10):5830–5847.
Chen N, Lagzi S, Milner J (2025) Using neural networks to guide data-driven operational decisions. Man-
agement Sci. forthcoming.
Devlin J, Chang MW, Lee K, Toutanova K (2019) Bert: Pre-training of deep bidirectional transformers for
language understanding. Proc. NAACL-HLT 2019, volume 1 (long and short papers), 4171–4186.


--- Page 35 ---
35
Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, Hu S, Chen Y, Chan CM, Chen W, et al. (2023) Parameter-
efficient fine-tuning of large-scale pre-trained language models. Nat. Mach. Intell. 5(3):220–235.
Efron B (2019) Bayes, oracle Bayes and empirical Bayes. Statist. Sci. 34(2):177–201.
Elmachtoub
AN,
Lam
H,
Zhang
H,
Zhao
Y
(2023)
Estimate-then-optimize
versus
integrated-
estimation-optimization versus sample average approximation: a stochastic dominance perspective.
arXiv:2304.06833 .
Farrell MH, Liang T, Misra S (2021) Deep neural networks for estimation and inference. Econometrica
89(1):181–213.
Feng Q, Li L, Shanthikumar JG (2023) Transfer learning, cross learning and co-learning across newsvendor
systems with operational data analytics (ODA). Available at SSRN 4629760 .
Feng Q, Shanthikumar JG, Wu J (2025) Contextual data-integrated newsvendor solution with operational
data analytics (ODA). Management Sci. .
Floridi L, Chiriatti M (2020) GPT-3: Its nature, scope, limits, and consequences. Minds Mach. 30:681–694.
Fu H, Yang Z, Wang M, Chen M (2024) Unveil conditional diffusion models with classifier-free guidance: A
sharp statistical theory. arXiv:2403.11968 .
Goldberger J, Gordon S, Greenspan H (2003) An efficient image similarity measure based on approximations
of KL-divergence between two Gaussian mixtures. Proc. IEEE Int. Conf. Comput. Vis., 487–493.
Gupta V, Kallus N (2022) Data pooling in stochastic optimization. Management Sci. 68(3):1595–1615.
Gupta V, Rusmevichientong P (2021) Small-data, large-scale linear optimization with uncertain objectives.
Management Sci. 67(1):220–241.
Han J, Hu M, Shen G (2025) Deep neural newsvendor. Management Sci. forthcoming.
Hollmann N, M¨uller S, Purucker L, Krishnakumar A, K¨orfer M, Hoo SB, Schirrmeister RT, Hutter F (2025)
Accurate predictions on small data with a tabular foundation model. Nature 637(8045):319–326.
Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W, et al. (2022) LoRA: Low-rank
adaptation of large language models. ICLR 1(2):3.
Hu H, Simchi-Levi D (2025) Pre-trained AI model assisted online decision-making under missing covariates:
A theoretical perspective. arXiv:2507.07852 .
Huang C, Tang Z, Hu S, Jiang R, Zheng X, Ge D, Wang B, Wang Z (2025) ORLM: A customizable framework
in training large models for automated optimization modeling. Oper. Res. forthcoming.
Iyengar G, Lam H, Wang T (2023) Optimizer’s information criterion: Dissecting and correcting bias in
data-driven optimization. arXiv:2306.10081 .
Jiang H, Li Q (2024) Approximation rate of the transformer architecture for sequence modeling. Advances
in Neural Information Processing Systems 37:68926–68955.


--- Page 36 ---
36
Jiang W, Zhang CH (2009) General maximum likelihood empirical Bayes estimation of normal means. Ann.
Stat. 1647–1684.
Jiao Y, Shen G, Lin Y, Huang J (2023) Deep nonparametric regression on approximate manifolds: Nonasymp-
totic error bounds with polynomial prefactors. Ann. Stat. 51(2):691–716.
Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, ˇZ´ıdek
A, Potapenko A, et al. (2021) Highly accurate protein structure prediction with alphafold. Nature
596(7873):583–589.
Kale V (2023) How Shein decoded the supply chain of fast fashion e-commerce? URL https://medium.com/
@vedantkale/how-shein-decoded-the-supply-chain-of-fast-fashion-e-commerce-011d3e4ffee0,
accessed: July 19, 2025.
Kaplan J, McCandlish S, Henighan T, Brown TB, Chess B, Child R, Gray S, Radford A, Wu J, Amodei D
(2020) Scaling laws for neural language models. arXiv:2001.08361 .
Kim H, Papamakarios G, Mnih A (2021) The lipschitz constant of self-attention. ICML, 5562–5571 (PMLR).
Kolmogorov AN (1957) On the representations of continuous functions of many variables by superposition
of continuous functions of one variable and addition. Dokl. Akad. Nauk USSR, volume 114, 953–956.
Levi R, Perakis G, Uichanco J (2015) The data-driven newsvendor problem: New bounds and insights. Oper.
Res. 63(6):1294–1306.
Lin M, Huh WT, Krishnan H, Uichanco J (2022) Data-driven newsvendor problem: Performance of the
sample average approximation. Oper. Res. 70(4):1996–2012.
Liu A, Feng B, Xue B, Wang B, Wu B, Lu C, Zhao C, Deng C, Zhang C, Ruan C, et al. (2024) Deepseek-v3
technical report. arXiv:2412.19437 .
Lu Z, Kannan P (2025) AI for customer journeys: A transformer approach. J. Mark. Res. forthcoming.
Miao S, Chen X, Chao X, Liu J, Zhang Y (2022) Context-based dynamic pricing with online clustering.
Prod. Oper. Management 31(9):3559–3575.
Miˇsi´c VV, Perakis G (2020) Data analytics in operations management: A review. Manufacturing & Service
Oper. Management 22(1):158–169.
Mohri M, Rostamizadeh A, Talwalkar A (2018) Foundations of Machine Learning (MIT Press).
Mukherjee G, Brown LD, Rusmevichientong P (2015) Efficient empirical Bayes prediction under check loss
using asymptotic risk estimates. arXiv:1511.00028 .
Qi X, Wang J, Chen Y, Shi Y, Zhang L (2023) Lipsformer: Introducing Lipschitz continuity to vision
transformers. arXiv:2304.09856 .
Romera-Paredes B, Barekatain M, Novikov A, Balog M, Kumar MP, Dupont E, Ruiz FJ, Ellenberg JS, Wang
P, Fawzi O, et al. (2024) Mathematical discoveries from program search with large language models.
Nature 625(7995):468–475.


--- Page 37 ---
37
Russo D, Zou J (2019) How much does your data exploration overfit? Controlling bias via information usage.
IEEE Trans. Inf. Theory. 66(1):302–323.
Schulman J, Lab TM (2025) LoRA without regret. Thinking Machines Lab: Connectionism URL http:
//dx.doi.org/10.64434/tml.20250929, https://thinkingmachines.ai/blog/lora/.
Siegel AF, Wagner MR (2021) Profit estimation error in the newsvendor model under a parametric demand
distribution. Management Sci. 67(8):4863–4879.
Siegel AF, Wagner MR (2023) Data-driven profit estimation error in the newsvendor model. Oper. Res.
71(6):2146–2157.
Soloff JA, Guntuboyina A, Sen B (2025) Multivariate, heteroscedastic empirical Bayes via nonparametric
maximum likelihood. J. R. Stat. Soc. Ser. B Stat. Method. 87(1):1–32.
Teh A, Jabbour M, Polyanskiy Y (2025) Solving empirical Bayes via transformers. arXiv:2502.09844 .
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser  L, Polosukhin I (2017) Attention
is all you need. Adv. Neural Inform. Processing Systems 30.
Von Oswald J, Niklasson E, Randazzo E, Sacramento J, Mordvintsev A, Zhmoginov A, Vladymyrov M
(2023) Transformers learn in-context by gradient descent. ICML, 35151–35174 (PMLR).
Wang H, Li X, Talluri K (2023) Transformer choice net: A transformer neural network for choice prediction.
arXiv:2310.08716 .
Wang Z, Gao R, Li S (2024) Neural-network mixed logit choice model: Statistical and optimality guarantees.
Available at SSRN 5118033 .
Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, Rault T, Louf R, Funtowicz M, et al.
(2020) Transformers: State-of-the-art natural language processing. EMNLP 2020 Demos, 38–45.
Xu A, Raginsky M (2017) Information-theoretic analysis of generalization capability of learning algorithms.
Adv. Neural Inf. Process. Syst. 30.
Ye Z, Zhang Z, Zhang DJ, Zhang H, Zhang R (2025) Deep learning-based causal inference for large-scale
combinatorial experiments: Theory and empirical evidence. Management Sci. forthcoming.
