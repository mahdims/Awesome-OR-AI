--- Page 1 ---
MIRROR: A Multi-Agent Framework with Iterative
Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in
Operations Research
Yifan Shi * 1 Jialong Shi * 1 Jiayi Wang 1 Ye Fan 2 Jianyong Sun ‚Ä† 1
Abstract
Operations Research (OR) relies on expert-driven
modeling‚Äîa slow and fragile process ill-suited
to novel scenarios. While large language mod-
els (LLMs) can automatically translate natural
language into optimization models, existing ap-
proaches either rely on costly post-training or
employ multi-agent frameworks, yet most still
lack reliable collaborative error correction and
task-specific retrieval, often leading to incorrect
outputs. We propose MIRROR, a fine-tuning-
free, end-to-end multi-agent framework that di-
rectly translates natural language optimization
problems into mathematical models and solver
code. MIRROR integrates two core mechanisms:
(1) execution-driven iterative adaptive revision for
automatic error correction, and (2) hierarchical
retrieval to fetch relevant modeling and coding ex-
emplars from a carefully curated exemplar library.
Experiments show that MIRROR outperforms
existing methods on standard OR benchmarks,
with notable results on complex industrial datasets
such as IndustryOR and Mamo-ComplexLP. By
combining precise external knowledge infusion
with systematic error correction, MIRROR pro-
vides non-expert users with an efficient and reli-
able OR modeling solution, overcoming the fun-
damental limitations of general-purpose LLMs in
expert optimization tasks.
1. Introduction
Operations Research (OR) serves as a foundational method-
ology for solving complex decision-making problems and
plays an indispensable role in domains such as manufactur-
1Xi‚Äôan
Jiaotong
University,
Xi‚Äôan,
Shaanxi,
China
2Northwestern Polytechnical University, Xi‚Äôan, Shaanxi, China.
Correspondence to: Jianyong Sun <jy.sun@xjtu.edu.cn
*Equal contribution. ‚Ä†Corresponding author.>.
Preprint. February 5, 2026.
ing, logistics, supply chain management, energy scheduling,
and public services (Cannas et al., 2024). By formulat-
ing mathematical optimization models and leveraging high-
performance solvers such as Gurobi(Gurobi Optimization,
LLC, 2024), COPT(Ge et al., 2024). OR significantly im-
proves resource utilization efficiency, reduces operational
costs, and enables enterprises to maximize economic returns.
Despite its well-established value, the practical adoption of
OR faces a fundamental bottleneck: real-world problems
are typically expressed in unstructured natural language,
while translating them into rigorous mathematical mod-
els and executable code requires deep domain expertise
and programming proficiency, making the process time-
consuming, labor-intensive, and poorly scalable. This high
knowledge barrier limits OR‚Äôs accessibility for small- and
medium-sized enterprises and non-technical users, and fur-
ther complicates adaptation to dynamic changes in oper-
ational environments‚Äîsuch as demand shifts or updated
constraints‚Äîthat necessitate frequent, expert-driven model
revisions.
Recent advances in large language models (LLMs) have
demonstrated remarkable capabilities in natural language
understanding, mathematical reasoning, symbolic manip-
ulation, and code generation‚Äîoften approaching or even
surpassing human expert performance. Current mainstream
LLMs can be broadly categorized into two paradigms: gen-
eral models and reasoning models. General models, includ-
ing GPT-4 (OpenAI et al., 2024a), DeepSeek-V3 (DeepSeek-
AI et al., 2025), and Qwen-3 (Yang et al., 2025a), excel in
semantic comprehension and broad knowledge coverage
through extensive pre-training on textual corpora, enabling
accurate interpretation of complex problem descriptions and
the generation of preliminary modeling insights. Mean-
while, reasoning model architectures such as GPT-o1 (Ope-
nAI et al., 2024b) and DeepSeek-R1 (Guo et al., 2025)
explicitly enhance systematic thinking, multi-step reasoning
chains, and self-correction mechanisms, thereby achieving
superior performance in structured tasks such as mathemati-
cal derivation, constraint formalization, and code synthesis.
Motivated by recent advances in large language models
(LLMs), researchers have explored various approaches to
1
arXiv:2602.03318v2  [cs.CL]  4 Feb 2026


--- Page 2 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
automatically translate unstructured natural language prob-
lem statements into solvable mathematical optimization
models, aiming to democratize operations research for non-
experts. Recent efforts such as LLMOPT (Jiang et al., 2025),
ORLM (Huang et al., 2025a), and MiniOpt (Anonymous,
2026a) train specialized models via supervised fine-tuning
or reinforcement learning on synthetic datasets; however,
they face two fundamental challenges: high-quality anno-
tated data is scarce and expensive to construct, and the
outputs inherently lack natural verifiability, making evalua-
tion and debugging difficult‚Äîparticularly for smaller-scale
models. To circumvent these limitations, a growing line of
work has shifted toward multi-agent frameworks that coor-
dinate multiple LLM agents to collaboratively construct op-
timization models without any additional training. Systems
like Chain-of-Experts (Xiao et al., 2023), OptiMUS (Ahma-
diteshnizi et al., 2024), and ORMind(Wang et al., 2025) de-
compose complex modeling tasks into specialized roles and
enable iterative interaction among agents, offering a flexible
and practical alternative. Nevertheless, existing multi-agent
frameworks are still constrained by closed architectures that
lack extensibility; their external knowledge is often gener-
ated by the large models themselves, introducing halluci-
nations, biases, or misalignments with task requirements,
which yields low-quality and poorly relevant contextual sup-
port. More critically, most of these frameworks lack reliable
correction mechanisms, making it difficult to detect and
rectify errors after code execution, thereby creating hidden
risks of solution failure and diminished system credibility.
To address these limitations, we propose MIRROR, a Multi-
Agent framework with Iterative Revision and Hierarchical
Retrieval for optimization modeling in Operation Research,
with the following key contributions. (Appendix A presents
a visual comparison of existing methods and MIRROR.)
‚Ä¢ We propose MIRROR, an end-to-end multi-agent
framework. This framework requires no fine-tuning
and automatically transforms natural language descrip-
tions of optimization problems into executable solver
code. Its dual-memory architecture consists of local
memory and shared global memory: the former records
the outputs required by the revision-stage agents to en-
sure intra-task consistency, while the latter enables
cross-task knowledge transfer.
‚Ä¢ We design an Iterative Adaptive Revision (IAR)
mechanism. Whenever solver code execution fails,
the mathematical modeling and code generation agents
switch to their respective revision experts, diagnose
errors in either the model or the solver code, and gener-
ate structured revision tips. The system then iteratively
refines both the model and solver code without hu-
man intervention until a correct solution is obtained or
a preset limit reached. Unlike existing methods, our
approach stores the historical model, code, and asso-
ciated revision tips in a local memory pool to provide
contextual history for subsequent corrective iterations.
‚Ä¢ We propose a Hierarchical Retrieval-Augmented
Generation (HRAG) mechanism. It based on an ex-
emplar library constructed via an automated synthe-
sis and labeling pipeline. The mechanism employs a
two-stage retrieval strategy‚Äîfirst coarse-grained filter-
ing by overall problem semantics and metadata, then
fine-grained reranking based on subproblem types and
deep semantic similarity‚Äîto provide highly relevant
exemplar contexts for modeling and code generation,
significantly improving model rationality and solver
code correctness.
MIRROR achieves state-of-the-art performance among cur-
rent multi-agent approaches on multiple operations re-
search benchmarks.It notably outperforms existing methods
on challenging datasets such as IndustryOR and Mamo-
ComplexLP. The framework also demonstrates effectiveness
with small open-source language models, enhancing their
optimization modeling capabilities without any task-specific
training.
2. Related Work
LLMs for Math and Code Generation
In recent years,
large language models (LLMs) have achieved remark-
able advances in mathematical reasoning and code gen-
eration.
On the mathematical side, specialized models
such as Qwen2.5-Math (Yang et al., 2024) and DeepSeek-
Prover (Ren et al., 2025) have enhanced formalized infer-
ence, while systems like AlphaEvolve (Novikov et al., 2025)
and MM-Agent (Liu et al., 2025) further integrate LLMs
into end-to-end mathematical problem-solving pipelines. In
code generation, frameworks including CodeAct (Wang
et al., 2024), KareCoder (Huang et al., 2024), SWE-
bench (Jimenez et al., 2024), and Web-bench (Xu et al.,
2025) demonstrate the potential of LLMs for autonomous
programming and debugging. Building on these develop-
ments, the present study focuses on the intersection of math-
ematical modeling and programming‚Äîautomated optimiza-
tion modeling‚Äîand categorizes existing solutions into two
paradigms: Learning-based LLM Optimization Modeling
and Agent-based LLM Optimization Modeling.
Learning-based LLM Optimization Modeling
Recent
learning-driven studies enhance LLM-based optimization
modeling via data synthesis, targeted fine-tuning, and rein-
forcement learning to address domain adaptation and data
scarcity in operations research. On the data synthesis front,
ORLM (Huang et al., 2025a) introduces the semi-automated
OR-Instruct framework. Moreover, OptMATH (Lu et al.,
2


--- Page 3 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
2025), Step-Opt (Wu et al., 2025), and ReSocratic (Yang
et al., 2025b) generate high-quality modeling data through
automated validation processes. On the learning mechanism
side, LLMOPT (Jiang et al., 2025), MiniOpt (Anonymous,
2026a), and SIRL (Chen et al., 2025) reduce modeling hal-
lucinations and improve generalization by integrating struc-
tured representations and verifiable learning mechanisms
such as external solver feedback. OR-R1 (Ding et al., 2025)
achieves similar effects via verifiable learning mechanisms,
particularly test-time reinforcement learning. Additionally,
CALM (Tang et al., 2025) and StepORLM (Zhou et al.,
2025) refine reasoning trajectories through corrective adap-
tation and process supervision, further augmenting the rea-
soning capabilities of LLMs for complex optimization tasks.
Agent-based LLM Optimization Modeling
To over-
come the inherent limitations of fixed decomposition strate-
gies, multi-agent frameworks enhance modeling perfor-
mance through specialized role allocation and collaborative
mechanisms. Chain-of-Experts (Xiao et al., 2023) intro-
duces a Conductor to orchestrate domain experts, whereas
OptiMUS (Ahmaditeshnizi et al., 2024) utilizes a modular
structure to decouple formulation, coding and evaluation
modules, effectively handling long-text descriptions and
complex data. At the execution level, OptimAI (Anony-
mous, 2026b) incorporates a Planner and a Code Critic to
enable strategic reflection, while OR-LLM-Agent (Zhang
& Luo, 2025) leverages the capabilities of reasoning LLMs
to decompose the task into three sub-tasks: modeling, code
generation and debugging. Furthermore, to enhance reli-
ability, ORMind (Wang et al., 2025) draws on cognitive
dual-process theory to implement counterfactual reasoning
for error detection, while LEAN-LLM-OPT (Liang et al.,
2026) employs a lightweight few-shot approach to reduce
computational overhead without compromising modeling
effectiveness.
Summary and Gaps
Despite recent progress, existing
methods face two main limitations. First, learning-based
models rely on annotated datasets that are scarce and expen-
sive to create. Their ‚Äúblack-box‚Äù nature also makes it diffi-
cult to diagnose errors or adapt to new constraints without
costly re-training. Second, most current multi-agent frame-
works often hallucinate due to a lack of external domain
knowledge. Moreover, they typically struggle to effectively
use solver feedback for error correction, leading to repeated
execution failures. To bridge these gaps, we propose MIR-
ROR, which integrates external knowledge retrieval with
execution-based iterative revision.
3. Methodology
The overall architecture of MIRROR is depicted in Figure 1.
MIRROR is a LLMs‚Äîbased multi-agent collaborative
framework designed for end-to-end automated modeling‚Äî
from natural language problem descriptions to formal math-
ematical models and executable solver code. The system
operates within a four-phase closed-loop paradigm: analy-
sis, modeling, implementation, and revision, ensuring cor-
rectness and robustness through IAR and HRAG mecha-
nisms(see Section 3.2 for details).
The entire workflow is divided into generation and revision
stages. In the generation phase, based on the input optimiza-
tion problem, the Parameter Extraction, Modeling Advisor,
Mathematical Modeling, and Code Generation agents se-
quentially perform their respective functions to finally pro-
duce the solver code. Problem analysis is performed via a
sequential dual-agent architecture: the Parameter Extraction
Agent first structures key problem parameters, and its out-
put is passed as contextual input to the Modeling Advisor
Agent to produce a structured interpretation of the problem.
Together, they produce a unified semantic representation.
This representation is then consumed by the Mathematical
Modeling Agent to generate a formal mathematical model,
which is subsequently compiled into executable solver code
by the Code Generation Agent. Both agents leverage the
same HRAG mechanism, performing coarse-grained filter-
ing based on metadata followed by fine-grained reranking
according to subproblem types, to dynamically integrate the
most relevant modeling patterns and solver code exemplars
from the exemplar library.
The generated solver code is immediately executed for vali-
dation. If execution succeeds, the process terminates; oth-
erwise, the system triggers the IAR phase: the Mathemat-
ical Modeling Agent and Code Generation Agent assume
revised roles as the Modeling Revision Agent and Code
Revision Agent, respectively. They jointly diagnose the
root cause of the failure and, leveraging their initial out-
puts stored in the local memory pool, generate structured
revision tips that serve as a reference strategy for further
corrections in the event of subsequent compile errors. The
system employs a dual-memory-pool architecture: The local
memory pool stores historical outputs of the agents involved
in the revision phase, providing the necessary contextual
history for error diagnosis and refinement, while the shared
global memory pool accumulates outputs from all functional
agents across tasks, enabling cross-task knowledge reuse
and continuous system evolution.
3.1. Component Overview
To enable end-to-end automated modeling from natural lan-
guage problem descriptions to executable optimization pro-
grams, we design a multi-agent framework composed of
specialized agents operating in coordination. We denote the
space of natural language optimization tasks by T , where
each t ‚ààT represents a complete problem statement. The
3


--- Page 4 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
Coding
Modeling
Problem Analysis
Provide modeling 
advice 
Parameter 
extraction
Example1
Example2
Code Revision 
Example1
Example2
 Modeling Revision 
embedding
model/Code
agent
exemplar library
local memory pool
mathematical Model
problem analysis
problem
code
global memory pool
Hierarchical retrieval-augmented generation process
Iterative adaptive revision process
Start IAR
vector store
Example1
Example2
......
ExampleN
embedding
Example1
Example2
query vectocr
Similarity Search 
with MMR
candidate examples
 Fine-grained reranking
metadata 
filtering
Final examples
The MIRROR Pipeline: A Multi-Agent Framework for Optimization Modeling
numerical answer
numerical 
answer
code tip:{
"tip_type": "",
"scenario": "",
"error_statement": "",
"code_error_location": 
"correct_code_snippet": "",
"incorrect_code_snippet": ""
}
model tip:{
"tip_type": "",
"scenario": "",
"error_statement": "",
"correct_component": "",
"incorrect_model": ""
}
{ "p1": 
{"Type": , 
"Definition": }
 .....}
**Category**: 
**Insight**: 
......
{
"VARIABLES": "",
"CONSTRAINTS": 
[ ],
"OBJECTIVE": ""
}
import 
gurobipy as gp
from gurobipy 
import GRB
......
Figure 1. MIRROR: An LLM-based multi-agent framework that automates end-to-end optimization modeling‚Äîfrom natural language
to executable solver code‚Äîthrough four phases: Understanding, Modeling, Implementation, and Revision. Hierarchical Retrieval-
Augmented Generation (HRAG) retrieves relevant exemplars for model and solver code synthesis; upon execution failure, the Iterative
Analysis and Revision (IAR) mechanism leverages local memory to diagnose and refine outputs. Local memory stores per-task agent
history for revision, while global memory accumulates cross-task knowledge for system-wide evolution.
unified exemplar library used in the hierarchical retrieval-
augmented generation mechanism is defined as L. For each
task t, the retrieval process yields a task-specific candidate
set Rt ‚äÜL, from which relevant modeling and solver
code exemplars are selected. The framework comprises
two phases: the Generation Phase, responsible for initial
solution construction, and the IAR Phase, which handles
iterative error correction.
3.1.1. AGENTS IN THE GENERATION PHASE
‚Ä¢ Parameter Extraction Agent fparam This agent is Re-
sponsible for identifying and structuring key parame-
ters from t. It then outputs p, a JSON object containing
parameter symbols, data types, and semantic defini-
tions, with its generation adhering to three principles:
real-world discreteness or continuity, practical feasi-
bility of values, and compatibility with mathematical
operations.
‚Ä¢ Modeling Advisor Agent fadv This agent provides
domain-aware semantic guidance through a structured
interpretation of t.
It then outputs g, a standard-
ized JSON list comprising operational clarifications
of domain-specific terms, salient problem details, and
a characterization of the problem‚Äôs essence‚Äîall strictly
derived from inferable information in the original text.
‚Ä¢ Mathematical Modeling Agent fmodel This agent acts
as the core formulator. It integrates the problem t, pa-
rameters p, advisory guidance g, and a set of retrieved
modeling exemplars Rmodel ‚äÜRt. It synthesizes a
formal optimization model m = (V, X, O), explicitly
defining the decision variables (V), constraints (X),
4


--- Page 5 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
and the objective function (O).
‚Ä¢ Code Generation Agent fcode This agent translates the
mathematical model into an executable program. Us-
ing the formal model m and retrieved code exemplars
Rcode ‚äÜRt, it produces the solver code c. The agent is
designed to handle implementation details and resolve
potential inconsistencies between the abstract model
and the specific syntax requirements of the solver such
as Gurobi
Executor
The external executor receives the generated
code c and attempts to run it. It returns either a numerical
solution n or a failure flag ‚ä•accompanied by a specific
error message e (e.g., syntax errors, execution timed out).
This feedback signal is crucial for triggering the subsequent
revision phase.
3.1.2. AGENTS IN THE IAR PHASE
When execution fails, the Mathematical Modeling and Code
Generation agents switch roles to function as revision spe-
cialists, leveraging the Dual Memory mechanism.
‚Ä¢ Modeling Revision Agent Œ¥model This agent focuses
on rectifying logical or formulation errors. It analyzes
the error message e and the previous modeling trace
stored in the local memory. It then reconstructs the
model to produce a corrected version Àúm and generates
a structured revision model tip œÑm. This tip serves as
a strategic reference to prevent similar errors in future
iterations.
‚Ä¢ Code Revision Agent Œ¥code This agent addresses
implementation-level failures. It takes the corrected
model Àúm, the error message e, and the history of the
previous code generation as context. It repairs the
solver code to yield a new version Àúc and produces a
code tip œÑc, ensuring the code is syntactically correct
and consistent with the revised mathematical model.
3.2. Mechanism Overview
MIRROR integrates three core mechanisms to enable robust,
reusable, and self-correcting automated modeling: IAR,
HRAG, and Dual Memory.
Iterative Adaptive Revision (IAR)
The system first per-
forms forward generation by sequentially invoking fparam ,
fadv , fmodel , and fcode to map the input task t to executable
solver code c . If Executor returns a failure, the IAR mech-
anism is triggered: The modeling revision agent Œ¥model and
the code revision agent Œ¥code collaboratively diagnose the
root cause of the error and generate structured revision tips .
These historical models, solver code, and their correspond-
ing revision tips are stored in a local memory pool to provide
contextual history for subsequent corrective iterations. They
then iteratively refine both the mathematical model and
its solver code implementation. Subsequently, the revised
solver code is executed again. This process continues until
a valid solution is produced or the maximum number of
revision rounds is reached, thereby establishing a closed-
loop, execution-driven self-correction mechanism.Here is
the main Structure of a Revision Instance(see Appendix D
for the concrete process).
Revision Instance
Generation Phase
problem: Consider a transportation problem with multiple prod-
ucts...?
...
Result: COMPILE ERROR
IAR Phase
‚Äì First Revision ‚Äì
...
Result: COMPILE ERROR
‚Äì Second Revision ‚Äì
{{‚Äúoriginal incorrect model objective‚Äù: ‚Äú...‚Äù,
‚Äúrevised correct model objective‚Äù: ‚Äú...‚Äù},
{‚Äúoriginal incorrect code snippet‚Äù: ‚Äú...‚Äù,
‚Äúrevised correct code snippet‚Äù: ‚Äú...‚Äù}}
Result: ACCEPT
Hierarchical Retrieval-Augmented Generation (HRAG)
The mechanism follows a sampling‚Äìallocation strategy. In
the sampling step, a unified exemplar library L‚Äîconsisting
of 602 high-quality and balanced optimization instances‚Äîis
constructed to enable contextual augmentation. Each exam-
ple is first structured as a (t, m, c) triple and then annotated
with a high-level category and a fine-grained subproblem
type (see Appendix C.1 for details of the sampling proce-
dure). The allocation step then retrieves relevant examples
and delivers them to the agent.
Retrieval follows a two-stage strategy:
1. Coarse-grained filtering: An embedding model Œòemb
combined with the Maximal Marginal Relevance
(MMR) (Adams et al., 2022) algorithm retrieves di-
verse yet semantically relevant candidate examples
(see Appendix C.2 for MMR algorithmic details), fol-
lowed by metadata-based filtering to ensure alignment
with problem type.
2. Fine-grained reranking: A large language model
reranks candidate examples based on problem cate-
gory alignment and deep semantic similarity, selecting
examples relevant to the target task.
At most two high-value examples are delivered as Rmodel
or Rcode for use by the modeling and code agents follows
this structure (see Appendix C.3 for details). respectively,
If no suitable example exists, an empty signal is returned
5


--- Page 6 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
to trigger robust fallback handling. Here is the format of a
data instance entry from the exemplar library.
Data Instance
{‚Äúen answer‚Äù: 10000.0, ‚Äúprompt‚Äù: ‚ÄúA construction company is
planning to allocate resources across four different tasks: ...?‚Äù,
‚Äúresponse‚Äù: ‚Äú## Mathematical Model: ... ‚ÄúVARIABLES‚Äù: ...
‚ÄúCONSTRAINTS‚Äù: ... ‚ÄúOBJECTIVE‚Äù: ... ## Python Code: ...‚Äù,
‚Äúproblem type‚Äù: ‚ÄúMixed-Integer Linear Programming (MILP)‚Äù,
‚Äúproblem subtype‚Äù: ‚ÄúDiscrete Scheduling & Assignment‚Äù}
Dual Memory
To support knowledge accumulation and
reuse, the system maintains two memory pools:
‚Ä¢ Local Memory Pool: Stores the primary agent‚Äôs own
historical outputs (e.g., past models, debugging tips),
providing contextual history for its revision phase. This
memory is used exclusively for the agent‚Äôs subsequent
tasks, reflecting its role-specific adaptability.
‚Ä¢ Global Memory Pool: Accumulates outputs from all
functional agents across tasks, enabling knowledge
reuse and system-level evolution through shared expe-
rience.
In summary, MIRROR integrates specialized agent roles
with the aforementioned dynamic mechanisms, not only
generating verifiable solver code from natural language by
dynamically incorporating relevant exemplar knowledge,
but also triggering iterative collaborative revisions upon
execution failure to progressively improve the correctness
of both the model and the solver code.
4. Experiments
In this section, we evaluate MIRROR on five diverse datasets
to assess its optimization solving capabilities.
4.1. Experimental Setup
Benchmarks
We evaluate our method on five standard
benchmarks in the domain of optimization modeling:
NL4Opt (Ramamonjison et al., 2023), Mamo (Huang
et al., 2025b), IndustryOR (Chen et al., 2025), and Com-
plexOR (Xiao et al., 2023). From the Mamo benchmark, we
specifically utilize two subsets: Mamo-EasyLP and Mamo-
ComplexLP. According to the complexity analysis in (Xiao
et al., 2025), which quantifies problem difficulty based on
the number of variables and constraints, NL4Opt, Mamo-
EasyLP, and ComplexOR are classified as simple tasks,
whereas IndustryOR and Mamo-ComplexLP are catego-
rized as complex tasks. Additionally, the last 163 instances
of Mamo-EasyLP are reserved as part of the source data for
the exemplar library L, while the remaining 489 instances
constitute the test set.
Baselines
We compare MIRROR against three categories
of methods:
(1) Traditional prompting: Standard Chain-of-Thought
(CoT) (Wei et al., 2022) prompting is applied on two mod-
els: the backbone model (qwen-plus-2025-09-11)
and DeepSeek-v3, to evaluate conventional inference un-
der different LLMs.
(2) Learning-based models: MiniOpt, LLMOPT, Opt-
MATH, ORLM, and SIRL‚Äîmethods that fine-tune spe-
cialized models on large-scale optimization datasets for
end-to-end modeling.
(3) Agent-based methods: OptiMUS, ORMind, and Chain-
of-Experts(COE), which use multi-agent LLM frameworks
with division-of-labor collaboration. All are implemented
using the same backbone model as our method for fair com-
parison.
Additionally, we validate MIRROR‚Äôs effectiveness on
a smaller model: qwen3-30b-a3b-instruct-2507
(abbreviated as qwen3-30B), comparing CoT prompting
against the full MIRROR framework.
Implementation Details
Except for the small-model abla-
tion, all experiments use qwen-plus-2025-09-11 as
the backbone model, with temperature set to 0 for determin-
istic and reproducible outputs. Generated code is formatted
for the Gurobi solver.
Evaluation Metric
We adopt pass@1 (Chen et al., 2021)
as the primary metric, defined as the proportion of problem
instances for which the model generates a correct solution
in a single attempt. A prediction ÀÜy is considered correct
if it satisfies the relative error tolerance with respect to the
ground-truth optimal value y‚àó:
|y‚àó‚àíÀÜy|
|y‚àó|
< 10‚àí3,
(1)
and to handle the case where y‚àó= 0 , we introduce an
absolute error criterion: |y‚àó‚àíÀÜy| < 10‚àí1.
4.2. Results Analysis
Overall Performance
MIRROR achieves the highest rank
among all fully evaluated methods in Table 1, setting a
new state of the art among multi-agent approaches for end-
to-end optimization modeling. Notably, this performance
is attained without any task-specific fine-tuning, relying
solely on collaborative reasoning, hierarchical retrieval, and
iterative adaptive revision.
Advancement over Agent-based Baselines
Among ex-
isting agent-based methods, COE achieves the highest
macro-average accuracy (68.20%), outperforming Opti-
MUS(53.57%) and ORMind (61.28%). MIRROR surpasses
6


--- Page 7 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
Table 1. Accuracy (pass@1, %) of different methods across benchmark datasets.
Category
Models / Methods
NL4Opt
Mamo
EasyLP
Mamo
ComplexLP
IndustryOR
ComplexOR
Macro Avg
Rank
Traditional prompting
Backbone model
72.24
85.07
44.83
46.00
44.44
58.52
7
Deepseek-v3
73.88
84.66
57.14
51.00
55.56
64.45
3
qwen3-30B
68.57
85.28
30.54
41.00
33.33
51.74
10
Learning-based methods
MiniOpt (14B)*
92.17
90.80
33.65
27.00
61.11
60.95
6
LLMOPT (14B)*
80.28
89.53
44.08
29.00
35.29
55.64
8
OptMATH (7B)*
78.70
84.20
34.12
19.00
33.33
49.87
11
ORLM (8B)*
85.70
82.30
37.40
38.00
‚Äî
‚Äî
‚Äî
SIRL (32B)*
98.00
94.60
61.10
48.00
‚Äî
‚Äî
‚Äî
Agent-based methods
OptiMUS
57.96
85.28
45.81
51.00
27.78
53.57
9
COE
84.90
87.93
57.63
55.00
55.56
68.20
2
ORMind
77.55
81.19
52.22
51.00
44.44
61.28
5
Ours
MIRROR
86.50
87.30
67.50
57.00
61.11
71.88
1
MIRROR (30B)
82.40
86.90
52.70
53.00
44.44
63.89
4
All agent-based methods use the default model. Bold denotes current SOTA; ‚Äú‚Äî‚Äù indicates unreported results; values marked with * are
from other papers: SIRL and ORLM from their original works, and all other learning-based methods from MiniOpt.
Table 2. Ablation study of MIRROR components (Accuracy (pass@1, %))
Variant
NL4Opt
Mamo
EasyLP
Mamo
ComplexLP
IndustryOR
ComplexOR
Macro Avg
MIRROR
86.50
87.30
67.50
57.00
61.11
71.88
W/o IAR
85.70
86.90
67.00
55.00
61.11
71.14
W/o HRAG
85.30
86.70
63.50
54.00
50.00
67.90
W/o Both
84.10
86.50
62.05
52.00
44.44
65.82
all of them, exceeding COE on four out of five bench-
marks and achieving improvements of +9.9% on Mamo-
ComplexLP and +5.6% on ComplexOR. This consistent im-
provement demonstrates that the iterative adaptive revision
mechanism and hierarchical retrieval-augmented generation
mechanism in our framework capture complex constraints
and non-standard problem structures more effectively than
previous agent designs.
Superiority to Learning-based Models
MIRROR re-
quires no task-specific training yet outperforms all learning-
based baselines in terms of average performance: MiniOpt
(14B), LLMOPT (14B), OptMATH (7B), ORLM (8B), and
SIRL (32B). On the challenging Mamo-ComplexLP and
IndustryOR benchmarks, it achieves 67.50% and 57.00% ac-
curacy, respectively‚Äîsurpassing the strongest prior model,
SIRL (32B), by 6.40 and 9.00 percentage points. This shows
that structured agent collaboration can excel at complex
optimization modeling without large-scale supervised fine-
tuning.
Advantage Over Traditional Prompting Approach
MIRROR, using the backbone model, achieves a macro-
average score of 71.88%, outperforming Chain-of-Thought
(CoT) prompting on the same model (58.52%) by 13.4
points.
It also exceeds CoT applied to the stronger
DeepSeek-v3 model (64.45%).
Effective Transfer to Small Model
When applied to
qwen3-30B, MIRROR boosts macro-average accuracy from
51.74% (achieved by COT) to 63.89%‚Äîa 12.15-point im-
provement without fine-tuning-confirming its plug-and-play
utility for accessible models.
4.3. Ablation Study
we conduct an ablation study on the five datasets: NL4Opt,
Mamo-EasyLP, Mamo-ComplexLP, IndustryOR, and Com-
plexOR. We evaluate four variants: the full MIRROR
(with both IAR and HRAG), and three ablated versions‚Äî
removing only IAR, only HRAG, or both. As shown in
Table 2, the full model achieves the highest macro-average
accuracy of 71.88%, outperforming the w/o HRAG variant
(67.90%), w/o IAR (71.14%), and w/o Both (65.82%). This
confirms that both mechanisms contribute to performance,
with HRAG yielding a larger gain.
7


--- Page 8 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
N L 4 O p t
M
a m o -
E a s y L P
M
a m o -
C o m p l e x L P
I n d u s t r y O R
C o m p l e x O R
0
2 0
4 0
W
r o n g  A n s w e r  R a t e ( %
)
 W
/ o  B o t h
 W
/ o  H R A G
 W
/ o  I A R
 M
I R R O R
D a t a s e t
(a) Wrong Answer Rate Across Datasets and Methods
N L 4 O p t
M
a m o -
E a s y L P
M
a m o -
C o m p l e x L P
I n d u s t r y O R
C o m p l e x O R
0
5
1 0
1 5
2 0
2 5
3 0
    	
           
D a t a s e t
 W
/ o  B o t h
 W
/ o  H R A G
 W
/ o  I A R
 M
I R R O R
(b) Compile Error Rate Across Datasets and Methods
Figure 2. Ablation Study of MIRROR
We further decompose errors into wrong answer rate (exe-
cutable solver code that generates incorrect numerical result)
and compile error rate (syntactically invalid solver code that
fails to execute), as these reflect distinct failure modes in op-
timization modeling: the former indicates flawed reasoning,
while the latter reveals structural or grammatical mistakes in
the generated solver code. Figure 2 visualizes these two er-
ror types across configurations. The light-colored regions in
Figure 2a represent the compile error rate. Since the wrong
answer rates are significantly higher than the compile error
rates in most cases across the datasets and mechanisms, the
light-colored regions for the former are omitted in Figure 2b
to ensure a clearer visualization of the latter.
For Wrong Answer Rate
all datasets except ComplexOR
show a consistent decrease or remain stable from the w/o
Both configuration to single-mechanism variants and finally
to the full model, indicating that both mechanisms help
produce correct solutions. Notably, removing HRAG leads
to a larger increase in wrong answers than removing IAR;
for example, on IndustryOR, the wrong answer rate is 3.00%
higher when only HRAG is removed compared to when only
IAR is removed. This suggests that HRAG enhances both
problem modeling and solver code generation through its
two-stage retrieval strategy, thereby improving the accuracy
of the final results.
For Compile Error Rate
the opposite trend is observed:
on NL4Opt, Mamo-EasyLP, and IndustryOR, the w/o
IAR variant incurs higher compile errors than w/o HRAG,
demonstrating that the IAR mechanism‚Äôs closed-loop it-
erative adaptive revision process effectively identifies and
resolves compilation failures. Averaged across all datasets,
the compile error rate decreases from 8.06% in the w/o Both
setting to 2.60% with IAR only and 2.65% with HRAG only,
and further declines to 1.91% when both mechanisms are
used. This highlights their complementary contributions to
generating valid and executable optimization models and
solver code.
5. Conclusion
We present MIRROR, a training-free multi-agent frame-
work for automated operations research (OR) modeling
that bridges the gap between natural language problem
descriptions and formal mathematical models with exe-
cutable solver code. By integrating Hierarchical Retrieval-
Augmented Generation (HRAG) for dynamic exemplar re-
trieval and an Iterative Analysis and Revision (IAR) mecha-
nism for execution-driven self-correction, MIRROR effec-
tively mitigates the hallucination and fragility of general-
purpose large language models in specialized OR tasks.
Experiments show that MIRROR achieves strong perfor-
mance across diverse OR modeling benchmarks, attaining
state-of-the-art results on complex industrial datasets and
significantly boosting the capabilities of small open-source
language models‚Äîwithout any task-specific training. This
work paves the way toward accessible, reliable, and scal-
able AI-assisted decision-making for real-world operations
research applications.
8


--- Page 9 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
Impact Statement
This work aims to make optimization modeling more acces-
sible by enabling non-experts to formulate and solve real-
world problems from natural language, without requiring
task-specific training. It has potential benefits in domains
such as logistics, energy, and supply chain management. As
with any AI-assisted tool, incorrect or ambiguous inputs
could lead to flawed outputs; thus, human oversight remains
essential. The system does not use personal data and is
intended solely as a decision-support aid.
References
Adams, D., Suri, G., and Chali, Y. Combining state-of-
the-art models with maximal marginal relevance for few-
shot and zero-shot multi-document summarization. arXiv
preprint arXiv:2211.10808, 2022.
Ahmaditeshnizi, A., Gao, W., and Udell, M. OptiMUS:
Scalable optimization modeling with (MI)LP solvers and
large language models. In International Conference on
Machine Learning, pp. 577‚Äì596. PMLR, 2024.
Anonymous.
Miniopt: Reasoning to model and solve
general optimization problems with limited resources,
2026a. URL https://openreview.net/forum?
id=nIWCUVJ6OU.
Anonymous. OptimAI: Optimization from natural language
using LLM-powered AI agents, 2026b. URL https:
//openreview.net/forum?id=JtgZkVdAIP.
Cannas, V. G., Ciano, M. P., Saltalamacchia, M., and Secchi,
R. Artificial intelligence in supply chain and operations
management: A multiple case study research. Interna-
tional Journal of Production Research, 62(9):3333‚Äì3360,
2024.
Chen, M., Tworek, J., Jun, H., Yuan, Q., de O. Pinto, H. P.,
Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,
G., et al. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374, 2021.
Chen, Y., Xia, J., Shao, S., Ge, D., and Ye, Y. Solver-
informed RL: Grounding large language models for
authentic optimization modeling. In The Thirty-ninth
Annual Conference on Neural Information Processing
Systems, 2025. URL https://openreview.net/
forum?id=80L235oVBe.
DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B.,
Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D.,
Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai,
F., ..., and Pan, Z. DeepSeek-V3 technical report. arXiv
preprint arXiv:2412.19437, 2025.
Ding, Z., Tan, Z., Zhang, J., and Chen, T. OR-R1: Au-
tomating modeling and solving of operations research
optimization problem via test-time reinforcement learn-
ing. arXiv preprint arXiv:2511.09092, 2025.
Ge, D., Huangfu, Q., Wang, Z., Wu, J., and Ye, Y. Car-
dinal optimizer (COPT) user guide.
arXiv preprint
arXiv:2208.14314, 2024.
Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q.,
Xu, R., Zhang, R., Ma, S., Bi, X., et al. DeepSeek-R1
incentivizes reasoning in LLMs through reinforcement
learning. Nature, 645(8081):633‚Äì638, 2025.
Gurobi Optimization, LLC. Gurobi optimizer reference
manual, 2024. URL https://www.gurobi.com.
Reference Manual.
Huang, C., Tang, Z., Hu, S., Jiang, R., Zheng, X., Ge, D.,
Wang, B., and Wang, Z. ORLM: A customizable frame-
work in training large models for automated optimization
modeling. Operations Research, 2025a.
Huang, T., Sun, Z., Jin, Z., Li, G., and Lyu, C. Knowledge-
aware code generation with large language models. In
2024 IEEE/ACM 32nd International Conference on Pro-
gram Comprehension (ICPC), pp. 52‚Äì63. IEEE Computer
Society, April 2024.
Huang, X., Shen, Q., Hu, Y., Gao, A., and Wang, B. Large
language models for mathematical modeling: Towards
bridging the gap between natural and mathematical lan-
guages. arXiv preprint arXiv:2405.13144, 2025b.
Jiang, C., Shu, X., Qian, H., Lu, X., Zhou, J., Zhou, A., and
Yu, Y. LLMOPT: Learning to define and solve general
optimization problems from scratch. In International
Conference on Learning Representations, pp. 101580‚Äì
101606, 2025.
Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press,
O., and Narasimhan, K. SWE-bench: Can language mod-
els resolve real-world GitHub issues? In International
Conference on Learning Representations, 2024.
LangChain Team.
Langchain, 2023.
URL https:
//github.com/langchain-ai/langchain.
GitHub repository.
Liang, K., Lu, Y., Mao, J., Sun, S., Yang, C., Zeng, C., Jin,
X., Qin, H., Zhu, R., and Teo, C.-P. LLM for large-scale
optimization model auto-formulation: Bridging flexibility
and standardization via agentic workflow. arXiv preprint
arXiv:2601.09635, 2026.
Liu, F., Yang, Z.-R., Liu, C., Song, T., Gao, X., and Liu,
H. MM-agent: LLM as agents for real-world mathemat-
ical modeling problem. In 2nd AI for Math Workshop
9


--- Page 10 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
@ ICML 2025, 2025. URL https://openreview.
net/forum?id=QyKBf7X98d.
Lu, H., Xie, Z., Wu, Y., Ren, C., Chen, Y., and Wen, Z. Opt-
MATH: A scalable bidirectional data synthesis framework
for optimization modeling. In Forty-second International
Conference on Machine Learning, 2025. URL https:
//openreview.net/forum?id=9P5e6iE4WK.
Novikov, A., VÀúu, N., Eisenberger, M., Dupont, E., Huang, P.-
S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F.
J. R., Mehrabian, A., et al. AlphaEvolve: A coding agent
for scientific and algorithmic discovery. arXiv preprint
arXiv:2506.13131, 2025.
OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
and Akkaya, I. GPT-4 technical report. arXiv preprint
arXiv:2303.08774, 2024a.
OpenAI, Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-
Kishky, A., et al. OpenAI o1 system card. arXiv preprint
arXiv:2412.16720, 2024b.
Ramamonjison, R., Yu, T., Li, R., Li, H., Carenini, G., Ghad-
dar, B., He, S., Mostajabdaveh, M., Banitalebi-Dehkordi,
A., Zhou, Z., et al. NL4Opt competition: Formulating
optimization problems based on their natural language
descriptions. In NeurIPS 2022 Competition Track, pp.
189‚Äì203. PMLR, 2023.
Ren, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W.,
Zhang, L., Fu, Z., Zhu, Q., Yang, D., et al. DeepSeek-
Prover-V2: Advancing formal mathematical reasoning
via reinforcement learning for subgoal decomposition.
arXiv preprint arXiv:2504.21801, 2025.
Tang, Z., Ye, Z., Huang, C., Huang, X., Li, C., Li, S., Chen,
G., Yan, M., Wang, Z., Zha, H., et al. CALM before the
STORM: Unlocking native reasoning for optimization
modeling. arXiv preprint arXiv:2510.04204, 2025.
Wang, X., Chen, Y., Yuan, L., Zhang, Y., Li, Y., Peng, H.,
and Ji, H. Executable code actions elicit better LLM
agents. In Proceedings of the 41st International Confer-
ence on Machine Learning, volume 235 of Proceedings
of Machine Learning Research, pp. 50208‚Äì50232. PMLR,
2024.
Wang, Z., Chen, B., Huang, Y., Cao, Q., He, M., Fan, J.,
and Liang, X. ORMind: A cognitive-inspired end-to-
end reasoning framework for operations research. arXiv
preprint arXiv:2506.01326, 2025.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems, 35:
24824‚Äì24837, 2022.
Wu, Y., Zhang, Y., Wu, Y., Wang, Y., Zhang, J., and Cheng,
J. Step-Opt: Boosting optimization modeling in LLMs
through iterative data synthesis and structured validation.
arXiv preprint arXiv:2506.17637, 2025.
Xiao, Z., Zhang, D., Wu, Y., Xu, L., Wang, Y. J., Han, X.,
Fu, X., Zhong, T., Zeng, J., Song, M., et al. Chain-of-
experts: When LLMs meet complex operations research
problems. In The Twelfth International Conference on
Learning Representations, 2023.
Xiao, Z., Xie, J., Xu, L., Guan, S., Zhu, J., Han, X., Fu,
X., Yu, W., Wu, H., Shi, W., Kang, Q., Duan, J., Zhong,
T., Yuan, M., Zeng, J., Wang, Y., Chen, G., and Zhang,
D. A survey of optimization modeling meets LLMs:
Progress and future directions. In Proceedings of the
Thirty-Fourth International Joint Conference on Artificial
Intelligence (IJCAI-25), pp. 10742‚Äì10750, 2025. doi:
10.24963/ijcai.2025/1192.
Xu, K., Mao, Y., Guan, X., and Feng, Z.
Web-Bench:
A LLM code benchmark based on web standards and
frameworks. arXiv preprint arXiv:2505.07473, 2025.
Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu,
D., Tu, J., Zhou, J., Lin, J., et al. Qwen2.5-Math techni-
cal report: Toward mathematical expert model via self-
improvement. arXiv preprint arXiv:2409.12122, 2024.
Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B.,
Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical
report. arXiv preprint arXiv:2505.09388, 2025a.
Yang, Z., Wang, Y., Huang, Y., Guo, Z., Shi, S., Han, X.,
Feng, L., Song, L., Liang, X., and Tang, J. Optibench
meets resocratic: Measure and improve llms for optimiza-
tion modeling. In International Conference on Learning
Representations, pp. 24726‚Äì24759, 2025b.
Zhang, B. and Luo, P. OR-LLM-Agent: Automating mod-
eling and solving of operations research optimization
problem with reasoning large language model. arXiv
preprint arXiv:2503.10009, 2025.
Zhou, C., Xu, T., Lin, J., and Ge, D. StepORLM: A self-
evolving framework with generative process supervision
for operations research language models. arXiv preprint
arXiv:2509.22558, 2025.
10


--- Page 11 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
A. Method
In this section, we provide a concise visualization along with a comparative analysis of learning-based methods, existing
multi-agent methods, and the MIRROR method.
Training
Data
output
problem
(a) Learning-based
review
problem
output
if
review
(b) Existing Agent-based
exemplar library
hierarchical 
retrieval
if
problem
output
revision history
Iterative adaptive revision
(c) MIRROR
Figure 3. Comparison of optimization modeling methods. (a) Learning-based methods train task-specific models on dedicated datasets
but lack execution feedback. (b) Existing agent-based methods employ a closed-loop to iteratively refine the multi-agent system using
execution errors. (c) MIRROR combines hierarchical retrieval, which dynamically incorporates exemplar knowledge, with iterative
adaptive revision to enable structured error diagnosis and collaborative correction, achieving accurate, robust, and fine-tuning-free
optimization modeling.
Learning-based methods focus on task-specific training. As illustrated in Figure 3a, specific optimization problems are
inputted into a Large Language Models (LLMs). Rather than being a general-purpose model, the LLM is pre-trained on
specialized, labeled datasets to acquire domain-specific expertise. While this approach enables rapid generation of Python
code, the output is produced in a single-pass manner. It lacks real-time interaction with the actual execution environment,
making it difficult to evaluate or debug the generated content. Consequently, such methods exhibit lower robustness when
encountering unseen or complex edge cases.
As illustrated in Figure 3b, existing agent-based methods utilize a dynamic closed-loop mechanism based on the multi-
agent system. Instead of single-model training, collaborative LLM experts generate code and iteratively refine it through
an execution feedback loop. While this self-correction enhances performance in complex tasks, the system‚Äôs closed
architecture limits its extensibility with external knowledge. Moreover, relying on internally generated context often triggers
hallucinations or task misalignment. More critically, most existing frameworks lack reliable error-correction mechanisms,
making it difficult to detect and rectify mistakes after code execution. Thus, despite its reactive correction capabilities, the
framework remains hindered by potential solution failures and limited reliability.
Our proposed MIRROR method introduces a knowledge-augmented multi-agent framework. As illustrated in Figure 3c, the
framework utilizes a hierarchical retrieval mechanism to access a curated exemplar library, providing reliable reference
knowledge that significantly reduces model hallucinations during the model and code generation process. Unlike existing
agent-based methods, our approach incorporates an iterative adaptive revision process and a dual-memory architecture, where
the system stores historical models, code, and associated revision tips in a local memory pool to provide contextual history
for subsequent corrective iterations. By integrating dynamic knowledge retrieval with the adaptive revision, MIRROR, as
a fine-tuning-free optimization modeling method, improves the accuracy of modeling and solving while overcoming the
limitations of existing frameworks in terms of the extensibility and reliability.
B. Dataset
B.1. The Introduction of Different Benchmarks
In this section, we provide an overview of the five benchmarks used for performance evaluation: NL4Opt, Mamo-EasyLP,
Mamo-ComplexLP, IndustryOR, and ComplexOR. These datasets represent a wide array of optimization tasks and constraints,
11


--- Page 12 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
thereby providing a rigorous foundation to validate the robustness and cross-domain adaptability of our proposed method.
NL4Opt is a standardized and widely used benchmark originating from the NeurIPS 2022 competition. It features high-
quality linear programming tasks across six diverse domains. Through a rigorous filtering process, the benchmark provides
a collection of refined instances for evaluating automated modeling performance. The dataset contains 245 problems.
Mamo is a mathematical modeling dataset consisting of two segments: Mamo-EasyLP and Mamo-ComplexLP. The
EasyLP segment comprises high school-level problems (652 instances), while the ComplexLP segment involves more
advanced, undergraduate-level formulations (203 instances).
IndustryOR is the first industrial benchmark specifically designed for operations research (OR), comprising 100 real-world
problems sourced from eight different industries. This dataset encompasses five major mathematical programming types:
linear programming (LP), integer programming (IP), mixed-integer programming (MIP), non-linear programming (NLP),
and others, distributed across three distinct levels of difficulty.
ComplexOR is the first benchmark dataset specifically designed for complex operations research (OR) problems. Its
instances are sourced from a wide range of materials, including academic papers, textbooks, real-world industrial scenar-
ios,covering multiple domains such as supply chain optimization and warehouse logistics, with a total of 18 instances.
B.2. Complexity Analysis of Evaluation Benchmarks
In this section, we provide a detailed analysis of the complexity of the five benchmark datasets to characterize the challenges
involved in the modeling task.
(Xiao et al., 2025) adopts standard prompting to generate models for each problem, using the number of variables and
constraints within the model as the metric for its complexity. The complexity results obtained from this research on different
datasets are shown in Table 3.
Table 3. Dataset Complexity
Dataset
Complexity
NL4Opt
5.59
Mamo-EasyLP
7.12
Mamo-ComplexLP
13.35
IndustryOR
14.06
ComplexOR
5.98
The results reveal a clear distinction in complexity levels among the evaluated datasets. Specifically, Mamo-ComplexLP
and IndustryOR are identified as the most challenging benchmarks, exhibiting significantly higher complexity compared to
the others. This indicates that problems within these two datasets involve a more extensive scale of decision variables and
constraints. In contrast, NL4Opt, Mamo-EasyLP and ComplexOR show relatively lower complexity levels. This hierarchical
distribution of complexity across the datasets ensures that the modeling tasks cover a wide range of difficulty, providing a
robust basis for evaluating model performance under different scales of problem constraints.
C. Hierarchical Retrieval-Augmented Generation (HRAG)
C.1. Exemplar Library Sampling Procedure
The exemplar library L is constructed through a multi-stage curation pipeline during the preparation phase, where ground-
truth verification signals are available to assess correctness‚Äînot only solver code executability but also solution optimality.
We start with raw instances from three sources: OptiBench (Yang et al., 2025b) (605 instances), OptMATH (359 instances),
and the last 163 instances from Mamo-EasyLP, totaling 1,127 optimization problems. Each instance is processed by the
MIRROR framework to complete missing mathematical formulations and executable solver code. Crucially, the generated
model‚Äìsolver code pairs are validated against oracle solver; only those yielding correct optimal solutions are retained,
resulting in 652 fully specified and verified problem‚Äìsolution pairs. Subsequently, we employ the large language model
qwen-plus to annotate each verified instance with a high-level problem category and a fine-grained subproblem type.
Instances whose categories cannot be confidently determined are filtered out. The final repository contains 602 high-quality,
balanced optimization instances, denoted as L.
12


--- Page 13 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
Instance
{"en_answer": 10000.0, "prompt": "A construction company is planning to allocate
resources across four different tasks: \$x1, x2, x3\$, and \$x4\$. These tasks
could represent various stages of the construction process such as site
preparation, foundation work, framing, and finishing. The objective is to
minimize the total cost associated with these tasks while meeting all task
requirements and constraints. Each unit of resource allocated to tasks \$x1, x2,
x3\$, and \$x4\$ incurs a cost of 100 dollars, 200 dollars, 300 dollars, and 400
dollars respectively. The allocation must adhere to the following conditions: -
The combined resources allocated for tasks \$x1\$ and \$x2\$ cannot exceed 30
units due to manpower or equipment limitations. - Twice the resources allocated
for task \$x1\$ combined with those for task \$x3\$ should be at least 40 units
in order to meet project milestones. - The difference in resources between task
\$x4\$ and task \$x2\$ should exactly equal 20 units possibly due to scheduling
or sequential dependencies among these tasks. - Task \$x1\$ along with half of
the resources allocated for task \$x3\$ should not be less than 10 units
considering certain quality standards or safety regulations. Given that all
resource allocations are integers because they may correspond to indivisible
items like workers or machines. Also each task has specific bounds on resource
allocation: What is the minimum total cost in dollars for the company given
these constraints?", "response": "## Mathematical Model:\n‚Äò‚Äò‚Äò{\n \"VARIABLES\":
\"x1, x2, x3, x4 are non-negative integer variables representing resource units
allocated to tasks 1, 2, 3, and 4 respectively.\",\n \"CONSTRAINTS\": [\n \"x1 +
x2 <= 30\",\n \"2*x1 + x3 >= 40\",\n \"x4 - x2 == 20\",\n \"x1 + 0.5*x3 >=
10\"\n ],\n \"OBJECTIVE\": \"minimize 100*x1 + 200*x2 + 300*x3 +
400*x4\"\n}‚Äò‚Äò‚Äò\n\n\n## Python Code:\n‚Äò‚Äò‚Äòpython\nimport gurobipy as gp\nfrom
gurobipy import GRB\n\n# Create model\nmodel =
gp.Model(\"ConstructionResourceAllocation\")\n\n# Define integer variables with
non-negativity constraints\nx1 = model.addVar(vtype=GRB.INTEGER, name=\"x1\",
lb=0)\nx2 = model.addVar(vtype=GRB.INTEGER, name=\"x2\", lb=0)\nx3 =
model.addVar(vtype=GRB.INTEGER, name=\"x3\", lb=0)\nx4 =
model.addVar(vtype=GRB.INTEGER, name=\"x4\", lb=0)\n\n# Set objective: minimize
total cost\nmodel.setObjective(100*x1 + 200*x2 + 300*x3 + 400*x4,
GRB.MINIMIZE)\n\n# Add constraints\nmodel.addConstr(x1 + x2 <= 30,
\"Constraint1\")\nmodel.addConstr(2*x1 + x3 >= 40,
\"Constraint2\")\nmodel.addConstr(x4 - x2 == 20,
\"Constraint3\")\nmodel.addConstr(x1 + 0.5*x3 >= 10, \"Constraint4\")\n\n#
Optimize model\nmodel.optimize()\n\n# Output the minimum total cost and
optimized
variables\nprint(int(model.objVal))\nprint(int(x1.x))\nprint(int(x2.x))\n
print(int(x3.x))\nprint(int(x4.x))\n\n‚Äò‚Äò‚Äò", "problem_type": "Mixed-Integer
Linear Programming (MILP)", "problem_subtype": "Discrete Scheduling &
Assignment"}
C.2. MMR Algorithm: Principle and Implementation
The Maximal Marginal Relevance (MMR) algorithm is a widely used retrieval strategy that balances relevance to the query
and diversity among selected results. In our HRAG framework, MMR is employed in the fine-grained matching stage to
select diverse yet highly relevant exemplars from the curated library L, ensuring both semantic alignment and contextual
variety.
The standard MMR formula is defined as:
MMR = arg max
Di‚ààD\S

Œª ¬∑ Sim1(Di, Q) ‚àí(1 ‚àíŒª) ¬∑ max
Dj‚ààS Sim2(Di, Dj)

(2)
where:
‚Ä¢ Q is the input query (e.g., problem description),
‚Ä¢ D is the full set of candidate exemplars,
13


--- Page 14 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
‚Ä¢ S is the current set of selected exemplars (initially empty),
‚Ä¢ Sim1(Di, Q) measures the relevance between candidate Di and query Q ,
‚Ä¢ Sim2(Di, Dj) measures the similarity between candidate Di and already-selected exemplar Dj ,
‚Ä¢ Œª ‚àà[0, 1] controls the trade-off between relevance ( Œª close to 1) and diversity ( Œª close to 0).
In our implementation, we use LangChain (LangChain Team, 2023)‚Äôs default MMR retriever, which selects diverse yet
relevant exemplars via cosine similarity in a shared embedding space. The retrieved sets Rmodel and Rcode thus provide
semantically aligned but non-redundant context for modeling and code generation.
C.3. Exemplar Structure
Modeling Exemplar Structure
{{
"Problem description": "...",
"Mathematical Model": {{
"VARIABLES": "...",
"CONSTRAINTS": "...",
"OBJECTIVE": "..."
}}
}}
Solver Code Exemplar Structure
{{
"Problem description": "...",
"Mathematical Model": {{
"VARIABLES": "...",
"CONSTRAINTS": "...",
"OBJECTIVE": "..."
}},
"Code": "..."
}}
C.4. Hierarchical Retrieval Algorithm
Hierarchical Retrieval Algorithm
Algorithm: Hierarchical Retrieval of MIRROR Framework
GLOBAL VARIABLES:
vector_store = NULL
rag_chain = NULL
data_path = "<knowledge_base_file_path>"
persist_dir = "<vector_db_persistence_directory>"
embedding_model_name = "text-embedding-v4"
FUNCTION load_json_lines_from_md(file_path: STRING) -> LIST[Document]:
documents = EMPTY LIST
OPEN file_path WITH ENCODING "utf-8" AS f:
FOR line_num FROM 1 TO total_lines IN f:
line = TRIM(f.readline())
IF line IS EMPTY OR line STARTS WITH ("#", "<!--", "‚Äò‚Äò‚Äò"):
CONTINUE
TRY:
data = PARSE JSON FROM line
problem_type = data.GET("problem_type", "general")
14


--- Page 15 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
problem_subtype = data.GET("problem_subtype", "general")
doc = Document(
page_content = line,
metadata = {
"source": file_path,
"line": line_num,
"problem_type": problem_type,
"problem_subtype": problem_subtype
}
)
ADD doc TO documents
CATCH JSONDecodeError:
CONTINUE
RETURN documents
FUNCTION _initialize_rag():
GLOBAL vector_store, rag_chain
IF vector_store IS NOT NULL AND rag_chain IS NOT NULL:
RETURN
load_dotenv()
api_key = GET ENVIRONMENT VARIABLE "DASHSCOPE_API_KEY"
IF api_key IS EMPTY:
RAISE ValueError("DASHSCOPE_API_KEY not found in environment variables.")
embeddings = DashScopeEmbeddings(
model = embedding_model_name,
dashscope_api_key = api_key
)
IF persist_dir DOES NOT EXIST:
PRINT "Building vector database from md ..."
docs = load_json_lines_from_md(data_path)
IF docs IS EMPTY:
RAISE RuntimeError("No valid JSON lines found in the data file.")
Chroma.from_documents(
documents = docs,
embedding = embeddings,
persist_directory = persist_dir,
collection_name = "knowledge_base"
)
PRINT "Vector DB built with " + LEN(docs) + " examples."
vector_store = Chroma(
persist_directory = persist_dir,
embedding_function = embeddings,
collection_name = "knowledge_base"
)
retriever = vector_store.as_retriever(
search_type = "mmr",
search_kwargs = {"k": 3, "fetch_k": 10}
)
prompt_template ="model/code retrieval prompt template"
prompt = PromptTemplate.from_template(prompt_template)
llm = ChatTongyi(
model_name = "qwen-plus-2025-09-11",
temperature = 0,
dashscope_api_key = api_key
)
15


--- Page 16 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
rag_chain = (
{"context": retriever, "question": RunnablePassthrough()}
| prompt
| llm
)
FUNCTION retrieve_knowledge(query: STRING) -> STRING:
CALL _initialize_rag()
TRY:
response = rag_chain.invoke(query)
IF response HAS ATTRIBUTE "content":
RETURN response.content
ELSE:
RETURN STRING(response)
CATCH Exception AS e:
RETURN "Retrieval failed: " + STRING(e)
D. Case Study
In this section, we provide a detailed case study to demonstrate the inner workings of our proposed MIRROR framework. By
tracing the iterative modeling process of a representative problem from the ComplexOR dataset, we specifically highlight the
effectiveness of the IAR mechanism in the problem-solving process. This case study illustrates how the MIRROR identifies
potential modeling flaws and iteratively refines the solution through its IAR mechanism, ultimately leading to a more robust
and accurate final output.
D.1. Problem Description
Consider a transportation problem with multiple products. Given a set of cities Cities and a set of links Links between
the cities. Each city i has a certain supply of each product p (Supplyi,p) and a certain demand for each product p
(Demandi,p). The cost of shipping one package of product p from city i to city j is ShipmentCosti,j,p. Each link
(i, j) has a certain capacity for each product p (Capacityi,j,p) and a joint capacity JointCapacityi,j for all products.
The problem aims to minimize the total cost of shipping products from the cities to the cities. The total number of
packages to be shipped on each link (i, j) should not exceed its joint capacity. How to decide the number of packages
of each product p to be shipped from each city i to each city j? The following parameters are included in this problem:
Cities: list, a list of cities; Links: list, a list of links between the cities; Products: list, a list of products; Supply:
list of lists, the supply of each product at each city; Demand: list of lists, the demand of each product at each city;
ShipmentCost: list of lists of lists, the cost of shipping each product from each city to each city; Capacity: list of lists of
lists, the capacity of shipping each product from each city to each city; JointCapacity: list of lists, the joint capacity
of each link. The following data is included in this problem: ‚ÄôCities‚Äô:
[‚ÄôA‚Äô, ‚ÄôB‚Äô], ‚ÄôLinks‚Äô:
[[‚ÄôA‚Äô,
‚ÄôB‚Äô]], ‚ÄôProducts‚Äô:
[‚ÄôProduct1‚Äô], ‚ÄôSupply‚Äô:
[[10], [0]], ‚ÄôDemand‚Äô:
[[0], [10]],
‚ÄôShipmentCost‚Äô:
[[[1]]], ‚ÄôCapacity‚Äô:
[[[10]]], ‚ÄôJointCapacity‚Äô:
[[10]].
D.2. Iterative Adaptive Revision Details
Revision Details
{
"second_reversion": {
"second_reversion": {
"original_incorrect_model_objective": "minimize sum_{(i,j) in Links} sum_{p in
range(len(Products))} ShipmentCost[i][j][p] * flow[i][j][p] - rationale: the
objective must only iterate over existing links (i,j) in Links to avoid
index out-of-range errors; the original code incorrectly assumed a full
Cartesian product of all city pairs, but only specified links are valid.",
"revised_correct_model_objective": "minimize sum_{(i,j) in Links} sum_{p in
range(len(Products))} ShipmentCost[i][j][p] * flow[i][j][p] - rationale:
ShipmentCost is a 3D array indexed by [link_index][product_index], where
16


--- Page 17 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
link_index corresponds to the position of (i,j) in the Links list. Thus, for
each link (i,j), we use its positional index in Links (not city-to-city
mapping) to access the correct cost. The original code incorrectly attempted
to map city names to link indices using a dictionary (link_to_idx), but the
cost array is already indexed by link position in Links, so we directly use
i,j as indices into ShipmentCost assuming the order of Links matches the
first dimension of ShipmentCost.",
"original_incorrect_code_snippet": "ShipmentCost[link_to_idx[(Cities[i],
Cities[j])]][p] * flow[i, j, p]", "model.addConstr(flow[i, j, p] <=
Capacity[link_idx][p], name=f"capacity_{i}_{j}_{p}")",
"revised_correct_code_snippet": "ShipmentCost[link_to_idx[(Cities[i],
Cities[j])]][0][p] * flow[i, j, p]", "model.addConstr(flow[i, j, p] <=
Capacity[link_idx][0][p], name=f"capacity_{i}_{j}_{p}")".
}
}
D.3. Workflow Execution
Initial Implementation
Parameter Extractor:
{"Cities";"Links";"Products";"Supply";
"Demand";"ShipmentCost";"Capacity";"JointCapacity"}
Modeling Advisor:
{"Domain Terminology";"Problem Key Point";"Problem Essence"}
Modeling Expert:
{"VARIABLES";"CONSTRAINTS";"OBJECTIVE"}
Code Generation Expert:
{Gurobi-based Python code}
[Initial Result]:COMPILE_ERROR
First Revision
Modeling Revision Expert:
{Inputs:
- Original Model:
{
"VARIABLES": "flow[i][j][p] is a non-negative continuous variable representing
the number of packages of product p shipped from city i to city j, where i
and j are indices of cities in Cities, and p is an index of products in
Products",
"CONSTRAINTS": ["sum_{j: (i,j) in Links} flow[i][j][p] - sum_{j: (j,i) in Links}
flow[j][i][p] = Supply[i][p] - Demand[i][p] for all i in Cities and all p in
Products", "0 <= flow[i][j][p] <= Capacity[i][j][p] for all (i,j) in Links
and all p in Products", "sum_{p in Products} flow[i][j][p] <=
JointCapacity[i][j] for all (i,j) in Links"],
"OBJECTIVE": "minimize sum_{(i,j) in Links} sum_{p in Products}
ShipmentCost[i][j][p] * flow[i][j][p]"
}
- Error Message:
{‚Äôiter_1‚Äô: ...ShipmentCost[i][j][p] * flow[i, j, p]\n
ÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀÜÀÜÀÜ\nIndexError: list index out of range‚Äô, ‚Äôstatus‚Äô: ‚ÄôRuntime
Error‚Äô}
- Memory Tip:
{No tip for reference}
}
Modeling Expert:
{output:
{
17


--- Page 18 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
"VARIABLES": "flow[i][j][p] is a non-negative continuous variable representing
the number of packages of product p shipped from city i to city j, where
(i,j) is a link in Links and p is a product in Products. The indices i and j
refer to the positions of cities in the Cities list, and p refers to the
position of the product in the Products list.",
"CONSTRAINTS": ["sum_{j: (i,j) in Links} flow[i][j][p] - sum_{j: (j,i) in Links}
flow[j][i][p] = Supply[i][p] - Demand[i][p] for all i in range(len(Cities))
and all p in range(len(Products))","0 <= flow[i][j][p] <= Capacity[i][j][p]
for all (i,j) in Links and all p in range(len(Products))","sum_{p in
range(len(Products))} flow[i][j][p] <= JointCapacity[i][j] for all (i,j) in
Links"
],
"OBJECTIVE": "minimize sum_{(i,j) in Links} sum_{p in range(len(Products))}
ShipmentCost[i][j][p] * flow[i][j][p] -rationale: the objective must only
iterate over existing links (i,j) in Links to avoid index out-of-range
errors; the original code incorrectly assumed a full Cartesian product of
all city pairs, but only specified links are valid."
}
}
Code Revision Expert:
{Inputs:
- Initial Code:
import gurobipy as gp
from gurobipy import GRB
# Extract parameters
Cities = [‚ÄôA‚Äô, ‚ÄôB‚Äô]
Links = [[‚ÄôA‚Äô, ‚ÄôB‚Äô]]
Products = [‚ÄôProduct1‚Äô]
Supply = [[10], [0]]
Demand = [[0], [10]]
ShipmentCost = [[[1]]]
Capacity = [[[10]]]
JointCapacity = [[10]]
# Map cities to indices
city_to_idx = {city: i for i, city in enumerate(Cities)}
n_cities = len(Cities)
n_products = len(Products)
n_links = len(Links)
# Create index mappings for links
link_to_idx = {}
for idx, link in enumerate(Links):
link_to_idx[(link[0], link[1])] = idx
# Initialize model
model = gp.Model("Multi_Product_Transportation")
# Decision variables: flow[i][j][p] = amount of product p shipped from city i to
city j
flow = model.addVars(
n_cities, n_cities, n_products,
lb=0, ub=GRB.INFINITY, name="flow"
)
# Objective: minimize total shipping cost
model.setObjective(
gp.quicksum(
ShipmentCost[i][j][p] * flow[i, j, p]
for i in range(n_cities)
for j in range(n_cities)
for p in range(n_products)
if (Cities[i], Cities[j]) in [(link[0], link[1]) for link in Links]
),
GRB.MINIMIZE
)
# Flow conservation constraints: supply - demand = net outflow
18


--- Page 19 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
for i in range(n_cities):
for p in range(n_products):
outflow = gp.quicksum(flow[i, j, p] for j in range(n_cities) if (Cities[i],
Cities[j]) in [(link[0], link[1]) for link in Links])
inflow = gp.quicksum(flow[j, i, p] for j in range(n_cities) if (Cities[j],
Cities[i]) in [(link[0], link[1]) for link in Links])
model.addConstr(outflow - inflow == Supply[i][p] - Demand[i][p],
name=f"flow_balance_{i}_{p}")
# Per-product capacity constraints
for link_idx, (i_city, j_city) in enumerate(Links):
i = city_to_idx[i_city]
j = city_to_idx[j_city]
for p in range(n_products):
model.addConstr(flow[i, j, p] <= Capacity[i][j][p],
name=f"capacity_{i}_{j}_{p}")
# Joint capacity constraint: total flow on link (sum over products) must not exceed
joint capacity
for link_idx, (i_city, j_city) in enumerate(Links):
i = city_to_idx[i_city]
j = city_to_idx[j_city]
total_flow = gp.quicksum(flow[i, j, p] for p in range(n_products))
model.addConstr(total_flow <= JointCapacity[i][j],
name=f"joint_capacity_{i}_{j}")
# Optimize model
model.optimize()
- Error Message:
{‚Äôiter_1‚Äô: ...ShipmentCost[i][j][p] * flow[i, j, p]\n
ÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀúÀÜÀÜÀÜ\nIndexError: list index out of range‚Äô, ‚Äôstatus‚Äô: ‚ÄôRuntime
Error‚Äô}
- Memory Tip:
{No tip for reference}
}
Code Revision Expert:
{output:"The new code(first revision)"}
[After Revision] Result: COMPILE_ERROR
Second Revision
Modeling Expert:
{inputs:
- Original Model:
{
"VARIABLES": "flow[i][j][p] is a non-negative continuous variable representing
the number of packages of product p shipped from city i to city j, where
(i,j) is a link in Links and p is a product in Products. The indices i and j
refer to the positions of cities in the Cities list, and p refers to the
position of the product in the Products list.",
"CONSTRAINTS": [
"sum_{j: (i,j) in Links} flow[i][j][p] - sum_{j: (j,i) in Links} flow[j][i][p] =
Supply[i][p] - Demand[i][p] for all i in range(len(Cities)) and all p in
range(len(Products))",
"0 <= flow[i][j][p] <= Capacity[i][j][p] for all (i,j) in Links and all p in
range(len(Products))",
"sum_{p in range(len(Products))} flow[i][j][p] <= JointCapacity[i][j] for all
(i,j) in Links"
],
"OBJECTIVE": "minimize sum_{(i,j) in Links} sum_{p in range(len(Products))}
ShipmentCost[i][j][p] * flow[i][j][p] - rationale: the objective must only
iterate over existing links (i,j) in Links to avoid index out-of-range
errors; the original code incorrectly assumed a full Cartesian product of
all city pairs, but only specified links are valid."
19


--- Page 20 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
}
- Error Message:
{‚Äôiter_2‚Äô: ...ShipmentCost[link_to_idx[(Cities[i], Cities[j])]][p] * flow[i, j,
p]...TypeError: can\‚Äôt multiply sequence by non-int of type \‚ÄôVar\‚Äô‚Äô, ‚Äôstatus‚Äô:
‚ÄôRuntime Error‚Äô}
- Memory Tip:
{
"tip_type": "modeling",
"scenario": "transportation",
"error_statement": "The cost of shipping one package of product p from city i to
city j is ShipmentCost_{i, j, p}",
"correct_component": "ShipmentCost[i][j][p] * flow[i, j, p] where i, j are indices
corresponding to the position of cities in the Cities list, and p is the index
of the product in the Products list, but only for (i,j) pairs that exist in
Links",
"incorrect_model": "ShipmentCost[i][j][p] * flow[i, j, p] for all i, j in Cities
and p in Products, without validating that (i,j) is in Links"
}
}
Modeling Expert:
{output:
{
"VARIABLES": "flow[i][j][p] is a non-negative continuous variable representing the
number of packages of product p shipped from city i to city j, where i and j
are indices corresponding to the position of cities in the Cities list, and p
is the index of the product in the Products list, defined only for (i,j) pairs
that exist in Links.",
"CONSTRAINTS": [
"sum_{j: (i,j) in Links} flow[i][j][p] - sum_{j: (j,i) in Links} flow[j][i][p] =
Supply[i][p] - Demand[i][p] for all i in range(len(Cities)) and all p in
range(len(Products))",
"0 <= flow[i][j][p] <= Capacity[i][j][p] for all (i,j) in Links and all p in
range(len(Products))",
"sum_{p in range(len(Products))} flow[i][j][p] <= JointCapacity[i][j] for all
(i,j) in Links"
],
"OBJECTIVE": "minimize sum_{(i,j) in Links} sum_{p in range(len(Products))}
ShipmentCost[i][j][p] * flow[i][j][p] - rationale: ShipmentCost is a 3D array
indexed by [link_index][product_index], where link_index corresponds to the
position of (i,j) in the Links list. Thus, for each link (i,j), we use its
positional index in Links (not city-to-city mapping) to access the correct
cost. The original code incorrectly attempted to map city names to link
indices using a dictionary (link_to_idx), but the cost array is already
indexed by link position in Links, so we directly use i,j as indices into
ShipmentCost assuming the order of Links matches the first dimension of
ShipmentCost."
}}
Programming Expert:
{inputs:
- Initial Code:
import gurobipy as gp
from gurobipy import GRB
# Extract parameters
Cities = [‚ÄôA‚Äô, ‚ÄôB‚Äô]
Links = [[‚ÄôA‚Äô, ‚ÄôB‚Äô]]
Products = [‚ÄôProduct1‚Äô]
Supply = [[10], [0]]
Demand = [[0], [10]]
ShipmentCost = [[[1]]]
Capacity = [[[10]]]
JointCapacity = [[10]]
# Map cities to indices
20


--- Page 21 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
city_to_idx = {city: i for i, city in enumerate(Cities)}
n_cities = len(Cities)
n_products = len(Products)
n_links = len(Links)
# Create index mappings for links
link_to_idx = {}
for idx, link in enumerate(Links):
link_to_idx[(link[0], link[1])] = idx
# Initialize model
model = gp.Model("Multi_Product_Transportation")
# Decision variables: flow[i][j][p] = amount of product p shipped from city i to
city j
flow = model.addVars(
n_cities, n_cities, n_products,
lb=0, ub=GRB.INFINITY, name="flow"
)
# Objective: minimize total shipping cost
model.setObjective(
gp.quicksum(
ShipmentCost[link_to_idx[(Cities[i], Cities[j])]][p] * flow[i, j, p]
for i in range(n_cities)
for j in range(n_cities)
for p in range(n_products)
if (Cities[i], Cities[j]) in link_to_idx
),
GRB.MINIMIZE
)
# Flow conservation constraints: supply - demand = net outflow
for i in range(n_cities):
for p in range(n_products):
outflow = gp.quicksum(flow[i, j, p] for j in range(n_cities) if (Cities[i],
Cities[j]) in link_to_idx)
inflow = gp.quicksum(flow[j, i, p] for j in range(n_cities) if (Cities[j],
Cities[i]) in link_to_idx)
model.addConstr(outflow - inflow == Supply[i][p] - Demand[i][p],
name=f"flow_balance_{i}_{p}")
# Per-product capacity constraints
for link_idx, (i_city, j_city) in enumerate(Links):
i = city_to_idx[i_city]
j = city_to_idx[j_city]
for p in range(n_products):
model.addConstr(flow[i, j, p] <= Capacity[link_idx][p],
name=f"capacity_{i}_{j}_{p}")
# Joint capacity constraint: total flow on link (sum over products) must not exceed
joint capacity
for link_idx, (i_city, j_city) in enumerate(Links):
i = city_to_idx[i_city]
j = city_to_idx[j_city]
total_flow = gp.quicksum(flow[i, j, p] for p in range(n_products))
model.addConstr(total_flow <= JointCapacity[link_idx][0],
name=f"joint_capacity_{i}_{j}")
# Optimize model
model.optimize()
- Error Message:
{‚Äôiter_2‚Äô: ...ShipmentCost[link_to_idx[(Cities[i], Cities[j])]][p] * flow[i, j,
p]...TypeError: can\‚Äôt multiply sequence by non-int of type \‚ÄôVar\‚Äô‚Äô, ‚Äôstatus‚Äô:
‚ÄôRuntime Error‚Äô}
- Memory Tip:
{
"tip_type": "code",
"scenario": "transportation",
"error_statement": "The cost of shipping one package of product p from city i to
city j is ShipmentCost_{i, j, p}",
21


--- Page 22 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
"code_error_location": {
"objective_setting": "IndexError: list index out of range",
"data_processing": "Mismatch between 3D ShipmentCost structure and assumed
indexing"
},
"correct_code_snippet": "ShipmentCost[link_to_idx[(Cities[i], Cities[j])]][p] *
flow[i, j, p]",
"incorrect_code_snippet": "ShipmentCost[i][j][p] * flow[i, j, p]"
}
}
Programming Expert:
{output:"The new code(second revision)"}
[After Revision] Result: ACCEPT
D.4. Analysis of Results
In the initial implementation phase, the Programming Expert erroneously assumed the transportation network to be a fully
connected graph and adopted a Cartesian product traversal of all city pairs to construct the objective function. However, the
data provided in the problem only contains specific sparse links. This exhaustive traversal caused the solver code to trigger
an ‚ÄòIndexError‚Äô (list index out of range) when attempting to access the ‚ÄòShipmentCost‚Äô for non-existent links.
In response to this error, the system triggered the first revision (Attempt 1). The Modeling Expert rapidly identified the
mismatch between the model and the data structure, pointing out that the objective function must iterate only over the
existing links. Accordingly, the Programming Expert introduced a ‚Äòlink to idx‚Äô mapping table and corrected the solver
code logic to traverse only valid links. However, although this correction resolved the index out of range issue, the newly
generated solver code encountered a ‚ÄòTypeError‚Äô during runtime. The error message indicated that the system was attempting
to multiply a sequence by a Gurobi variable, suggesting that the solver code‚Äôs handling of the indexing levels for the 3D
array ‚ÄòShipmentCost‚Äô remained biased, failing to extract scalar values.
Faced with this more subtle data type error, the system conducted a second revision (Attempt 2). During this stage, the
MIRROR performed an in-depth analysis of the 3D list structure of ‚ÄòShipmentCost‚Äô. The Modeling Expert provided a
crucial Memory Tip, explicitly specifying that the correct indexing method must precisely match the data dimensions. Based
on this, the Programming Expert fine-tuned the solver code again, correcting the indexing for the cost and capacity arrays
(e.g., adjusting from ‚ÄòShipmentCost[...][p]‚Äô to ‚ÄòShipmentCost[...][0][p]‚Äô to adapt to the data structure).
After these two rounds of feedback-based iterative optimization, the solver code finally passed compilation and was solved
successfully (Final result: ACCEPT). This process demonstrates that the IAR mechanism can not only repair surface-level
syntax errors but also progressively resolve deep-seated logical flaws and data structure alignment issues through multiple
rounds of ‚Äútrial-analysis-correction‚Äù loops, thereby ensuring the correctness and robustness of the final solution.
E. Prompt Templates for Agents
Prior to acceptance, and to prevent unintended disclosure of the full implementation, only partial prompt templates are
shown. In this section, we present representative excerpts of the prompt templates used by each expert agent in the MIRROR
framework. These excerpts illustrate the core instruction structure that guides the Large Language Models in performing
their specialized roles. Note that content enclosed in curly braces denotes dynamic placeholders. At runtime, these are filled
with context-specific information‚Äîsuch as the user‚Äôs problem statement, feedback from the Iterative Adaptive Revision
(IAR) mechanism, or relevant modeling exemplars retrieved via hierarchical retrieval. This modular and context-aware
prompting design enables MIRROR to generalize across diverse optimization tasks while maintaining high fidelity and
controllability.
Parameter Extractor
ROLE_DESCRIPTION = ‚ÄôYou are an assistant that extracts parameters and their types or
shape from the given problem.‚Äô
GENERATION_TASK = ‚Äô‚Äô‚Äô
22


--- Page 23 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
Please Extract parameters along with their concise definitions from the problem
description:
{problem_description}
The comment from your colleague is:
{comment_text}
**Key Principles for Parameter Type:
...
Your output should be in JSON format as follows:
{{
"Parameter1": {{"Type": ...,
"Definition": ...},
"Parameter2": {{"Type": ...,
"Definition": ...},
...
}}
Provide only the requested JSON output without any additional information.
‚Äô‚Äô‚Äô
Modeling Advisor
ROLE_DESCRIPTION = """You are a senior operations research expert. Your role is to
provide constructive, positive insights about the problem‚Äôs domain terminology,
key points, and essential nature-based strictly on the given text. Never
critique, imply missing information, or speculate."""
FORWARD_TASK = """Review the following problem description and your colleague‚Äôs
comment:
{problem_description}
{comment_text}
Provide 2-3 concise, helpful insights that may support accurate modeling.
Each insight must belong to exactly one of these categories:
- "Domain Terminology":...
- "Problem Key Point": ...
- "Problem Essence": ...
All insights must be directly inferable from the provided text-do not invent
assumptions.
Output ONLY a JSON list in this exact format:
[
{{
"category": "Domain Terminology" | "Problem Key Point" | "Problem Essence",
"insight": "A clear, practical sentence."
}}
]
No other text, formatting, or explanation."""
Modeling Expert
ROLE_DESCRIPTION = ‚ÄôYou are a modeling assistant specialized in the field of
Operations Research for mathematical formulation.‚Äô
FORWARD_TASK = ‚Äô‚Äô‚ÄôNow the origin problem is as follows:
{problem_description}
You can refer to the parameters and other information provided by your colleagues:
{comments_text}
please generate a optimization model for the problem.
Your output format should be a JSON like this:
{{
"VARIABLES": ...
"CONSTRAINTS": ...
"OBJECTIVE": ...
}}
Don‚Äôt give any other information.
23


--- Page 24 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
‚Äô‚Äô‚Äô
Modeling Revision Expert
REVISION_TASK = ‚Äô‚Äô‚ÄôYou are a senior operations research expert performing debugging
and knowledge distillation.
Your task is to Diagnose the root cause and produce a fully corrected model.
and Extract a structured tip encoding: what went wrong, where, and how to fix
it - for future reuse. based on:
- The original problem description
- The incorrect model
- The solver error
- The last tip
### Input
Problem Description:
{problem_description}
###Original (Incorrect) Model:
{original_model}
###Execution Error (from the code):
{error_message}
### The Last tip for reference
The last tip is provided for reference only and may be incorrect - please carefully
verify and use your judgment
{last_tip}.
### Output Requirement
Return a tuple of two JSON objects:
First: A tip JSON with EXACTLY the following keys::
{{
"tip_type":...
"scenario": ...
"error_statement": ...
"correct_component":...
"incorrect_model": ...
}}
Second: The fully corrected model in standard JSON format:
{{
"VARIABLES": ...
"CONSTRAINTS": ...
"OBJECTIVE": ...
}}
You MUST output in exactly this format:
TIP_JSON
<split>
CORRECTED_MODEL_JSON
No additional text, explanations, or formatting marks are allowed.‚Äô‚Äô‚Äô
Code Expert
ROLE_DESCRIPTION = ‚ÄôYou are a Python programmer specializing in operations research
and optimization.‚Äô
FORWARD_TASK = ‚Äô‚Äô‚ÄôYou are presented with a specific problem and tasked with
developing an efficient Python program to solve it.
The original problem is as follows:
{problem_description}
Your colleague has constructed a mathematical model for reference:
{comments_text}
Please note that this model may contain errors and is used as a reference.
...
Requirements:
1. ...
24


--- Page 25 ---
MIRROR: A Multi-Agent Framework for Optimization Modeling
2. ...
3. ...
4. ...
Code Revision Expert
REVISION_TASK = ‚Äô‚Äô‚ÄôYou are debugging an optimization implementation that encountered
errors during execution.
The original problem and attempted solution are as follows:
### Problem Context
{problem_description}
### Original Code
{initial_code}
### Execution Error
{error_message}
### The Last tip for reference
- The last tip is provided for reference only and may be incorrect - please
carefully verify and use your judgment
{last_tip}.
Your goal is twofold:
1. Diagnose the root cause and produce a fully corrected implementation.
2. Extract a structured tip encoding: what went wrong, where, and how to fix it -
for future reuse.
Output a tuple of two JSON objects:
(
{{
"tip_type": ...,
"scenario": ...
"error_statement": ...
"correct_code_snippet": ...
"incorrect_code_snippet": ...
}},
"Full corrected Python code using Gurobipy"
)
Rules:
-...
- ...
- ...
- ...
Return exactly:
TIP_JSON
<split>
CORRECTED_CODE_JSON
‚Äô‚Äô‚Äô
25
