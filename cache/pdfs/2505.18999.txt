--- Page 1 ---
arXiv:2505.18999v1  [cs.IR]  25 May 2025
Lightweight Embeddings with Graph Rewiring for
Collaborative Filtering
XURONG LIANG, The University of Queensland, Australia
TONG CHEN, The University of Queensland, Australia
WEI YUAN, The University of Queensland, Australia
HONGZHI YINâˆ—, The University of Queensland, Australia
GNN-based recommender systems have become increasingly popular in academia and industry due to their
ability to capture high-order information from user-item interaction graphs. However, as recommendation
services scale rapidly and their deployment now commonly involves resource-constrained edge devices, GNN-
based models face significant challenges, including high embedding storage costs and runtime latency from
graph propagations. Our previous work, LEGCF, effectively reduced embedding storage costs but struggled to
maintain recommendation performance under stricter storage limits. Additionally, LEGCF did not address the
extensive runtime computation costs associated with graph propagation, which involves heavy multiplication
and accumulation operations (MACs). These challenges consequently hinder effective training and inference
on resource-constrained edge devices. To address these limitations, we propose Lightweight Embeddings
with Rewired Graph for Graph Collaborative Filtering (LERG), an improved extension of LEGCF. LERG
retains LEGCFâ€™s compositional codebook structure but introduces quantization techniques to reduce the
storage cost of embedding weights, enabling the inclusion of more meta-embeddings within the same storage
constraints for improved model expressiveness. To optimize graph propagation for edge devices, we pretrain
the quantized compositional embedding table using the full interaction graph on resource-rich servers, after
which a fine-tuning stage is engaged to identify and prune low-contribution entities via a gradient-free binary
integer programming approach, constructing a rewired graph that excludes these entities (i.e., user/item
nodes) from propagating signals. The quantized compositional embedding table with selective embedding
participation and sparse rewired graph are transferred to edge devices which significantly reduce computation
memory and inference time. Experiments on three public benchmark datasets, including an industry-scale
dataset, demonstrate that LERG achieves superior recommendation performance while dramatically reducing
storage and computation costs for graph-based recommendation services.
CCS Concepts: â€¢ Information systems â†’Recommender systems.
Additional Key Words and Phrases: Lightweight Recommender System; Compositional Embedding; Graph
Collaborative Filtering
ACM Reference Format:
Xurong Liang, Tong Chen, Wei Yuan, and Hongzhi Yin. 2018. Lightweight Embeddings with Graph Rewiring
for Collaborative Filtering. In Proceedings of Make sure to enter the correct conference title from your rights
âˆ—Hongzhi Yin is the corresponding author.
Authorsâ€™ Contact Information: Xurong Liang, xurong.liang@uq.edu.au, The University of Queensland, Brisbane, Australia;
Tong Chen, tong.chen@uq.edu.au, The University of Queensland, Brisbane, Australia; Wei Yuan, w.yuan@uq.edu.au, The
University of Queensland, Brisbane, Australia; Hongzhi Yin, h.yin1@uq.edu.au, The University of Queensland, Brisbane,
Australia.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 2 ---
2
Xurong Liang et al.
confirmation emai (Conference acronym â€™XX). ACM, New York, NY, USA, 30 pages. https://doi.org/XXXXXXX.
XXXXXXX
1
Introduction
Recommender systems have been widely deployed in our daily lives to help people make better
decisions when shopping, listening to music, watching movies, and more [12, 19, 41, 78, 112].
Early recommender systems directly compute the dot product between representation vectors (i.e.,
embeddings) of users and items for recommendation score calculation [65]. Following the recent
surge of study in graph neural networks (GNNs) [48, 87], GNN-based recommender systems, which
leverage the power of GNNs to learn user and item representations, have been a popular deployment
choice and achieved remarkable success in both academia [25, 81, 102, 103] and industry [23, 58]
due to their strong capability in capturing and propagating user/item nodesâ€™ collaborative signals
across their neighbors for interaction modeling [87].
As the need for scalability and decentralized services arises, researchers have shifted their
attention to optimizing the training efficiency and model deployment [9, 92], especially on edge
devices. Traditionally, recommender systems follow a cloud-based deployment mechanism, where
the recommendation model is wholly trained and stored on a resource-rich server. The edge
devices only act as agents that query and receive the recommendation results from the central
server via a communication link [101]. This working mechanism has been criticized for being
overly reliant on internet access and central servers as well as the high response latency caused by
communication overhead [101]. To this end, recent studies [9, 39, 46, 51, 53, 55, 60, 61, 73, 92, 111]
have been focusing on deploying lightweight recommender systems on resource-constrained edge
devices directly. A general deployment pipeline for this kind of work involves initially training
a full-scale recommender system on a central server, followed by compressing and deploying a
lightweight version to edge devices to accommodate their hardware constraints. The on-device
model is expected to perform inference fully on board instead of querying the central server in
real-time [101]. Considering the versatility and robust performance of GNN-based recommender
systems in cloud-based settings, how to efficiently train and deploy GNN-based recommender
systems on edge devices raises extensive demands [87]. However, the transition to edge-based
recommendation services faces two major challenges.
The first major challenge is the substantial storage cost of the recommendation model caused
by the heavily parameterized embedding table, a problem widely acknowledged in embedding
optimization research [34]. While many approaches attempt to address this, they often fail to
achieve a satisfactory balance between storage cost and recommendation performance. For instance,
dimension search algorithms [9, 43, 53, 60, 61, 109] determine the optimal embedding dimension
size for each user/item. However, the objectives of these methods typically prioritize maintaining
model accuracy and have weak or none model size constraints, making it difficult to adhere to tight
storage budgets. Pruning-based methods [46, 59, 97] are effective in reducing storage by eliminating
redundant or irrelevant embedding parameters, but this is at the cost of significantly reducing
the number of usable embedding dimensions available to each user/item. Under strict storage
constraints, this aggressively weakens the embeddingsâ€™ expressiveness and can significantly hinder
recommendation performance. Moreover, both dimension search and pruning-based approaches
involve resource-intensive and time-consuming search or retraining processes, limiting their
feasibility for large-scale deployment. The other promising optimization strategy, compositional
embedding [38, 39, 69, 84], maps multiple users/items to shared meta-embeddings to reduce the
total number of embeddings stored. Our prior work, LEGCF [38], exemplifies this approach. LEGCF
introduces a compact, dense codebook combined with a scalable, sparse assignment weight matrix,
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 3 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
3
generating entity1 embeddings by a unique combination of two meta-embeddings with assignment
weights. Additionally, the assignment weight matrix is learnable, allowing dynamic updates to better
capture evolving entity representations. Despite significantly reducing storage costs, LEGCF still
faces challenges in balancing storage constraints with recommendation performance. Specifically,
under strict storage constraints, LEGCF may encounter severe "collisions" of meta-embeddings
when using a codebook to represent entity embeddings, i.e., a large number of entity embeddings
may be represented by the same meta-embedding, resulting in the degradation of recommendation
performance.
The second major challenge in deploying GNN-based recommender systems on resource-
constrained edge devices is the high runtime computation cost. GNN-based recommender systems
rely extensively on repetitive graph propagation operations to exchange collaborative signals
between entities and encode them into their embeddings [87]. For simplicity, consider a graph
convolution network (GCN) [25, 31] without nonlinear activation, where graph propagation is
effectively a matrix multiplication between the graph adjacency and entity embedding matrices.
The computation cost of matrix multiplication is typically measured by the number of multipli-
cation and accumulation operations (MACs) it requires. As an example, our largest experimental
dataset iFashion (see Section 4.1.1) entails over 2.6 million interactions among a total of 2 million
users and items. Propagating embeddings across such a graph using a GCN requires 10 billion
MACs in a single pass, even before taking multiple convolution layers into account. Although
some embedding compression techniques can compact GNN-based recommender systems to fit on
a device [34], they often overlook the runtime computation costs. As a result, the accumulative
MACs of GNN-based recommendation models not only hurt inference efficiency, but also negate
the potential of performing on-device fine-tuning to handle new interactions over time. Our prior
work, LEGCF, for example, constructs an expanded interaction graph and propagates both entity
and codebook embeddings during the assignment weight update phase. This process imposes a
substantial runtime computation cost, hindering deployment on edge devices. Several general-
purpose techniques [7, 22, 24, 64, 85, 104] address this issue by performing mini-batch training
during graph propagation, using sampled neighbors [22, 24] or subgraphs [11, 64, 104] for feature
propagation in each batch. However, the sampling process may take a considerably long time
for large-scale graphs. Also, the sampling heuristics play an important role in determining entity
embedding quality, and the accuracy of GNN-based recommender systems may be heavily affected
when the graph-propagated entity embeddings do not correctly reflect the neighbor relations and
collaborative semantics.
In this paper, we extend our previous work [38] and propose an efficient framework designed
for lightweight GNN-based recommendation, namely Lightweight Embeddings with Rewired
Graph for Graph Collaborative Filtering (LERG). To address the substantial storage cost of the
embedding table, we build upon the design of LEGCF to employ a compositional embedding table
comprising a dense codebook and a highly sparse assignment weight matrix. We identified that
LEGCFâ€™s performance degradation stems from its reliance on conventional 32-bit floating-point
precision for meta-embeddings, which consumes significant storage. Under stringent storage
constraints, reducing the number of meta-embeddings in the codebook becomes necessary, leading
to diminished compositional diversity and compromised entity embedding uniqueness, ultimately
impacting model accuracy. In this regard, quantization [21, 54, 57, 71, 77] offers an effective solution
to reduce the storage size of individual meta-embedding elements. Widely validated in the field
of large language models (LLMs) [6, 54, 76, 77, 89], quantization preserves model weight quality
while accelerating training and inference. While one can apply quantization techniques to the
1In this paper, we term both users and items as entities for convenience.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 4 ---
4
Xurong Liang et al.
full embedding table directly [21, 33, 93] to optimize the entity embeddings, an aggressively low
precision is often used to meet a tight storage constraint, which heavily impedes the fidelity of
entity embeddings. Therefore, instead of applying quantization techniques to the full embedding
table, LERG quantizes each full-precision meta-embedding in the dense codebook into a low-bit
integer representation. This allows the embedding table to accommodate more meta-embeddings
within the same storage space, reducing the risk of hash collisions and enhancing the uniqueness
of entity embeddings.
To enable computationally efficient graph propagation on resource-constrained edge devices, we
first pretrain the compositional embedding table on resource-rich servers using the full user-item
interaction graph. Next, we evaluate each entityâ€™s contribution to collaborative signal propagation
by framing this as a gradient-free binary integer programming problem. Entities with the lowest
contributions are identified, and a rewired graph is generated that excludes these entities from
propagating collaborative signals to their neighbors. The pretrained, quantized compositional em-
bedding table is then efficiently fine-tuned using the rewired graph. To further reduce peak memory
consumption and interference during fine-tuning, we disable the embeddings of pruned entities
from refining the quantized compositional embedding table. Instead, these embeddings are drawn
from a small set of placeholder meta-embeddings derived from the pretrained graph-propagated
entity embeddings. Consequently, only a subset of entity embeddings participates in graph propa-
gation, further lowering runtime computation costs. For deployment, only the pretrained quantized
compositional embedding table, the rewired graph and the set of placeholder meta-embeddings
along with its assignment vector for pruned entities are transmitted to resource-constrained edge de-
vices. This approach minimizes the storage cost of the embedding table while significantly reducing
the number of MACs required for graph propagation, alleviating computational complexity.
We summarize our main contributions as follows:
â€¢ We analyze the features and drawbacks of common embedding optimization methods and
graph computation improvement work. We reinforce the importance of controlling the
storage cost of the embedding table and the runtime memory consumption of GNN-based
recommender systems on resource-constrained edge devices.
â€¢ We put forward a substantially improved GNN-based lightweight recommender system,
namely LERG, as an extension to our prior work LEGCF. Essentially, LEGCF proposes using
a dense codebook for entity embedding storage, in which all meta-embeddings are stored in
32-bit floating-point precision. A highly sparse meta-embedding assignment matrix is learned,
controlling the meta-embedding composition of each entity. In LERG, we further reduce
the storage cost of meta-embeddings in the codebook by applying quantization techniques.
Under the same storage budget, the quantized compositional codebook allows more meta-
embeddings to be stored, thus massively improving entity composed embedding uniqueness.
Meanwhile, to address the runtime memory overhead of LEGCF, LERG uses a fixed assignment
matrix, and a graph rewiring method is proposed to allow efficient propagation on a pruned
interaction graph. The design of the rewired sparse graph significantly reduces runtime
memory usage and computational time for graph propagation, enabling efficient fine-tuning
and inference on edge devices.
â€¢ We conduct extensive experiments on three public benchmark datasets, including one on
an industry scale. The experimental results verify the state-of-the-art recommendation
performance of LERG compared with recent lightweight baselines.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 5 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
5
2
Related Work
In this section, we outline work that is relevant to our research. We separate the related work into
three categories: embedding storage optimization, server-edge computing paradigm and graph
computation complexity optimization.
Embedding Storage Optimization. To reduce the storage cost of the embedding table, most di-
mension search algorithms [27, 43, 60, 61, 63, 109] formulate automated machine learning (AutoML)
processes to select optimal dimension sizes from a predefined candidate size set. Some others [9, 53]
employ evolutionary search to identify an optimal embedding structure. Pruning-based methods
[32, 39, 46, 59, 62, 74, 97] instead define a pruning constraint on the embedding table. As the value
update process continues, elements considered redundant or unimportant will be nullified to save
space. The compositional embedding scheme is another major direction for embedding storage
optimization. These methods reduce the number of embeddings to be stored on devices by letting
multiple entities share the same set of embeddings (i.e., meta-embeddings). The most common meta-
embedding assignment strategy is to design a set of fixed hash functions [35, 69, 84, 105]. LEGCF
[38] introduces a learnable meta-embedding assignment scheme to improve flexibility in generating
entity embeddings. Our work roots from the backbone of LEGCF but avoids the assignment update
process as it requires the involvement of meta-embeddings in graph propagation, which introduces
additional graph computation overhead. Locality-sensitive hashing (LSH) can also be deployed
for meta-embedding assignment [14, 15]. Tensor train (TT) decomposition [64, 79, 90, 99, 100] is
another form of compositional embedding technique which forms entity embeddings as a product
of tensors. Some researchers propose to learn several codebooks [36, 37, 42, 44, 91, 108], each entity
embedding is composed of one codeword drawn from each codebook. This strategy is termed vector
quantization [101] and can be regarded as a form of compositional embedding technique as well.
Server-Edge Computing Paradigm. The main usage of server-edge computing paradigm is in
federated learning [45, 47, 107], in which the edge devices first train a local model using a small local
dataset, the parameters of locally trained models are then uploaded to central servers to perform a
global aggregation [101]. After that, the aggregated parameters are distributed to edge devices for
inference. Such a training paradigm avoids the exposure of local data to the public, thus providing
data security and user privacy. Another branch of research lines in on-edge fine-tuning, wherein
a foundation model is first pretrained on resource-rich servers. After that, the foundation model
is fine-tuned on edge devices to fit downstream tasks [83]. [80, 94] conducts fine-tuning on edge
devices to update the entire pretrained foundation model, which elevates concerns on feasibility due
to limited hardware specifications. [55] proposes to only update a part of the pretrained model to
reduce the computation overhead on edge devices. Patch-learning [52, 96] is another direction that
introduces a small patch model on top of the pretrained model. In the fine-tuning stage, only the
weights of the patch model are updated to adopt the model to local deployment. Our work fine-tunes
the quantized compositional embedding table by leveraging only a part of entity embeddings to
control the peak runtime memory usage. The propagation graph is also rewired to reduce the graph
computational complexity.
Graph Computation Complexity Optimization. To reduce the graph computation complexity,
a node sampling strategy is commonly used, which samples a subset of nodes from the neighborhood
of the target node to perform graph propagation at each mini-batch [16]. Initially, the uniform
sampling strategy is implemented [4, 24]. To further optimize the sample time for large graphs,
the sampling is later conducted in a layer-wise manner [3, 26, 113]. The sampling strategy can
also be conducted at the graph level so that a large graph is split into multiple subgraphs, each
of which contains sampled nodes and edges. GraphSAINT [104] is a typical example of using
the graph sampling technique to bring down the computation cost for graph propagation. Other
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 6 ---
6
Xurong Liang et al.
methods [11, 40] seek to partition the original large graph into multiple subgraphs so that graph
propagation is only conducted within each partition, or leverage the fast Fourier transform algorithm
to map the graph vertex space into Fourier domain for efficient cross-correlation computation
[106]. Another research direction for graph computation complexity optimization focuses on
graph sparsification, in which the number of connected edges in the graph is lowered so that the
graph computation overhead, runtime memory consumption and communication latency can be
improved when performing graph propagation over the entire graph [48]. There are methods, such
as DropEdge [67], which adopt handcrafted heuristics to decide the edges to be dropped. However,
the heuristics for edge removal can be hard to tune and yet the model accuracy will be highly
reliant upon the quality of the sparsified graph. Some other techniques [8, 95, 110] construct an
optimization constraint to learn a sparsified version of the interaction graph, despite this strategy
often introducing additional trainable parameters. In our work, we leverage the pretrained graph-
propagated embeddings to identify entity contributions in graph propagation and remove edges of
entities with limited contributions, which can be regarded as a graph sparsification technique as
well.
3
Method
In this section, we detail the components of LERG, which includes a quantized compositional
embedding table and a rewired propagation graph for embedding fine-tuning on edge devices.
For convenience, we summarize the list of important symbols and notations introduced in this
section in Tab. 1. We also attach the framework visualization diagram in Fig. 1, which includes
4 important stages: (a) quantized compositional embedding table pretraining; (b) graph rewiring
for graph sparsification; (c) placeholder meta-embedding generation and assignment for pruned
entities and; (d) entity embedding generation on edge devices for inference (and fine-tuning).
3.1
Preliminaries
In this work, we focus on the common ID-based recommendation, wherein GNN-based recom-
mender systems are a primary choice for interaction modeling [5, 25, 56, 70, 81, 102, 103]. We denote
the set of users U and the set of items I. The total number of users and items is ğ‘= |U| + |I| and
each user/item is assigned a unique ID ğ‘—âˆˆ[1, ğ‘]. The user-item interaction matrix can be defined
as a binary matrix R âˆˆ{0, 1}|U|Ã—|I| with 0 and 1 respectively indicating unobserved and observed
interactions. In GNN-based recommender systems, the signal propagation is often conducted via a
graph adjacency matrix A âˆˆRğ‘Ã—ğ‘created using the interaction matrix R:
A =
 0
R
RâŠ¤
0

.
(1)
Conventional GNN-based recommender systems make use of a full embedding table E âˆˆRğ‘Ã—ğ‘‘,
where each user/item is assigned a physically unique embedding vector ğ‘’ğ‘—âˆˆRğ‘‘. To obtain graph-
propagated embeddings H âˆˆRğ‘Ã—ğ‘‘, the full embedding table E is treated as the input embedding
H(0). The hidden embeddings of the next layer H(ğ‘™+1) is generated by propagating the hidden
embeddings of the current layer Hğ‘™with the symmetrically normalized graph adjacency matrix
Dâˆ’1
2 ADâˆ’1
2 , where ğ‘™âˆˆ[1, ğ¿] is the current layer number, ğ¿is the total number of propagation
layers and D âˆˆRğ‘Ã—ğ‘is the diagonal degree matrix of A. The final graph-propagated embeddings
H âˆˆRğ‘Ã—ğ‘‘can then be generated by taking the mean of all hidden embeddings. The above
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 7 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
7
Table 1. Table of symbols and notations.
Symbol
Explanation
U
Set of users.
I
Set of items.
N
Total number of users and items.
R
Binarized user-item interaction matrix.
A
Full user-item interaction graph.
Eğ‘šğ‘’ğ‘¡ğ‘
Original compositional codebook.
ğ‘
Number of meta-embeddings in the compositional codebook.
S
Highly sparse assignment weight matrix.
Î”
Learnable step size vector for quantization.
Â¯Eğ‘šğ‘’ğ‘¡ğ‘
Quantized compositional codebook.
ğ‘
Bit length for quantization.
Ë†Eğ‘šğ‘’ğ‘¡ğ‘
De-quantized compositional codebook.
Ë†E
Inferred full embedding table from the quantized compositional embedding table.
Â¯Eğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘’ğ‘¡ğ‘
Pretrained quantized compositional codebook.
Î”ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
Pretrained learnable step size vector.
Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
Pretrained graph-propagated full embedding table.
ğ¿
Total number of graph propagation layers.
B
Entity-entity similarity matrix.
ğ‘š
Number of entities to be retained in the rewired graph.
ğ‘œ
Predefined rounding boundary for the linear programming problem formulated for the graph rewiring process.
ğ‘‡
The highest propagation degree used in the graph rewiring algorithm.
Nğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›
Set of retained entities in the rewired graph.
Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
Set of entities to be refrained from propagating their collaborative signals in the rewired graph.
Aâ€²
Rewired propagation graph used in the embedding fine-tuning and inference stages.
Hğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›
graph-propagated embeddings for retained entities in the fine-tuning stage.
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
Placeholder meta-embeddings for pruned entities.
ğ‘
Number of placeholder meta-embeddings in Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’.
ğ‘„
Placeholder assignment vector for pruned entities.
Ë†Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
Imputed embedding table for pruned entities.
descriptions are formulated as:
H(0) = E,
H(ğ‘™+1) = (Dâˆ’1
2 ADâˆ’1
2 )H(ğ‘™),
H =
1
ğ¿+ 1
ğ¿
âˆ‘ï¸
ğ‘™=0
H(ğ‘™).
(2)
In downstream recommendation settings, for user ğ‘¢âˆˆU and item ğ‘–âˆˆI, their graph-propagated
embeddings hğ‘¢, hğ‘–âˆˆRğ‘‘are drawn from H to perform affinity score calculation:
Ë†ğ‘¦ğ‘¢ğ‘–= hâŠ¤
ğ‘¢hğ‘–.
(3)
The predicted score Ë†ğ‘¦ğ‘¢ğ‘–is then inputted to common recommendation loss functions, such as the
Bayesian personalized ranking (BPR) loss [66], for embedding optimization:
LBPR =
âˆ‘ï¸
(ğ‘¢,ğ‘–+,ğ‘–âˆ’)âˆˆB
âˆ’lnğœ(bğ‘¦ğ‘¢ğ‘–+ âˆ’bğ‘¦ğ‘¢ğ‘–âˆ’) + ğœ†||Î˜||2,
(4)
where B is either the whole training set or a training batch, (ğ‘¢,ğ‘–+,ğ‘–âˆ’) is a triplet that contains
sampled user ğ‘¢â€™s observed interacted item ğ‘–+ and unvisited item ğ‘–âˆ’, ||Î˜||2 is the ğ¿2 regularization
over trainable parameters and ğœ†controls its weight in the loss.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 8 ---
8
Xurong Liang et al.
Fig. 1. The overall workflow of LERG. (a) corresponds to the quantized compositional embedding table
pretraining stage described in Sec. 3.4, (b) corresponds to graph rewiring for graph sparsification in Sec. 3.5,
(c) corresponds to pruned entity embedding imputation in Sec. 3.6, (d) corresponds to entity embeddings
generation in the inference and fine-tuning stages described in Sec. 3.6. (a), (b), (c) are all conducted on the
resource-rich server side, the fine-tuning stage in (d) can be conducted either on-server or on-edge. The red
dotted arrows in (b) indicate the constructed edges between ğ‘£4 and her indirect neighbors after the graph
rewiring process.
3.2
Compositional Embedding Table
In Sec. 3.1, we mention that the full embedding table is widely deployed in GNN-based recom-
menders. However, amid the increasing scale of users and items in recommendation services, the
storage cost of the embedding table also soars rapidly, introducing difficulties in hosting large-scale
datasets on edge devices as they are typically equipped with limited storage space.
To reduce the storage cost, we instead opt for a compositional embedding table, which contains far
fewer entries than the original full embedding table E. We follow our previous work [38] to devise
a dense codebook Eğ‘šğ‘’ğ‘¡ğ‘âˆˆRğ‘Ã—ğ‘‘and a highly sparse assignment weight matrix S âˆˆRğ‘Ã—ğ‘
â‰¥0 , where
ğ‘is the number of meta-embeddings stored in the codebook Eğ‘šğ‘’ğ‘¡ğ‘. Each row of the assignment
matrix S corresponds to a user/item, indicating which of the meta-embeddings are used to compose
this entityâ€™s embedding vector as well as their weights in the composition.
Assignment Matrix Initialization. Given that similar entities are expected to own similar
meta-embeddings, a good initialization of assignment matrix S can potentially contribute to higher
recommendation accuracy and a speedup in training. In S, for each entity ğ‘, we term its highest-
weighted meta-embedding (among the ğ‘¡selected) the anchor meta-embedding indexed by ğ‘âˆ—
ğ‘, and
it forms the base image of the entityâ€™s embedding eğ‘since it takes the largest proportion in eğ‘â€™s
composition. To let correlated entities have similar base embedding images during initialization, we
propose to assign two entities ğ‘1, ğ‘2 the same anchor meta-embedding weight if they demonstrate
a high affinity with each other, i.e., S[ğ‘1,ğ‘âˆ—
ğ‘1] = S[ğ‘1,ğ‘âˆ—
ğ‘2]. Because the entities lying within each
community in fact form tightly connected subgraphs in the full interaction graph, assigning them
the same anchor meta-embedding helps pass the same proximity onto the latent space. In our work,
we take advantage of a well-established multilevel graph partitioning algorithm, namely METIS
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 9 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
9
[29] for partitioning the user-item interaction graph. Although other advanced graph clustering
methods can also be considered, METIS well serves the one-off initialization purpose due to its
fast and accurate computation (no learning involved), balanced and non-overlapping partitions (all
anchor meta-embeddings are fairly utilized), as well as deterministic results (ease of replication).
We set the desired partition number to ğ‘, where entities in the same subgraph share one specific
anchor meta-embedding in Eğ‘šğ‘’ğ‘¡ğ‘. To reflect this, on initialization of the assignment matrix S, we
perform the following for every entity ğ‘:
I. Given entity ğ‘â€™s subgraph index ğ‘ğ‘âˆˆ{1, 2, ...,ğ‘}, we set S[ğ‘,ğ‘âˆ—
ğ‘] = ğ‘¤âˆ—, where ğ‘âˆ—
ğ‘= ğ‘ğ‘and ğ‘¤âˆ—is a
universal hyperparameter.
II. We uniformly sample ğ‘¡âˆ’1 indexes from {1, 2, ...,ğ‘} \ ğ‘ğ‘with replacement, denoted as a set Q.
For every ğ‘âˆˆQ, we set the corresponding assignment weight S[ğ‘,ğ‘] = (1âˆ’ğ‘¤âˆ—)
ğ‘¡âˆ’1 .
III. For all remaining entries at ğ‘â€² âˆ‰Q âˆªğ‘ğ‘, we set S[ğ‘,ğ‘â€²] = 0.
In short, each entity receives an initial anchor meta-embedding based on the subgraph assigned,
while the remaining ğ‘¡âˆ’1 meta-embedding assignments are randomly initialized in favor of diversity.
Our hyperparameter study in [38] shows that so long as there is more than one meta-embedding
assigned to each user/item with the anchor meta-embedding ğ‘âˆ—
ğ‘carefully selected on initialization,
a competitive recommendation performance is guaranteed. Hence, we select 2 meta-embeddings
for each user/item, one of which is the anchor meta-embedding ğ‘âˆ—
ğ‘selected by following step I of
the sampling strategy mentioned above. The other auxiliary meta-embedding ğ‘is randomly drawn
from the codebook as depicted in step II above to encourage the uniqueness of composed entity
embeddings. Following step III, all remaining entries in S have the value of 0. In this way, we ensure
the assignment matrix S is highly sparse, as each row has only 2 nonzero elements. In our previous
work [38], we study the impact of different weights assigned to the anchor meta-embedding ğ‘¤âˆ—.
Our finding is that setting ğ‘¤âˆ—= 0.9 yields a satisfactory recommendation performance when
the dataset contains a relatively large number of users and items. Therefore, LERG follows the
weight distribution scheme of 0.9/0.1 split between the anchor meta-embedding ğ‘âˆ—
ğ‘and auxiliary
meta-embedding ğ‘as well.
To infer the full embedding table Ë†E âˆˆRğ‘Ã—ğ‘‘for downstream recommendation tasks, one can
simply perform the matrix multiplication between S and Eğ‘šğ‘’ğ‘¡ğ‘:
Ë†E = SEğ‘šğ‘’ğ‘¡ğ‘.
(5)
With the assignment matrix S, one can also flexibly update composition weights assigned to each
meta-embedding for each user/item to reflect the change of tendency toward meta-embeddings
over time. For example, in LEGCF [38], we design an expanded interaction graph that treats meta-
embeddings as virtual nodes to perform simultaneous graph propagation between meta-embeddings
and entity embeddings. The graph-propagated embeddings are then used to update the assignment
weight matrix S by solving a similarity constraint.
By replacing the full embedding table E with a dense codebook Eğ‘šğ‘’ğ‘¡ğ‘and a highly sparse
assignment weight matrix S, the storage cost of the embedding layer can be greatly reduced from
ğ‘‚(ğ‘ğ‘‘) to ğ‘‚(ğ‘ğ‘‘+ 2ğ‘). The compositional embedding structure performs row-wise compression on
the embedding table, since the number of unique embedding vectors to be stored on the disk is
lowered.
3.3
Codebook Quantization-Aware Training
In Sec. 3.2, we have detailed the core components and working mechanisms of our prior work
LEGCF. From this section onward, we unfold the design of its enhanced version, namely LERG.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 10 ---
10
Xurong Liang et al.
In LEGCF, despite that we leverage the codebook to bring down the number of embedding rows
to be stored on disk, each unique embedding vector is still saved as a sequence of floating point
numbers. Since the size of each floating point number is 4 bytes, to store a single meta-embedding
with a relatively large dimension size, say 128, 512 bytes will be utilized. For edge devices with
limited storage space, the number of usable meta-embeddings can be throttled by the size of a
single meta-embedding vector. To solve this, there has been a broad coverage of research focusing
on compressing elements in embeddings or model parameters using quantization [49, 57, 77, 98].
The key idea is to map each continuous floating point number element in the embedding vector
into discrete integer chunks so that the elements can be stored efficiently using low-bit fixed-
point representations, like INT8/16. As such, the storage size of each embedding element can be
squeezed to 1 byte and 2 bytes respectively, reducing the original storage size effectively by 75%
and 50%. To learn accurate fixed-point representations, quantization-aware training (QAT) has
been a popular training paradigm for quantizing the model parameters [6, 17, 49, 54, 77] due to its
compatibility with gradient descent update and ease of implementation [101]. In QAT, a floating
point precision embedding table is used as a backbone parameter for back-propagation updates.
In forward passes, the full-precision embeddings are quantized into low-bit integer precision
first and then de-quantized to floating point precision by multiplying a step size. In backward
passes, the gradients of the full-precision embeddings are approximated by the straight through
estimator (STE) [1] to perform value update. Since the downstream recommendation task takes the
de-quantized embeddings from their quantized counterparts, the optimization objective is aware
of the quantization process and aims to learn a full-precision embedding table that encodes the
quantization semantics.
In our framework, we adopt the learned step size quantization strategy (LSQ) [18] on the dense
codebook Eğ‘šğ‘’ğ‘¡ğ‘to further perform element-wise compression. The low-bit integer representation
of the codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘âˆˆ{âˆ’2ğ‘âˆ’1, .., 2ğ‘âˆ’1 âˆ’1}ğ‘Ã—ğ‘‘can be obtained as follows:
Â¯Eğ‘šğ‘’ğ‘¡ğ‘= Round(Clip( Eğ‘šğ‘’ğ‘¡ğ‘
Î”
,ğ‘„ğ‘šğ‘–ğ‘›,ğ‘„ğ‘šğ‘ğ‘¥)),
(6)
where Round(Â·) is the integer rounding function, Clip(x, min, max) ensures values in x that are lower
than min are set to min and values larger than max are set to max; ğ‘„ğ‘šğ‘–ğ‘›= âˆ’2ğ‘âˆ’1,ğ‘„ğ‘šğ‘ğ‘¥= 2ğ‘âˆ’1 âˆ’1
represents the lowest and largest integer values that can be used to represent floating point numbers,
ğ‘is the bit length of the fixed-point representations, Î” âˆˆRğ‘is the learnable step size vector. The
de-quantized codebook Ë†Eğ‘šğ‘’ğ‘¡ğ‘âˆˆRğ‘Ã—ğ‘‘is computed by multiplying Â¯Eğ‘šğ‘’ğ‘¡ğ‘by the learnable step size
vector Î” as:
Ë†Eğ‘šğ‘’ğ‘¡ğ‘= Â¯Eğ‘šğ‘’ğ‘¡ğ‘Ã— Î”,
(7)
one can then substitute Eğ‘šğ‘’ğ‘¡ğ‘with Ë†Eğ‘šğ‘’ğ‘¡ğ‘in Eq. 5 to infer the full embedding table.
Since the dense codebook can now be stored as low-bit integer representations Â¯Eğ‘šğ‘’ğ‘¡ğ‘and
a floating point precision step size vector Î”, the storage size of the codebook is reduced from
ğ‘‚(4 bytes Ã— ğ‘ğ‘‘) to ğ‘‚(4 bytes Ã— ğ‘( ğ‘
32ğ‘‘+ 1)). This implies that under a fixed storage budget, replac-
ing the full-precision codebook Eğ‘šğ‘’ğ‘¡ğ‘with our low-bit quantized codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘allows more
meta-embeddings to be included in the codebook. Thus, enriching the diversity of compositional
embeddings and reducing the risk of entity embedding collisions.
As most modern hardware has native support for INT8/INT16 data type 2, the quantized codebook
Â¯Eğ‘šğ‘’ğ‘¡ğ‘can be easily stored on edge devices without any external libraries or dedicated hardware.
2https://www.intel.com/content/www/us/en/developer/articles/technical/int8-quantization-for-x86-cpu-in-
pytorch.html;
https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/quantization.html;
https:
//docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/quantization.html.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 11 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
11
3.4
Codebook Pretraining
Due to the hardware limitation of edge devices and the desiderata of fast adaptation of GNN-based
recommender systems that satisfy various hardware specifications on the edge side, it is strongly
motivated to first conduct a one-off pretraining stage on the server side. In this way, only the
transfer of pretrained framework components and a lightweight refinement process is required on
the resource-constrained edge side to quickly adapt the recommender system.
We propose to pretrain the quantized compositional embedding table. In each training batch, we
combine the quantization-aware training process defined in Sec. 3.3 with the graph propagation
operation conducted on the inferred full embedding table Ë†E âˆˆRğ‘Ã—ğ‘‘using the user-item interaction
graph A. This is done by replacing the input embeddings H(0) in Eq. 2 to the full embedding table
inferred from the quantized compositional codebook:
H(0) = Ë†E = SË†Eğ‘šğ‘’ğ‘¡ğ‘= S(Â¯Eğ‘šğ‘’ğ‘¡ğ‘Ã— Î”).
(8)
Despite in LEGCF [38], we propose an assignment update strategy on the assignment weight matrix
S, our ablation studies [38] find out that the initialization of S via the METIS graph partitioning
algorithm [29] is a dominating factor for the performance boost compared with performing ad-
ditional updates on S. Moreover, the update strategy involves the graph propagation between an
expanded interaction graph and a larger embedding table, which introduces an even higher runtime
computation cost than the original full embedding table setting. For these reasons, in LERG, we no
longer update the assignment weight matrix S after it has been initialized.
Once the learning of the quantized codebook and learnable step size vector converges, the
pretraining stage is complete. We obtain the pretrained quantized codebook Â¯Eğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘’ğ‘¡ğ‘
and pretrained
step size vector Î”ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. We also save the pretrained graph-propagated full embedding table
Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and use it to perform graph rewiring in Sec. 3.5.
3.5
Graph Rewiring for Graph Sparsification
For on-device deployment of GNN-based recommenders, the computational efficiency and runtime
memory consumption can also be bottlenecks for large-scale recommendation scenarios. Due to
the varying importance of nodes in the message passing process within a graph [20], we point
out that performing graph propagation using the full user-item interaction graph on edge devices
introduces difficulty in deployment and thus, should be avoided.
To alleviate the graph computational complexity problem, one effective solution is graph rewiring
[72], which modifies the structure of the graph by removing unnecessary nodes (i.e., entities) and
edges (i.e., interactions). Our objective here for graph rewiring is to identify and remove less
prominent propagation links from the user-item interaction graph A so that the sparsity of the
graph is increased to lower required MACs for graph propagation. To do this, it is intuitive to first
identify the impact of each user/item in participating in collaborative signal propagation. Those that
contribute little to no impact can be refrained from propagating their embeddings to their neighbors
without causing a significant drop in recommendation performance. It is worth noting that many
proposed GNN computation cost improvement work that utilizes the sampling strategy [11, 22, 24,
64, 104] can also be regarded as a form of graph rewiring, as the unsampled nodes and/or edges are
turned off temporarily for graph propagation. However, these methods either require a complicated
search process to identify an optimal subgraph, or a time-consuming node/edge sampling procedure
is often introduced. There are also existing graph sparsification methods available [8, 67, 95, 110], but
most of them still involve complicated optimization objectives that are computationally intensive,
or require handcrafted sparsification rules that suffer from limited generalizability. We argue that
involving such a cumbersome and lengthy graph rewiring/sparsification strategy in our framework
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 12 ---
12
Xurong Liang et al.
increases the difficulty in quick deployment, and instead look for a more time- and computation-
efficient alternative.
To determine which nodes/edges are to be dropped from the full interaction graph, the simplest
way is to sort the nodes/edges by their degrees and remove the ones that provide limited connectivity
within the graph. However, such a naÃ¯ve solution overlooks the node affinity in the latent space,
which may yield an ill-defined rewired graph and hurt the embedding quality when performing
graph propagation. Notice that from Sec. 3.4, we have the graph-propagated embedding table
Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›pretrained using the full user-item interaction graph A, which conceals rich entity
features and collaborative semantics. We leverage the entity-entity similarity matrix B âˆˆRğ‘Ã—ğ‘
computed based on the pretrained full embedding table Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›to determine entitiesâ€™ contribution
in graph collaborative signal propagation:
B = Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›âŠ¤.
(9)
Each row of B corresponds to an entity ğ‘—and stores the similarity scores between this entity and
all entities in the dataset. We denote each row as Bğ‘—âˆˆRğ‘so Bğ‘—ğ‘˜indicates the similarity score
between an entity pair (ğ‘—,ğ‘˜). With the similarity scores calculated from Eq. 9, we can now interpret
the high-contribution entity selection task as a binary integer programming (BIP) problem with
the objective of maximizing the datasetâ€™s overall entity-entity similarity scores:
maximize
ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘£ğ‘—(
ğ‘
âˆ‘ï¸
ğ‘˜=1
Bğ‘—ğ‘˜),
s.t.
ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘£ğ‘—= ğ‘š,
ğ‘£ğ‘—âˆˆ{0, 1}, ğ‘—âˆˆ{1, .., ğ‘},
(10)
where ğ‘£ğ‘—is a binary variable that corresponds to the entity ğ‘—, such that entities having little to no
contribution to signal propagation are assigned a value of 0. The ones with higher impact have
a value of 1 instead. ğ‘šâˆˆ{1, .., ğ‘} is a hyperparameter that controls the number of entities to be
retained after the graph rewiring process. By setting different values of ğ‘š, one can accommodate
various graph computation and memory efficiency requirements. For convenience, we define ğ‘š
to be proportional to the total number of users and items in the dataset, and we name this ratio
the retention ratio. For example, if the retention ratio is set to 0.7 for a dataset, the value of
ğ‘šis calculated as ğ‘š= âŒŠ0.7ğ‘âŒ‹. Our intention of formulating a BIP problem on the entity-entity
similarity matrix constructed using graph pretrained embeddings to realize the impact of entities
is intuitive: given that at most ğ‘šnodes can remain in the rewired graph, we aim to find the
ones such that the sum of the similarity scores in the rewired graph is maximized. Since graph
propagation encodes neighbor collaborative signals in entity embeddings [25, 81], an entity that is
similar to a large number of entities in the graph hidden space implies her significance in graph
propagation and strong connectivity in the user-item interaction graph. Thus, retaining the set of
such entities in the rewired graph maximally preserves the overall collaborative semantics in the
user-item interaction graph. On the contrary, entities with embeddings different from the majority
are usually less connected or exhibit high heterophily in the user-item interaction graph. These
entities are deemed less impactful on graph propagation. Hence, we prevent them from participating
in collaborative signal propagation in the rewired graph to lower graph computation complexity.
Despite the simplicity of Eq. 10, solving BIP directly has been proven to be NP-complete [28], which
means there is no guarantee that a valid solution can be found in polynomial time, challenging the
computational efficiency. To ensure a low computational overhead for graph rewiring, we propose
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 13 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
13
to first relax the BIP into a linear programming (LP) problem by substituting binary variables
ğ‘£ğ‘—, ğ‘—âˆˆ{1, .., ğ‘} with continuous numbers Ë†ğ‘£ğ‘—, ğ‘—âˆˆ{1, .., ğ‘} in the [0, 1] range:
maximize
ğ‘
âˆ‘ï¸
ğ‘—=1
Ë†ğ‘£ğ‘—(
ğ‘
âˆ‘ï¸
ğ‘˜=1
Bğ‘—ğ‘˜),
s.t.
ğ‘
âˆ‘ï¸
ğ‘—=1
Ë†ğ‘£ğ‘—= ğ‘š,
Ë†ğ‘£ğ‘—âˆˆ[0, 1], ğ‘—âˆˆ{1, .., ğ‘},
(11)
With this relaxation, the problem can be solved using the simplex algorithm [13], which yields
polynomial-time complexity [68]. We then approximate the binary variables by applying value
rounding with a predefined boundary ğ‘œâˆˆ[0, 1]:
ğ‘£ğ‘—=
(
1
, if Ë†ğ‘£ğ‘—> ğ‘œ,
0
, otherwise.
(12)
Taking the set of binary variables ğ‘£ğ‘—, ğ‘—âˆˆ{1, .., ğ‘} gathered from Eq. 12, we define the set of entities
with high impact on graph propagation as Nğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›(i.e., retained entities), the set of entities
with relatively lower contribution on graph propagation as Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’(i.e., pruned entities) and
|Nğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›| = ğ‘š, |Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’| = ğ‘âˆ’ğ‘š, Nğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›âˆ©Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’= âˆ…. Given an undirected graph A âˆˆRğ‘Ã—ğ‘
represented in the form of an adjacency matrix, we define a graph sparsification function that
increases the graph sparsity by nullifying the columns with indices corresponding to the IDs of the
pruned entities:
prune(A:,ğ‘—) =
(
0
, if ğ‘—âˆˆNğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’,
A:,ğ‘—
, otherwise,
(13)
where A:,ğ‘—is the ğ‘—ğ‘¡â„column of the full user-item interaction graph. In this way, the sparsified graph
becomes a directed graph, where the entities in Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’no longer propagate their collaborative sig-
nals to their neighbors, yet they can still receive signals from their neighbors to update embeddings,
given their corresponding rows are not all zeros. If we apply Eq. 13 to the full interaction graph A,
however, the resultant sparsified graph may introduce additional all zero rows. The entities with
all zero rows mean that the set of first-hop neighbors is a subset of the pruned entity set Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’, so
there are no neighbors to receive the collaborative signals from.
We argue that it is crucial to have some neighbors for each entity so that their embeddings can
be improved during the graph propagation step. To this end, we construct a rewired propagation
graph Aâ€² âˆˆRğ‘Ã—ğ‘by taking the row-wise disjoint of column nullified interaction graphs of various
propagation degrees. This process is displayed in Alg. 1, where ğ‘‡is the highest propagation
degree used, count_nonzero(Â·) returns the number of nonzero elements in a vector, sign(Â·) is the
signum function. Simply put, line 1 nullifies pruned entitiesâ€™ corresponding columns in the first-hop
neighborhood. After that, we find all entities that do not have first-hop neighbors left and fill their
receptive fields with their second-hop neighbors or third-hop neighbors given that they are not
members in Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’, so on and so forth until the rows contain some nonzero values (lines 4 - 8).
The signum function in line 7 ensures all indirect neighbors have the same weight as the first-hop
neighbors. Following this, the number of all zero rows caused by the column nullification process
can be greatly reduced, while the overall sparsity of the interaction graph increases, which lowers
the required MACs when performing graph propagation. In practice, setting ğ‘‡= 4 in most cases is
sufficient to entirely recover all zero rows caused by the nullification process by rewiring these
entities with their indirect neighbors. Since Alg. 1 utilizes various multi-hop adjacency matrices,
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 14 ---
14
Xurong Liang et al.
Algorithm 1: Graph rewiring algorithm.
1 Input: full user-item interaction graph A, total number of entities ğ‘, the highest
propagation degree ğ‘‡and multi-hop adjacency matrices ğ´1, ..,ğ´ğ‘‡
2 Output: the sparsified rewired propagation graph Aâ€²
3 Aâ€² â†prune(A)
4 for ğ‘—âˆˆ[1, .., ğ‘] do
5
if count_nonzero(Aâ€²
ğ‘—) = 0 then
6
for ğ‘¡âˆˆ[2, ..,ğ‘‡] do
7
ğ‘”â€² â†prune(Ağ‘¡
ğ‘—)
8
if count_nonzero(ğ‘”â€²) > 0 then
9
Aâ€²
ğ‘—â†sign(ğ‘”â€²)
10
break
they are precomputed and stored on the resource-rich server side so that one can reuse them for
multiple graph rewiring procedures without incurring noticeable counter effects on framework
efficiency.
The rewired propagation graph Aâ€² will be transmitted to resource-constrained edge devices
for inference. To meet various hardware capacities of edge devices, one can adjust the number of
retained entities ğ‘što modify the message passing complexity of the rewired graph and achieve
a desired level of efficiency-accuracy trade-off. To ensure the optimality of the quantized compo-
sitional embedding table, we propose conducting a quick fine-tuning process using the rewired
propagation graph Aâ€². We detail the embedding fine-tuning process in Sec. 3.6.
3.6
Fine-tuning with Rewired Graph
With the pretrained quantized codebook Â¯Eğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘’ğ‘¡ğ‘
, the pretrained learnable step size vector Î”ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
and the rewired propagation graph Aâ€² obtained from Sec. 3.4 and Sec. 3.5, we can perform a light
embedding fine-tuning process to fit the embedding table to various storage and memory constraints.
We take the inferred embeddings Ë†Eğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›âˆˆRğ‘šÃ—ğ‘‘of the retained entities as the input embeddings
and perform propagation over the newly generated rewired graph Aâ€² to conduct quantization-aware
training:
H(0)
ğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›= Ë†Eğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›= Sğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›Ë†Eğ‘šğ‘’ğ‘¡ğ‘= Sğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›(Â¯Eğ‘šğ‘’ğ‘¡ğ‘Ã— Î”),
H(ğ‘™+1)
ğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›= (Dâˆ’1
2 Aâ€²Dâˆ’1
2 )H(ğ‘™)
ğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›,
Hğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›=
1
ğ¿+ 1
ğ¿
âˆ‘ï¸
ğ‘™=0
H(ğ‘™)
ğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›,
(14)
where Sğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›= S[Nğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›, :] âˆˆRğ‘šÃ—ğ‘is the assignment weight matrix for entities in Nğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘–ğ‘›; Î” âˆˆRğ‘
is the learnable step size vector; D âˆˆRğ‘šÃ—ğ‘šis the diagonal degree matrix of Aâ€². On initialization,
we set Â¯Eğ‘šğ‘’ğ‘¡ğ‘to be Â¯Eğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘’ğ‘¡ğ‘
and Î” to be Î”ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. Essentially, in the fine-tuning step, only the
embeddings of retained entities will be updated using the rewired graph Aâ€². The advantages of
this embedding refinement procedure are three-fold. Firstly, updating only the retained entitiesâ€™
embeddings caps the size of the computable matrices at forward and backward passes at ğ‘‚(ğ‘šğ‘‘),
which effectively controls the peak runtime memory of LERG at training and inference. The use of
the rewired graph, on the other hand, lowers the total MACs in graph propagation. Thus, the issue
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 15 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
15
with the expensive graph computational cost on edge devices is alleviated. The two innovations
together enable the smooth deployment and fine-tuning (if necessary) of a GNN-based recommender
system directly on-edge. Secondly, a fast and lightweight embedding fine-tuning strategy boosts
the time efficiency for large-scale edge device deployment, as most embedding optimization work
requires a complete training process for each hardware specification. In LERG, instead, for each
hardware configuration, one only needs to input a suitable retained entity budget ğ‘šand fine-
tune the quantized compositional embedding table using the generated rewired graph to obtain a
lightweight GNN-based recommender with satisfactory accuracy. Lastly, refining the quantized
codebook and learnable step size vector using only retained entitiesâ€™ graph-propagated embeddings
encodes the entity importance in the quantized codebook as now the quantized codebook is driven
to improve the retained entitiesâ€™ embeddings, of who is deemed impactful in graph propagation.
Since only the embeddings of retained entities are inferred from the quantized codebook and
propagated through the rewired graph, the graph-propagated embeddings of pruned entities need to
be imputed using other strategies. Despite one can directly draw pruned entitiesâ€™ graph-propagated
embeddings Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
= Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›[Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’, :] âˆˆR(ğ‘âˆ’ğ‘š)Ã—ğ‘‘from the pretrained full embedding
table Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and store it on edge devices for inference, this naÃ¯ve solution contradicts with the
concern on embedding table storage cost. As a wraparound, we design a placeholder codebook
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’âˆˆRğ‘ŸÃ—ğ‘‘generated by applying the clustering technique such as the K-means algorithm [50]
on Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
for pruned entity embeddings imputation:
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’,ğ‘„= clustering(Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
,ğ‘Ÿ),
(15)
where ğ‘„âˆˆ[1, ..,ğ‘Ÿ]ğ‘âˆ’ğ‘šis the placeholder assignment vector for entities in Nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’, ğ‘Ÿis the number
of placeholder meta-embeddings in the codebook Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’. To save the storage space, we have
ğ‘Ÿâ‰ª(ğ‘âˆ’ğ‘š). By letting pruned entities share a smaller pool of placeholder embeddings, one only
needs to store Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’and ğ‘„on disk. Assuming ğ‘„has the precision of INT32, the space complexity
of pruned entity embedding is ğ‘‚(4 bytes Ã— (ğ‘Ÿğ‘‘+ (ğ‘âˆ’ğ‘š)). It is reasonable to set ğ‘Ÿâ‰ªğ‘since the
pruned entities have less impact in graph propagation than the retained entities, whose embeddings
are inferred from the quantized codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘and the rewired propagation graph Aâ€². At inference
time, the imputed embedding table of pruned entities Ë†Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’âˆˆR(ğ‘âˆ’ğ‘š)Ã—ğ‘‘is created by taking the
placeholder embeddings with respect to the assignment vector:
Ë†Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’= Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’[ğ‘„].
(16)
Algorithm 2: Fine-tuning process.
1 Input: pretrained quantized codebook Â¯Eğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘’ğ‘¡ğ‘
and pretrained step size vector Î”ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›,
rewired propagation graph Aâ€², placeholder codebook Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’and placeholder assignment
vector ğ‘„
2 Output: fine-tuned quantized codebook Â¯Eâˆ—
ğ‘šğ‘’ğ‘¡ğ‘and fine-tuned step size vector Î”âˆ—
3 Initialize Â¯Eğ‘šğ‘’ğ‘¡ğ‘as Â¯Eğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘’ğ‘¡ğ‘
4 Initialize Î” as Î”ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
5 while not converged do
6
Follow Eq. 14 to refine quantized codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘and step size vector Î” using retained
entity embeddings and rewired propagation graph Aâ€²
7
Draw pruned entitiesâ€™ embeddings Ë†Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’w.r.t. Eq. 16 for inference
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 16 ---
16
Xurong Liang et al.
The fine-tuning process is depicted in Alg. 2. Note that the placeholder meta-embedding genera-
tion for pruned entities is not part of the fine-tuning process as we refine the quantized compositional
embedding table only using the retained entity embeddings. Due to the use of a rewired graph
and only a part of the entity embeddings participate in graph propagation, the fine-tuning process
incurs low runtime memory consumption and hence, can be conducted either on-sever or on-edge
efficiently.
3.7
Framework Overview
Algorithm 3: LERG implementation.
1 Initialize S âˆˆRğ‘Ã—ğ‘as defined in Sec. 3.2
2 Formulate codebook quantization-aware training as stated in Sec. 3.3
3 while not converged do
4
Pretrain quantized codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘and learnable step size vector Î” by following Sec. 3.4
5 Follow Sec. 3.5 and Alg. 1 to generate rewired propagation graph Aâ€² using the pretrained full
embedding table Hğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
6 Generate placeholder meta-embeddings Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’and placeholder assignment vector ğ‘„for
pruned entities w.r.t. Eq. 15
7 Follow Alg. 2 to fine-tune the quantized compositional embedding table using the rewired
graph.
To illustrate the implementation of LERG, we provide the overall algorithm in Alg. 3. Lines 3-4
conduct quantized compositional codebook pretraining. Line 5 performs the graph rewiring step to
obtain a sparsified propagation graph to be used on edge devices. Line 6 generates the placeholder
meta-embeddings and assigns them to pruned entities. Line 7 carries out the fine-tuning procedure
on the pretrained quantized compositional codebook. Lines 3-6 are executed on the resource-rich
server due to their relatively high computation cost and runtime memory consumption. Line 7 has
the option to be executed on-edge or on-server.
To analyze the space complexity of LERG, the quantized compositional codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘and the
floating point precision learnable step size vector Î” exert space of ğ‘‚(4 bytes Ã— ğ‘( ğ‘
32ğ‘‘+ 1)), the
floating point placeholder meta-codebook Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’along with the INT32 placeholder assignment
vector ğ‘„incur space of ğ‘‚(4 bytes Ã— (ğ‘Ÿğ‘‘+ (ğ‘âˆ’ğ‘š)), so the total space complexity is ğ‘‚(ğ‘( ğ‘
32ğ‘‘+1)) +
(ğ‘Ÿğ‘‘+ (ğ‘âˆ’ğ‘š))), which is far lower than the space complexity of a full embedding table ğ‘‚(ğ‘ğ‘‘).
Since commonly ğ‘Ÿâ‰ªğ‘, the former term has higher significance when evaluating the overall space
efficiency.
4
Experiments
In this section, we carry out experiments to verify the effectiveness of LERG. This section is
organized to answer the following research questions (RQs):
â€¢ RQ1: Does our framework work well compared to other baselines under resource-constrained
settings?
â€¢ RQ2: What is the effect of important components proposed in LERG?
â€¢ RQ3: How do different hyperparameter settings affect our frameworkâ€™s performance?
â€¢ RQ4: Does LERG work well when equipped with other GNN-based recommenders?
â€¢ RQ5: How does the precision of the quantized codebook affect recommendation performance?
â€¢ RQ6: How efficient is our proposed on-device fine-tuning process?
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 17 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
17
4.1
Experimental Settings
4.1.1
Datasets. We conduct experiments on three publicly available datasets: two medium-scale
datasets Yelp2020 and Amazon-book as well as one industrial large-scale dataset Alibaba-
iFashion (iFashion). Yelp2020 dataset is downloaded from [70], Amazon-book dataset can be
found in [25] and iFashion dataset is readily available from [10]. For Yelp2020 and Amazon-book,
we use all user-item interactions in the dataset. For iFashion dataset, due to the high sparsity of
observed interactions, we follow the practices detailed in [86] to sample 500, 000 users and include
all user-item interactions made by the sample users to form the dataset for experiment. The detailed
statistics of the three datasets are depicted in Tab. 2. We adopt the train/test/validation split ratios
of 80%/20%/10% on observed interactions. For each observed user-item interaction in the training
set, we sample 5 negative items and add them to the training set.
Table 2. Statistics of datasets used in experiments.
Dataset
#User
#Item
#Interaction
Density
Yelp2020
71,135
45,063
1,782,999
0.056%
Amazon-book
52,643
91,599
2,984,108
0.062%
iFashion
500,000
1,465,817
26,069,309
0.004%
4.1.2
Based Recommenders and Baselines. Since LERG is a lightweight framework dedicated to
GNN-based recommender systems, we select LightGCN [25] as the base recommender for main
experiments considering its effectiveness and popularity, and we also verify our frameworkâ€™s
generalization with other GNN-based recommender in Section 4.5.
To show the effectiveness of our framework, we compare the performance of LERG against a
wide range of embedding optimization state-of-the-art baselines:
â€¢ ESAPN [43]: An AutoML-based dimension search algorithm that devises reinforcement learn-
ing to find the suitable user and item embedding sizes that yield an optimal recommendation
performance.
â€¢ CIESS [61]: The state-of-the-art AutoML-based lightweight embedding framework which
allows userâ€™s input of desirable storage budget.
â€¢ PEP [46]: The iconic pruning-based strategy which identifies and nullifies unimportant
elements in the full embedding table.
â€¢ OptEmbed [53]: An embedding optimization work that combines both pruning and evolu-
tionary search.
â€¢ CERP [39]: The state-of-the-art pruning method which conducts simultaneous pruning on
two compositional codebooks.
â€¢ LEGCF [38]: A state-of-the-art compositional embedding method that utilizes a codebook
and a learnable assignment matrix to infer the full embedding table. LERGis an improved
work of LEGCF.
â€¢ Post4bits: [21]: A post-training quantization method that devises the greedy search algorithm
to find the best min/max bound for quantization scaling.
Moreover, we also conduct experiments using the unified dimensionality setting (UD) with
dimension sizes of 64 and 128 to discuss the effect of LERG in retaining the recommendation
performance amid massive storage and runtime computation complexity reduction.
4.1.3
Evaluation Metrics. We use the common recommendation ranking quality metrics NDCG@N
[82] and Recall@N with ğ‘set to {10, 20} to evaluate the performance of LERG and baselines
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 18 ---
18
Xurong Liang et al.
mentioned above. We also indicate the storage cost of the resultant embedding tables and the
peak runtime memory consumption generated by each work to compare their robustness in
fulfilling tight storage constraints.
4.1.4
Implementation Details. In terms of the dimensional size of embedding vectors, for LERG and
all baselines other than Post4bits, we set dimension size ğ‘‘of embeddings/meta-embeddings to 128.
Since Post4bits only supports 4-bit quantization, we first calculate the storage cost of the embedding
table generated by our work and then convert it to a suitable dimension size for Post4bits so that
the total storage cost is similar to ours for fair comparison.
For LERG, in default settings, the number of meta-embeddings in the quantized codebook ğ‘
is set to 2, 000 for Yelp2020 and Amazon-book and 10, 000 for iFashion. We set the quantization
bit length ğ‘= 16, which means each meta-embedding on Eğ‘šğ‘’ğ‘¡ğ‘will be quantized to an INT16
vector. By default, we set the retention ratio to 0.7 (i.e., ğ‘š= âŒŠ0.7ğ‘âŒ‹). We set the maximum neighbor
hops for graph rewiring ğ‘‡= 4 for all three datasets. We set the predefined boundary for linear
integer programming ğ‘œ= 0.5. For both pretraining and fine-tuning processes, the number of GNN
propagation layers ğ¿is set to 4. The number of placeholder meta-embeddings in Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’is set
ğ‘Ÿ= 500 for Yelp2020 and Amazon-book and ğ‘Ÿ= 2000 for the large-scale dataset iFashion. The study
of the impact of the number of quantized meta-embedding ğ‘is presented in Fig. 3a. The study of the
number of the placeholder meta-embedding ğ‘Ÿis available in Fig. 3b. For the training process, we
devise the Adam optimizer [30] with a learning rate 1eâˆ’3 and weight decay 1eâˆ’5. The ğ¿2 penalty
factor ğœ†is fixed to 5eâˆ’4. For Yelp2020 and Amazon-book datasets, we train and test our work on a
workstation equipped with an Intel Core i7-12700K processor and an NVIDIA GeForce RTX 3090
GPU. For iFashion dataset, we use an HPC equipped with an AMD EPYC 7443 processor and an
NVIDIA GeForce H100 GPU. We set a RAM upper bound of 128GB and a maximum wall time of 7
days for each running process in every work to avoid overly extensive computation.
To create a fair comparison between our work and baselines, we first calculate the storage
cost of the embedding table generated by our work and then input the cost as the storage target
for work that accommodates various storage cost requirements. For work that does not offer the
option to customize storage targets, like ESAPN [43] and OptEmbed [53], we tune the candidate
embedding size set to reach a balanced trade-off between the final storage cost and recommendation
performance.
4.2
Overall Performance (RQ1)
The overall performance benchmark is revealed in Tab. 3. We also plot the relationship between
various retention ratios with recommendation performance and graph computational efficiency in
Fig. 2. We organize our findings into the following subsections:
Comparison with Baselines. It is observed that LERG achieves the best performance across all
three datasets, followed by our previous work LEGCF, both of which implement a compositional
codebook and an assignment weight matrix. This verifies the robustness of compositional codebook
structure in enhancing entity embedding uniqueness amid a tight storage budget. Post4Bits and PEP
work reasonably well on datasets with fewer users and items such as Yelp2020. On iFashion dataset,
however, both work fails to maintain a satisfactory performance. The cause of poor performance may
be the overly sparse embedding table/small quantization precision bit length that limits embedding
expressiveness. Comparing the results of LERG with UD settings, our work aces in retaining a
robust recommendation performance while reducing the storage cost of the embedding table to
a tiny proportion of full embedding tables. This observation is especially obvious on the largest
dataset iFashion, as the storage cost of LERG is reduced by 97.7% of that consumed by the UD setting
with a dimension size of 128, yet the performance is only degraded by 15.1% when evaluated using
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 19 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
19
Table 3. Performance comparison between our work and baselines. â€œStorageâ€ indicates the total storage cost
of the embedding layer of each method. â€œMEMâ€ means the peak runtime memory consumption measured in
each setting. â€œUD - dim 128â€ and â€œUD - dim 64â€ are the full embedding table settings with unified dimensions
ğ‘‘= 128 and ğ‘‘= 64, respectively. â€œâ€”â€ indicates the results are not available. In each column, we use bold font
and underlined text to mark the best and second best results achieved by lightweight embedding methods
respectively.
Yelp2020
Amazon-book
iFashion
Method
Storage MEM N@10 R@10 N@20 R@20 Storage MEM N@10 R@10 N@20 R@20
Storage
MEM
N@10 R@10 N@20 R@20
UD - dim 128 56.74MB 1.15GB 0.0284 0.0426 0.0382 0.0721 70.43MB 1.66GB 0.0172 0.0215 0.0230 0.0367 760.46MB 26.22GB 0.0053 0.0079 0.0077 0.0145
UD - dim 64 28.37MB 0.99GB 0.0274 0.0409 0.0366 0.0687 35.22MB 1.28GB 0.0165 0.0204 0.0222 0.0353 380.23MB 15.81GB 0.0048 0.0070 0.0069 0.0128
ESAPN
4.29MB 0.77GB 0.0067 0.0086 0.0087 0.0142 5.49MB 1.06GB 0.0045 0.0037 0.0051 0.0057 37.50MB 19.50GB 0.0019 0.0016 0.0023 0.0029
OptEmbed
1.85MB 2.02GB 0.0084 0.0089 0.0108 0.0158 6.55MB 2.37GB 0.0037 0.0032 0.0050 0.0062
â€”
â€”
â€”
â€”
â€”
â€”
PEP
1.76MB 1.47GB 0.0185 0.0208 0.0235 0.0351 2.01MB 1.71GB 0.0047 0.0050 0.0058 0.0081 21.55MB 31.87GB 0.0001 0.0001 0.0002 0.0003
CERP
1.76MB 1.06GB 0.0149 0.0214 0.0203 0.0375 2.01MB 1.48GB 0.0092 0.0112 0.0125 0.0198 17.12MB 30.70GB 0.0012 0.0013 0.0016 0.0025
CIESS
1.85MB 2.12GB 0.0178 0.0306 0.0241 0.0521 2.12MB 2.75GB 0.0041 0.0046 0.0055 0.0084
â€”
â€”
â€”
â€”
â€”
â€”
Post4bits
1.77MB 1.05GB 0.0191 0.0274 0.0253 0.0463 2.06MB 1.46GB 0.0132 0.0152 0.0172 0.0261 17.82MB 22.57GB 0.0014 0.0016 0.0019 0.0028
LEGCF
1.76MB 1.09GB 0.0194 0.0272 0.0260 0.0475 2.01MB 1.48GB 0.0142 0.0161 0.0172 0.0244 17.12MB 30.72GB 0.0025 0.0029 0.0032 0.0047
LERG
1.76MB 0.57GB 0.0216 0.0311 0.0294 0.0550 2.01MB 0.83GB 0.0146 0.0165 0.0188 0.0280 17.12MB 16.54GB 0.0045 0.0050 0.0057 0.0081
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
retention ratio
0.0221
0.0272
0.0322
NDCG@20
retention ratio vs performance
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
retention ratio
5.32
5.73
6.14
epoch time (s)
retention ratio vs efficiency
1.70
2.25
2.81
MACs (billions)
(a) Yelp2020
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
retention ratio
0.0131
0.0174
0.0217
NDCG@20
retention ratio vs performance
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
retention ratio
12.14
14.72
17.30
epoch time (s)
retention ratio vs efficiency
2.26
3.53
4.80
MACs (billions)
(b) Amazon-book
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
retention ratio
0.0038
0.0048
0.0058
NDCG@20
retention ratio vs performance
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
retention ratio
442.29
508.93
575.58
epoch time (s)
retention ratio vs efficiency
25.05
33.19
41.32
MACs (billions)
(c) iFashion
Fig. 2. The plots on the left hand side show the recommendation performance of LERG w.r.t. different
retention ratios. The plots on the right hand side depict the relationship between retention ratios and the
computational efficiency of our algorithm. The trend of training epoch elapsed time (in seconds) is shown in
purple color. The trend of MACs (in billions) incurred during graph propagation is shown in green color. Here
we record the training time per epoch as an indicator of algorithm time efficiency to align with the potential
need for fine-tuning the model directly on-edge once deployed.
NDCG@10. In terms of the storage cost of baselines, AutoML-based dimension search methods like
ESAPN and OptEmbed tend to obtain a much larger embedding table. The possible cause of this is
that these methods assign higher priority to optimizing the performance over memory cost. In terms
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 20 ---
20
Xurong Liang et al.
Table 4. Performance comparison between default settings and settings with a particular component altered.
The default settings are the ones fully trained on the quantized compositional codebook. The BIP constraint
is solved to identify contributions of entities in graph propagation for graph sparsification. In the fine-tuning
stage, graph-propagated embeddings of pruned entities are drawn from the placeholder meta-embeddings
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’with Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’discarded.
Dataset
Yelp2020
Amazon-book
iFashion
Retention Ratio
Variant
N@10
R@10
N@20
R@20
N@10
R@10
N@20
R@20
N@10
R@10
N@20
R@20
0.7
Default
0.0216
0.0311
0.0294
0.0550
0.0146
0.0165
0.0188
0.0280
0.0045
0.0050
0.0057
0.0081
w/o Fine-tuning
0.0214
0.0309
0.0291
0.0545
0.0139
0.0161
0.0182
0.0278
0.0043
0.0049
0.0055
0.0079
w/o BIP
0.0218
0.0309
0.0287
0.0521
0.0129
0.0147
0.0165
0.0245
0.0038
0.0041
0.0048
0.0065
w/o Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
0.0164
0.0235
0.0222
0.0415
0.0116
0.0126
0.0142
0.0202
0.0046
0.0049
0.0057
0.0079
Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’retained
0.0166
0.0228
0.0247
0.0440
0.0142
0.0180
0.0159
0.0263
0.0044
0.0055
0.0049
0.0080
Post-compression
0.0136
0.0207
0.0190
0.0374
0.0101
0.0116
0.0132
0.0202
0.0029
0.0032
0.0038
0.0055
0.5
Default
0.0190
0.0279
0.0266
0.0514
0.0132
0.0151
0.0171
0.0258
0.0044
0.0048
0.0055
0.0078
w/o Fine-tuning
0.0182
0.0269
0.0256
0.0495
0.0124
0.0146
0.0164
0.0254
0.0042
0.0047
0.0054
0.0076
w/o BIP
0.0194
0.0269
0.0256
0.0457
0.0131
0.0148
0.0167
0.0247
0.0033
0.0034
0.0041
0.0055
w/o Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
0.0121
0.0170
0.0166
0.0310
0.0095
0.0099
0.0116
0.0160
0.0043
0.0045
0.0054
0.0073
Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’retained
0.0146
0.0200
0.0215
0.0381
0.0139
0.0174
0.0156
0.0255
0.0044
0.0056
0.0049
0.0080
Post-compression
0.0118
0.0175
0.0165
0.0320
0.0099
0.0113
0.0130
0.0198
0.0029
0.0032
0.0038
0.0055
0.1
Default
0.0157
0.0232
0.0221
0.0427
0.0097
0.0108
0.0131
0.0199
0.0031
0.0032
0.0038
0.0052
w/o Fine-tuning
0.0129
0.0194
0.0185
0.0365
0.0088
0.0102
0.0120
0.0186
0.0031
0.0032
0.0038
0.0052
w/o BIP
0.0118
0.0175
0.0168
0.0326
0.0072
0.0073
0.0095
0.0135
0.0015
0.0015
0.0018
0.0024
w/o Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
0.0063
0.0085
0.0092
0.0173
0.0041
0.0037
0.0051
0.0066
0.0021
0.0020
0.0025
0.0033
Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’retained
0.0074
0.0107
0.0113
0.0215
0.0108
0.0138
0.0121
0.0205
0.0036
0.0035
0.0042
0.0052
Post-compression
0.0062
0.0093
0.0093
0.0187
0.0080
0.0091
0.0105
0.0160
0.0026
0.0028
0.0034
0.0048
of the peak runtime memory consumption, our work exerts the lowest volume of memory across
all three datasets. Interestingly, it is discovered that some embedding optimization work has an
even higher peak runtime memory consumption than the UD settings, creating excessive burdens
in the pipeline. We also notice that AutoML-based methods OptEmbed and CIESS fail to generate
recommendation results on the largest dataset iFashion, due to long elapsed time and/or excessive
runtime memory consumption. This phenomenon proves the statement that the AutoML-based
methods are not suitable for large-scale deployment.
Retention Ratio vs Performance/Efficiency. To measure the training epoch time, we fix
the training batch for each dataset for settings with various retention ratios. To measure the
MACs incurred during the graph propagation operation, we first generate propagation graphs with
various ratios of entities retained, and then conduct the calculation of multiply-addictive counts on
each propagation graph. It is observed that for smaller datasets Yelp2020 and Amazon-book, the
recommendation performance is relatively sensitive w.r.t. the retention ratio. On iFashion, LERG
still manages to maintain a satisfactory performance when 40% of the total entities are retained,
creating an optimal trade-off between recommendation performance and graph computational
efficiency. In terms of the relationship between retention ratios and algorithm efficiency, it is
noticed that as more entities are pruned, the sparsity of the rewired propagation graph increases,
followed by fewer computation costs (i.e., lower MACs). The epoch training time also drops w.r.t.
to retention ratios, due to lighter graph computational overhead and fewer entity embeddings to
undergo backward propagation. The experiments conducted in this subsection verify LERGâ€™s ability
to fit state-of-the-art GNN-based recommender systems to resource-constrained edge devices.
4.3
Discussions on Key Model Components (RQ2)
To validate the effectiveness of key components of LERG, we conduct ablation studies on the
embedding fine-tuning process, the BIP procedure of identifying pruned entities for the graph
rewiring process, the placeholder meta-embeddings Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’from clustering for pruned entities in
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 21 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
21
the fine-tuning stage, the revocation of Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’for pruned entities in the fine-tuning stage, and the
end-to-end training on the quantized compositional codebook, respectively. The experiment results
are recorded in Tab. 4. In order to thoroughly study the impact of key components of LERG w.r.t.
different edge device specifications, we display the results conducted with retention ratios set to
{0.7, 0.5, 0, 1} respectively.
Embedding Fine-tuning Process. We denote the settings where the pretrained quantized
compositional embedding table is used directly on edge devices without fine-tuning as w/o Fine-
tuning in Tab. 4. It is obvious that applying the rewired graph directly for inference without the
embedding finefining step tends to achieve sub-optimal recommendation performance. The effect
of fine-tuning is particularly obvious when evaluated on Yelp2020 dataset with the retained ratio
set to 0.1, as performance discrepancy between the setting with and without fine-tuning shows
a 17.8% difference when observing the NDCG@10 values. The experiment results indicate the
necessity of a lightweight embedding fintuning process to be conducted on edge devices.
BIP for Graph Rewiring. We denote the scenarios where the pruned entities are randomly
drawn from the entity set as w/o BIP in Tab. 4. It is discovered that as the retention ratio decreases,
the performance discrepancy between default settings and settings without the use of BIP enlarges
on all three datasets. This indicates the importance of solving the BIP constraint for the graph
rewiring process as it effectively identifies the entities whose contribution to graph propagation is
more impactful than others. In the case that only a small percentage of entities are retained in the
rewired graph, the rewired graph maximally preserves the collaborative semantics by retaining
entities that are deemed to have significant contributions to graph propagation.
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’for Entity Imputation. We denote the settings where the pruned entity embeddings in
the fine-tuning stage are imputed as the mean of all pruned entity graph-propagated pretrained
embeddings as w/o Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’in Tab. 4. It is observed that on smaller datasets Yelp2020 and Amazon-
book, the use of Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’for pruned entity embedding imputation plays a critical role in attaining
excellent accuracy. This is especially true when the retention ratio is set to 0.1. On large-scale
dataset iFashion, when the retention ratios are 0.7 and 0.5, the performance gap between settings
with and without Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’is not particularly noticeable. However, when the retention ratio is lowered
to 0.1, the performance of the setting with Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’for pruned entity embedding imputation prevails.
We deduce the reason why using Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’in low retention ratio cases works well is that when the
majority of entities are pruned, imputing their embeddings from the placeholder codebook Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’
provides sufficient embedding uniqueness for the recommender system to distinguish entities,
rather than simply taking the unanimous mean placeholder embedding for all pruned entities.
Revocation of Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’in Fine-tuning. In Sec. 3.6, we conclude that the primary reason for
deriving the embeddings of pruned entities ( Ë†Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’) from the fixed placeholder meta-embeddings
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’, rather than updating them like the retained entities, is to significantly reduce runtime
memory consumption and computational complexity. To study the impact on recommendation
accuracy after discarding pruned entitiesâ€™ full graph-propagated embeddings Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’in favor of
Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’, we show the results computed in scenarios where Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’were also allowed to participate
in the fine-tuning process (denoted as Hğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’retained) in Tab. 4. The indicated results show that
this modification leads to only marginal improvements, and in some cases, slightly degraded
performance, particularly when the retention ratio is relatively high. The experiments conducted
in this sub-section confirm that our design will not significantly affect the performance caused by
the different updating methods of pruned and retained entities.
End-to-end Training on Quantized Compositional Codebook. In our work, we initialize
the compositional embedding framework and perform quantization-aware training directly on it,
as opposed to the work [21] that performs a post-training quantization process on the well-trained
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 22 ---
22
Xurong Liang et al.
Yelp2020
Amazon-book
iFashion
0.0294
0.0309
0.0323
0.0163
0.0181
0.0199
1k
2k
5k
10k
15k
0.0029
0.0043
0.0057
NDCG@20
(a) Effect of ğ‘
0.0280
0.0292
0.0305
0.0171
0.0190
0.0208
100 500
1k
2k
3k
r
0.005700
0.005705
0.005710
NDCG@20
(b) Effect of ğ‘Ÿ
0.0087
0.0215
0.0342
0.0044
0.0136
0.0227
Random
METIS
Spectral
RB
0.0019
0.0052
0.0085
NDCG@20
(c) Effect of assignment init. method
Fig. 3. The performance of LERG w.r.t. various hyperparameter settings.
uncompressed full embedding table. To examine the performance superiority of our end-to-end
training design, we conduct an additional experiment adopting a post-compression strategy. In this
setting, we first train a full embedding table to convergence, and then apply our proposed graph
rewiring, meta-embedding generation, and quantization modules as post hoc compression steps.
This setup mirrors traditional compression approaches that operate after full-capacity training. The
results, reported in Tab. 4 under the Post-compression setting, indicate a notable degradation in per-
formance compared to our end-to-end training scheme. We hypothesize that the post-compression
strategy fails to establish a high-quality and semantically aligned mapping between entities and
meta-embeddings, leading to suboptimal compositions. In contrast, our joint learning framework
enables better adaptation of the codebook and assignments throughout training.
4.4
Hyperparameter Sensitivity (RQ3)
The three tunable hyperparameters in LERG are the number of meta-embeddings in the quantized
codebook ğ‘, the number of placeholder meta-embeddings for pruned entity embedding imputation
ğ‘Ÿ, as well as the initialization method of the assignment matrix.
Number of Quantized Meta-Embeddingsğ‘. The impact of varying number of meta-embeddings
ğ‘in the quantized codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘is revealed in Fig. 3a. The set of testing value for ğ‘include
{1, 000; 2, 000; 5, 000; 10, 000; 15, 000}. The performance plot shows that setting ğ‘= 5, 000 yields
optimal performance on all three datasets. Setting ğ‘to values larger than 5, 000 does not lead to
great performance improvement. We also notice that for the large-scale dataset iFashion, setting
ğ‘= 2, 000 results in even better performance than setting ğ‘= 5, 000. Based on the two observations,
we conclude that LERG is designed to work under a tight storage cost constraint for large-scale
datasets, effectively easing the deployment difficulty of GNN-based recommender systems on
resource-constrained devices while maintaining satisfactory recommendation performance.
Number of Placeholder Meta-Embeddings ğ‘Ÿ. The effect of different placeholder meta-
embeddings ğ‘Ÿin the codebook Cğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’is visualized in Fig. 3b. We examine the settings with ğ‘Ÿ
set to {100; 500; 1, 000; 2, 000; 3, 000}. It is discovered that for smaller datasets Yelp2020 and Amazon-
book, increasing ğ‘Ÿyields better recommendation performance. However, for large-scale dataset
iFashion, changing the value of ğ‘Ÿbarely causes fluctuations in the performance trend. The cause
of this might be that iFashion contains a large number of entities that actually have very few
interactions with other entities. As such, in settings with a relatively high retention ratio (i.e., 0.7),
the size of the pruned entities is too small such that the placeholder embeddings are barely drawn
for downstream recommendation tasks. Hence, the little accuracy change is spotted despite using
various sizes of placeholder meta-embeddings. Our experiments in this section indicate that the
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 23 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
23
Table 5. Performance comparison between our work and unified dimensionality settings. â€œrecommender
- LERG - 1/0.7" means the setting evaluated on our work with all/70% of the entities retained when the
recommender is used. â€œrecommender - UDâ€ means the setting evaluated on unified dimensionality when the
recommender is used.
Yelp2020
Amazon-book
iFashion
Setting
N@10 R@10 N@20 R@20 N@10 R@10 N@20 R@20 N@10 R@10 N@20 R@20
LightGCN - LERG - 1
0.0241 0.0347 0.0322 0.0596 0.0168 0.0195 0.0217 0.0330 0.0046 0.0051 0.0058 0.0082
LightGCN - LERG - 0.7 0.0216 0.0311 0.0294 0.0550 0.0146 0.0165 0.0188 0.0280 0.0045 0.0050 0.0057 0.0081
LightGCN - UD
0.0157 0.0226 0.0208 0.0386 0.0080 0.0092 0.0106 0.0164 0.0014 0.0016 0.0019 0.0028
NGCF - LERG - 1
0.0088 0.0121 0.0136 0.0264 0.0107 0.0121 0.0132 0.0190 0.0039 0.0038 0.0042 0.0049
NGCF - LERG - 0.7
0.0086 0.0119 0.0131 0.0253 0.0083 0.0094 0.0101 0.0146 0.0011 0.0013 0.0015 0.0022
NGCF - UD
0.0046 0.0071 0.0068 0.0137 0.0051 0.0056 0.0067 0.0101 0.0000 0.0000 0.0001 0.0001
xSimGCL - LERG - 1
0.0232 0.0325 0.0306 0.0550 0.0213 0.0246 0.0275 0.0416 0.0084 0.0088 0.0104 0.0142
xSimGCL - LERG - 0.7
0.0173 0.0247 0.0235 0.0434 0.0170 0.0198 0.0222 0.0338 0.0052 0.0056 0.0066 0.0092
xSimGCL - UD
0.0163 0.0230 0.0216 0.0395 0.0088 0.0100 0.0115 0.0174 0.0022 0.0026 0.0030 0.0045
number of placeholder meta-embeddings for pruned entities is not a key performance factor for
large-scale datasets. One can simply opt for a small ğ‘Ÿvalue for large-scale datasets so the storage
cost of the placeholder meta-embeddings is negligible.
Initialization Method of Assignment Matrix. The effect of various methods used to select
the anchor meta-embedding on initialization is shown in Fig. 3c. We showcase the performance
results conducted under 4 initialization strategies: random selection (dubbed â€œRandomâ€), METIS
k-way graph partitioning (dubbed â€œMETISâ€) [29], spectral clustering (dubbed â€œSpectralâ€) [75]
and METIS recursive bisection graph partitioning (dubbed â€œRBâ€) [2]. It is obvious that while the
performance achieved by the three graph partitioning algorithms varies, the magnitude of change
is barely noticeable across the three initialization methods. In contrast, the performance achieved
by randomly selecting an anchor meta-embedding is much worse than the settings with a graph
partitioning algorithm in use. This implies that the main factor of a robust meta-embedding-based
recommender system is the proper initialization of the assignment matrix. With a reasonably well
anchor meta-embedding assignment, our framework is capable of obtaining satisfactory accuracy
without further refinement of the assignment matrix.
4.5
Framework Generalizability (RQ4)
Since LERG is designed to enable the deployment of lightweight GNN-based recommender systems
on resource-constrained edge devices, it is necessary to check whether the proposed framework
performs well on various GNN-based recommenders. To do this, we select three popular GNN-based
recommenders, namely LightGCN [25], NGCF [81] and the self-supervised contrastive learning
recommender xSimGCL [102], to carry out performance benchmark in this section. For each base
recommender, we reveal the performance achieved by our work when all entities are retained, 70%
of the entities are retained, as well as the settings evaluated on the full embedding table (UD) with
dimension sizes adjusted to meet similar storage cost to our settings. The results are revealed in
Tab. 5.
From the table, it is noticed that recommendation accuracy of LERG consistently outperforms the
UD settings across all three datasets, regardless of the choice of base recommenders. One extreme
case is when NGCF is adopted as the base recommender on iFashion, the accuracy attained by the
UD setting is ill-defined. In contrast, the setting using our work with 70% of entities retained still
maintains a satisfactory recommendation quality. This indicates that the quantized compositional
embedding table, along with the graph rewiring technique, is tailored for GNN-based recommender
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 24 ---
24
Xurong Liang et al.
INT4
INT8
INT16
0.7
0.5
0.1
retention ratio
0.0201
0.0248
0.0294
NDCG@20
(a) Yelp2020
0.7
0.5
0.1
retention ratio
0.0054
0.0121
0.0188
NDCG@20
(b) Amazon-book
0.7
0.5
0.1
retention ratio
0.0019
0.0038
0.0057
NDCG@20
(c) iFashion
Fig. 4. The performance of LERG when using different quantization precisions for Â¯Eğ‘šğ‘’ğ‘¡ğ‘.
systems. Regarding the performance drop caused by graph rewiring, it is discovered that when
LERG is equipped with LightGCN or NGCF base recommenders, the performance degradation is not
obvious on Yelp2020 and iFashion datasets. The settings with xSimGCL base recommender seem to
be more sensitive to retention ratio, although the performance achieved by settings with retention
ratio of 0.7 is still much better than the UD settings. Our guess is that xSimGCL itself implements a
graph perturbation technique. Using the rewired propagation graph implements another form of
graph perturbation. The two techniques together may have caused slight interference, which leads
to sensitivity to retention ratios.
4.6
Impact of Quantization Precision (RQ5)
In LERG, we deploy a quantized compositional codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘to further reduce the storage cost
of the embedding table. Commonly, the elements in the codebook are quantized into INT8 or
INT16 precision due to the native hardware support by most edge devices. Recent quantization
literature has pushed down the quantization precision to 4 bits [71, 88, 89], despite most of them
defining a self-implemented format and GEMM for storage and computation. From our perspective,
it is insightful to study the relationship between the recommendation performance of our work
and the choice of quantization precisions applied to the compositional codebook. To do this, we
conduct experiments using the quantized compositional codebook Â¯Eğ‘šğ‘’ğ‘¡ğ‘with precision formats
INT4, INT8 and INT16 respectively. We once again show the results evaluated on retention ratios
of {0.7, 0.5, 0.1} for a comprehensive study of the subject. The performance results can be found in
Fig. 4.
From the plots, it is inspected that as the retention ratio decreases, the recommendation per-
formance, in general, follows the same fashion, regardless of the quantization precision formats.
It is also discovered that INT4 settings are more sensitive to retention ratios on Yelp2020 and
Amazon-book datasets. On the other hand, INT8 and INT16 settings attain similar performance
on these two datasets, indicating using INT8 for meta-embedding compression should suffice for
smaller datasets. However, on iFashion dataset, the performance gap between INT4 and INT8
settings is tiny. The recommendation performance achieved by INT16 settings shows a great leap
forward compared to INT4 and INT8 settings. This means that for large-scale datasets, using a
small precision format may limit the expressiveness of the meta-embeddings, and so do entity
embeddings.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 25 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
25
Table 6. Batch-wise runtime efficiency comparison between the pretraining process and settings with different
retention ratios. The settings with a retention ratio of 1 correspond to the pretraining process. â€œMEMâ€ is the
total runtime memory consumption. â€œMFLOPSâ€ is the magnitude of the exerted megaFLOPS. â€œTimeâ€ is the
training elapsed time. The three computation-related metrics are measured for a forward and backward pass
that occurred in a single batch.
Yelp2020
Amazon-book
iFashion
Retention Ratio
MEM
MFLOPS
Time
MEM
MFLOPS
Time
MEM
MFLOPS
Time
1
52.45MB
58.54
20.81ms
74.06MB
77.39
37.05ms
0.80GB
944.82
513.39ms
0.7
48.56MB
55.05
19.78ms
66.40MB
63.12
33.20ms
0.79GB
939.22
510.74ms
0.5
37.83MB
38.71
18.75ms
61.62MB
60.99
28.77ms
0.63GB
767.26
495.65ms
0.1
33.84MB
36.57
16.40ms
45.99MB
56.62
22.14ms
0.58GB
767.26
374.26ms
4.7
On-device Fine-tuning Computation Improvement (RQ6)
Since LERG offers the option to perform a lightweight embedding fine-tuning process on-edge, it
is crucial to study the runtime computation cost of LERGin the fine-tuning stage. To do this, we
simulate the pretraining and fine-tuning processes using a pure GPU computation environment. For
the three datasets, we fix the batch size and record the runtime memory usage, the measured floating
point operations per second (FLOPS) and the elapsed time to perform a forward and backward
pass in a single batch. Note that since the main innovation of our work lies in the improvement
of the embedding update strategy, the three computation efficiency metrics are measured on the
graph propagation operation in the forward and backward passes only. To be specific, we track
the accumulative computation cost incurred during the low-level multiplication and accumulation
operations. The result comparison is shown in Tab. 6. It is evident that, for the three datasets, the
pertaining process always incurs the most expensive computation cost and the longest training
elapsed time. For fine-tuning, as the retention ratio is lowered, the corresponding runtime memory
consumption and training elapsed time is also reduced. It is also apparent that our work works
well for large-scale datasets in optimizing the runtime computation cost. This is shown in the
iFashion dataset, as the runtime memory consumption and training elapsed time of the setting are
respectively 27.1% and 27.7% lower than that measured on the pertaining process with a retention
ratio of 0.1. Thus, compared with pretraining, the fine-tuning process is more computationally
efficient, thus practically supporting on-device deployment.
5
Conclusion
In this paper, we analyze the deployment of GNN-based recommender systems on resource-
constrained edge devices and conclude the inherited challenges of high embedding storage cost
and low graph computational efficiency. We also study currently available embedding optimization
frameworks and realize that most work fails to address graph computational efficiency and high
peak runtime memory consumption issues. A storage- and memory-efficient GNN-based frame-
work LERG is then proposed by means of alleviating the difficulty of employing state-of-the-art
GNN-based recommender systems on resource-constrained edge devices. We conduct extensive
experiments to validate the effectiveness of LERG in lowering the embedding storage cost and
graph computational complexity while preserving satisfactory recommendation performance.
Acknowledgment
The Australian Research Council partially supports this work under the streams of Future Fellowship
(Grant No. FT210100624), Discovery Early Career Researcher Award (Grant No. DE230101033),
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 26 ---
26
Xurong Liang et al.
Discovery Project (Grant No. DP240101108 and DP240101814), and Linkage Project (Grant No.
LP230200892 and LP240200546).
References
[1] Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic
neurons for conditional computation. (2013).
[2] Pak K Chan, Martine DF Schlag, and Jason Y Zien. 1993. Spectral k-way ratio-cut partitioning and clustering. In
Proceedings of the 30th international Design Automation Conference. 749â€“754.
[3] Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph convolutional networks via importance
sampling. In ICLR.
[4] Jianfei Chen, Jun Zhu, and Le Song. 2018. Stochastic Training of Graph Convolutional Networks with Variance
Reduction. In ICML.
[5] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting graph based collaborative filtering: A
linear residual graph convolutional network approach. In AAAI.
[6] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024.
EfficientQAT: Efficient Quantization-Aware Training for Large Language Models. (2024).
[7] Ming Chen, Zhewei Wei, Bolin Ding, Yaliang Li, Ye Yuan, Xiaoyong Du, and Ji-Rong Wen. 2020. Scalable graph neural
networks via bidirectional propagation. NeurIPS (2020).
[8] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. 2021. A unified lottery ticket hypothesis
for graph neural networks. In ICML.
[9] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning elastic embeddings
for customizing on-device recommenders. In KDD. 138â€“147.
[10] Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li, Andreas Pfadler, Huan Zhao, and
Binqiang Zhao. 2019. POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion. In
KDD.
[11] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. Cluster-gcn: An efficient
algorithm for training deep and large graph convolutional networks. In KDD.
[12] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Recsys.
[13] George B Dantzig. 1990. Origins of the simplex method. In A history of scientific computing. 141â€“151.
[14] Abhinandan S Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google news personalization: scalable
online collaborative filtering. In WWW.
[15] Aditya Desai, Yanzhou Pan, Kuangyuan Sun, Li Chou, and Anshumali Shrivastava. 2021. Semantically Constrained
Memory Allocation (SCMA) for Embedding in Efficient Recommendation Systems. (2021).
[16] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022. Data augmentation for deep graph learning: A survey.
KDD 24, 2 (2022).
[17] DaYou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, and Ningyi Xu. 2024. BitDistiller: Unleashing
the Potential of Sub-4-Bit LLMs via Self-Distillation. In ACL. 102â€“116.
[18] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. 2020.
LEARNED STEP SIZE QUANTIZATION. In ICLR.
[19] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social
recommendation. In WWW.
[20] Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin,
Xiangnan He, et al. 2023. A survey of graph neural networks for recommender systems: Challenges, methods, and
directions. TORS 1, 1 (2023), 1â€“51.
[21] Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. Post-training 4-bit quantization on
embedding tables. (2019).
[22] Vipul Gupta, Xin Chen, Ruoyun Huang, Fanlong Meng, Jianjun Chen, and Yujun Yan. 2024. GraphScale: A Framework
to Enable Machine Learning over Billion-node Graphs. In CIKM.
[23] Saket Gurukar, Nikil Pancha, Andrew Zhai, Eric Kim, Samson Hu, Srinivasan Parthasarathy, Charles Rosenberg, and
Jure Leskovec. 2022. MultiBiSage: A Web-Scale Recommendation System Using Multiple Bipartite Graphs at Pinterest.
VLDB 16, 4 (2022), 781â€“789.
[24] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. NeurIPS 30
(2017).
[25] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and
Powering Graph Convolution Network for Recommendation. In SIGIR. 639â€“648.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 27 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
27
[26] Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. 2018. Adaptive sampling towards fast graph representa-
tion learning. NeurIPS 31 (2018).
[27] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and
Quoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In KDD. 2387â€“2397.
[28] Richard M Karp. 2009. Reducibility among combinatorial problems. Springer. 219â€“241 pages.
[29] George Karypis and Vipin Kumar. 1997. METIS: A software package for partitioning unstructured graphs, partitioning
meshes, and computing fill-reducing orderings of sparse matrices. (1997).
[30] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. (2014).
[31] Thomas N Kipf and Max Welling. 2017. SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL
NETWORKS. In ICLR.
[32] Pan Li, Maofei Que, and Alexander Tuzhilin. 2023. Dual contrastive learning for efficient static feature representation
in sequential recommendations. TKDE 36, 2 (2023), 544â€“555.
[33] Shiwei Li, Huifeng Guo, Lu Hou, Wei Zhang, Xing Tang, Ruiming Tang, Rui Zhang, and Ruixuan Li. 2023. Adaptive
low-precision training for embeddings in click-through rate prediction. In AAAI, Vol. 37. 4435â€“4443.
[34] Shiwei Li, Huifeng Guo, Xing Tang, Ruiming Tang, Lu Hou, Ruixuan Li, and Rui Zhang. 2024. Embedding Compression
in Recommender Systems: A Survey. Comput. Surveys 56, 5 (2024), 1â€“21.
[35] Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight Self-Attentive Sequential Recommendation.
In CIKM. 967â€“977.
[36] Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing Xie. 2020. LightRec: A Memory and
Search-Efficient Recommender System. In WWW.
[37] Defu Lian, Xing Xie, Enhong Chen, and Hui Xiong. 2020. Product quantized collaborative filtering. TKDE 33, 9 (2020).
[38] Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, and Hongzhi Yin. 2024. Lightweight Embeddings
for Graph Collaborative Filtering. In SIGIR.
[39] Xurong Liang, Tong Chen, Quoc Viet Hung Nguyen, Jianxin Li, and Hongzhi Yin. 2023. Learning compact composi-
tional embeddings via regularized pruning for recommendation. In ICDM.
[40] Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong Xu. 2020. Pagraph: Scaling gnn training on large graphs
via computation-aware caching. In ACM Symposium on Cloud Computing.
[41] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon. com recommendations: Item-to-item collaborative
filtering. IEEE Internet computing 7, 1 (2003), 76â€“80.
[42] Chong Liu, Defu Lian, Min Nie, and Xia Hu. 2020. Online optimized product quantization. In ICDM.
[43] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated Embedding Size Search
in Deep Recommender Systems. In SIGIR.
[44] Qi Liu, Jin Zhang, Defu Lian, Yong Ge, Jianhui Ma, and Enhong Chen. 2021. Online additive quantization. In KDD.
[45] Ruixuan Liu, Yang Cao, Yanlin Wang, Lingjuan Lyu, Yun Chen, and Hong Chen. 2023. Privaterec: Differentially
private model training and online serving for federated news recommendation. In KDD.
[46] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding Sizes for Recommender
Systems. ICLR (2021).
[47] Weiming Liu, Chaochao Chen, Xinting Liao, Mengling Hu, Jianwei Yin, Yanchao Tan, and Longfei Zheng. 2023.
Federated Probabilistic Preference Distribution Modelling with Compactness Co-Clustering for Privacy-Preserving
Multi-Domain Recommendation.. In IJCAI.
[48] Xin Liu, Mingyu Yan, Lei Deng, Guoqi Li, Xiaochun Ye, Dongrui Fan, Shirui Pan, and Yuan Xie. 2022. Survey on
Graph Neural Network Acceleration: An Algorithmic Perspective. In IJCAI.
[49] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman
Krishnamoorthi, and Vikas Chandra. 2024. LLM-QAT: Data-Free Quantization Aware Training for Large Language
Models. In Findings of the ACL. 467â€“484.
[50] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on information theory 28, 2 (1982), 129â€“137.
[51] Jing Long, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2023. Decentralized collaborative learning
framework for next POI recommendation. TOIS 41, 3 (2023), 1â€“25.
[52] Jing Long, Guanhua Ye, Tong Chen, Yang Wang, Meng Wang, and Hongzhi Yin. 2024. Diffusion-Based Cloud-Edge-
Device Collaborative Learning for Next POI Recommendations. In KDD.
[53] Fuyuan Lyu, Xing Tang, Hong Zhu, Huifeng Guo, Yingxue Zhang, Ruiming Tang, and Xue Liu. 2022. OptEmbed:
Learning Optimal Embedding Table for Click-through Rate Prediction. In CIKM. 1399â€“1409.
[54] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong
Xue, and Furu Wei. 2024. The era of 1-bit llms: All large language models are in 1.58 bits. (2024).
[55] Nattaya Mairittha, Tittaya Mairittha, and Sozo Inoue. 2020. Improving activity data collection with on-device
personalization using fine-tuning. In UbiComp/ISWC.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 28 ---
28
Xurong Liang et al.
[56] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: ultra simplification of
graph convolutional networks for recommendation. In CIKM.
[57] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort.
2021. A white paper on neural network quantization. (2021).
[58] Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. 2020. Pinnersage:
Multi-modal user embedding framework for recommendations at pinterest. In KDD.
[59] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot embedding
dimension search in recommender system. In SIGIR.
[60] Yunke Qu, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024. Budgeted embedding table for recommender
systems. In WSDM.
[61] Yunke Qu, Tong Chen, Xiangyu Zhao, Lizhen Cui, Kai Zheng, and Hongzhi Yin. 2023. Continuous Input Embedding
Size Search For Recommender Systems. In SIGIR.
[62] Yunke Qu, Liang Qu, Tong Chen, Xiangyu Zhao, Jianxin Li, and Hongzhi Yin. 2024. Sparser Training for On-Device
Recommendation Systems. (2024).
[63] Yunke Qu, Liang Qu, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024. Scalable Dynamic
Embedding Size Search for Streaming Recommendation. In CIKM.
[64] Zheng Qu, Dimin Niu, Shuangchen Li, Hongzhong Zheng, and Yuan Xie. 2023. TT-GNN: Efficient On-Chip Graph
Neural Network Training via Embedding Reformation and Hardware Optimization. In MICRO.
[65] Steffen Rendle. 2010. Factorization Machines. In ICDM. 995â€“1000.
[66] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized
ranking from implicit feedback. In UAI. 452â€“461.
[67] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. Dropedge: Towards deep graph convolutional
networks on node classification. In ICLR.
[68] A Schrijver. 1998. Theory of linear and integer programming. John Wiley & Sons.
[69] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using
Complementary Partitions for Memory-Efficient Recommendation Systems. KDD (2020), 165â€“175.
[70] Jianing Sun, Zhaoyue Cheng, Saba Zuberi, Felipe PÃ©rez, and Maksims Volkovs. 2021. Hgcf: Hyperbolic graph
convolution networks for collaborative filtering. In WWW.
[71] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar
El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. 2020. Ultra-low precision 4-bit training of
deep neural networks. NeurIPS 33 (2020).
[72] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. 2022.
Understanding over-squashing and bottlenecks on graphs via curvature. In ICLR.
[73] Hung Vinh Tran, Tong Chen, Nguyen Quoc Viet Hung, Zi Huang, Lizhen Cui, and Hongzhi Yin. 2025. A Thorough
Performance Benchmarking on Lightweight Embedding-based Recommender Systems. TOIS (2025).
[74] Hung Vinh Tran, Tong Chen, Guanhua Ye, Quoc Viet Hung Nguyen, Kai Zheng, and Hongzhi Yin. 2025. On-device
content-based recommendation with single-shot embedding pruning: A cooperative game perspective. In WWW.
772â€“785.
[75] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing 17 (2007), 395â€“416.
[76] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu
Zhang, et al. 2023. Efficient large language models: A survey. Transactions on Machine Learning Research (2023).
[77] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu,
and Furu Wei. 2023. BitNet: Scaling 1-bit Transformers for Large Language Models. (2023).
[78] Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. 2018. Billion-scale commodity
embedding for e-commerce recommendation in alibaba. In KDD.
[79] Qinyong Wang, Hongzhi Yin, Tong Chen, Zi Huang, Hao Wang, Yanchang Zhao, and Nguyen Quoc Viet Hung. 2020.
Next Point-of-Interest Recommendation on Resource-Constrained Mobile Devices. In WWW. 906â€“916.
[80] Qinyong Wang, Hongzhi Yin, Tong Chen, Junliang Yu, Alexander Zhou, and Xiangliang Zhang. 2022. Fast-adapting
and privacy-preserving federated recommender system. VLDB 31, 5 (2022), 877â€“896.
[81] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering.
In SIGIR. 165â€“174.
[82] Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. 2013. A theoretical analysis of NDCG
ranking measures. In COLT, Vol. 8. 6.
[83] Yingchao Wang, Chen Yang, Shulin Lan, Liehuang Zhu, and Yan Zhang. 2024. End-edge-cloud collaborative computing
for deep learning: A comprehensive survey. IEEE Communications Surveys & Tutorials (2024).
[84] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In ICML.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 29 ---
Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
29
[85] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying graph
convolutional networks. In ICML.
[86] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised
graph learning for recommendation. In SIGIR.
[87] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural networks in recommender systems: a
survey. Comput. Surveys 55, 5 (2022), 1â€“37.
[88] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. 2023. Understanding int4 quantization
for language models: latency speedup, composability, and failure cases. In ICML. 37524â€“37539.
[89] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. 2023. Training transformers with 4-bit integers. NeurIPS 36
(2023).
[90] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc Viet Hung Nguyen. 2022. On-device
next-item recommendation with self-supervised knowledge distillation. In SIGIR. 546â€“555.
[91] Xin Xia, Junliang Yu, Qinyong Wang, Chaoqun Yang, Nguyen Quoc Viet Hung, and Hongzhi Yin. 2023. Efficient
on-device session-based recommendation. TOIS 41, 4 (2023).
[92] Xin Xia, Junliang Yu, Guandong Xu, and Hongzhi Yin. 2023. Towards communication-efficient model updating for
on-device session-based recommendation. In CIKM.
[93] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li. 2021. Agile and accurate
CTR prediction model training for massive-scale online advertising systems. In SIGMOD. 2404â€“2409.
[94] Yikai Yan, Chaoyue Niu, Renjie Gu, Fan Wu, Shaojie Tang, Lifeng Hua, Chengfei Lyu, and Guihai Chen. 2022.
On-device learning for model personalization with large-scale cloud-coordinated domain adaption. In KDD.
[95] Liang Yang, Zesheng Kang, Xiaochun Cao, Di Jin 0001, Bo Yang, and Yuanfang Guo. 2019. Topology Optimization
based Graph Convolutional Network.. In IJCAI.
[96] Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia Yang. 2021. Device-cloud collaborative
learning for recommendation. In KDD.
[97] Yao Yao, Bin Liu, Haoxun He, Dakui Sheng, Ke Wang, Li Xiao, and Huanhuan Cao. 2023. i-Razor: A Differentiable
Neural Input Razor for Feature Selection and Dimension Search in DNN-Based Recommender Systems. TKDE 36, 9
(2023), 4736â€“4749.
[98] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida
Wang, Michael Mahoney, et al. 2021. Hawq-v3: Dyadic neural network quantization. In ICML.
[99] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021. TT-REC: Tensor train compression for deep learning
recommendation model embeddings. Proceedings of Machine Learning and Systems 3 (2021), 448â€“462.
[100] Chunxing Yin, Da Zheng, Israt Nisa, Christos Faloutsos, George Karypis, and Richard Vuduc. 2022. Nimble GNN
Embedding with Tensor-Train Decomposition. In KDD. 2327â€“2335.
[101] Hongzhi Yin, Liang Qu, Tong Chen, Wei Yuan, Ruiqi Zheng, Jing Long, Xin Xia, Yuhui Shi, and Chengqi Zhang. 2024.
On-device recommender systems: A comprehensive survey. (2024).
[102] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and Hongzhi Yin. 2023. XSimGCL: Towards
extremely simple graph contrastive learning for recommendation. TKDE 36, 2 (2023).
[103] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmenta-
tions necessary? simple graph contrastive learning for recommendation. In SIGIR.
[104] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2020. Graphsaint: Graph
sampling based inductive learning method. In ICLR.
[105] Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani, Akshay Gupta, Pranay Kumar Myana, Deepak
Dilipkumar, Suvadip Paul, Ikuhiro Ihara, et al. 2020. Model Size Reduction Using Frequency Based Double Hashing
for Recommender Systems. In RecSys. 521â€“526.
[106] Chengyuan Zhang, Yang Wang, Lei Zhu, Jiayu Song, and Hongzhi Yin. 2021. Multi-graph heterogeneous interaction
fusion for social recommendation. TOIS 40, 2 (2021), 1â€“26.
[107] Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2023. LightFR: Lightweight federated recom-
mendation with privacy-preserving matrix factorization. TOIS 41, 4 (2023).
[108] Jin Zhang, Qi Liu, Defu Lian, Zheng Liu, Le Wu, and Enhong Chen. 2022. Anisotropic additive quantization for fast
inner product search. In AAAI, Vol. 36.
[109] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing
Liu, and Xiwang Yang. 2021. Autoemb: Automated embedding dimensionality search in streaming recommendations.
In ICDM. 896â€“905.
[110] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. 2020.
Robust graph representation learning via neural sparsification. In ICML.
[111] Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, and Hongzhi Yin. 2024. Personalized elastic embedding
learning for on-device recommendation. TKDE 36, 7 (2024), 3363â€“3375.
, Vol. 1, No. 1, Article . Publication date: June 2018.


--- Page 30 ---
30
Xurong Liang et al.
[112] Shangfei Zheng, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wei Chen, and Lei Zhao. 2025. Do as I can, not
as I get: Topology-aware multi-hop reasoning on multi-modal knowledge graphs. TKDE (2025).
[113] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. 2019. Layer-dependent importance
sampling for training deep and large graph convolutional networks. NeurIPS 32 (2019).
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
, Vol. 1, No. 1, Article . Publication date: June 2018.
