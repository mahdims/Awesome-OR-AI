--- Page 1 ---
OR-Toolformer: Modeling and Solving Operations Research Problems with
Tool Augmented Large Language Models
Jianzhang Zhang, Jialong Zhou and Chuang Liu†
Alibaba Business School, Hangzhou Normal University
{zjzhang,liuchuang}@hznu.edu.cn, jialongzhouzj@gmail.com
Abstract
Large language models (LLMs) demonstrate
strong mathematical reasoning, but reliance
on closed-source APIs for OR tasks raises pri-
vacy concerns, and training open-source mod-
els from scratch incurs high compute costs.
We introduce OR-Toolformer, which fine-tunes
Llama-3.1-8B-Instruct with a semi-automatic
data synthesis pipeline that generates diverse
OR problem-answer pairs and augments the
model with external solvers to produce API
calls. On three of four standard benchmarks,
OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by
over 4.3%. In zero-shot evaluation on two un-
seen OR problem types, it attains 54% average
accuracy, a 21 percentage-point improvement
over the strongest baseline. These findings vali-
date the efficacy of tool-augmented fine-tuning
LLMs for accurate and generalizable OR prob-
lem modeling and solving.
1
Introduction
Operations Research (OR) offers rigorous meth-
ods to formalize and solve complex decision prob-
lems in various sectors. OR workflows involve (1)
translating natural-language descriptions into math-
ematical optimization models and (2) obtaining
solutions via general-purpose solvers (Petropoulos
et al., 2024), yet this pipeline remains dependent
on domain expertise, limiting scalability.
Large language models (LLMs) have demon-
strated strong text comprehension and multi-
step mathematical reasoning on complex bench-
marks (Romera-Paredes et al., 2024; Xia et al.,
2025), indicating their potential to automate both
formulation and solution of OR tasks. However,
reliance on closed source LLM APIs raises data pri-
vacy concerns (Das et al., 2025), as sensitive prob-
lem descriptions and data often constitute commer-
cial confidential information and must be transmit-
†Corresponding author.
ted to proprietary platforms beyond the user’s con-
trol. Moreover, training open-source models from
scratch incurs prohibitive computational costs (Xia
et al., 2024).
Fine-tuning pre-trained LLMs for domain-
specific tasks offers a resource-efficient alterna-
tive, but vanilla LLMs struggle with precise arith-
metic (McLeish et al., 2024). Tool-learning tech-
niques enable LLMs to invoke external tools,
such as calculators or specialized APIs, thereby
combining generative flexibility with solver accu-
racy (Schick et al., 2023; Shi et al., 2025). We in-
troduce OR-Toolformer 1, which fine-tunes Llama-
3.1-8B-Instruct to extract structured solver param-
eters from natural-language OR problem descrip-
tions and generate corresponding API calls, fully
automating the modeling and solution phases.
Figure 1: Overview of OR-Toolformer.
2
The Methodology of OR-Toolformer
OR-Toolformer automates OR tasks through three
integrated components (Figure 1):
• Problem–Answer Data Generation, a semi-
automated pipeline that synthesizes diverse
1publicly available after finishing peer reviewing
arXiv:2510.01253v1  [cs.AI]  24 Sep 2025


--- Page 2 ---
OR problem-answer pairs across problem
types, industry contexts, and representation
formats to ensure domain and expression di-
versity;
• LLM Fine-Tuning, which adapts pre-trained
LLMs to parse natural-language descriptions
and extract structured solver parameters;
• Problem Solving with OR Solvers, where
the fine-tuned model issues API calls to ex-
ternal optimization solvers, uniting language
comprehension with computational precision.
2.1
Problem-Answer Data Generation
High-quality instruction tuning for robust gener-
alization requires OR problem-answer pairs that
capture both domain-specific variation and diverse
linguistic expressions (Albalak et al., 2024). Given
the scarcity of datasets that include detailed mod-
eling steps and solver API calls (Huang et al.,
2025a; Mostajabdaveh et al., 2025), we introduce
a three-stage, semi-automated pipeline for large-
scale synthesis of OR problem-answer pairs. Fig-
ure 2 presents an example linear programming
(LP) problem-answer instance, highlighting the in-
put key information (left-top) and the generated
problem-answer pair (right) along with the corre-
sponding API call (left-bottom).
Figure 2: Snippet of the generation process of an LP
problem-answer pair.
Stage 1: Parameter sampling. We randomly
sample OR problem parameters from realistic
ranges (e.g., positive unit consumption rates).
These values are converted into structured API in-
puts and validated by the solvers. To ensure do-
main diversity, we vary application contexts (e.g.,
agriculture, logistics, finance) and objective types
(profit maximization, cost minimization). For ex-
pression diversity, the parameter set of OR problem
is rendered in free-form text, matrix notation, and
tabular lists.
Stage 2: Prompt-based statement and answer
synthesis. We embed the key information (the sam-
pled parameters and context as illustrated in top-
left of Figure 2) into a problem generation prompt
template that instructs Gemini 2.0 Flash to gen-
erate coherent OR problem statements. We then
augment the same key information with API usage
descriptions in answer generation prompt template
that instructs Gemini 2.0 Flash to produce both the
chain of thoughts and the corresponding API call
(bottom-right of Figure 2). These two prompts are
shown in Appendix A.1 and A.2 respectively.
Stage 3: Quality filtering and formatting. To
mitigate hallucinations (Huang et al., 2025b), we
execute the generated API call (dotted box in the
right-bottom of Figure 2) and compare its result
against that of the sampled parameters based API
call (left bottom of Figure 2).
Only problem-
answer pairs with matching results are retained.
Finally, we cast validated instances into a dialogue
format aligning with instruction-tuning best prac-
tices (Ouyang et al., 2022; Qin et al., 2024; Patil
et al., 2024). System messages list one correct tool
and three distractors, user messages present the
problem, and assistant messages deliver the chain
of thoughts plus API calls.
2.2
LLM Fine-Tuning
We fine-tune Llama-3.1-8B-Instruct (Grattafiori
et al., 2024) on our synthesized dataset via in-
struction tuning. Let D = {(Qi, Ai)}N
i=1 denote
the set of N OR problem–answer pairs, where
each prompt Qi comprises a system message and
a user message, and Ai is the corresponding as-
sistant message. The model’s prediction for Qi is
ˆAi = LLMθ(Qi). We optimize the parameters θ
by minimizing the negative log-likelihood (cross-
entropy) loss:
L(θ) = −
N
X
i=1
log Pθ(Ai | Qi)
(1)
where Pθ(Ai | Qi) is the probability assigned by
the LLM to the reference output Ai.


--- Page 3 ---
2.3
Problem Solving with OR Solvers
After generating an OR problem solution, we ex-
tract the embedded API call strings and parse them
into structured invocations as depicted by the con-
nected dotted boxes in the bottom of Figure 2. We
execute these on two external OR services, NEOS
Server2 and Google Operations Research API3, to
compute numerical solutions for each instance4.
3
Experiments
3.1
Experimental Setup
Data generation. We synthesize two datasets: one
for instruction fine-tuning and another to evalu-
ate zero-shot generalization on unseen OR prob-
lem types.
Table 1 details the number of in-
stances per problem category.
The fine-tuning
dataset includes the same types of problems as
those found in the four benchmarks including
NL4OPT (Ramamonjison et al., 2023), MAMO-
EasyLP, MAMO-ComplexLP (Huang et al., 2024),
and IndustryOR (Huang et al., 2025a).
Training Dataset
LP
IP
MILP
TSP
MF
Total
3502
3501
3493
3516
3496
17508
Test Dataset
TSP
MF
AP
MCF
50
50
50
25
175
Abbreviations: LP = Linear Programming; IP = In-
teger Programming; MILP = Mixed-Integer Linear
Programming; TSP = Traveling Salesman Problem;
MF = Maximum Flow; AP = Assignment Problem;
MCF = Minimum-Cost Flow.
Table 1: Summary statistics of training and test datasets.
Training. We fine-tune Llama-3.1-8B-Instruct
on the full training set with a batch size of 64 and a
learning rate of 2 × 10−4. Using the Unsloth (Han
and Han, 2023) framework on a single GPU (10
GB VRAM), we perform parameter-efficient fine-
tuning via LoRA, 8-bit AdamW, and 4-bit quanti-
zation, updating only 0.52% of parameters.
Evaluation. We measure execution accuracy
following Huang et al. (Huang et al., 2025a),
deeming a prediction correct if the solver’s re-
turned optimum matches any ground-truth value.
We benchmark OR-Toolformer against general-
purpose LLMs (ChatGPT, Gemini, DeepSeek-
2https://neos-server.org/neos/
3https://developers.google.com/optimization/service
4Google OR API is used to solve MF, MCF, and AP prob-
lems, as NEOS does not offer solvers for these problems.
R1) and size-matched baselines: general LLMs
(DeepSeek-7B, Mistral-7B, Qwen-2.5-7B) and
math-focused LLMs (JiuZhang-3.0).
3.2
Results Analysis
Results on benchmarks. Table 2 summarizes the
execution accuracy of OR-Toolformer and base-
line models on four standard benchmarks. All
models achieve substantially higher accuracy on
simpler tasks (NL4OPT, MAMO-EasyLP) than
on more complex ones (MAMO-ComplexLP, In-
dustryOR). Consistent with scaling laws (Kaplan
et al., 2020), larger general-purpose LLMs outper-
form their smaller counterparts on three of the four
benchmarks. Accordingly, we focus our analysis
on size-matched general-purpose and math-specific
LLMs. Among 7-8 B models, OR-Toolformer de-
livers the highest accuracy across all benchmarks
except IndustryOR, where it places second. In par-
ticular, OR-Toolformer attains 80.1% on MAMO-
EasyLP and approximately 14% on both MAMO-
ComplexLP and IndustryOR, substantially outper-
forming other size-matched baselines, all of which
fall below 18%. Although Qwen-2.5-7B-Instruct
ranks second, math-specific LLMs generally out-
perform other size-matched models, underscoring
the value of domain-specific fine-tuning (Zhang
et al., 2024).
Results on the test dataset. Table 3 compares
OR-Toolformer and Qwen-2.5-7B-Instruct on two
OR problem types (AP and MCF) not included
in the benchmark suites. On two familiar prob-
lem types (TSP and MF), which were generated
identically to our training data, OR-Toolformer
achieves 100% and 98% execution accuracy, re-
spectively, confirming the consistency of our syn-
thesis pipeline. Crucially, on two entirely unseen
problem types (AP and MCF), OR-Toolformer at-
tains 68% and 40% accuracy versus 62% and 4%
for Qwen-2.5-7B-Instruct, representing an aver-
age improvement of 21 percentage points. These
results demonstrate OR-Toolformer’s strong zero-
shot generalization to novel OR tasks.
Output token efficiency. We evaluate the av-
erage output length of each model to assess token
efficiency. OR-Toolformer generates concise re-
sponses, averaging 449 tokens, compared to 500
tokens for Qwen-2.5-7B-Instruct and 1,422 tokens
for Qwen-2.5-Math-7B, the latter of which typi-
cally includes extensive mathematical derivations
and embedded code. As illustrated in the bottom-
right of Figure 2, OR-Toolformer produces succinct


--- Page 4 ---
Method
NL4OPT
MAMO-
EasyLP
MAMO-
ComplexLP
IndustryOR
General LLMs
GPT-3.5
42.4%
61.8%
20.9%
19.0%
GPT-4
47.3%
66.5%
14.6%
28.0%
Gemini-2.0 Flash
79.6%
77.3%
26.1%
23.0%
DeepSeek-R1-685B
66.1%
73.6%
48.3%
27.0%
General LLMs
in similar scale
DeepSeek-LLM-7B-Chat
5.7%
2.3%
0.5%
1.0%
Llama-3.1-8B-Instruct
6.9%
8.3%
7.6%
3.0%
Mistral-7B-Instruct-v0.3
0.0%
0.0%
0.0%
3.0%
Qwen-2.5-7B-Instruct
44.1%
43.6%
9.5%
18.0%
Math LLMs
in similar scale
DeepSeek-Math-7B-Instruct
20.0%
30.7%
6.6%
10.0%
DeepSeek-Math-7B-RL
23.7%
27.5%
10.4%
10.0%
Qwen-2.5-Math-7B
40.8%
41.1%
10.9%
9.0%
JiuZhang-3.0-7B
13.9%
4.6%
3.3%
4.0%
JiuZhang-3.0-8B
23.7%
4.3%
4.3%
2.0%
Ours
OR-Toolformer-8B
59.6%
80.1%
14.7%
14.0%
Note. The best results are in bold, and the second-best are underlined. Results in the first section are not
included in the ranking.
Table 2: Performance of OR-Toolformer and three types of baselines on four benchmarks.
Method
TSP
MF
AP
MCF
Qwen-2.5-7B-
Instruct
0.0%
16.0%
62.0%
4.0%
Ours
100.0%
98.0%
68.0%
40.0%
Table 3: Performance of OR-Toolformer and Qwen-2.5-
7B-Instruct on test dataset.
natural-language outputs that satisfy both optimiza-
tion and API-invocation requirements, thereby sub-
stantially reducing token consumption.
4
Related Work
Tool learning enables LLMs to extend generative
capacity by invoking external APIs.
STE has
models imagine, execute, and refine tool-usage se-
quences via simulated trial-and-error (Wang et al.,
2024). Cooperative multi-agent methods decom-
pose tool use into grounding, execution, and review
stages (Shi et al., 2024), and budget-constrained
planning generates cost-optimal call sequences un-
der resource limits (Zheng et al., 2024).
Self-
instruction pipelines synthesize diverse API-call
examples from documentation (Yang et al., 2023),
further scaled by Shi et al.(Shi et al., 2025). Large-
scale benchmarks such as StableToolBench(Guo
et al., 2024) and RoTBench (Ye et al., 2024b) stan-
dardize evaluation, and ToolSword exposes safety
vulnerabilities across tool-learning stages (Ye et al.,
2024a). Unlike prior work focused on calculator-
based tools (Schick et al., 2023), our method em-
phasizes solver learning for OR, leveraging self-
instruction generated training data (Yang et al.,
2023).
LLMs have been applied to automate OR task
formulation and solution.
The NL4OPT com-
petition provides a widely used benchmark (Ra-
mamonjison et al., 2023), and Mostajabdaveh
et al.(Mostajabdaveh et al., 2025) evaluate open-
source LLMs on complex OR problems. Chain-of-
Experts and Optimus combine prompt engineering
and multi-agent pipelines using GPT-4 for OR for-
mulation (Xiao et al., 2023; AhmadiTeshnizi et al.,
2024). LLMs have also been used to help inter-
pret optimization results and identify infeasible
optimization problems (Li et al., 2023; Chen et al.,
2024). To mitigate privacy and computational costs,
ORLM fine-tunes open-source models end-to-end
for solver-code generation (Huang et al., 2025a). In
contrast, we employ parameter-efficient fine-tuning
to yield concise natural language formulations and
structured API calls.
5
Conclusion
We present OR-Toolformer, a fine-tuned Llama-
3.1-8B-Instruct model augmented with external
OR solvers. It achieves 80.1% execution accu-
racy on three standard benchmarks, outperforming
size-matched LLMs, and 54% average zero-shot
accuracy on two novel problem types (a 21 pp im-
provement). These results confirm the efficacy of
tool-augmented LLM fine-tuning for both accuracy
and generalization in OR tasks. Future work will
explore integrating agents via a model-context pro-
tocol.


--- Page 5 ---
Limitations
Our study has several limitations.
First, OR-
Toolformer’s accuracy on complex or industry-
scale OR tasks (e.g., MAMO-ComplexLP, Indus-
tryOR) remains substantially lower than on simpler
academic benchmarks, which may impede real-
world deployment. Second, due to computational
constraints, we fine-tuned and evaluated only a sin-
gle open-source LLM; a broader comparison across
additional models is left to future work. Third, our
synthetic data pipeline relies on heuristic prompt
templates and limited domain context; incorporat-
ing stronger LLMs and richer industrial scenarios
could enhance data realism and diversity. Finally,
we have not yet conducted user-centered evalua-
tions to measure the framework’s usability and util-
ity in practical optimization workflows.
References
Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell.
2024. Optimus: scalable optimization modeling with
(mi) lp solvers and large language models. In Pro-
ceedings of the 41st International Conference on Ma-
chine Learning, pages 577–596.
Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne
Longpre, Nathan Lambert, Xinyi Wang, Niklas
Muennighoff, Bairu Hou, Liangming Pan, Hae-
won Jeong, Colin Raffel, Shiyu Chang, Tatsunori
Hashimoto, and William Yang Wang. 2024. A survey
on data selection for language models. Transactions
on Machine Learning Research. Survey Certifica-
tion.
Hao Chen, Gonzalo E Constante-Flores, and Can Li.
2024. Diagnosing infeasible optimization problems
using large language models. INFOR: Information
Systems and Operational Research, 62(4):573–587.
Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu.
2025. Security and privacy challenges of large lan-
guage models: A survey. ACM Computing Surveys,
57(6):1–39.
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
Alex Vaughan, and 1 others. 2024. The llama 3 herd
of models. arXiv preprint arXiv:2407.21783.
Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang,
Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and
Yang Liu. 2024. StableToolBench: Towards stable
large-scale benchmarking on tool learning of large
language models. In Findings of the Association
for Computational Linguistics: ACL 2024, pages
11143–11156, Bangkok, Thailand. Association for
Computational Linguistics.
Daniel Han and Michael Han. 2023. Unsloth.
Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing
Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and
Zizhuo Wang. 2025a. Orlm: A customizable frame-
work in training large models for automated optimiza-
tion modeling. Operations Research.
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 oth-
ers. 2025b. A survey on hallucination in large lan-
guage models: Principles, taxonomy, challenges, and
open questions. ACM Transactions on Information
Systems, 43(2):1–55.
Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao,
and Benyou Wang. 2024. Mamo: a mathematical
modeling benchmark with solvers. arXiv preprint
arXiv:2405.13144.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models.
arXiv
preprint arXiv:2001.08361.
Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan
Pathuri, and Ishai Menache. 2023. Large language
models for supply chain optimization. arXiv preprint
arXiv:2307.03875.
Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John
Kirchenbauer, Brian Bartoldson, Bhavya Kailkhura,
Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
and 1 others. 2024. Transformers can do arithmetic
with the right embeddings.
In Advances in Neu-
ral Information Processing Systems, pages 108012–
108041.
Mahdi Mostajabdaveh, Timothy Tin Long Yu, Samaren-
dra Chandan Bindu Dash, Rindra Ramamonjison,
Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou,
and Yong Zhang. 2025. Evaluating llm reasoning in
the operations research domain with orqa. In Pro-
ceedings of the AAAI Conference on Artificial Intelli-
gence, pages 24902–24910.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, and
1 others. 2022. Training language models to fol-
low instructions with human feedback. In Advances
in Neural Information Processing Systems, pages
27730–27744.
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
Gonzalez. 2024.
Gorilla: Large language model
connected with massive apis. In Advances in Neu-
ral Information Processing Systems, pages 126544–
126565.
Fotios Petropoulos, Gilbert Laporte, Emel Aktas,
Sibel A Alumur, Claudia Archetti, Hayriye Ayhan,
Maria Battarra, Julia A Bennell, Jean-Marie Bour-
jolly, John E Boylan, and 1 others. 2024. Operational


--- Page 6 ---
research: methods and applications. Journal of the
Operational Research Society, 75(3):423–617.
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan
Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,
Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li,
Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM:
Facilitating large language models to master 16000+
real-world APIs. In The Twelfth International Con-
ference on Learning Representations.
Rindranirina Ramamonjison, Timothy Yu, Raymond
Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar,
Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-
Dehkordi, Zirui Zhou, and 1 others. 2023. Nl4opt
competition: Formulating optimization problems
based on their natural language descriptions. In Pro-
ceedings of the NeurIPS 2022 Competitions Track,
pages 189–203.
Bernardino
Romera-Paredes,
Mohammadamin
Barekatain,
Alexander Novikov,
Matej Balog,
M Pawan Kumar, Emilien Dupont, Francisco JR
Ruiz, Jordan S Ellenberg, Pengming Wang, Omar
Fawzi, and 1 others. 2024. Mathematical discoveries
from program search with large language models.
Nature, 625(7995):468–475.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. 2023.
Toolformer: Language models can teach themselves
to use tools. In Advances in Neural Information Pro-
cessing Systems, volume 36, pages 68539–68551.
Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng,
Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren,
Suzan Verberne, and Zhaochun Ren. 2024. Learning
to use tools via cooperative and interactive agents.
In Findings of the Association for Computational
Linguistics: EMNLP 2024, pages 10642–10657, Mi-
ami, Florida, USA. Association for Computational
Linguistics.
Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng,
Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Ver-
berne, and Zhaochun Ren. 2025. Tool learning in
the wild: Empowering language models as automatic
tool agents. In Proceedings of the ACM on Web Con-
ference 2025, pages 2222–2237.
Boshi Wang, Hao Fang, Jason Eisner, Benjamin
Van Durme, and Yu Su. 2024. LLMs in the imag-
inarium: Tool learning through simulated trial and
error. In Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 10583–10604, Bangkok,
Thailand. Association for Computational Linguistics.
Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu,
and Pengfei Liu. 2025. Evaluating mathematical
reasoning beyond accuracy. In Proceedings of the
AAAI Conference on Artificial Intelligence, pages
27723–27730.
Yuchen Xia, Jiho Kim, Yuhan Chen, Haojie Ye, Souvik
Kundu, Cong Callie Hao, and Nishil Talati. 2024. Un-
derstanding the performance and estimating the cost
of llm fine-tuning. In 2024 IEEE International Sym-
posium on Workload Characterization, pages 210–
223.
Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu,
Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao
Zhong, Jia Zeng, Mingli Song, and 1 others. 2023.
Chain-of-experts: When llms meet complex opera-
tions research problems. In The twelfth international
conference on learning representations.
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge,
Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching
large language model to use tools via self-instruction.
In Advances in Neural Information Processing Sys-
tems, pages 71995–72007.
Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang,
Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and
Xuanjing Huang. 2024a.
ToolSword: Unveiling
safety issues of large language models in tool learn-
ing across three stages. In Proceedings of the 62nd
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2181–
2211, Bangkok, Thailand. Association for Computa-
tional Linguistics.
Junjie Ye, Yilong Wu, Songyang Gao, Caishuang
Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang,
Tao Gui, and Xuanjing Huang. 2024b. RoTBench: A
multi-level benchmark for evaluating the robustness
of large language models in tool learning. In Proceed-
ings of the 2024 Conference on Empirical Methods
in Natural Language Processing, pages 313–333, Mi-
ami, Florida, USA. Association for Computational
Linguistics.
Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan
Firat. 2024. When scaling meets LLM finetuning:
The effect of data, model and finetuning method. In
The Twelfth International Conference on Learning
Representations.
Yuanhang Zheng, Peng Li, Ming Yan, Ji Zhang, Fei
Huang, and Yang Liu. 2024. Budget-constrained
tool learning with planning. In Findings of the As-
sociation for Computational Linguistics: ACL 2024,
pages 9039–9052, Bangkok, Thailand. Association
for Computational Linguistics.


--- Page 7 ---
A
Question and Answer Generation
Prompts
A.1
Problem Generation Prompt Template
Figure 3 shows the prompt template for generating
linear programming problem statements, with key
information of problems (as illustrated in Figure 2)
inserted into {}.
Figure 3: Problem generation prompt template.
A.2
Answer Generation Prompt Template
Figure 4 shows the prompt template for generating
answers, with the API usage description and OR
question statements (as illustrated in Figure 2) in-
serted into {}. This template is used to generate
input for OR-Toolformer and all baselines.
Figure 4: Answer generation prompt template.
B
Data, Code, and Model Availability
The dataset, source code, and model check-
points are available at the anonymized URL
for peer review:
https://figshare.com/s/
262251a08ea7f79113d7.
