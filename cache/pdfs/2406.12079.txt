--- Page 1 ---
Multi-Dimensional Pruning: Joint Channel, Layer
and Block Pruning with Latency Constraint
Xinglong Sun1,2‚ãÜ, Barath Lakshmanan1, Maying Shen1, Shiyi Lan1, Jingde
Chen1, and Jose Alvarez1
1 NVIDIA
2 Stanford University
{xinglongs, blakshmanan, mshen, shiyil, joshchen, josea}@nvidia.com
2500
3000
3500
4000
4500
5000
FPS
69
70
71
72
73
74
75
Top1
EagleEye (ECCV'20)
MetaPruning (ICCV'19)
AutoSlim (NeurIPS'19)
0.50 x ResNet50
ResNet18
HALP (NeurIPS'22)
SMCP (ECCV'22)
Ours
+4.2
Top1
+ 426
FPS
+ 1161 FPS 
+2.1
mAP
+5.6 FPS
Fig. 1: MDP exhibits Pareto dominance across different tasks. In contrast to existing
methods: [Left] On Imagenet classification, we achieve a 6.2% relative accuracy gain
with a 2.6% FPS speedup, and even greater gains at higher pruning ratio: a 2% relative
gain with a substantial 28.3% FPS speedup. [Right] On NuScenes 3D object detection,
we observe a 5.6% relative mAP improvement alongside a 1.8% FPS increase.
Abstract. As we push the boundaries of performance in various vi-
sion tasks, the models grow in size correspondingly. To keep up with this
growth, we need very aggressive pruning techniques for efficient inference
and deployment on edge devices. Existing pruning approaches are limited
to channel pruning and struggle with aggressive parameter reductions.
In this paper, we propose a novel multi-dimensional pruning framework
that jointly optimizes pruning across channels, layers, and blocks while
adhering to latency constraints. We develop a latency modeling technique
that accurately captures model-wide latency variations during pruning,
which is crucial for achieving an optimal latency-accuracy trade-offs at
high pruning ratio. We reformulate pruning as a Mixed-Integer Nonlinear
Program (MINLP) to efficiently determine the optimal pruned structure
with only a single pass. Our extensive results demonstrate substantial im-
provements over previous methods, particularly at large pruning ratios.
In classification, our method significantly outperforms prior art HALP
‚ãÜPerformed during an internship at NVIDIA
arXiv:2406.12079v1  [cs.CV]  17 Jun 2024


--- Page 2 ---
2
X.Sun et al.
with a Top-1 accuracy of 70.0(v.s. 68.6) and an FPS of 5262 im/s(v.s.
4101 im/s). In 3D object detection, we establish a new state-of-the-art
by pruning StreamPETR [60] at a 45% pruning ratio, achieving higher
FPS (37.3 vs. 31.7) and mAP (0.451 vs. 0.449) than the dense baseline.
Keywords: Network Pruning, Model Acceleration, MINLP.
1
Introduction
Deep neural networks have become the de-facto standards of advanced com-
puter vision applications, ranging from image classification [25] to object de-
tection [43] and segmentation [46]. Contemporary networks [15, 60, 65] usually
consist of both convolutional neural network (CNN) based feature extractors
and transformer blocks to capture global cues. As the performance advances,
the models swell in size correspondingly, containing millions or even billions of
parameters [33]. This growth in model size presents challenges for deployment
on resource-constrained edge devices, hinders real-time inference tasks such as
autonomous driving, and incurs significant costs for training and inference on
cloud systems. Pruning [23,50,54], which involves removing redundant parame-
ters from the network, has emerged as an effective strategy to reduce the model
computation and size to reach real-time requirements without significantly com-
promising its accuracy. To keep pace with the ever-expanding model sizes, we
need very aggressive pruning techniques to significantly reduce latency for effi-
cient and real-time model deployment.
Channel pruning [31,38,39,50,52‚Äì54,59], in particular, has garnered signif-
icant attention as an effective pruning technique to reduce model computation,
usually 30% - 50%, practically without requiring changes in the hardware. Chan-
nel pruning involves removing redundant convolution filters identified by some
importance criterion [39,40,50], usually starting from a pre-trained model. De-
spite advancements, these methods have two critical limitations. First, channel
pruning methods are confined to pruning on the channel level, while we can
not avoid the structural removal of entire blocks or layers to achieve the larger
pruning ratios required (70%-90%). Only a few works [11, 19, 58, 61, 62, 64] ad-
dress layer or block pruning. These methods can provide greater acceleration
than channel pruning, but they are restricted to pruning at the layer or block
granularity and cannot simultaneously introduce channel sparsity, resulting in
suboptimal accuracy.
Second, current pruning approaches to directly reduce inference latency use
latency models that only account for variations in output channel count at each
layer, ignoring the simultaneous impact of pruning on input channels [10, 13,
31, 34, 49, 52, 54, 66]. This inaccurate latency estimation leads to sub-optimal
trade-offs between accuracy and latency, especially at the larger pruning ratios
required for inference on the edge. With large pruning ratios, guiding pruning
toward an optimal structure becomes more challenging while adhering closely to
the desired latency without precise modeling.


--- Page 3 ---
Multi-Dimensional Pruning
3
INPUT
CONV + RELU
POOLING
2x (CONV + RELU) 
Residual Block 1
Block
Decision
Variable
Layer
Channel 
Variable
1
ùíöùüè
1
ùëß" ‚àà{0,1}
2
ùíöùüê
3
ùíöùüë
2
ùëß% ‚àà{0,1}
4
ùíöùüí
5
ùíöùüì
6
ùíöùüî
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
0.04
0.07
0.12
0.17
0.08
0.11
0.18
0.2
0.11
0.15
0.21
0.23
0.16
0.19
0.24
0.28
0.18
0.24
0.29
0.32
latency cost matrix
MINLP 
Solver
layers
channels
Extract Pruned Subnetwork
3x (CONV + RELU) 
Residual Block 2
‚Ä¶
Pretrained Model
‚Äúcat‚Äù
Œò
ùëò
ùëò
ùíéùüë= ùüí
ùíéùüê= ùüì
Zoomed-in Layer 3
Œò! ‚ààùëÖ"√ó$√ó%√ó%
0.9
1.3
0.2
0.5
Channel
Importance
0.9
1.3
0.2
0.5
Sorting
1.3
2.2
2.7
2.9
Layer
Importance
Compute Layer Importance
Score every possible channel count
$ùêº!
Output Channel Count 
Input Channel Count 
ùê∂!
Compute Latency
Capture both in. and out. dims. changes
Block Grouping
ùë¶+
, = 1
ùë¶-- = 1
ùë¶., ùë¶/, ùë¶0: ùëõ/ùëé
ùë¶,+ = 1
argmax ‚Ä¶ + ùëß& ‚ãÖ(ùë¶!
' ‚ãÖ$ùêº!)+‚Ä¶
s.t. ‚Ä¶ + ùëß& ‚ãÖùë¶! ‚ãÖùë¶(
' ‚ãÖùê∂!
+ ‚ãØ‚â§Œ®
MINLP Formulation w/
Latency and Importance Expression of Layer 3
ùëß, = 1
ùëß+ = 0
blocks
Finetuning
TopK &
Sum
One-hot Vector
0
1
0
0
Eg.
ùíöùüë
ùüë= ùüè: Keep 3 Output Channels at Layer 3
Fig. 2: Paradigm of our proposed method MDP. We start by computing layer im-
portance and constructing latency cost matrices for each layer. We then group layers
within the same block and solve an MINLP to optimize pruning decisions at both
channel and block levels. Finally, we extract the pruned subnetwork and finetune it.
This paper presents a novel pruning framework that effectively overcomes the
limitations of existing methods. Specifically, we do not model channels or layers
separately. Instead, we first group channels and layers within the same block in
our formulation, allowing them to be handled jointly in the optimization process.
This unified approach seamlessly integrates channel, layer, and block pruning,
enabling us to identify the optimal pruned structure at all levels efficiently. Sec-
ond, for accurate modeling of latency in different configurations at each layer,
we propose the concept of bilayer configuration latency, which considers simul-
taneous variations in both input and output channel counts across all layers.
To incorporate these two strategies, we reformulate pruning as a Mixed-Integer
Nonlinear Program (MINLP) [6, 7, 37]. This allows us to directly solve for the
optimal pruned structure adhering to a specific latency budget with only a single
pass. As a result, our framework enhances pruning performance with significant
latency reductions. All together, we refer to our method as Multi-Dimensional
Pruning (MDP). Our code will be provided upon acceptance.
Our extensive experiments, with a glimpse showcased in Figure 1, validate
our method‚Äôs superior performance, particularly at high pruning ratios. In clas-
sification with an aggressive 85% pruning ratio, we significantly outperform the
previous work, HALP [54], with a speed increase of +1161 im/s and an accuracy
improvement of +1.4. For 3D object detection, we prune the StreamPETR [60]
model comprising of a CNN feature extractor and transformer based decoder.
We establish a new state-of-the-art at a 45% pruning ratio, achieving higher FPS
(37.3 vs. 31.7) and mAP (0.451 vs. 0.449) than the dense baseline. At a larger
pruning ratio of 70%, we significantly outpace HALP [54] in both FPS (43.3 vs.
42.5) and mAP (0.394 vs. 0.373). We summarize our contributions as follows:
‚Äì We introduce a block grouping strategy for simultaneous channel and block
pruning, allowing collective decision-making in optimization.
‚Äì We propose a method to accurately formulate latency for different layer
configurations, capturing variations in both input and output channels.


--- Page 4 ---
4
X.Sun et al.
‚Äì We organically combine the above strategies with a novel pruning framework
redefining pruning as a Mixed-Integer Nonlinear Program (MINLP) which
directly solves a globally optimal pruned structure within specific latency
constraints efficiently with a single-pass.
‚Äì We conduct extensive experiments and observe state-of-the-art accuracy-
latency trade-offs in a wide range of settings, covering (a) ImageNet [16] for
classification, Pascal VOC [20] for 2D detection, and Nuscenes [9] for 3D
detection (b) with three model architectures: ResNet-50 [25], SSD [43], and
StreamPETR [60] (c) across various latency reduction pruning ratios.
2
Related Works
Our work can be categorized as a pruning method in general. We will now provide
a brief overview of the field and highlight our differences from the previous
approaches. Pruning methods [1, 23, 24, 36, 39, 40, 48, 50, 54, 56] mostly design
importance criterion to rank parameters and remove the lowest-ranked ones,
followed by an additional finetuning for accuracy recovery.
Channel Pruning Some pruning methods [14, 26‚Äì28, 31, 39, 40, 50, 54, 57, 67]
operate under structural constraints, for example removing convolutional chan-
nels [39] from CNNs, thus enjoy immediate performance improvement without
specialized hardware or library support. Exemplary channel importance criterion
relied on metrics like weight norm [14,26,27,39,67], Taylor expansion [41,50,68],
geometric median [28], and feature maps rank [40]. Our method leverages the
Taylor [50] channel importance criterion but extend it to evaluate the configu-
rations of entire layers and blocks, going beyond just pruning channel but also
combining layer and block removals.
Layer and Block Pruning Channel pruning methods have been effective in
reducing performance loss while removing a moderate number of parameters.
However, their effectiveness is limited when it comes to more extensive pruning.
This is because they focus only on removing channels, but to achieve optimal
results with significant pruning, it becomes necessary to remove entire layers or
blocks. Only a limited number of works [11, 19, 58, 61, 62, 64] focus on pruning
layers and blocks. [12] and [19] employ intermediate features at each layer to
compute a layer ranking score with linear classifier probes [12] or imprinting [19]
and remove the lowest ranked layers. [61] introduces linear classifier probe after
each block to check performance and remove blocks with the smallest improve-
ments from the previous. Recent method [62] also studies individually removing
each block and calculating importance based on the performance drop.
Though shown to provide larger speedups than channel pruning, all of these
approaches [11, 19, 58, 61, 62, 64] only operate at the layer or block granularity
and fail if we want to introduce channel sparsity simultaneously. Additionally,
some add extra module or parameters like linear probes [12, 61] that require
additional training and complicate the process. Our method seamlessly unites
channel and block pruning, which allows us to efficiently determine an optimal
pruned structure at both the channel and block levels with just a single pass on
the pre-trained model, without any need for extra training or parameters.


--- Page 5 ---
Multi-Dimensional Pruning
5
Hardware-aware Pruning Since parameter compression ratio does not di-
rectly translate into computation reduction ratio, some works [38, 63, 67] focus
primarily on reducing model FLOPs. Latest methods go one step further and
perform hardware-aware pruning which aims to directly reduce the hardware
inference latency. The representative work HALP [54] first prepares a latency
lookup table for all configurations of prunable parameters measured on the tar-
get hardware then formulate pruning as a knapsack problem [55], maximizing
total parameter importance while constraining the total associated latency un-
der a given budget. [52] later demonstrates that HALP can be applied to au-
tonomous systems to achieve real-time detection performance. To enhance the
learning capacity of pruned models, SMCP [31] introduces soft masking within
the HALP framework, enabling the reconsideration of earlier pruned weights in
an iterative pruning setting.
Although these methods [31,52,54] have made notable progress in accuracy
and speed, their reliance on an inaccurate latency estimation leads to subopti-
mal accuracy-latency trade-offs. They account for changes in output channels
but overlook simultaneous variations in input channels caused by pruning the
preceding layer. This issue is more pronounced when aiming for large latency
reductions, as it becomes more challenging to guide pruning to meet the desired
latency budget without accurate latency modeling. In our work, we also focus
on hardware-aware pruning but introduce a more accurate latency modeling tech-
nique that accounts for simultaneous variations in both input and output channel
counts across all layers, allowing us to determine optimal configurations globally.
Mixed-Integer Nonlinear Program (MINLP) [6, 7, 37] As our strategies
of block pruning and accurate latency modeling are unified with a MINLP for-
mulation, we will briefly introduce the field. Formally defined in [37], MINLPs
are optimization problems with both integer and continuous variables, where the
objective and constraints are nonlinear functions. Effectively and efficiently solv-
ing MINLP [3‚Äì5,17,18,21,22] is a critical area in optimization research, with key
component often involving decomposing the problem into Mixed-Integer Linear
Programs (MILP) and Nonlinear Programs (NLP). Recently, conveniently mod-
eling and solving MINLPs in Python has been made possible with Pyomo [8]
and MindtPy [2]. In this paper, we use the Outer Approximation (OA) [17,21]
method with Pyomo [8] and MindtPy [2] to solve our pruning MINLP.
3
Methodology
We will now present our pruning framework. We begin by establishing prelimi-
naries, defining our goals and declaring relevant notations. We then describe our
proposed pruning formulation in detail.
Preliminaries For the neural network with L convolution layers in total, we rep-
resent the convolution parameters as Œò = SL
l=1 Œòl, s.t.Œòl ‚ààRml√óml‚àí1√óKl√óKl,
where ml, ml‚àí1, Kl denote the number of output channels, input channels, and
kernel size at layer l respectively. Following [25,47], a block is defined as the set
of layers(e.g. bottleneck [25]) skipped by a residual connection. Suppose there is


--- Page 6 ---
6
X.Sun et al.
a total of B blocks in the network Œò. Given a desired inference latency, Œ®, our
goal is to find the most performant subnetwork ÀÜŒò ‚äÜŒò through pruning, such
that the inference latency of ÀÜŒò is below the budget Œ® on the target hardware.
Additionally, we declare the following entities(all 1-indexed):
Name
Notation
Explanation
layer2block
Œ≤(l) ‚àà[1, B]
map layer l to ID of the block it belongs to
layer channel variable yl ‚àà{0, 1}ml, one-hot yi
l = 1 if layer l keeps i out of ml channels
block decision variable zb ‚àà{0, 1}, b ‚àà[1, B]
zb = 0 if the entire bth block is pruned
To elaborate, the layer channel variables yl is defined as one-hot vector,
where the index of the hot bit represents the total number of selected channels
in the pruned network ÀÜŒò, ranging from 1 to ml. If the bth block is pruned (i.e.
zb = 0), all layers in this block(Œ≤(l) = b) are removed, regardless of the value of
yl. In this case, the channel count becomes 0.
The layer channel variables y and block decision variables z describe the
pruning decisions and encode the pruned subnetwork ÀÜŒò, and they are our targets
to jointly optimize. In the following sections, we will describe how to solve them
based on information we collected from the pre-trained network Œò.
3.1
Multi-Dimensional Pruning (MDP)
To understand how performant a pruned subnetwork ÀÜŒò is, following previous
works [31, 39, 40, 50, 54], we leverage importance score as a proxy. The optimal
subnetwork ÀÜŒò is considered to be the one that maximizes the importance score
while closely adhering to the latency constraint Œ®. With large latency reduction
from the original model Œò, we need to consider potential removal of layers and
blocks while guiding the pruning decisions with accurate latency estimations.
To identify this optimal subnetwork encoded by layer channel variables (y) and
block decision variables (z), we begin by defining two key components, the im-
portance score for different values of y and z and accurate latency estimation
for these varying configurations, for each individual layer. The final objective is
the aggregation of these components across all layers of the model.
Next, to seamlessly combine complete layer and block removal with channel
sparsity, we perform a block grouping step which groups the latency and impor-
tance expression for all layers within the same block under a single block decision
variable. We then formulate the above as a Mixed-Integer Nonlinear Program-
ming (MINLP) to jointly determine y and z for an optimal pruned subnetwork
ÀÜŒò at both the channel and block levels. Finally, we extract the pruned subnet-
work structure ÀÜŒò from the solver‚Äôs output of y and z and carry out a finetuning
session on ÀÜŒò for E epochs to recover the accuracy. All together, we refer to our
method as Multi-Dimensional Pruning (MDP). A paradigm diagram of MDP
is demonstrated in Fig. 2. We are now going to describe the details of each step.
Layer Importance Computation As discussed in Sec.2, the quality of the
pruned subnetworks could be conveniently assessed with channel importance


--- Page 7 ---
Multi-Dimensional Pruning
7
scores. With Taylor importance score [50], the importance of the jth channel at
layer l can be computed as:
  
\ l abel
 {e
q n :im
p or
t ance} \mathcal {I}_l^j = |g_{\gamma _l^j}\gamma _l^j + g_{\beta _l^j}\beta _l^j|,
(1)
where Œ≥ and Œ≤ are BatchNorm layer‚Äôs weight and bias for the corresponding
channel, and gŒ≥ and gŒ≤ are the gradients.
As the number of channels kept in a layer l is directly encoded by the one-
hot variables yl, we associate an importance score for each possible configuration
yl could take, with the one-hot bit index ranging from 1 to ml. We leverage a
greedy approach to derive this. For example, if at layer l pruning keeps i channels
(i.e. yi
l = 1), we would like these i channels to be the top-i most important.
Therefore, we first rank individual channel importance scores Il at layer l in
ascending order. Then to compute layer importance score ÀÜIi
l corresponding to
yl with yi
l = 1, we aggregate the i highest channel importance scores.
Formally, this is expressed as:
  \
h a
t
 {\mathcal  { I }_l ^i} = \sum \text {Top-i}(\Vec {\mathcal {I}}_l), \forall i \in [1, m_l] \label {eqn:config_imp}
(2)
The vector ÀÜIl ‚ààRml fully describes the importance scores for all possible number
of channels layer l could take from 1 to ml. We thereby define the importance at
layer l for a specific configuration yl as a simple dot-product: yl‚ä§¬∑ ÀÜIl.
Latency Modeling In order to accurately guide our pruning, we fully describe
the latency variations with respect to both the number of its output and input
channels and construct a latency cost matrix Cl for each convolution layer l as
follows:
  \
l
abel
 {eqn :l
atv2}  {
 \ m
athbf  {C
}_l =  \
left [ 
{ \ b
egin {ar
ray
}{c
ccc
} T
_l(1,1) & T_l(1,2) & \ c d ots & T_ l(1
,
m_l)\\ T_l(2, 1) & T_l(2,2) & \cdots & T_l(2, m_l)\\ \vdots & \vdots & \ddots & \vdots \\ T_l(m_{l-1}, 1) & T_l(m_{l-1}, 2) & \cdots & T_l(m_{l-1}, m_l)\\ \end {array} } \right ] }
(3)
Here, Tl is a pre-built latency lookup table, which could be measured on the
target hardware beforehand as prior works [31, 52, 54], and Tl(i, j) returns the
latency of layer l with i input channels and j output channels, upper-bounded by
the total channel count ml‚àí1 and ml in Œò. Cl enumerates latency corresponding
to all possibilities layer l could be, varying the input and output channel numbers.
With these configurations encoded in the one-hot layer channel variables yl‚àí1
and yl, we define the bilayer configuration latency at layer l for specific yl‚àí1 and
yl simply as two dot-products: yl ¬∑ (yl‚àí1‚ä§¬∑ Cl).
We can observe that each yl appears twice in the expressions, once at com-
puting latency for layer l, and once at layer l+1. While this poses some challenges
in optimization, it manages to accurately capture the full overall latency land-
scape of network. This approach enables us to guide the pruning process with
more precise latency estimations, significantly improving the precision from pre-
vious methods [31, 52, 54] that did not consider the simultaneous contributions
from both output and input channel dimension.


--- Page 8 ---
8
X.Sun et al.
Block Grouping Notice that we define the layer channel variables yl to only
describe channel count from 1 to ml, excluding the case when pruning removes all
channels from layer l(i.e. channel count of 0). This means if we only use variables
y to represent the pruned model ÀÜŒò, we cannot represent completely removing
a layer from the network; at best, we can reduce it to just one channel, similar
to previous methods [31,52,54]. This is intentional because arbitrarily pruning
a single layer could easily lead to network disconnection, causing discontinuity
in the information flow of the network. However, residual blocks are inherently
resilient to removal of all their internal layers at once, as the skip connection
allows information to bypass the removed layers, preserving gradient flow.
To handle the removal of an entire residual block structure, we introduce
block grouping where layers are grouped into the block it belongs to. Specifically,
we parse the network architecture to obtain the layer2block mapping Œ≤(l) for
every layer l. Then we group all importance and latency expressions within the
same block under a single block decision variable. If pruning decides to remove
the bth block, the importance and latency contributions from all layers within
that block, where Œ≤(l) = b, should be simultaneously set to 0.
We model this group decision with the binary block decision variables zb.
Subsequently, for each layer l, we first determine whether its associated block
decision variable, denoted by zŒ≤(l), is active (zŒ≤(l) = 1). Only if it is active, we
evaluate the layer importance and latency expressions determined by y; other-
wise, they are simply zeroed. The importance for layer l is determined by both yl
and zŒ≤(l), and can be expressed as zŒ≤(l) ¬∑(yl‚ä§¬∑ ÀÜIl). Similarly, the bilayer configu-
ration latency at convolution layer l can be represented as zŒ≤(l)¬∑(yl¬∑(yl‚àí1‚ä§¬∑Cl)).
Block removal is properly handled by giving the block decision variables zs
higher ‚Äôpriority‚Äô than the layer channel variables ys. For example, deactivating
z1 results in the exclusion of all layers within the first block(where Œ≤(l) = 1) by
simultaneously setting their importance and latency expressions to 0, regardless
of the values taken by their yls. Also, notice that for the layers that do not belong
to any block structures, their corresponding zs are simply always 1.
Solve MINLP We aim to jointly determine the optimal layer channel and block
decisions(y and z) that maximize the summation of their importance scores while
ensuring the cumulative bilayer configuration latency remains below the budget
Œ®. Formally, this can be represented with the following Mixed-Integer Nonlinear
Programming (MINLP) formulation:
  \ lab
el 
{
e
qn:
progr a m} 
\ a rgma
x _
{\Ve
c
 
{y}
, \Vec {z } } & \
q u ad \ s um _{l=1}^L z_{\beta (l)} \cdot ( \Vec {y_l}^\top \cdot \Vec {\hat {\mathcal {I}_l}}) \\ \textsc {s.t.} \quad \sum _{l=1}^L z_{\beta (l)} \cdot & (\Vec {y_{l}} \cdot (\Vec {y_{l-1}}^\top \cdot \mathbf {C}_l)) \leq \Psi \nonumber
We restrict all decision variables y and z to binary values, while the layer
importances ÀÜIl and latency cost matrices Cl contain floating-point numbers,
hence making the program mixed-integer in nature. Recall each layer channel


--- Page 9 ---
Multi-Dimensional Pruning
9
Algorithm 1 MDP Framework
Input: Pretrained weights Œò, latency lookup table T, total finetuning epochs E, train-
ing dataset D, latency budget Œ®
1: Declare layer channel variables y and block decision variables z
2: //Layer Importance Computation
3: for sample (x, y) in D do
4:
Perform forward pass and backward pass with Œò
5:
Calculate Taylor channel importance score Il (Eqn. 1)
6:
Calculate and accumulate layer importance score ÀÜIl (Eqn. 2)
7: end for
8: Construct importance expression: y‚ä§
l ¬∑ ÀÜIl
9: //Latency Modeling
10: Construct latency matrices Cl (Eqn.3)
11: Construct bilayer configuration latency expression: yl ¬∑ (yl‚àí1
‚ä§¬∑ Cl)
12: //Block Grouping
13: Obtain the layer2block mapping Œ≤(l) for each layer l
14: Group importance and latency expressions under z
15: //Solve MINLP
16: Set up the MINLP (Eqn. 4) and solve it with Pyomo and MindtPy
17: //Extract Pruned Structure
18: Extract pruned subnetwork ÀÜŒò from solver output y and z
19: Finetune the pruned model ÀÜŒò as usual for E epochs
variable yl is one-hot, which can be formally formulated as follows as an addi-
tional constrain to Eqn. 4:
  
\ V e c  { y_ l }^\ top &\cdot \mathbf {1} = 1, \Hquad \forall l\in [1, L]
(5)
To solve this MINLP 4, we leverage the Python numerical decomposition
framework Pyomo [8] and MindtPy [2], and employ the Feasibility Pump (FP)
method [5] to enhance efficiency. Since we jointly optimize all variables, we can
directly determine a globally optimal set of y and z with only a single pass.
Extract Pruned Structure Once we solved the MINLP program 4, we proceed
to extract the pruned subnetwork ÀÜŒò based on the variables y and z determined
by the solver. If block decision variable zb is set to 0 for a particular block b,
we completely remove that block in ÀÜŒò and disregard the solver‚Äôs output for the
layer channel variables(yl) of the layers within that block (where Œ≤(l) = b). On
the other hand, if the block is active with zb = 1 and the solver returns the value
of y with yi
l = 1, we keep i channels in ÀÜŒò at layer l according to ArgTopK(Il, i),
mapping layer importance ÀÜIl in Eqn.2 back to the i top-performing channels.
In practice, since the layer importances ÀÜI are built from Taylor score [50] mea-
sured using gradient information from data batches, we perform pruning after
one epoch when the model has seen all of the samples in the dataset and accumu-
late their importance estimation. After pruning, we finetune the pruned model
ÀÜŒò for a duration of E epochs to recover accuracy. An algorithmic description of
the above process is provided in Algorithm 1.


--- Page 10 ---
10
X.Sun et al.
4
Experiments
To validate the proposed method, we perform extensive experiments across a
comprehensive set of scenarios. We demonstrate our pruning results on 3 tasks:
image classification with ImageNet [16] and ResNet50 [25], 2D object detection
with Pascal VOC [20] and SSD [43], and 3D object detection with Nuscenes [9]
and StreamPETR [60].
Our method improves upon previous approaches by providing (a) more accu-
rate latency estimation and (b) the ability to handle the removal of entire layer
and block structures. These improvements are reflected in our superior result,
as we achieve a new state-of-the-art with a significantly better accuracy-latency
trade-off compared to prior arts [31,54] and other competitive baselines [38,59],
especially at higher pruning ratios. To provide a comprehensive understanding of
our proposed pruning framework, we also conducted an in-depth ablation study,
highlighting the individual contribution of our improvement from (a) and (b).
Settings For ResNet50 and SSD, we aimed to optimize their inference latency
on the Nvidia TITAN V GPU with batch size of 256. When pruning Stream-
PETR [60], we adapted our approach to target the Nvidia GeForce RTX 3090
GPU batch size of 1, aligning with the focus of StreamPETR‚Äôs original paper.
This allowed us to fairly evaluate the speedup benefits and also demonstrate the
generalization of our method targeting different hardware platforms.
All of our trainings use 8 Nvidia Tesla V100 GPUs and conducted with
PyTorch [51] V1.4.0. Our CPU is Intel Xeon E5-2698 and is used to solve the
MINLP optimization(Eqn. 4).
4.1
Classification Results on ImageNet
In Table 1, We report our pruning results and comparison with the baseline
methods on ResNet50 [25] and ImageNet [16]. We evaluate these results using
Top-1 Accuracy and Top-5 Accuracy to gauge the recovered accuracy after fine-
tuning. In addition, we include inference FPS (im/s, i.e. images per second) to
directly showcase the speedups on the target hardware. We also present FLOPs
for completeness.
Compared with previous methods like HALP [54] and SMCP [31], we achieve
a significantly improved accuracy-latency trade-off. For instance, SMCP reaches
a Top-1 accuracy of 72.7 with an inference speed of 3784 im/s; our method
slightly surpasses its Top-1 with an accuracy of 72.8 but with a considerably
faster inference speed of 4210 im/s. With larger pruning, HALP achieves a
Top-1 accuracy of 68.6 with an inference speed of 4101 im/s, our method signif-
icantly outperforms it with a Top-1 accuracy of 70.0 and an impressive FPS of
5262 im/s. Notably, we can observe from Table 1 that our method particularly
excels when targeting high FPS with substantial pruning from pre-trained
models, corroborating the effectiveness of improvements from our method. Our
improvements could be observed more clearly in the FPS v.s. Top-1 Pareto curve
displayed in Figure 1.


--- Page 11 ---
Multi-Dimensional Pruning
11
Method
Top-1 Acc(%)‚ÜëTop-5 Acc(%)‚ÜëFLOPs(√óe9)‚ÜìFPS(im/s)‚Üë
ResNet50 [25]
Dense
76.2
92.9
4.1
1019
ResConv-Prune [64]
70.0
90.0
1.6
‚àí‚àí
DBP-0.5 [61]
72.4
‚àí‚àí
‚àí‚àí
1630‚àó
LayerPrune7-Imprint [19]
74.3
‚àí‚àí
‚àí‚àí
1828‚àó
MetaPrune [45]
73.4
‚Äì
1.0
2381
AutoSlim [69]
74.0
‚Äì
1.0
2390
GReg-2 [59]
73.9
‚Äì
1.3
1514
HALP-70% [54]
74.5
91.8
1.2
2597
SMCP-70% [31]
74.6
92.0
1.0
2947
Ours-70%
74.6
92.2
1.1
3092
HALP-85% [54]
68.1
88.4
0.6
3971
Ours-85%
70.0
89.3
0.5
5306
ResNet50 - EagleEye [38]
Dense [38]
77.2
93.7
4.1
1019
EagleEye-1G [38]
74.2
91.8
1.0
2429
HALP-70% [54]
74.5
91.9
1.2
2597
SMCP-70% [31]
75.1
92.3
1.1
2589
Ours-65%
75.2
92.5
1.3
2774
Ours-70%
75.0
92.2
1.2
3052
HALP-80% [54]
71.2
90.1
0.7
3691
SMCP-80% [31]
72.7
‚Äì
‚Äì
3784
Ours-80%
72.8
90.9
0.7
4210
HALP-85% [54]
68.6
88.5
0.6
4101
Ours-85%
70.0
89.2
0.5
5262
Table 1: ImageNet results with ResNet-50. FPS is measured on NVIDIA TITAN
V with batch size of 256. Results with similar FPS are grouped. ‚àíX% denote the
pruning ratio. ‚àódenotes latency estimated from the reported ratio. Ours achieve much
better accuracy-FPS tradeoffs than the baselines, specifically when pruning ratio is
large. Averaged results over two runs.
We also include direct comparison with methods [19,61,64] which also specif-
ically discuss layer and block removal. As shown in Table 1, our results are sig-
nificantly better. For instance, compared to LayerPrune [19], we achieve a higher
Top-1 accuracy (74.6 vs. 74.3) and a substantially greater FPS (3092 vs. 1828).
4.2
2D Object Detection Results on PascalVOC
To illustrate the broad applicability of our approach, we also conducted exper-
iments in the realm of 2D object detection using the widely recognized Pascal
VOC dataset [20]. In Figure 3b, we present the outcomes of our pruning method-
ology applied to an SSD512 [43] model with a ResNet50 backbone. Our perfor-
mance is assessed against various competitive baselines, including HALP [54]
and SMCP [31]. We depict the Pareto frontier, showcasing the trade-off between
FPS and mean Average Precision (mAP).
Our results distinctly outshine existing methods in the field, marking a sub-
stantial advancement. In direct comparison to SMCP, our approach consistently


--- Page 12 ---
12
X.Sun et al.
40
60
80
100
120
140
FPS
76
77
78
79
80
mAP(%)
HALP
SSD512-RN50
SSD512-RN50-Slim
SSD300-RN50
RetinaNet-RN50
SSD300-VGG16
SMCP
Ours
(a) FPS versus mAP are plotted(top-right is
better). FPS measured on NVIDIA TITANV.
20
40
60
80
100
FLOPs
76
77
78
79
80
mAP(%)
HALP
SSD512-RN50
SSD512-RN50-Slim
SSD300-RN50
RetinaNet-RN50
SSD300-VGG16
SMCP
Ours
(b) FLOPs versus mAP are plotted(top-left is
better).
Fig. 3: PascalVOC results with SSD512. FPS is measured on NVIDIA TITAN V
with batch size of 256. Ours achieve much better mAP-FPS and mAP-FLOPs tradeoffs
than the baselines.
Method
mAP‚ÜëNDS‚ÜëmATE‚ÜìmASE‚ÜìmAOE‚ÜìmAVE‚ÜìmAAE‚ÜìFPS‚Üë
BEVPoolv2 [30]
0.406
0.526
0.572
0.275
0.463
0.275
0.188
16.6
BEVDet4D [29]
0.322
0.457
0.703
0.278
0.495
0.354
0.206
16.7
PETRv2 [44]
0.349
0.456
0.700
0.275
0.580
0.437
0.187
18.9
Sparse4Dv2 [42]
0.439
0.539
0.598
0.270
0.475
0.282
0.179
20.3
StreamPETR [60] 0.449
0.546
0.613
0.267
0.413
0.265
0.196
31.7
HALP-45% [54]
0.446
0.547
0.605
0.271
0.409
0.269
0.211
36.8
Ours-45%
0.451 0.551
0.596
0.272
0.413
0.259
0.207
37.3
HALP-50% [54]
0.439
0.543
0.605
0.270
0.419
0.265
0.211
38.6
Ours-50%
0.441 0.544
0.606
0.269
0.421
0.268
0.205
39.0
HALP-60% [54]
0.427 0.533
0.620
0.269
0.438
0.271
0.209
39.5
Ours-60%
0.427 0.532
0.608
0.272
0.457
0.269
0.207
40.7
HALP-70% [54]
0.373
0.489
0.674
0.277
0.534
0.293
0.197
42.5
Ours-70%
0.394 0.512
0.642
0.275
0.449
0.278
0.204
43.3
Table 2: Nuscenes results with StreamPETR. FPS is measured on NVIDIA
GeForce RTX 3090 with batch size of 1. Results with similar FPS are grouped. ‚àíX%
denote the pruning ratio. Ours achieve much better accuracy-FPS tradeoffs than HALP
and even surpass performance of dense StreamPETR with much higher FPS.
achieves significantly higher mAP scores across various inference FPS levels.
For instance, we outperform SMCP with an mAP of 79.2 (compared to 78.3)
while also slightly increasing the FPS to 146.4 (compared to 144.4).Notably, our
pruned model even surpasses the mAP of the pre-trained dense SSD512-RN50 by
a margin(80.0 v.s. 78.0)while substantially enhancing its FPS(125.4 v.s. 68.2).
4.3
3D Object Detection Results on Nuscenes
So far, we have shown that our pruning method is effective for models composed
entirely of convolutional layers, such as ResNet50 and SSD. Modern systems


--- Page 13 ---
Multi-Dimensional Pruning
13
deploy convolutional layers for features extraction and transformer layers for
capturing global cues [15,65]. In this section, we explore our pruning effectiveness
for these hybrid models. We focus on the challenging task of 3D object detection,
using the widely recognized Nuscenes [9] dataset and the state-of-the-art model
StreamPETR [60], composed of a heavy CNN-based encoder and a transformer-
based decoder. Our analysis of the system‚Äôs latency revealed that the CNN-based
encoder has a higher latency (16.7ms) than the transformer decoder (14ms).
This indicates that applying our method to the convolutional layers can still
effectively accelerate the entire network.
Detailed results and comparisons with several competitive baselines are pre-
sented in Table 2. Our evaluation incorporated a diverse set of metrics commonly
adopted for 3D object detection tasks [9,42,60], including the mean Average Pre-
cision (mAP) and Normalized Detection Score (NDS). Additionally, we report
the FPS to highlight the improvements in speed.
Significantly, when compared to the dense pre-trained StreamPETR model,
our technique achieved a substantial acceleration of approximately 18%, result-
ing in an impressive 37.3 FPS as opposed to the baseline‚Äôs 31.7 FPS. Impor-
tantly, this speed boost was achieved without sacrificing performance: our pruned
model attained superior mAP (0.451 vs. 0.449) and NDS (0.551 vs. 0.546). In
comparison to the previous pruning method HALP [54], our approach exhibited
remarkable improvements in accuracy-latency trade-offs across various pruning
ratios. For instance, HALP managed to produce a pruned StreamPETR model
with an mAP of 0.373, an NDS of 0.489, and an inference FPS of 42.5. In con-
trast, our approach surpassed these results, achieving an mAP of 0.394, an NDS
of 0.512, and an inference FPS of 43.3.
4.4
Ablation Study
4.4.1
As discussed in detail in Sec. 3.1, our pruning method introduces two
key improvements from prior methods: (a)‚Äúbilayer configuration latency" for
accurate latency modeling; (b) "block grouping" for integration of block removal
with channel sparsity.
We‚Äôll now explore the individual impacts of (a) and (b) on pruning perfor-
mance. The baseline here is thus a bare latency pruning algorithm without both
(a) and (b). We then ablate each component by separately adding them on top
of the baseline to check inidividual improvement. The baseline performance is
depicted in Fig. 4 with label ‚ÄúBaseline".
Bilayer Configuration Latency In this setting, we add our ‚Äúbilayer configura-
tion latency" on top of the baseline but drop the block grouping step to exclude
the block decision variables from the MINLP program 4. This variant accurately
estimates the latency impacts of pruning by considering variations in both input
and output channel counts, but it cannot handle removal of entire blocks. The
result, labeled ‚ÄúOurs(Only Bilayer Latency)" in Fig. 4, show a markedly better
accuracy-latency tradeoff than the baseline, demonstrating its effectiveness even
when used alone.


--- Page 14 ---
14
X.Sun et al.
2500 3000 3500 4000 4500 5000
FPS
69
70
71
72
73
74
75
Top1
Baseline
Ours(Only Bilayer Latency)
Ours(Only Block Grouping)
Ours
Fig. 4: Ablation study results on Ima-
geNet with ResNet50. We show results
of each improvement acting individually.
Top-right is better.
Method
Steps Top-1‚ÜëFPS‚Üë
HALP-70%
30
74.3
2505
HALP-70%
10
73.9
2622
HALP-70%
1
65.1
4444
Ours30-70%
30
74.5
2660
Ours10-70%
10
74.8
2860
Ours-70%
1
75.2
2774
Table 3: Ablation study results on
ImageNet with ResNet50. We show re-
sults of ours and HALP [54] with different
pruning steps.
Block Grouping In this setting, we add our ‚Äúblock grouping" step to the base-
line but do not use our ‚Äúbilayer configuration latency" to model latency impacts
from pruning. Instead, we use previous methods‚Äô latency modeling [31, 52, 54],
which only account for variations in output channel counts. This variant can
effectively handle the removal of block strucutres to accommodate high pruning
ratios but but cannot accurately estimate latency impacts by considering changes
in both input and output channel counts. The results, labeled ‚ÄúOurs(Only Block
Grouping)" in Fig. 4, show an evidently improved accuracy-latency tradeoff com-
pared to the baseline, particularly at large pruning ratios and latency reduc-
tion(rightmost points in the curves). This indicates the effectiveness of "block
grouping" even when used independently.
4.4.2
By integrating our above two strategies into a unified MINLP frame-
work, we enable efficient and single-pass pruning.
Single-pass v.s. Iterative Pruning Our single-pass pruning approach achieves
the target latency in just one step, while iterative methods like HALP [54] re-
quire up to 30 steps. Performance comparisons between our method and HALP
across different pruning steps are shown in Table 3.
As observed, our approach performs consistently well regardless of the prun-
ing steps. Our single-pass performance is even better than our 30-steps iterative
pruning. We believe this is likely due to the benefit of using importance scores
from all samples in the dataset at once.
In contrast, HALP‚Äôs performance worsens with fewer pruning steps, espe-
cially in single-pass pruning where it defies the latency budget and over-prunes,
leading to a Top-1 of 65.1 and FPS of 4444. This behavior is because the over-
sights of (a) and (b) can be somewhat mitigated with multiple pruning steps,
but become more pronounced with just one or fewer steps.
5
Conclusion
In this paper, we introduce a novel pruning framework called MDP that inte-
grates channel, layer, and block pruning within a unified optimization process
and develop an accurate latency modeling technique that captures simultane-
ous input and output channel variations. To incorporate these strategies, we


--- Page 15 ---
Multi-Dimensional Pruning
15
reformulate pruning as a Mixed-Integer Nonlinear Program (MINLP) to effi-
ciently identify the optimal pruned structure within a specific latency budget
in a single pass. Our results demonstrate substantial improvements over pre-
vious methods, especially in scenarios requiring large pruning. We also pro-
vide an in-depth ablation study to investigate each contribution individually.
References
1. Alvarez, J.M., Salzmann, M.: Learning the number of neurons in deep networks.
In: Advances in Neural Information Processing Systems. pp. 2270‚Äì2278 (2016) 4
2. Bernal, D.E., Chen, Q., Gong, F., Grossmann, I.E.: Mixed-integer nonlinear decom-
position toolbox for pyomo (mindtpy). In: Computer Aided Chemical Engineering,
vol. 44, pp. 895‚Äì900. Elsevier (2018) 5, 9
3. Bernal, D.E., Vigerske, S., Trespalacios, F., Grossmann, I.E.: Improving the perfor-
mance of dicopt in convex minlp problems using a feasibility pump. Optimization
Methods and Software 35(1), 171‚Äì190 (2020) 5
4. Bonami, P., Biegler, L.T., Conn, A.R., Cornu√©jols, G., Grossmann, I.E., Laird,
C.D., Lee, J., Lodi, A., Margot, F., Sawaya, N., et al.: An algorithmic framework
for convex mixed integer nonlinear programs. Discrete optimization 5(2), 186‚Äì204
(2008) 5
5. Bonami, P., Cornu√©jols, G., Lodi, A., Margot, F.: A feasibility pump for mixed
integer nonlinear programs. Mathematical Programming 119(2), 331‚Äì352 (2009)
5, 9, 20
6. Burer, S., Letchford, A.N.: Non-convex mixed-integer nonlinear programming: A
survey. Surveys in Operations Research and Management Science 17(2), 97‚Äì106
(2012) 3, 5
7. Bussieck, M.R., Pruessner, A., et al.: Mixed-integer nonlinear programming.
SIAG/OPT Newsletter: Views & News 14(1), 19‚Äì22 (2003) 3, 5
8. Bynum, M.L., Hackebeil, G.A., Hart, W.E., Laird, C.D., Nicholson, B.L., Siirola,
J.D., Watson, J.P., Woodruff, D.L., et al.: Pyomo-optimization modeling in python,
vol. 67. Springer (2021) 5, 9
9. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,
Pan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous
driving. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 11621‚Äì11631 (2020) 4, 10, 13
10. Castro, R.L., Andrade, D., Fraguela, B.B.: Probing the efficacy of hardware-aware
weight pruning to optimize the spmm routine on ampere gpus. In: Proceedings
of the International Conference on Parallel Architectures and Compilation Tech-
niques. pp. 135‚Äì147 (2022) 2
11. Chen, S., Zhao, Q.: Shallowing deep networks: Layer-wise pruning based on feature
representations. IEEE transactions on pattern analysis and machine intelligence
41(12), 3048‚Äì3056 (2018) 2, 4
12. Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y.: Compressing neural
networks with the hashing trick. In: ICML. pp. 2285‚Äì2294. PMLR (2015) 4
13. Chen, Z., Liu, C., Yang, W., Li, K., Li, K.: Lap: Latency-aware automated pruning
with dynamic-based filter selection. Neural Networks 152, 407‚Äì418 (2022) 2
14. Chin, T.W., Ding, R., Zhang, C., Marculescu, D.: Towards efficient model com-
pression via learned global ranking. In: CVPR. pp. 1518‚Äì1528 (2020) 4


--- Page 16 ---
16
X.Sun et al.
15. Dai, Z., Liu, H., Le, Q.V., Tan, M.: Coatnet: Marrying convolution and attention
for all data sizes. Advances in neural information processing systems 34, 3965‚Äì3977
(2021) 2, 13
16. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: CVPR. pp. 248‚Äì255. Ieee (2009) 4, 10
17. Duran, M.A., Grossmann, I.E.: An outer-approximation algorithm for a class of
mixed-integer nonlinear programs. Mathematical programming 36, 307‚Äì339 (1986)
5, 20
18. D‚ÄôAmbrosio, C., Lodi, A.: Mixed integer nonlinear programming tools: an updated
practical overview. Annals of Operations Research 204, 301‚Äì320 (2013) 5
19. Elkerdawy, S., Elhoushi, M., Singh, A., Zhang, H., Ray, N.: To filter prune, or
to layer prune, that is the question. In: Proceedings of the Asian Conference on
Computer Vision (2020) 2, 4, 11
20. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal
visual object classes (voc) challenge. International journal of computer vision 88(2),
303‚Äì338 (2010) 4, 10, 11
21. Fletcher, R., Leyffer, S.: Solving mixed integer nonlinear programs by outer ap-
proximation. Mathematical programming 66, 327‚Äì349 (1994) 5, 20
22. G√ºnl√ºk, O., Linderoth, J.: Perspective reformulations of mixed integer nonlin-
ear programs with indicator variables. Mathematical programming 124, 183‚Äì205
(2010) 5
23. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding (2015) 2, 4
24. Hassibi, B., Stork, D.: Second order derivatives for network pruning: Optimal brain
surgeon. NeurIPS 5 (1992) 4
25. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR. pp. 770‚Äì778 (2016) 2, 4, 5, 10, 11, 20
26. He, Y., Ding, Y., Liu, P., Zhu, L., Zhang, H., Yang, Y.: Learning filter pruning
criteria for deep convolutional neural networks acceleration. In: CVPR. pp. 2009‚Äì
2018 (2020) 4
27. He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft filter pruning for accelerating
deep convolutional neural networks. In: IJCAI (2018) 4, 21
28. He, Y., Liu, P., Wang, Z., Hu, Z., Yang, Y.: Filter pruning via geometric median for
deep convolutional neural networks acceleration. In: CVPR. pp. 4340‚Äì4349 (2019)
4
29. Huang, J., Huang, G.: Bevdet4d: Exploit temporal cues in multi-camera 3d object
detection. arXiv preprint arXiv:2203.17054 (2022) 12
30. Huang, J., Huang, G.: Bevpoolv2: A cutting-edge implementation of bevdet toward
deployment. arXiv preprint arXiv:2211.17111 (2022) 12
31. Humble, R., Shen, M., Latorre, J.A., Darve, E., Alvarez, J.: Soft masking for cost-
constrained channel pruning. In: European Conference on Computer Vision. pp.
641‚Äì657. Springer (2022) 2, 4, 5, 6, 7, 8, 10, 11, 14, 19, 21
32. Kim, J., Yoo, J., Song, Y., Yoo, K., Kwak, N.: Dynamic collective intelligence
learning: Finding efficient sparse model via refined gradients for pruned weights.
arXiv preprint arXiv:2109.04660 (2021) 21
33. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023) 2
34. Kong, Z., Dong, P., Ma, X., Meng, X., Niu, W., Sun, M., Shen, X., Yuan, G., Ren,
B., Tang, H., et al.: Spvit: Enabling faster vision transformers via latency-aware


--- Page 17 ---
Multi-Dimensional Pruning
17
soft token pruning. In: European Conference on Computer Vision. pp. 620‚Äì640.
Springer (2022) 2
35. Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S.,
Farhadi, A.: Soft threshold weight reparameterization for learnable sparsity. In:
ICML. pp. 5544‚Äì5555. PMLR (2020) 21
36. LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: NeurIPS. pp.
598‚Äì605 (1990) 4
37. Lee, J., Leyffer, S.: Mixed integer nonlinear programming, vol. 154. Springer Sci-
ence & Business Media (2011) 3, 5
38. Li, B., Wu, B., Su, J., Wang, G.: Eagleeye: Fast sub-net evaluation for efficient
neural network pruning. In: ECCV. pp. 639‚Äì654 (2020) 2, 5, 10, 11
39. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient
convnets. In: ICLR (2017) 2, 4, 6
40. Lin, M., Ji, R., Wang, Y., Zhang, Y., Zhang, B., Tian, Y., Shao, L.: Hrank: Filter
pruning using high-rank feature map. In: CVPR. pp. 1529‚Äì1538 (2020) 2, 4, 6
41. Lin, S., Ji, R., Li, Y., Wu, Y., Huang, F., Zhang, B.: Accelerating convolutional
networks via global & dynamic filter pruning. In: IJCAI. vol. 2, p. 8. Stockholm
(2018) 4
42. Lin, X., Lin, T., Pei, Z., Huang, L., Su, Z.: Sparse4d v2: Recurrent temporal fusion
with sparse model. arXiv preprint arXiv:2305.14018 (2023) 12, 13
43. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd:
Single shot multibox detector. In: ECCV. pp. 21‚Äì37. Springer (2016) 2, 4, 10, 11
44. Liu, Y., Yan, J., Jia, F., Li, S., Gao, A., Wang, T., Zhang, X.: Petrv2: A unified
framework for 3d perception from multi-camera images. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 3262‚Äì3272 (2023)
12
45. Liu, Z., Mu, H., Zhang, X., Guo, Z., Yang, X., Cheng, K.T., Sun, J.: Metapruning:
Meta learning for automatic neural network channel pruning. In: ICCV. pp. 3296‚Äì
3305 (2019) 11
46. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: CVPR. pp. 3431‚Äì3440 (2015) 2
47. Luo, J.H., Wu, J.: Neural network pruning with residual-connections and limited-
data. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition. pp. 1458‚Äì1467 (2020) 5
48. Molchanov, D., Ashukha, A., Vetrov, D.: Variational dropout sparsifies deep neural
networks. In: ICML. pp. 2498‚Äì2507. PMLR (2017) 4
49. Molchanov, P., Hall, J., Yin, H., Kautz, J., Fusi, N., Vahdat, A.: Lana: latency
aware network acceleration. In: European Conference on Computer Vision. pp.
137‚Äì156. Springer (2022) 2
50. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J.: Importance estimation
for neural network pruning. In: CVPR. pp. 11264‚Äì11272 (2019) 2, 4, 6, 7, 9, 21
51. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch. NeurIPS
Workshop (2017) 10
52. Shen, M., Mao, L., Chen, J., Hsu, J., Sun, X., Knieps, O., Maxim, C., Alvarez,
J.M.: Hardware-aware latency pruning for real-time 3d object detection. In: 2023
IEEE Intelligent Vehicles Symposium (IV). pp. 1‚Äì6. IEEE (2023) 2, 5, 7, 8, 14, 19
53. Shen, M., Yin, H., Molchanov, P., Alvarez, J.M.: When to prune? a policy towards
early structural pruning. CVPR (2022) 2


--- Page 18 ---
18
X.Sun et al.
54. Shen, M., Yin, H., Molchanov, P., Mao, L., Liu, J., Alvarez, J.: Structural prun-
ing via latency-saliency knapsack. In: Advances in Neural Information Processing
Systems (2022) 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 19, 21
55. Sinha, P., Zoltners, A.A.: The multiple-choice knapsack problem. Operations Re-
search 27(3), 503‚Äì515 (1979) 5
56. Sun, X., Hassani, A., Wang, Z., Huang, G., Shi, H.: Disparse: Disentangled spar-
sification for multitask model compression. In: CVPR. pp. 12382‚Äì12392 (2022) 4
57. Sun, X., Shi, H.: Towards better structured pruning saliency by reorganizing con-
volution. In: Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision. pp. 2204‚Äì2214 (2024) 4
58. Tang, H., Lu, Y., Xuan, Q.: Sr-init: An interpretable layer pruning method. In:
ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). pp. 1‚Äì5. IEEE (2023) 2, 4
59. Wang, H., Qin, C., Zhang, Y., Fu, Y.: Neural pruning via growing regularization.
In: ICLR (2021) 2, 10, 11
60. Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.: Exploring object-centric tem-
poral modeling for efficient multi-view 3d object detection. arXiv preprint
arXiv:2303.11926 (2023) 2, 3, 4, 10, 12, 13
61. Wang, W., Zhao, S., Chen, M., Hu, J., Cai, D., Liu, H.: Dbp: Discrimination based
block-level pruning for deep model acceleration. arXiv preprint arXiv:1912.10178
(2019) 2, 4, 11
62. Wu, C.E., Davoodi, A., Hu, Y.H.: Block pruning for enhanced efficiency in convo-
lutional neural networks. arXiv preprint arXiv:2312.16904 (2023) 2, 4
63. Wu, Y.C., Liu, C.T., Chen, B.Y., Chien, S.Y.: Constraint-aware importance es-
timation for global filter pruning under multiple resource constraints. In: CVPR
Workshops. pp. 686‚Äì687 (2020) 5
64. Xu, P., Cao, J., Shang, F., Sun, W., Li, P.: Layer pruning via fusible residual con-
volutional block for deep neural networks. arXiv preprint arXiv:2011.14356 (2020)
2, 4, 11
65. Yang, C., Qiao, S., Yu, Q., Yuan, X., Zhu, Y., Yuille, A., Adam, H., Chen, L.C.:
Moat: Alternating mobile convolution and attention brings strong vision models.
In: The Eleventh International Conference on Learning Representations (2022) 2,
13
66. Yang, H., Yin, H., Shen, M., Molchanov, P., Li, H., Kautz, J.: Global vision trans-
former pruning with hessian-aware saliency. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 18547‚Äì18557 (2023)
2
67. Yang, T.J., Howard, A., Chen, B., Zhang, X., Go, A., Sandler, M., Sze, V., Adam,
H.: Netadapt: Platform-aware neural network adaptation for mobile applications.
In: ECCV. pp. 285‚Äì300 (2018) 4, 5
68. You, Z., Yan, K., Ye, J., Ma, M., Wang, P.: Gate decorator: Global filter pruning
method for accelerating deep convolutional neural networks. NeurIPS 32 (2019) 4
69. Yu, J., Huang, T.: Autoslim: Towards one-shot architecture search for channel
numbers. NeurIPS Workshop (2019) 11
70. Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., Li, H.: Learning
n: m fine-grained structured sparse neural networks from scratch. ICLR (2021) 21


--- Page 19 ---
Multi-Dimensional Pruning
19
Appendix
A
Detailed Analysis on Prior Latency Estimation
In the main paper, we note that previous approaches [31,52,54] use an imprecise
estimation of latency, resulting in suboptimal trade-offs between accuracy and
latency. These methods consider changes in output channels but ignore concur-
rent variations in input channels that occur due to pruning the previous layer.
This problem becomes more significant when targeting substantial reductions
in latency. In this section, we will examine their limitations through detailed
formulations.
To model the latency variations, previous methods compute a latency cost
for the j-th channel at the l-th layer Œòj
l as:
  
\ l abel {eq n: latency} R_l^j = T_l(p_{l-1}, j) - T_l(p_{l-1}, j-1)
(6)
Here, T(.) is the pre-built lookup table, and Tl(i, j) returns the latency of layer l
with i input channels and j output channels. Since previous methods leverage an
iterative pruning technique to gradually reach the desired latency target, pl‚àí1 is
the total number of input channels kept from the last pruning step. It is used as
a constant input channel count in the current pruning step to query the latency
from the lookup table Tl. However, we can not ascertain that the input channel
count will remain the same throughout the current pruning step, which can only
happen if previous layer is not pruned at all. The actual value of input channel
count ÀÜpl‚àí1 will be smaller than this constant estimate pl‚àí1 if pruning takes place
in the previous layer. Therefore, we have the following inequalities:
 \ha t  {p}
_{l-1} &\leq p_{l-1}\\ T_l(\hat {p}_{l-1}, j) &\leq T_l(p_{l-1}, j)\\ T_l(\hat {p}_{l-1}, j-1) &\leq T_l(p_{l-1}, j-1)
(9)
We can write the error of the latency estimation for each channel from the
true value as:
  
\ epsilon _l^j &= |\hat {R}_l^j - R_l^j| \\ &= |T_l(\hat {p}_{l-1}, j) - T_l(\hat {p}_{l-1}, j-1) - T_l(p_{l-1}, j) + T_l(p_{l-1}, j-1)|\\ &= |(T_l(p_{l-1}, j-1) - T_l(\hat {p}_{l-1}, j-1)) + (T_l(\hat {p}_{l-1}, j) - T_l(p_{l-1}, j))| \\ &\leq |T_l(p_{l-1}, j-1) - T_l(\hat {p}_{l-1}, j-1)| + |T_l(p_{l-1}, j) + T_l(\hat {p}_{l-1}, j) |
(13)
When targeting large pruning ratio, the difference between pl‚àí1 and ÀÜpl‚àí1 will be
enlarged since we expect to aggressively prune each layer and the input channel
count will change substantially overtime. We could see from the formula that
this latency modeling will be much more prone to make mistakes in measuring
the channel latency cost under such scenarios.
In terms of our proposed latency estimation technique, instead of capturing
latency cost for each channel, we directly model the latency for all configurations


--- Page 20 ---
20
X.Sun et al.
Fig. 5: Pruned architecture.
Method
Top-1 FPS
Autoslim[67]
74.0
33.3
EagleEye[36]
74.2
31.3
MetaPrune[43]
73.4
33.3
HALP-70%[52]
74.5
45.9
Ours
75.2 118.2
Table 4: Results on Intel CPU Xeon
E5.
with differing input and output channel counts to enumerate the entire latency
landscape at layer l. Recall our latency cost matrix Cl looks like:
  {
 
\mat
hbf { C}
_l = \l
e f t
 [ {\ beg
in {a rr
ay}{c cc
c }  
T_l(1 ,1)
 & 
T_l
(1,
2)&
 \cdots & T_l(1,m_ l) \ \  T_l(2, 1 ) &
 
T_l(2,2) & \cdots & T_l(2, m_l)\\ \vdots & \vdots & \ddots & \vdots \\ T_l(m_{l-1}, 1) & T_l(m_{l-1}, 2) & \cdots & T_l(m_{l-1}, m_l)\\ \end {array} } \right ] }
(14)
Previous approaches could be viewed as only leveraging the pl‚àí1th row
of Cl:
 
 { \left  [  {\begin  { a r r ay}{cccc } T
_l(p_{l-1},1) & T_l(p_{l-1},2)& \cdots & T_l(p_{l-1},m_l)\\ \end {array} } \right ] }
(15)
.
B
Solving MINLPs
In order to solve our MINLP program, we leverage the method called OA [17,21]
which decomposes the problem into solving an alternating finite sequence of NLP
subproblems and relaxed versions of MILP master program. We also leverage a
method called Feasibility Pump [5] to to expedite the process of finding feasible
solutions within constraints.
The entire program could be efficiently solved on common CPUs for modern
network sizes. For instance, when applied to a model like ResNet50 [25], the
entire optimization problem can be solved in approximately 5 seconds on an
Intel Xeon E5-2698 CPU.
C
Pruned Architecture
To gain deeper understanding of our pruning framework, we demonstrate the
pruned structure in Fig. 5. Our optimization approach maximizes global impor-
tance of a neural network under user-defined latency constraint (Eqn.4), elimi-
nating the need to pre-define pruning ratios. This automatically finds the optimal
solution, pruning blocks only if their impact on overall importance is minimal


--- Page 21 ---
Multi-Dimensional Pruning
21
compared to other options. The resulting pruned structure (Fig. 5) reveals that
pruning is concentrated in shallower layers, contrary to the general expectation
of layer collapse in deeper layers due to smaller gradients.
D
Performance on CPU Platform
To demonstrate the adaptability and effectiveness of our approach across plat-
forms, we instantiated and evaluated our method with the goal of optimizing
latency on the Intel Xeon E5 CPU. The results, shown in Table 4, demonstrate
significant performance improvements over previous work. Specifically, compared
to HALP [54], we achieve higher Top-1 accuracy (75.2 vs. 74.5) while more than
doubling the FPS (118.2 vs. 45.9). This improvement is even more pronounced
than our GPU results presented in the main paper. We attribute this to our
framework‚Äôs ability to prune blocks, resulting in a shallower structure that is
more conducive to CPU inference. This further underscores our method‚Äôs po-
tential for widespread applicability. Algorithmically, to enable different latency
optimization targets (e.g., CPU vs. GPU), our framework only requires gener-
ating a latency lookup table Tl (Section 3.1 Latency Modeling) on the target
platform and feeding it into the MINLP program.
E
Integration with Soft Masking
Recent advances in pruning [27, 31, 32, 35, 70] have increasingly adopted soft
masking techniques to retain the learning capacity of pruned models by not di-
rectly removing the pruned weights. Notably, SMCP [31] integrates this method
into the HALP hardware-aware pruning framework, resulting in an enhanced
accuracy-latency tradeoff for pruned models. Here, we explore the potential of
soft masking to enhance our model‚Äôs performance.
We conduct this study on ImageNet with ResNet50 and depict the Pareto
frontier of FPS versus Top-1 in Figure 6b. For clarity, we also include the perfor-
mance of SMCP [31] and ours. The results reveal that soft masking offers limited
advantages at lower FPS levels with modest pruning ratios and latency reduc-
tion. Nonetheless, targeting higher FPS levels leads to notable improvements in
Top-1 accuracy. This outcome may be attributed to the Taylor channel impor-
tance score we employed [50], which gauges parameter significance based on its
impact on loss. Though it maintains precision with minor parameter deletions,
its reliability may diminish when a larger number of parameters are pruned con-
currently. The iterative reassessment inherent to the soft masking technique may
counteract this shortcoming.
F
Comparison with Smaller Networks
In our paper, we adopt a "training large - pruning to small" paradigm. Our
approach is particularly effective at large pruning ratios. Therefore, instead of


--- Page 22 ---
22
X.Sun et al.
(a) Comparison with smaller networks on Im-
ageNet with pruning ResNet50. Our approach of
pruning large models across various ratios achieves a
superior accuracy-speed trade-off compared to existing
smaller networks. Top-right is better.
2500 3000 3500 4000 4500 5000
FPS
70
71
72
73
74
75
Top1
SMCP
Ours
Ours + Soft Masking
(b) Results of ours with soft masking on
ImageNet with ResNet50. Improvement is
observed in Top1 at a high FPS level. Top-
right is better.
starting with and pruning smaller architectures like MobileNet, EfficientNet, and
ResNet18, we aggressively prune a larger portion from ResNet50 to attain an
efficient model. This strategy outperforms other smaller networks in both Top-1
accuracy and FPS, as illustrated in Figure 6a. Larger networks provide a richer
manifold space for learning more complex features during training compared
to smaller ones, giving the pruned models greater potential. Our framework
excels at identifying an optimal pruned subnetwork structure, surpassing the
performance of training smaller networks from scratch.
G
Training Detail
The hyperparameters and settings used for experiments are detailed in Table 5.
Dataset
Epochs Optimizer, Momentum, WeightDecay
Learning Rate
ImageNet
90
SGD, 0.875, 3e ‚àí5
Init=1.024, LinearDecay
PascalVOC
800
SGD, 0.9, 2e ‚àí3
Init=8e ‚àí3, StepDecay
NuScenes
60
AdamW,n/a, 0.01
Init=6e ‚àí4, CosineAnneal
Table 5: Training Detail.
