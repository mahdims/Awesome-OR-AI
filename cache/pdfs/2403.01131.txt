--- Page 1 ---
LLaMoCo: Instruction Tuning of Large Language Models for
Optimization Code Generation
Zeyuan Ma 1 Hongshu Guo 1 Jiacheng Chen 1 Guojun Peng 1
Zhiguang Cao 2 Yining Ma 3 Yue-Jiao Gong 1
Abstract
Recent research explores optimization using large
language models (LLMs) by either iteratively
seeking next-step solutions from LLMs or di-
rectly prompting LLMs for an optimizer. How-
ever, these approaches exhibit inherent limitations,
including low operational efficiency, high sensi-
tivity to prompt design, and a lack of domain-
specific knowledge. We introduce LLaMoCo,
the first instruction-tuning framework designed to
adapt LLMs for solving optimization problems
in a code-to-code manner. Specifically, we es-
tablish a comprehensive instruction set contain-
ing well-described problem prompts and effec-
tive optimization codes. We then develop a novel
two-phase learning strategy that incorporates a
contrastive learning-based warm-up procedure be-
fore the instruction-tuning phase to enhance the
convergence behavior during model fine-tuning.
The experiment results demonstrate that a Code-
Gen (350M) model fine-tuned by our LLaMoCo
achieves superior optimization performance com-
pared to GPT-4 Turbo and the other competi-
tors across both synthetic and realistic problem
sets. The fine-tuned model and the usage instruc-
tions are available at https://anonymous.
4open.science/r/LLaMoCo-722A.
1. Introduction
Nowadays, Large Language Models (LLMs) are posing
a profound impact on human society (Floridi & Chiriatti,
2020; Lund & Wang, 2023). Through text generation, LLMs
exhibit extraordinary prowess in natural language under-
1School of Computer Science and Engineering, South China
University of Technology, Gungzhou, Guangdong, China 2School
of Computing and Information Systems, Singapore Management
University, Singapore. 3Nanyang Technological University, Sin-
gapore. Correspondence to: Yining Ma <yiningma@u.nus.edu>,
Yue-Jiao Gong <gongyuejiao@gmail.com>.
standing and adeptness in solving complex tasks (Biswas,
2023a; Lund & Wang, 2023; Biswas, 2023b). This prompts
a research question: Can LLMs even handle the challenging
Optimization problems that are usually difficult for humans
to address? This forms the core of our study in this paper.
In the literature, several existing works have been developed
to explore the possibilities of solving optimization problems
using LLMs. A typical way is to iteratively prompt LLMs
to output better solutions through a multi-turn conversa-
tion with LLMs (Yang et al., 2023; Guo et al., 2023b; Liu
et al., 2023b;a). Typically, they leverage LLMs through an
iterative process, sometimes incorporating the concept of
in-context learning. This involves the steps of presenting the
LLMs with a set of initial or current best-so-far solutions and
iteratively requesting LLMs to generate potentially superior
solutions. While showing certain effectiveness in solving
optimization tasks, these solution-to-solution approaches
have several limitations: 1) the scale of target optimization
tasks (e.g., the number of variables, historical solutions and
newly generated solutions) is constrained by the context
window length of LLMs; 2) the iterative process typically
involves hundreds rounds of conversations, consuming mul-
titudinous resources; and 3) due to the LLMs’ sensitivity to
prompt design, it is nontrivial to provide coherent prompts
for LLMs to guarantee ideal outputs.
An alternative way is to directly prompt LLMs for optimiza-
tion programs, namely a piece of executable code that either
reuses existing optimization toolboxes (AhmadiTeshnizi
et al., 2023) or combines multiple high-performing optimiz-
ers to create a novel one (Pluhacek et al., 2023). It could
be more efficient than the solution-to-solution methods for
two reasons: 1) only a simple or few rounds of conversation
are necessary for generating codes; and 2) the prompts and
the generated codes do not include solution information,
making it compatible as the problem scales. However, it
remains crucial to carefully craft prompts to ensure logi-
cal coherence of the generated codes. For example, Opti-
Mus (AhmadiTeshnizi et al., 2023) integrates hints about
the optimizer to be generated directly into the prompts, ne-
cessitating a deep understanding of optimization techniques
and expertise in the field. Additionally, using the LLMs
1
arXiv:2403.01131v2  [math.OC]  5 Mar 2024


--- Page 2 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Sure, here is the code for solving 
this problem:
“””
m.setObjective(7*x1+10+x2,GRB.
MAXIMIZE)
“””
Prompt for Optimizer
I will give you an LP optimization 
problem, and want you to write a 
python code using Gurobi to solve it.
The problem is as follows:
Nature language 
description of the 
problem
Instruction Tuning
Our LLaMoCo 
According to the provided problem, 
the best solver is:
import scipy
config = .... # hyper-params
res = scipy.optimize.minimize(obj 
fun, x0,method='SLSQP'...)                      
                                      
Use python to solve the following 
problem:
Stipulated Prompts Protocol
   -dimension: D
   -maxFEs: K
   -obj func: python/latex code
   -constraints: (optional)
Sure, here is the suggestions: 
[2.3, 1.5,...., 4.7]
...
[1.1, 3.5,..., -4.0]
Prompt for Solution
Now you will help me minimize a 
function. I have some points and the 
function values, lower is better:
 [-7.1, 0.5,...., 10.8], 17.8
...
[-1.7, -3.5,..., 4.2], 61.2
Give me some new points that is 
different from the above, and has
lower function values.
Instruction Set
Problem
Prompt
Code
Demonstration
Pre-trained
LLM
Pre-trained
LLM
Pre-trained
LLM
Instruction-tuned
 LLM
Figure 1. Conceptual overview of LLMs as optimizers.
Left: optimization through iteratively prompting LLMs for better solu-
tions (solution-to-solution style), such as OPRO (Yang et al., 2023). Middle: optimization through directly prompting LLMs for
an optimizer with code implementation, such as OptiMus (AhmadiTeshnizi et al., 2023). To ensure rational output, the prompts should
include hints about the type of problem and the suggested optimizer. Right: our LLaMoCo, which first tunes general LLMs on a
problem-code instruction set, then can be used to generate proper optimization code given the formatted problem prompts.
pre-trained on a wide range of corpus currently falls short
in generating a customized optimizer tailored to a specific
optimization problem instance. This limitation is identified
as the lack of domain-specific expert knowledge (Zhao et al.,
2023), which also extends to other intricate tasks with struc-
tured data, such as knowledge-base question answering and
semantic parsing (Jiang et al., 2023; Xie et al., 2022).
In this paper, we propose LLaMoCo, a novel framework
that fine-tunes general-purpose Large Language Models for
optimization Code generation. Different from the above
approaches that are sorely based on prompt engineering, our
LLaMoCo fine-tunes the LLMs on a well-formatted instruc-
tion set comprising code-to-code pairs of problem prompts
and executable optimization programs. Once the training is
completed, the fine-tuned model can be generalized to un-
seen optimization problems, i.e., crafting an optimizer based
on the specific problem structure. Our LLaMoCo holds the
following advantages against the preliminary works. 1) The
solution information-free setting, where an optimization pro-
gram is generated in a single round of conversation, makes it
easier to handle large-scale problems with higher efficiency
than the solution-to-solution methods. 2) The stipulated
prompt protocol for users to describe their optimization
problems minimizes the domain knowledge and efforts re-
quired for prompt design. 3) The fine-tuned LLMs by our
LLaMoCo provide users with more robust and expert-level
optimizers than those obtained by directly using general
code-generating LLMs. In Figure 1, we illustrate the dif-
ference between LLaMoCo and existing approaches that
leverage LLMs for optimization.
However, achieving expert-level LLMs for optimization
tasks presents certain challenges. To overcome these chal-
lenges, we contribute to the following aspects: 1) We estab-
lish the first instruction set for fine-tuning LLMs as expert-
level optimizer generators. This instruction set offers metic-
ulously crafted problem descriptions and corresponding
well-performing optimizer implementations selected from
a wide spectrum of advanced optimizers, refined through
extensive benchmarking with fine-grained hyper-parameter
search; 2) We put forth a two-phase adaption strategy, which
first enhances the latent space representation of a given prob-
lem instance through contrastive learning (Hadsell et al.,
2006), followed by the conventional sequence-to-sequence
loss for instruction tuning. Such design significantly ac-
celerates the convergence of fine-tuned LLMs, resulting in
superior performance; 3) LLaMoCo has been meticulously
designed for user-friendliness. Users can focus on the opti-
mization problem itself following a stipulated prompt proto-
col, and then the prompt is automatically constructed and
fed into the LLMs fine-tuned by our LLaMoCo.
Our benchmark experiments reveal the remarkably robust
optimization performance of our LLaMoCo, surpassing ex-
isting methods. Notably, we show that instruction tuning
of a relatively small LLM, e.g., CodeGen-350M (Nijkamp
et al., 2023), on domain-specific tasks can yield substan-
tial performance enhancements, even surpassing the very
large and powerful models like GPT-4 (Achiam et al., 2023).
Moreover, we provide in-depth analyses of the proposed
2


--- Page 3 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
two-phase adapting strategy, the sensitivity to training data
distribution, and the zero-shot generalization performance.
In summary, our contributions are four folds: 1) Intro-
duction of LLaMoCo, the first instruction tuning frame-
work for adapting general-purpose LLMs for generating
expert-level optimizers. 2) Establishment of the large-scale
instruction set on optimization domain, providing copious
code implementation of advanced optimizers at instance
level (Section 3.1). 3) Development of a novel two-phase
training strategy that reinforces the latent space representa-
tions of the prompts through efficient contrastive warm-up
training, boosting the subsequent instruction tuning perfor-
mance (Section 3.2). 4) Demonstration of LLaMoCo’s supe-
rior optimization performance against existing LLM-based
optimizers. The fine-tuned LLMs exhibit remarkable zero-
shot generalization ability to realistic optimization tasks,
with certain efficiency and code robustness (Section 4).
2. Related Works
2.1. Fine-tuning LLMs
Pre-trained Large Language Models (LLMs) can be refined
by additional parameter updates on specific tasks through
a fine-tuning process. We introduce two prominent fine-
tuning strategies: Instruction Tuning (IT) (Ouyang et al.,
2022) and Alignment Tuning (AT) (Christiano et al., 2017;
Ziegler et al., 2019), each serving distinct purposes. Gen-
erally, IT involves fine-tuning pre-trained LLMs using a
moderate collection of formatted task instances (Wei et al.,
2022). The fine-tuning process typically includes two steps:
1) prepare instruction-formatted training examples by asso-
ciating a task description with each task instance, which aids
LLMs in understanding tasks through the instructions (Sanh
et al., 2022); and 2) leverage the prepared instruction set
to fine-tune LLMs using a sequence-to-sequence super-
vised loss (Gupta et al., 2023). By incorporating a well-
established task-specific instruction set, IT can be an ef-
fective approach to inject domain-specific knowledge into
general LLMs. This enables the transfer of LLMs to spe-
cific experts in domains like medicine (Singhal et al., 2023),
law (Huang et al., 2023) and finance (Zhang et al., 2023).
Differently, AT aims to correct unexpected behaviors of
LLMs by aligning the models with human values and prefer-
ences (Ouyang et al., 2022; Ziegler et al., 2019). A practical
algorithm for AT is the Reinforcement Learning from Hu-
man Feedback (RLHF) (Ziegler et al., 2019), which firstly
estimates a reward model on a human-preference data col-
lection via maximum likelihood. It then uses the learned
reward model to provide feedback and post-trains the LLMs
through Proximal Policy Optimization (PPO) (Schulman
et al., 2017). A recent work named Direct Preference Opti-
mization (DPO) (Rafailov et al., 2023) first reparameterizes
the reward function based on the parameters of the pre-
trained LLMs, saving the modelling and training of the re-
ward function. DPO is mathematically equivalent to RLHF
but is even more efficient, which is widely adopted in the
latest LLMs such as Mistral 8x7B (Jiang et al., 2024).
2.2. LLMs for Code Generation
The task of generating code from natural language descrip-
tions is both exciting and inherently complex (Zan et al.,
2023; Chen et al., 2021). Although general-purpose LLMs
such as GPT (Brown et al., 2020), Llama 2 (Touvron et al.,
2023) and Mistral (Jiang et al., 2024) show competitive per-
formance on the widely used LLM benchmarks including
HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)
and DS-1000 (Lai et al., 2023), their performance on a par-
ticular task may still be limited. Recent efforts have focused
on developing Large Language Models (LLMs) specifically
tailored for code generation. These models are either trained
exclusively on code, such as AlphaCode (Li et al., 2022) and
StarCoder (Li et al., 2023), or fine-tuned from general LLMs,
like Codex (Chen et al., 2021) and Code Llama (Roziere
et al., 2023). Notably, Codex shows that a 12B LLM can
solve 72.31% of complex programming tasks posed by hu-
mans. This success has led to the emergence of various
Code LLMs, such as CodeGen (Nijkamp et al., 2023) that
factorizes a potentially long specification into multiple steps
to enhance program synthesis, and Code Llama that in-
creases Llama 2 models through a cascade of fine-tuning
steps. Other models such as Phi-2 (Javaheripi et al., 2023),
InCoder (Fried et al., 2023) and CodeGeeX (Zheng et al.,
2023) have also gained great attention.
2.3. LLMs as Optimizers
Optimization plays a crucial role in numerous science and
engineering fields but poses significant challenges. Unlike
simpler tasks such as language understanding that can be
easily handled by humans, optimization tasks can hardly
be solved by humans without efficient algorithms. The un-
derlying complexity of solving optimization problems tests
the reasoning and generalization abilities of LLMs. Re-
cently, there are several works that explore the potential
use of LLMs as optimizers (Yang et al., 2023; Pluhacek
et al., 2023; Yang et al., 2023; Guo et al., 2023c), mostly
based on prompt engineering and sometimes in an in-context
learning way (Min et al., 2022). Typically, these methods
consider a set of candidate solutions to be improved. LLMs
receive prompts containing these solutions and their objec-
tive values to propose improved ones. This process iterates
until a termination condition is reached. Moreover, several
studies introduce additional instructions, such as the muta-
tion and crossover operations, to the naive prompts. This
enables LLMs to mimic human-developed evolutionary op-
erators, thereby achieving improved performance. (Liu et al.,
3


--- Page 4 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
2023b;a; Lehman et al., 2023; Chen et al., 2023). However,
these approaches have limitations in efficiency due to the
need for extensive iterations. In contrast, several studies con-
sider prompting LLMs directly for optimization programs,
focusing on either creating new optimizers (Pluhacek et al.,
2023) or leveraging the combination of existing ones (Ah-
madiTeshnizi et al., 2023). To the best of our knowledge, all
the aforementioned works focus on prompt engineering of
pre-trained LLMs, and the area of fine-tuning general LLMs
with optimization-domain knowledge remains unexplored.
3. LLaMoCo
We introduce LLaMoCo, the first instruction tuning frame-
work for adapting general-purpose LLMs as optimizers.
It operates on a code-to-code basis, whereby, given anop-
timization problem where its objective function and con-
straints are described using Python or LaTeX codes, the
fine-tuned LLMs would generate a code implementation of
an optimizer for solving this problem (illustrated in the right
of Figure 1). In Section 3.1, we introduce how to establish
a high-quality instruction set that comprises expert-level
knowledge about solving optimization problems. Based on
the proposed instruction set, we design a novel two-phase in-
struction tuning strategy to smoothly boost the performance,
which is detailed in Section 3.2.
3.1. Construction of Instruction Set
Task synthesis. An optimization problem can be mathemat-
ically formulated as follows:
Minimize :
f(x),
x = (x1, x2, ..., xD)
s.t. :
gi(x) ≤0,
i = 1, ..., Mg
hj(x) = 0,
j = 1, ..., Mh
(1)
where f(·) is the objective function, x is a D-dimensional
vector denoting a solution, gi(·) and hj(·) denote Mg in-
equality constraints and Mh equality constraints respec-
tively. Without loss of generality, we assume a minimization
problem where the optimal solution x∗attains the minimum
objective value, adhering to all specified constraints.
The main concern in this context is how to create an ade-
quate amount of problem instances that possess both high
quality and diversity, which is crucial for instruction tun-
ing (Sanh et al., 2022; Zhou et al., 2023). Since it is not
feasible to gather all types of optimization problems that
arise in realistic scenarios, we opt for a more feasible ap-
proach by generating synthetic problem instances. Specif-
ically, we collect a basic function set 𭟋comprising many
different optimization problems and a basic constraint set
Ωcomprising a wide variety of constraints from the well-
known optimization benchmarks (Boyd & Vandenberghe,
2004; Wu et al., 2017; Guo et al., 2023a). Following the
methodology of Mohamed et al. (2021), we synthesize a
new objective function based on K basic functions in 𭟋
through two different paradigms as given by Equation (2).
1) Composition: this involves a linear combination of the K
basic functions on the complete decision space, where each
wi is uniformly sampled in [0, 1]. 2) Hybrid: we randomly
decompose x into K segments s1 to sK. The K basic func-
tions then operate on these K segments, respectively, and
the final objective function is the summation of these basic
functions’ values on the corresponding decision subspace.
Composition :
f(x) =
K
X
i=1
wi · fi(x)
Hybrid :
f(x) =
K
X
i=1
fi(x[si])
(2)
More concretely, we obtain a problem instance by three
steps: 1) Firstly, we indicate the problem dimension D, the
search bounds for each dimension (e.g., −10 ≤xi ≤10),
and the number of basic functions K; 2) Secondly, if K = 1,
we randomly select a basic function in 𭟋as f(x), otherwise,
we apply Composition/Hybrid paradigm to synthesize f(x);
and 3) Lastly, we randomly select a group of constraints
{{gi}, {hj}} in Ω. Note that step 3) is optional, as some
optimization problems may not have constraints.
In this work, we generate 3k problem instances without
constraints, denoted as Pnc, and another 3k problem in-
stances with constraints, denoted as Pc. The complete set
P is the union of Pnc and Pc, consisting of 6k instances.
These instances showcase different characteristics of global
landscapes, including unimodal or multimodal, separable or
nonseparable, and symmetrical or asymmetrical. They also
exhibit various local landscape properties, such as distinct
properties around different local optima, continuous every-
where yet differentiable nowhere, and optima situated in
flattened areas. This guarantees that the generated instances
comprehensively mirror various realistic problems.
Knowledge gathering. In our study, the term ‘knowledge’
refers to expertise on how to deal with an optimization prob-
lem, which involves identifying a well-performing optimizer
and configuring its hyper-parameters. After synthesizing the
task set, we conduct exhaustive benchmarking to determine
one effective optimizer for each instance p ∈P. Concretely,
we filter a wide range of optimizers from the published litera-
ture (Stork et al., 2022; Zhan et al., 2022), competitions (Wu
et al., 2017; Mohamed et al., 2021; Turner et al., 2021),
and benchmarks (R.Turner & D.Eriksson, 2019; Guo et al.,
2023a). The 23 selected optimizers form an algorithm pool,
which covers various algorithm families, including Evolu-
tionary Algorithms (e.g., GA (Holland, 1992; Clune et al.,
2008; Wang et al., 2023), DE (Storn & Price, 1997; Xu et al.,
2020; Biswas et al., 2021; Ye et al., 2023), PSO (Kennedy
4


--- Page 5 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
# objective function
obj = np.sum(x**2, axis=-1)
# constraints
g = x[:,1] - x[:,0]
# searching bounds
bounds = [[-10,10],[-5,5]]
# budget
maxFEs = 200
Generate an optimizer to find its optimal.   
Accoding to the provided problem, 
the effective solver is:   
from scipy.optimize import minimize
# provide your obj func
def obj_func(x):
# provide your constraints
constraints = {}
# provide your bounds for variables
bounds = []
# provide your budget
maxFEs = None
# provide a initial solution
x0 = np.random.rand(2)
# I suggest following options:
method = ‘SLSQP’
eps = 1e-8
# call optimization api
minimize(obj_func, x0, method, bounds, 
constraints, {‘maxiter’:maxFEs, ‘eps’:eps})
)
,
(
,
)
(
:
2
1
2
1
2
x
x
x
x
x
f
Minimize
i
i


# objective function
f(x) = \sum_{i=1}^{2}{x_i^2}
0
)
(
:
1
2



x
x
x
g
s.t.
# constraints
g(x) = x_2 - x_1
Python version
Latex version
Given an 2-D optimization problem:   
# searching bounds
bounds = [[-10,10],[-5,5]]
# budget
maxFEs = 200
Figure 2. Input-output example of our instruction set. For a given
problem (red box), diverse task descriptions in Python/LaTeX for-
mats construct the prompt (yellow box). The code implementation
of an effective optimizer is provided as the answer (green box).
& Eberhart, 1995; Gong et al., 2015; Wu & Wang, 2022; Lu
et al., 2023) and ES (Hansen & Ostermeier, 2001; Ros &
Hansen, 2008; Hansen, 2009; He et al., 2020)), Bayesian
Optimization (Snoek et al., 2012; Wang et al., 2020), Local
Search strategies (Kirkpatrick et al., 1983; Van Laarhoven
et al., 1987; Xiang et al., 1997; Fontes et al., 2023), and
Numerical Optimization methods (Kraft, 1988; Conn et al.,
2000; Powell, 2007; Bollapragada et al., 2018). To deter-
mine the most effective optimizer among our algorithm pool
for each instance p, we employ a two-step process. Firstly,
we perform a grid search to identify the best configuration
for each optimizer on p (conducted multiple times to re-
duce the impact of variance). Subsequently, we select the
optimizer that yields the best performance among all the
configured optimizers. The selected optimizer and its config-
uration are implemented as a piece of Python code, serving
as the knowledge of the desired optimizer’s implementation
for instance p. Refer to Appendix A for details of those
selected optimizers (configurations, implementations etc.)
and the benchmarking process.
Enhancement with diverse task descriptions. Recent stud-
ies suggest that enhancing the diversity of the task descrip-
tions for each task instance can lead to additional generaliza-
tion gains for the instruction-tuned LLMs (Sanh et al., 2022;
Wei et al., 2022; Chung et al., 2022). We hence augment
each problem instance in P by rephrasing the writing style
of its objective function and constraints. Specifically, we in-
vited a total of 500 university students majoring in computer
science to write Python or LaTeX codes for describing a
variety of optimization problems. Different writing patterns
are observed during this process. Based on these different
patterns, for each problem instance p ∈P, we can obtain
a number of rephrased versions for describing its objective
function and constraints in either Python or LaTeX code.
Refer to Appendix B for the detailed rephrasing process and
the different writing styles we have found.
After the data augmentation, we obtain the final instruction
Similar prompt
q+
Anchor prompt
q
Dissimilar prompt
q−
Transformer layers
o(q+)
o(q)
o(q−)
Positive sample loss:
���= �(�+, �)
Negative sample loss:
���= ���(0, �−�(�−, �))
Figure 3. The workflow of contrastive warm-up strategy. Given
an anchor problem prompt, we collect its similar prompts and
dissimilar prompts within a mini-batch. The positive and negative
sample losses are calculated on the latent space representations.
set by transforming each instance p, along with its rephrased
versions, into a text prompt q (input), and setting the source
code of the selected optimizer with configurations as the
answer a (output). This results in an instruction set I com-
prising 32570 pairs of input-output examples (q, a), where
an input-output example is illustrated in Figure 2.
3.2. Two-Phase Instruction Tuning
Contrastive warm-up. A key observation during the in-
struction set construction process in Section 3.1 is that: even
two prompts qm and qn are very different to each other (e.g.,
they adopt different descriptions of the same problem), they
can share the same desired optimizer a. On the contrary,
for two prompts hold similar descriptions, the selected op-
timizers may differ. This phenomenon challenges the con-
vergence of the models during fine-tuning. An appealing
approach to alleviate this issue is to adopt contrastive learn-
ing to align the latent space representation for different
prompts that share the same semantics. Such contrastive
learning task has shown its effectiveness in several code un-
derstanding scenarios (Guo et al., 2022). In LLaMoCo, we
adopt constrastive learning (Hadsell et al., 2006) to warm
up the LLMs before instruction tuning.
The workflow of the loss calculation is illustrated in Figure 3.
Given an anchor prompt, we collect its similar prompts and
dissimilar prompts within a mini-batch, which are then ap-
plied to calculate the positive and negative sample loss, re-
spectively. Specifically, for the decoder-only LLMs adopted
for code generation tasks in this paper, we activate the Trans-
former layers (Vaswani et al., 2017) and regard the output
embedding of the final self-attention block as latent space
representation for the prompt q.
In LLaMoCo, we measure the distance between two prompts
qm and qn, denoted as G(qm, qn), by considering the cosine
similarity between their latent space representations −→o (qm)
5


--- Page 6 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
and −→o (qn):
G(qm, qn) = 1
2

1 −
⃗o (qm) · ⃗o (qn)
∥⃗o (qm)∥∥⃗o (qn)∥

(3)
The above distance G(qm, qn) ∈[0, 1]. Then, the con-
trastive loss of qm and qn, denoted as Lcl(qm, qn), is as
Lcl =
(
G(qm, qn)
am = an
max(0, φ −G(qm, qn))
am ̸= an
(4)
where am and an are the corresponding designated opti-
mizer of qm and qn, respectively, φ is a margin parameter.
By minimizing Lcl, we could efficiently pull together the
representations of two prompts which share the same de-
sired optimizer yet have different forms, and vice versa. In
LLaMoCo, we consume a small number of epochs to warm
up the fine-tuning of LLMs by Lcl and then instruction-
tune the LLMs with the normal language modelling loss for
next-token prediction (Wolf et al., 2019). We note that the
contrastive warm-up phase does not require context genera-
tion, hence the time cost is relatively smaller compared with
the subsequent instruction tuning phase. We validate the ef-
fectiveness of this contrastive learning phase in Section 4.3.
Balanced data sampling. The instruction set I exhibits
certain imbalance in the distribution of data. Notably, we
observe that several optimizers dominate on thousands of
problem instances, while the others only outperform on a
few problem instances. Dealing with imbalanced data poses
a challenge during the process of fine-tuning models (Batista
et al., 2004; Zhao et al., 2023). To address the issue, we
follow the example-proportional mixing strategy (Raffel
et al., 2020) to re-balance the data distribution in I. Each
data pair (q, a) is sampled with a probability ρ as:
ρ(q, a) =
1
Na × Nq,a
(5)
where Na denotes the number of optimizers in the gathered
algorithm pool, Nq,a denotes the number of instances whose
desired optimizer is a. In this way, the number of sampled
pairs dominated by each optimizer is approximately equal in
each training epoch. Note that we apply this strategy in both
the contrastive warm-up phase and the instruction tuning
phase. The approach aids in avoiding biased training of
the LLMs and enables them to effectively learn the knowl-
edge from minority instances. In addition, a homogeneous
mini-batch sampling strategy is applied, due to the space
limitation, it is presented in Appendix C.
4. Results and Discussions
4.1. Experimental Setup
Fundamental models. We adopt CodeGen-Mono (350M),
Phi-2 (2.7B) and Code Llama (7B) as fundamental models
and fine-tune them on our instruction set. The reasons are
two-fold: 1) these models show robust programming lan-
guage reasoning and code generation ability, serving as a
good start point for the code-to-code scenario in our work;
2) the relatively small model size helps to reduce computa-
tional resources required for training and deploying.
Training settings. For generating the task set P, the prob-
lem dimension D for each pi is randomly chosen from
[2, 50], and the number of components K is randomly cho-
sen from [1, 5]. We randomly split the instruction set I into
a training set Itrain with 30k input-output pairs and a test set
Ieval with the rest examples. For our two-phase instruction
tuning, we deploy 5 epochs of contrastive warm-up and 20
epochs of instruction tuning for all fundamental models.
Specifically, we first apply SGD (Amari, 1993) with a fixed
learning rate 5 × 10−4 in the contrastive warm-up phase,
alongside φ = 0.3. Then, we apply AdamW (Loshchilov
& Hutter, 2019) to optimize the LLMs in the instruction
tuning phase. During the initial 1k iterations, the learning
rate gradually increases from 0 to 5 × 10−4 in a linear man-
ner. Subsequently, it decreases to 0 according to a cosine
schedule. The batch size in both phases is set to 4. Note
that we fine-tune the CodeGen-Mono (350M) with full pa-
rameters, but apply LoRA (Hu et al., 2022) to fine-tune the
larger Phi-2 (2.7B) and Code Llama (7B) models, with the
rank r = 8, scaling factor α = 32, and a dropout rate of
0.05. All experiments are performed on a platform with
an Intel(R) Xeon(R) Gold 6348 CPU, 504GB RAM and a
Nvidia A800 (80GB) GPU. Upon the settings, the training
duration for CodeGen is one day, whereas Phi-2 and Code
Llama require 2.5 days and 4 days of training, respectively.
Competitors.
We include two solution-to-solution ap-
proaches, OPRO (Yang et al., 2023) and LMEA (Liu
et al., 2023b), which prompt pre-trained LLMs (e.g., GPT-4
Turbo) repeatedly to generate and improve solutions for the
given problems. Compared to OPRO, LMEA additionally
engineered its prompt with an explicit indication of using
some evolutionary operators to let LLMs act as an evolution-
ary optimizer for performance boost. We also include three
general LLMs for code generation, namely Code Llama-
7B (Roziere et al., 2023), Llama 2-70B (Touvron et al.,
2023), and GPT-4 Turbo (Achiam et al., 2023). We prompt
these three general LLMs with the same format as in our
instruction set I to generate an optimizer for each problem
instance. The configurations of the competitors are set by
default according to the corresponding references.
Performance metrics. When evaluating the performance
of LLMs for optimization, we consider four metrics: 1) the
code error rate, which indicates the proportion of problems
for which the LLMs generate optimization codes with bugs
(lower values are preferable); 2) the code recovery cost,
which measures the proportion of lines of code that need
6


--- Page 7 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Table 1. Results of different approaches in terms of Code Error Rate (Err.), Code Recovery Cost (Rec.), Optimization Perfor-
mance (Perf.), and Computational Overhead (Comp.) on the unconstrained problems (Ieval/Pc), constrained problems (Ieval/Pnc),
and all test problems (Ieval), where “-” denotes that the approach does not generate code (it follows a solution-to-solution paradigm).
Testset
Metrics
Prompt for Solution
Prompt for Optimizer
Our LLaMoCo
OPRO
LMEA
GPT-4 Turbo
Code Llama-7B
Llama2-70B
LLaMoCo-S
LLaMoCo-M
LLaMoCo-L
Ieval/Pc
Err. ↓
-
-
43.333%
98.184%
99.673%
5.437%
4.414%
4.697%
Rec. ↓
-
-
9.942%
67.857%
62.232%
9.684%
10.101%
9.947%
Perf. ↑
29.499%
20.350%
71.783%
14.089%
18.922%
85.360%
86.412%
85.810%
Comp. ↓
115k
249k
3.4k
1.7k
1.5k
2.3k
2.3k
2.3k
Ieval/Pnc
Err. ↓
-
-
39.944%
90.474%
99.521%
5.697%
6.130%
5.977%
Rec. ↓
-
-
16.463%
44.938%
49.202%
11.861%
10.443%
10.584%
Perf. ↑
4.514%
7.541%
75.678%
46.968%
22.460%
77.576%
79.718%
83.404%
Comp. ↓
115k
249k
3.5k
2.0k
2.0k
2.5k
2.5k
2.5k
Ieval
Err. ↓
-
-
41.667%
95.156%
99.617%
5.580%
5.434%
5.509%
Rec. ↓
-
-
13.072%
57.001%
55.717%
10.826%
10.349%
10.461%
Perf. ↑
17.821%
14.762%
74.248%
29.717%
20.579%
81.843%
83.369%
83.451%
Comp. ↓
115k
249k
3.5k
1.9k
1.7k
2.4k
2.4k
2.4k
Table 2. Performance comparison on realistic problems.
Metrics
OPRO
GPT-4 Turbo
LLaMoCo-S
Err. ↓
-
79.483%
4.168%
Rec. ↓
-
22.985%
7.426%
Perf. ↑
73.995%
59.174%
87.227%
Comp. ↓
241k
3.6k
2.5k
to be corrected in order to fix the bugs in the erroneous
codes (lower values are preferable); 3) the average optimiza-
tion performance on the test problems (higher values are
preferable); and 4) the average computational overhead for
solving a problem, which is determined by the number of
tokens used for both the input and output of LLMs (lower
values are preferable). These four metrics could provide
a comprehensive evaluation on existing baselines and our
LLaMoCo in aspects of code generation robustness, opti-
mization performance and runtime complexity. The detailed
calculations for these metrics can be found in Appendix D.
4.2. Performance Analysis
We use LLaMoCo-S(mall), -M(edium) and -L(arge) to de-
note the fine-tuned CodeGen-Mono (350M), Phi-2 (2.7B)
and Code Llama (7B) models on Itrain, respectively.
Performance on test sets. First, we evaluate the perfor-
mance of our fine-tuned LLMs and the competitors on three
test sets, Ieval/Pc, Ieval/Pnc, and Ieval that represent the un-
constrained task set, constrained task set, and the complete
set mixing unconstrained and constrained tasks, respectively,
each with 5 independent runs. The results in terms of the
four metrics are reported in Table 1, which show that:
1) The LLMs fine-tuned by our LLaMoCo framework con-
sistently achieve superior performance, which validates that
instruction tuning the general LLMs with moderate expert-
level knowledge would gain substantial performance rein-
forcement in optimization. For example, LLaMoCo-L fine-
tuned on the Code Llama (7B) demonstrate an optimization
performance boost from 29.717% to 81.843% on Ieval.
2) Although LLaMoCo-S is fine-tuned from a relatively
small fundamental model, it achieves competitive perfor-
mance to those of LLaMoCo-M and LLaMoCo-L. This may
reveal a potential marginal effect in instruction tuning, since
the data scale should match the capacity of the model.
3) The solution-to-solution approaches OPRO and LMEA
achieve unsatisfactory performance on our complex opti-
mization task sets. Considering the tremendous tokens these
approaches consume to solve one optimization problem
through iteratively prompting solutions, both the efficacy
and efficiency (as shown in the ‘Perf.’ and ‘Comp.’ rows of
Table 1) of them require further improvement.
4) Among the three ‘prompt for optimizer’ models we com-
pared, the GPT-4 Turbo dominates the other two, which
shows the powerfulness of a general-purpose LLM with high
capacity. Nevertheless, it still underperforms our domain-
specific LLaMoCo. Our models effectively reduce the error
rates and the required recovery efforts for generating the
codes of an optimizer through the instruction tuning. Mean-
while, note that the Code Llama (7B) model achieves better
overall performance than the Llama 2 (70B) model in our
experiments. The above observations validate that, although
LLMs with larger capacity may show strong performance
for solving general tasks, a smaller model could be sufficient
to be fine-tuned as a domain-specific task solver.
Zero-shot performance on realistic problems. We intro-
duce a realistic optimization problem set collected by Kumar
et al. (2020) to further evaluate the zero-shot generalization
performance of the LLMs fine-tuned by our LLaMoCo. This
problem set serves as an ideal testbed for our framework
for two reasons: 1) an optimizer that performs very well on
synthetic benchmark suites may not provide robust perfor-
7


--- Page 8 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
0
30K
60K
90K
120K
150K
Learning Steps
1.41
1.42
1.43
1.44
Loss
w/o contrastive
contrastive
2
6
10
14
18
Epoch
76%
78%
80%
82%
Performance
w/o contrastive
contrastive
Figure 4. Effectiveness of the contrastive warm-up strategy on
training curves (Left) and performance gains (Right).
mance on real-world problems, and 2) this set of problems
shows very different structures compared to our synthetic
problem set P. These problems come from various real-
world scenarios including the industrial chemical process,
mechanical engineering, and the power system, many of
them feature high-dimensional problem spaces and compli-
cated constraints. As an illustration, we test OPRO, GPT-4
Turbo and our LLaMoCo-S on these realistic problems (in-
tegrate their problem definitions into our formatted prompts)
for 5 independent runs. The results in Table 2 demonstrate
the best generalization performance and hence the practical
availability of our LLaMoCo.
4.3. Ablation study
Diversity enhancement. To improve the generalization
of the fine-tuned LLMs in LLaMoCo, we enrich the task
descriptions for each problem instance by augmenting the
description of its objective function and constraints with
Python or LaTeX codes of different writing styles. We
illustrate the effect of this procedure in the left of Fig-
ure 5 by showing the optimization performance of six
LLaMoCo-S models trained on pure Python, pure LaTeX
and Python+LaTeX data, with or without the diversity en-
hancement by rephrasing. The results show that providing
multi-lingual descriptions of optimization problems signifi-
cantly boosts the generalization performance, while rephras-
ing each description with multiple writing styles further
enhances the final training results.
Contrastive warm-up. The contrastive warm-up phase
in our proposed two-phase instruction tuning strategy (see
Section 3.2) aims to reduce the cross-modal ambiguity by
aligning the latent space representations of different prompts
that share the same desired optimizer. We illustrate the train-
ing curves and performance gain curves on Ieval with or
without the contrastive warm-up during the instruction tun-
ing phase in Figure 4, where LLaMoCo-S is applied as
a showcase. The results show that incorporating such a
contrastive warm-up strategy aids in accelerating the con-
vergence of the subsequent instruction tuning. Furthermore,
it is advantageous for the LLMs to generate accurate codes
and enhance the overall optimization performance.
Python
Latex
Python+Latex
40%
50%
60%
70%
80%
Performance
55.02%
49.29%
81.84%
49.16%
44.14%
79.61%
with rephrase
w/o rephrase
Minority
Majority
Overall
20%
40%
60%
80%
53.45%
88.36%
81.84%
30.30%
69.45%
62.14%
with balancing
w/o balancing
Figure 5. Effectiveness of the diversity enhancement strategy
(Left) and the data distribution balancing strategy (Right).
Balanced data sampling. In LLaMoCo, we address the
imbalanced data distribution (caused by dominate optimiz-
ers) through performing example-proportional sampling
on Itrain. To investigate its effectiveness, we train two
LLaMoCo-S models on Itrain, with or without the data bal-
ancing strategy, respectively. The optimization performance
of the two models is presented in the right of Figure 5, by
separately considering the majority instances (which request
the dominating optimizers), the minority instances (which
request the others), and the overall instances of Ieval. The
results consistently show that keeping a balanced training
data distribution significantly boosts performance.
4.4. Open-Ended Discussion: Is GPT-4 a True
Optimization Expert?
Considering the competitive performance of GPT-4 on opti-
mization tasks, as shown in Table 1, we delve into whether
GPT-4 can be deemed as a genuine optimization expert.
Upon viewing the optimization codes generated by GPT-4
for both the test and the realistic problem set, a noteworthy
pattern emerges. GPT-4 consistently leans towards gener-
ating a specific numerical optimizer, SLSQP (Kraft, 1988),
for almost all tested problems. While SLSQP is a classical
solver for convex quadratic programming and is included
in our chosen advanced optimizers, our benchmarking re-
sults identify that on a proportion of tested problems, it
underperforms the others such as the Vanilla DE (Storn &
Price, 1997). To investigate further, we experiment by pro-
viding GPT-4 with a hint to use Vanilla DE to solve these
specific problems. Surprisingly, GPT-4 successfully out-
puts a code implementation of DE and achieves competitive
results. This observation suggests that while GPT-4 may
have included sufficient domain knowledge on how to solve
optimization problems, it still exhibits an underfitting issue
concerning how to solve a ‘particular’ problem. This un-
derscores the importance of the LLaMoCo framework for
fine-tuning general LLMs to fit the task of generating an ap-
propriate optimizer tailored for specific problem instances.
5. Conclusion
We introduce LLaMoCo, the first instruction-tuning frame-
work to adapt general LLMs to function as expert-level
8


--- Page 9 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
systems to solve optimization problems. To achieve this,
we meticulously construct an instruction set with more than
30k demonstration examples and then employ a novel two-
phase instruction tuning strategy to fine-tune a series of
LLMs. The results show that our models consistently out-
perform existing approaches. Notably, we observe that a
relatively small LLM is sufficient to be tuned as an expert-
level optimization code generator superior to GPT-4. As
a preliminary exploratory research endeavour, LLaMoCo
certainly has limitations, such as the need to augment the
instruction set with more instances to enhance generaliza-
tion performance. Additionally, we consider enhancing the
LLMs fine-tuned by LLaMoCo through further alignment
tuning as a promising future direction.
Impact Statements
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none of which we feel must be
specifically highlighted here.
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,
Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774, 2023.
AhmadiTeshnizi, A., Gao, W., and Udell, M. Optimus: Op-
timization modeling using mip solvers and large language
models. arXiv preprint arXiv:2310.06116, 2023.
Amari, S.-i. Backpropagation and stochastic gradient de-
scent method. Neurocomputing, 1993.
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732, 2021.
Batista, G. E., Prati, R. C., and Monard, M. C. A study of the
behavior of several methods for balancing machine learn-
ing training data. ACM SIGKDD Explorations Newsletter,
2004.
Biswas, S., Saha, D., De, S., Cobb, A. D., Das, S., and
Jalaian, B. A. Improving differential evolution through
bayesian hyperparameter optimization. In 2021 IEEE
Congress on evolutionary computation (CEC), 2021.
Biswas, S. S. Role of chat gpt in public health. Annals of
Biomedical Engineering, 2023a.
Biswas, S. S. Potential use of chat gpt in global warming.
Annals of Biomedical Engineering, 2023b.
Bollapragada, R., Nocedal, J., Mudigere, D., Shi, H.-J.,
and Tang, P. T. P. A progressive batching l-bfgs method
for machine learning. In International Conference on
Machine Learning, 2018.
Boyd, S. P. and Vandenberghe, L. Convex optimization.
Cambridge university press, 2004.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learn-
ers. Advances in Neural Information Processing Systems,
2020.
Chen, A., Dohan, D. M., and So, D. R. Evoprompting: Lan-
guage models for code-level neural architecture search.
arXiv preprint arXiv:2302.14838, 2023.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,
G., et al. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374, 2021.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
S., and Amodei, D. Deep reinforcement learning from
human preferences. Advances in Neural Information
Processing Systems, 2017.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Clune, J., Misevic, D., Ofria, C., Lenski, R. E., Elena, S. F.,
and Sanju´an, R. Natural selection fails to optimize mu-
tation rates for long-term adaptation on rugged fitness
landscapes. PLoS Computational Biology, 2008.
Conn, A. R., Gould, N. I., and Toint, P. L. Trust region
methods. SIAM, 2000.
Duan, Q., Zhou, G., Shao, C., Wang, Z., Feng, M., Yang, Y.,
Zhao, Q., and Shi, Y. Pypop7: A pure-python library for
population-based black-box optimization. arXiv preprint
arXiv:2212.05652, 2022.
Floridi, L. and Chiriatti, M. Gpt-3: Its nature, scope, limits,
and consequences. Minds and Machines, 2020.
Fontes, D. B., Homayouni, S. M., and Gonc¸alves, J. F. A
hybrid particle swarm optimization and simulated anneal-
ing algorithm for the job shop scheduling problem with
transport resources. European Journal of Operational
Research, 2023.
Fortin, F.-A., De Rainville, F.-M., Gardner, M.-A. G.,
Parizeau, M., and Gagn´e, C. Deap: Evolutionary al-
gorithms made easy. The Journal of Machine Learning
Research, 2012.
9


--- Page 10 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,
Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., and Lewis,
M. Incoder: A generative model for code infilling and
synthesis. In The Eleventh International Conference on
Learning Representations, 2023.
Gong, Y.-J., Li, J.-J., Zhou, Y., Li, Y., Chung, H. S.-H., Shi,
Y.-H., and Zhang, J. Genetic learning particle swarm
optimization. IEEE transactions on cybernetics, 2015.
Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., and Yin,
J. Unixcoder: Unified cross-modal pre-training for code
representation. arXiv preprint arXiv:2203.03850, 2022.
Guo, H., Ma, Z., Chen, J., Li, Z., Peng, G., Gong, Y.-J.,
Ma, Y., and Cao, Z. Metabox: A benchmark platform for
meta-black-box optimization with reinforcement learning.
In Thirty-seventh Conference on Neural Information Pro-
cessing Systems Datasets and Benchmarks Track, 2023a.
Guo, P.-F., Chen, Y.-H., Tsai, Y.-D., and Lin, S.-D. Towards
optimizing with large language models. arXiv preprint
arXiv:2310.05204, 2023b.
Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu,
G., Bian, J., and Yang, Y. Connecting large language mod-
els with evolutionary algorithms yields powerful prompt
optimizers. arXiv preprint arXiv:2309.08532, 2023c.
Gupta, H., Sawant, S. A., Mishra, S., Nakamura, M., Mitra,
A., Mashetty, S., and Baral, C. Instruction tuned models
are quick learners. arXiv preprint arXiv:2306.05539,
2023.
Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality
reduction by learning an invariant mapping. In 2006
IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’06), 2006.
Hansen, N. Benchmarking a bi-population cma-es on the
bbob-2009 function testbed. In Proceedings of the 11th
annual conference companion on genetic and evolution-
ary computation conference: late breaking papers, 2009.
Hansen, N. and Ostermeier, A. Completely derandomized
self-adaptation in evolution strategies. Evolutionary Com-
putation, 2001.
He, X., Zheng, Z., and Zhou, Y. Mmes: Mixture model-
based evolution strategy for large-scale optimization.
IEEE Transactions on Evolutionary Computation, 2020.
Holland, J. H. Adaptation in natural and artificial systems:
an introductory analysis with applications to biology, con-
trol, and artificial intelligence. 1992.
Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y.,
Wang, S., Wang, L., and Chen, W. LoRA: Low-rank
adaptation of large language models. In International
Conference on Learning Representations, 2022.
Huang, Q., Tao, M., An, Z., Zhang, C., Jiang, C., Chen,
Z., Wu, Z., and Feng, Y. Lawyer llama technical report.
arXiv preprint arXiv:2305.15062, 2023.
Javaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,
S., Mendes, C. C. T., Chen, W., Del Giorno, A., Eldan,
R., Gopi, S., et al. Phi-2: The surprising power of small
language models, 2023.
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,
E. B., Bressand, F., et al. Mixtral of experts. arXiv
preprint arXiv:2401.04088, 2024.
Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W. X., and
Wen, J.-R. Structgpt: A general framework for large
language model to reason over structured data. arXiv
preprint arXiv:2305.09645, 2023.
Kennedy, J. and Eberhart, R. Particle swarm optimization.
In Proceedings of ICNN’95-International Conference on
Neural Networks, 1995.
Kirkpatrick, S., Gelatt Jr, C. D., and Vecchi, M. P. Optimiza-
tion by simulated annealing. science, 1983.
Kraft, D. A software package for sequential quadratic pro-
gramming. Forschungsbericht- Deutsche Forschungs-
und Versuchsanstalt fur Luft- und Raumfahrt, 1988.
Kumar, A., Wu, G., Ali, M. Z., Mallipeddi, R., Sugan-
than, P. N., and Das, S. A test-suite of non-convex con-
strained optimization problems from the real-world and
some baseline results. Swarm and Evolutionary Compu-
tation, 2020.
Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettle-
moyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T.
Ds-1000: A natural and reliable benchmark for data sci-
ence code generation. In International Conference on
Machine Learning, 2023.
Lange, R. T. evosax: Jax-based evolution strategies. In
Proceedings of the Companion Conference on Genetic
and Evolutionary Computation, 2023.
Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C.,
and Stanley, K. O. Evolution through large models. In
Handbook of Evolutionary Machine Learning, 2023.
Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D.,
Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al.
Starcoder: may the source be with you! arXiv preprint
arXiv:2305.06161, 2023.
10


--- Page 11 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J.,
Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago,
A., et al. Competition-level code generation with alpha-
code. Science, 2022.
Liu, F., Lin, X., Wang, Z., Yao, S., Tong, X., Yuan, M., and
Zhang, Q. Large language model for multi-objective evo-
lutionary optimization. arXiv preprint arXiv:2310.12541,
2023a.
Liu, S., Chen, C., Qu, X., Tang, K., and Ong, Y.-S. Large lan-
guage models as evolutionary optimizers. arXiv preprint
arXiv:2310.19046, 2023b.
Loshchilov, I. and Hutter, F. Decoupled weight decay reg-
ularization. In International Conference on Learning
Representations, 2019.
Louppe, G. and Kumar, M. Scikit-optimize, 2016. URL
https://github.com/scikit-optimize/
scikit-optimize.
Lu, H.-C., Tseng, H.-Y., and Lin, S.-W. Double-track parti-
cle swarm optimizer for nonlinear constrained optimiza-
tion problems. Information Sciences, 2023.
Lund, B. D. and Wang, T. Chatting about chatgpt: how may
ai and gpt impact academia and libraries? Library Hi
Tech News, 2023.
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of
demonstrations: What makes in-context learning work?
arXiv preprint arXiv:2202.12837, 2022.
Mohamed, A. W., Hadi, A. A., Mohamed, A. K., Agrawal,
P., Kumar, A., and Suganthan, P. N. Problem definitions
and evaluation criteria for the cec 2021 on single objective
bound constrained numerical optimization. In Proceed-
ings of the IEEE Congress of Evolutionary Computation,
2021.
Morales, J. L. and Nocedal, J. Remark on “algorithm 778:
L-bfgs-b: Fortran subroutines for large-scale bound con-
strained optimization”. ACM Transactions on Mathemat-
ical Software (TOMS), 2011.
Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,
Y., Savarese, S., and Xiong, C. Codegen: An open large
language model for code with multi-turn program synthe-
sis. In The Eleventh International Conference on Learn-
ing Representations, 2023.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,
Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,
et al. Training language models to follow instructions
with human feedback. Advances in Neural Information
Processing Systems, 2022.
Pluhacek, M., Kazikova, A., Kadavy, T., Viktorin, A., and
Senkerik, R.
Leveraging large language models for
the generation of novel metaheuristic optimization al-
gorithms. In Proceedings of the Companion Conference
on Genetic and Evolutionary Computation, 2023.
Powell, M. J. A view of algorithms for optimization without
derivatives. Mathematics Today-Bulletin of the Institute
of Mathematics and its Applications, 2007.
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,
C. D., and Finn, C. Direct preference optimization: Your
language model is secretly a reward model. arXiv preprint
arXiv:2305.18290, 2023.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research,
2020.
Ros, R. and Hansen, N. A simple modification in cma-es
achieving linear time and space complexity. In Inter-
national conference on parallel problem solving from
nature, 2008.
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950, 2023.
R.Turner and D.Eriksson. Bayesmark: Benchmark frame-
work to easily compare bayesian optimization methods
on real machine learning tasks, 2019. URL https:
//github.com/uber/bayesmark.
Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L.,
Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey,
M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S.,
Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta,
D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M.,
Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T.,
Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T.,
Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao,
L., Wolf, T., and Rush, A. M. Multitask prompted training
enables zero-shot task generalization. In International
Conference on Learning Representations, 2022.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.
Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung,
H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,
et al. Large language models encode clinical knowledge.
Nature, 2023.
11


--- Page 12 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Snoek, J., Larochelle, H., and Adams, R. P.
Practical
bayesian optimization of machine learning algorithms.
In Advances in Neural Information Processing Systems,
2012.
Stork, J., Eiben, A. E., and Bartz-Beielstein, T. A new
taxonomy of global optimization algorithms. Natural
Computing, 2022.
Storn, R. and Price, K. Differential evolution-a simple and
efficient heuristic for global optimization over continuous
spaces. Journal of Global Optimization, 1997.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., et al. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288,
2023.
Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laakso-
nen, E., Xu, Z., and Guyon, I. Bayesian optimization is
superior to random search for machine learning hyper-
parameter tuning: Analysis of the black-box optimiza-
tion challenge 2020. In NeurIPS 2020 Competition and
Demonstration Track, 2021.
Van Laarhoven, P. J., Aarts, E. H., van Laarhoven, P. J., and
Aarts, E. H. Simulated annealing. Springer, 1987.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. Advances in Neural Information
Processing Systems, 2017.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Weckesser, W., Bright, J., et al. Scipy 1.0: fundamental
algorithms for scientific computing in python. Nature
methods, 2020.
Wang, F., Xu, G., and Wang, M. An improved genetic
algorithm for constrained optimization problems. IEEE
Access, 2023.
Wang, L., Fonseca, R., and Tian, Y. Learning search space
partition for black-box optimization using monte carlo
tree search. Advances in Neural Information Processing
Systems, 2020.
Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester,
B., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-
guage models are zero-shot learners. In International
Conference on Learning Representations, 2022.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
et al. Huggingface’s transformers: State-of-the-art natural
language processing. arXiv preprint arXiv:1910.03771,
2019.
Wu, D. and Wang, G. G. Employing reinforcement learning
to enhance particle swarm optimization methods. Engi-
neering Optimization, 2022.
Wu, G., Mallipeddi, R., and Suganthan, P. N. Problem defini-
tions and evaluation criteria for the cec 2017 competition
on constrained real-parameter optimization. National
University of Defense Technology, Changsha, Hunan, PR
China and Kyungpook National University, Daegu, South
Korea and Nanyang Technological University, Singapore,
Technical Report, 2017.
Xiang, Y., Sun, D., Fan, W., and Gong, X. Generalized
simulated annealing algorithm and its application to the
thomson model. Physics Letters A, 1997.
Xie, T., Wu, C. H., Shi, P., Zhong, R., Scholak, T., Ya-
sunaga, M., Wu, C.-S., Zhong, M., Yin, P., Wang, S. I.,
Zhong, V., Wang, B., Li, C., Boyle, C., Ni, A., Yao, Z.,
Radev, D., Xiong, C., Kong, L., Zhang, R., Smith, N. A.,
Zettlemoyer, L., and Yu, T. UnifiedSKG: Unifying and
multi-tasking structured knowledge grounding with text-
to-text language models. In Proceedings of the 2022
Conference on Empirical Methods in Natural Language
Processing, 2022.
Xu, T., He, J., and Shang, C. Helper and equivalent ob-
jectives: Efficient approach for constrained optimization.
IEEE transactions on cybernetics, 2020.
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and
Chen, X. Large language models as optimizers. arXiv
preprint arXiv:2309.03409, 2023.
Ye, C., Li, C., Li, Y., Sun, Y., Yang, W., Bai, M., Zhu, X.,
Hu, J., Chi, T., Zhu, H., et al. Differential evolution with
alternation between steady monopoly and transient com-
petition of mutation strategies. Swarm and Evolutionary
Computation, 2023.
Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B.,
Yongji, W., and Lou, J.-G. Large language models meet
nl2code: A survey. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics,
2023.
Zhan, Z.-H., Shi, L., Tan, K. C., and Zhang, J. A survey on
evolutionary computation for complex continuous opti-
mization. Artificial Intelligence Review, 2022.
Zhang, J., Xie, R., Hou, Y., Zhao, W. X., Lin, L., and Wen,
J.-R. Recommendation as instruction following: A large
language model empowered recommendation approach.
arXiv preprint arXiv:2305.07001, 2023.
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,
Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of
large language models. arXiv preprint arXiv:2303.18223,
2023.
12


--- Page 13 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y.,
Shen, L., Wang, Z., Wang, A., Li, Y., et al. Codegeex: A
pre-trained model for code generation with multilingual
benchmarking on humaneval-x. In Proceedings of the
29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, 2023.
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,
Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for
alignment. arXiv preprint arXiv:2305.11206, 2023.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning
language models from human preferences. arXiv preprint
arXiv:1909.08593, 2019.
13


--- Page 14 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
A. Benchmarking for Knowledge Gathering
A.1. Optimizer Pool and The Used Assets
To match each problem instance in the generated problem set P with an appropriate optimizer with corresponding code
implementation, we construct an optimizer pool Λ which integrates 23 well-performing optimizers from various algorithm
families. These selected optimizers can be divided into two groups considering their compatibility for constraint handling.
We briefly list the two groups as below:
Unconstrained group Λuc: Simulated Annealing (Kirkpatrick et al., 1983), Vanilla PSO (Kennedy & Eberhart, 1995),
Vanilla DE (Storn & Price, 1997), Dual Annealing (Xiang et al., 1997), SAMR-GA (Clune et al., 2008), SEP-CMA-ES (Ros
& Hansen, 2008), BIPOP-CMA-ES (Hansen, 2009), DEAP-DE (Fortin et al., 2012), Vanilla BO (Snoek et al., 2012),
GLPSO (Gong et al., 2015), MMES (He et al., 2020), LA-MCTS (Wang et al., 2020), MadDE (Biswas et al., 2021),
sDMS-PSO (Wu & Wang, 2022), AMCDE (Ye et al., 2023), NSA (Fontes et al., 2023).
Constrained group Λc: SLSQP (Kraft, 1988), Trust-Constr (Conn et al., 2000), COBYLA (Powell, 2007), L-BFGS-
B (Morales & Nocedal, 2011), HECO-DE (Xu et al., 2020), DTPSO (Lu et al., 2023), GA-TDX (Wang et al., 2023).
We benefit from open-source libraries, including DEAP (Fortin et al., 2012), PyPop7 (Duan et al., 2022), evosax (Lange,
2023), SciPy (Virtanen et al., 2020) and Scikit-Optimizer (Louppe & Kumar, 2016) etc., for the easy implementation of the
selected optimizers. We list the codebases we adopt for the implementation of these optimizers and their licenses in Table 3.
We note that the development and deployment of our framework strictly follow those licenses.
Table 3. Used assets and their licenses
Asset
Codebase
License
DEAP-DE (Fortin et al., 2012)
DEAP (Fortin et al., 2012)
LGPL-3.0 License
Vanilla PSO (Kennedy & Eberhart, 1995)
SAMR-GA (Clune et al., 2008)
evosax (Lange, 2023)
Apache-2.0 license
BIPOP-CMA-ES (Hansen, 2009)
Simulated Annealing (Kirkpatrick et al., 1983)
SEP-CMA-ES (Ros & Hansen, 2008)
PyPop7 (Duan et al., 2022)
GPL-3.0 license
MMES (He et al., 2020)
LA-MCTS (Wang et al., 2020)
NSA (Fontes et al., 2023)
Dual Annealing (Xiang et al., 1997)
SciPy (Virtanen et al., 2020)
BSD-3-Clause license
SLSQP (Kraft, 1988)
COBYLA (Powell, 2007)
Vanilla BO (Snoek et al., 2012)
Scikit-Optimizer (Louppe & Kumar, 2016)
BSD-3-Clause License
GA-TDX (Wang et al., 2023)
advanced-global-optimizers
https://pypi.org/project/advanced-global-optimizers/
MIT License
Vanilla DE (Storn & Price, 1997)
MadDE (Biswas et al., 2021)
AMCDE (Ye et al., 2023)
HECO-DE (Xu et al., 2020)
GLPSO (Gong et al., 2015)
sDMS-PSO (Wu & Wang, 2022)
DTPSO (Lu et al., 2023)
A.2. Benchmarking Process
The benchmarking process aims to find an appropriate configured optimizer for each problem instance p ∈P. To this
end, for each optimizer Λk ∈Λ, we span a grid-search configuration space Ck based on its tunable hyper-parameters,
which is listed in Table 4. Take DEAP-DE (Fortin et al., 2012) as an example, it has three hyper-parameters and each of
them has 5 optional values (pre-defined by us). We hence span the Ck of DEAP-DE as a configuration space comprising
5 × 5 × 5 = 125 configurations, each denoted as Cj
k. We now establish the target of our benchmarking process:
arg max
Λk∈Λ,Cj
k∈Ck
E
h
eval(p, Λk, Cj
k)
i
where p denotes the tested problem instance, eval denotes the final optimization performance by calling Λk with configuration
Cj
k to solve p, and E denotes the expectation of the optimization performance, which is unbiased-estimated by 5 independent
14


--- Page 15 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
runs in this work. For constrained problems, we benchmark Λc, while for unconstrained problems we benchmark Λnc. We
note that the benchmarking process for each problem instance may encounter execution failures, e.g., some optimizers in Λc
can not handle equality constraints, some optimizers in Λnc are incompatible with non-convex problems, BO optimizers are
extremely time-consuming on high-dimensional problems. When failures occur, the corresponding eval(p, Λk, Cj
k) is set to
0. After benchmarking Λ on P, we provide a configured optimizer a(Λk, Cj
k), and the corresponding code implementation
as the desired optimizer for each p.
Table 4: Configurations and the hyperparameter tuning settings of the optimizers.
Type
Algorithm
Parameters
Search range
GA
SAMR-GA
(Clune et al., 2008)
NP
[10, 20, 50, 100, 200]
elite ratio
0.0
sigma init
[0, 0.5, 1]
sigma meta
[1, 2, 3, 4, 5]
sigma best limit
[0.0001, 0.001, 0.1]
GA-TDX
(Wang et al., 2023)
beta
[0.1, 0.2, 0.3, 0.4, 0.5]
gamma
[1, 3, 5, 7, 9]
m
1e10
NP
[10, 20, 50, 100, 200]
DE
Vanilla DE
(Storn & Price, 1997)
NP
[10, 20, 50, 100, 200]
F
[0, 0.5, 0.9]
Cr
[0, 0.5, 0.9]
mutation
{best1, best2, rand2, current2rand,
current2best, rand2best2}
bound
{clip, periodic, reflect, rand}
DEAP-DE
(Fortin et al., 2012)
NP
[10, 20, 50, 100, 200]
F
[0.1, 0.3, 0.5, 0.7, 0.9]
Cr
[0.1, 0.3, 0.5, 0.7, 0.9]
HECO-DE
(Xu et al., 2020)
F0
0.5
Cr0
0.5
Arate
[2, 4, 6, 8]
Hm
[1, 3, 5]
NPm
12
NPmin
40
lamda
[10, 20, 30, 40]
n0
[1, 2, 3]
gamma
[0.05, 0.1, 0.2]
MadDE
(Biswas et al., 2021)
p
[0.09, 0.18, 0.27, 0.36]
PqBX
[0.01, 0.1, 0.2, 0.3, 0.5]
F0
0.2
Cr0
0.2
Arate
[1.3, 1.8, 2.3, 2.8 ,3.3]
Hm
[5, 10 ,15, 20]
NPm
[2, 4, 6, 8]
NPmin
4
AMCDE
(Ye et al., 2023)
F0
0.2
Arate
[1.6, 2.1, 2.6, 3.1, 3.6]
Hm
[5, 10, 15, 20]
NPm
[3, 6, 9]
NPmin
4
Gn
5
pbc1
[0.4, 0.5, 0.6]
pbc2
[0.4, 0.5, 0.6]
pw
[0.1, 0.2, 0.3]
pr
[0.005, 0.01, 0.05]
plssucc
0.1
plsfail
0.0001
PSO
Vanilla PSO
(Kennedy & Eberhart, 1995)
NP
[10, 20, 50, 100, 200]
phi1
[1, 2, 3]
phi2
[1, 2, 3]
GLPSO
(Gong et al., 2015)
pm
[0.01, 0.1, 0.2]
NP
[10, 20, 50, 100, 200]
nsel
10
w
0.7298
c1
1.49618
sg
7
rho
[0.1, 0.2, 0.3]
sDMS-PSO
(Wu & Wang, 2022)
w
[0.729, 0.271, 0.5]
NP
[33, 66, 99, 198]
c1
[1.49445, 3., 0.75]
c2
[1.49445, 3., 0.75]
m
[1, 3, 5]
R
[5, 10, 15]
LP
[5, 10, 15]
LA
8
L
100
L FEs
200
15


--- Page 16 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Table 4 continued from previous page
Type
Algorithm
Parameters
Search range
PSO
DTPSO
(Lu et al., 2023)
p
[0.1, 0.5, 0.9]
sigma
[0.25, 0.5, 0.75]
gamma
[0.25, 0.5, 0.75]
u1
[0, 0.5]
u2
[0, 0.5]
c1,1
[0, 1.711897]
c1,2
[0, 1.711897]
c2,1
[0, 1.711897]
c2,2
[0, 1.711897]
ws
0.9
we
0.4
NPinit
[50, 100, 200]
radius
[0.05, 0.1, 0.2]
ES
SEP-CMA-ES
(Ros & Hansen, 2008)
n individuals
[10, 20, 50, 100]
c c
[1, 2, 3, 4, 5]
sigma
[0.1, 0.3, 0.5]
BIPOP-CMA-ES
(Hansen, 2009)
NP
[10, 20, 50, 100]
elite ratio
[0.2, 0.5, 0.7]
sigma init
1
mean decay
0
min num gens
[10, 30, 50]
popsize multiplier
[1, 2, 3, 4, 5]
MMES
(He et al., 2020)
a z
[0.05, 0.1, 0.2]
c s
[0.1, 0.3, 0.5]
ms
[2, 4, 6]
n individuals
[25, 50, 100]
n parents
[25, 50, 100]
sigma
[0.1, 0.3, 0.5]
BO
Vanilla BO
(Snoek et al., 2012)
acq func
[LCB, EI, PI, gp hedge, EIps, PIps]
n initial points
[5, 10, 20]
initial point generator
[random, sobol, halton, hammersly, lhs]
LA-MCTS
(Wang et al., 2020)
n individuals
[10, 20, 50, 100]
c e
[0.01, 0.05, 0.1]
leaf size
[10, 20, 30, 40, 50]
LS
Simulated Annealing
(Kirkpatrick et al., 1983)
NP
[10, 20, 50, 100, 200]
sigma init
[0.1, 0.3, 0.5]
sigma decay
1
sigma limit
[0.01, 0.05, 0.1]
temp init
1
temp limit
0.1
temp decay
[0.9, 0.99, 0.999]
boltzmann const
[1, 5, 10]
Dual Annealing
(Xiang et al., 1997)
initial temp
[523, 5230, 50000]
visit
[1.62, 2.62, 3.62]
restart temp ratio
[2e-5, 2e-3, 2e-1]
NSA
(Fontes et al., 2023)
sigma
[0.1, 0.3, 0.5]
schedule
[linear, quadratic]
n samples
[10, 20, 50, 100, 200]
rt
[0.9, 0.99, 0.999]
NO
SLSQP
(Kraft, 1988)
eps
[1e-12, 1e-10, 1e-8, 1e-6, 1e-4]
Trust-Constr
(Conn et al., 2000)
initial tr radius
[0.5, 1, 1.5, 2]
initial constr penalty
[0.5, 1, 1.5, 2]
factorization method
[equality constrained sqp, tr interior point]
COBYLA
(Powell, 2007)
rhobeg
[0.5, 1, 1.5, 2]
L-BFGS-B
(Morales & Nocedal, 2011)
maxcor
[5, 10, 15, 20]
eps
[1e-12, 1e-10, 1e-8, 1e-6, 1e-4]
B. Details of Data Augmentation
It is a common practice to augment the training data for boosting the generalization performance in recent LLMs
works (Sanh et al., 2022; Wei et al., 2022; Chung et al., 2022). In LLaMoCo, we alter different writing styles of a problem’s
definition to generate moderate diverse prompts for each problem instance generated in P. For the different writing styles,
we investigate 500 university students majoring in computer science, invite them to write Python or LaTeX code that they
believe is correct for defining the given problem instances. After systematic statistics, we have empirically summarized
several writing patterns, which we believe could approximately represent the major writing patterns of different users. Based
on these different patterns, for each problem instance p ∈P, we can obtain moderate rephrased versions for its objective
function and constraints written by either Python or LaTeX code. We showcase the found patterns on a toy Katsuura
problem which holds the formulation as:
16


--- Page 17 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
Minimize :
f(x) = 10
D2
D
Y
i=1

1 + i
32
X
j=1
2jxi −round
 2jxi

2j


10
D1.2
−10
D2 , X ∈RD
For LaTeX patterns, we found three different writing styles from the 500 testees, which differ from each other mainly based
on the laws of arithmetic, e.g., commutative law, distributive law and associative law. We illustrate some different LaTeX
codes for our toy problem in Figure 6.
$\begin{aligned}
Minimize:\quad &f(x) = \frac{10}{D^2} 
\prod_{i=1}^D\left(1+i \sum_{j=1}^{32} 
\frac{\left|2^j x_i-\operatorname{round}\left(2^j 
x_i\right)\right|}{2^j}\right)^{\frac{10}{D^{1
2}}}-\frac{10}{D^2} , X\in R^{D}\\
\end{aligned}$
$\begin{aligned}
Minimize:\quad &f(x) = \frac{10}{D^2} 
\left[\prod_{i=1}^D\left(1+i \sum_{j=1}^{32} 
\frac{\left|2^j x_i-\operatorname{round}\left(2^j 
x_i\right)\right|}{2^j}\right)^{\frac{10}
{D^{12}}}-1\right] , X\in R^{D}\\
\end{aligned}
$\begin{aligned}
Minimize:\quad &f(x) =\frac{10\prod_{i=1}^D
\left(1+i \sum_{j=1}^{32} \frac{\left|2^j x_i-
\operatorname{round}\left(2^j x_i\right)\right|}
{2^j}\right)^{\frac{10}{D^{12}}}-10}{D^2} , x 
=  \left({x_1,x_2,...,x_D}\right)\\
\end{aligned}$
Figure 6. Three writing styles in LaTeX of the toy problem.
For Python patterns, the testees show different coding preferences on the writing styles of the objective functions and the
constraints, e.g., some may prefer using temporary variables to store interim calculation results, some leverage numpy
to facilitate matrix operations while others use a for loop, some may encapsulate the calculation details into a functional
module etc. In Figure 7 we list some of these writing styles on the toy problem.
D = np.shape(x)[-1]
temp1 = np.power(D, 1.2)
temp2 = np.repeat(np.power(np.ones((1, 32)) * 2, 
np.arange(1, 33)), x.shape[0], 0)
temp3 = np.ones(x.shape[0])
for i in range(D):
    temp4 = temp2 * np.repeat(x[:, i, None], 32, 1)
    temp5 = np.sum(np.fabs(temp4 - 
np.floor(temp4 + 0.5)) / temp2, -1)
    temp3 *= np.power(1 + (i + 1) * temp5, 10 / 
temp1)
temp6 = 10 / D / D
result = temp3 * temp6 - temp6
D = np.shape(x)[-1]
result = np.zeros(x.shape[0])
for i in range(x.shape[0]):
    result[i] = 10 / (D ** 2)
    for j in range(D):
        round_x = 0
        for k in range(32):
            round_x += np.abs(2**(k+1) * x[i][j] - 
np.round(2**(k+1) * x[i][j])) / (2**(k+1))
        result[i] *= np.power(1 + (j + 1) * round_x, 
10 / (np.power(D, 1.2)))
result[i] -= 10 / (D ** 2)
def f1(x):
    D = np.shape(x)[-1]
    temp1 = np.power(D, 1.2)
    temp2 = np.repeat(np.power(np.ones((1, 32)) * 
2, np.arange(1, 33)), x.shape[0], 0)
    temp3 = np.ones(x.shape[0])
    for i in range(D):
        temp4 = temp2 * np.repeat(x[:, i, None], 32, 
1)
        temp5 = np.sum(np.fabs(temp4 - 
np.floor(temp4 + 0.5)) / temp2, -1)
        temp3 *= np.power(1 + (i + 1) * temp5, 10 / 
temp1)
    return temp3
D = np.shape(x)[-1]
temp1 = f1(x)
temp2 = 10 / D / D
result = temp1 * temp2 - temp2
Figure 7. Three writing styles in Python of the toy problem.
C. Details in Training Process
Homogeneous batch sampling. We further apply a homogeneous batch sampling strategy at the instruction tuning phase to
reinforce the alignment of the different rephrasing version prompts for a problem p ∈P. Concretely, we force the LLMs
to sample data pairs which come from the same problem instances in a mini-batch. We observe consistent boosts in the
training of LLaMoCo-S, LLaMoCo-M and LLaMoCo-L. By presenting the LLMs with a batch of homogeneous samples,
they can learn patterns specific to these cross-modal prompts data more effectively.
Batch size. We would clarify that due to the resource limitation, all of our experiments are run on an NVIDIA A800 GPU.
When we train the CodeGen-Mono (350M), the batch size is 4 for both phases in our two-phase learning strategy. However,
for one A800, Phi-2 (2.7B) and Code Llama (7B) are too large to include a batch of 4 samples, even if we adapt LoRA for
them. For Phi-2, the batch size is 3 and 2 for each learning phase, while 3 and 1 for Code Llama.
17


--- Page 18 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
D. Calculation of Experimental Statistics
To provide a thorough evaluation on the LLMs fine-tuned by our LLaMoCo and the other approaches, for a group of Np
problem instances, we first leverage the optimization programs generated by each LLM to optimize them, for 5 independent
runs. Then we calculate the average error rate, recovery cost, optimization performance and computational cost of an
approach as the performance metrics of overall performance. The calculation details of these four performance metrics in
our experimental results are as follows:
Error rate (Err.) The robustness of the generated optimization program is a very important index for quality of service (QoS).
We measure the robustness by the proportion of error programs generated by an LLM, named as error rate. For each instance,
we use the optimization program generated by an LLM (ours or the others) to optimize that instance for 5 independent runs.
We count the number of the generated programs which encounter compilation error or runtime error when being executed,
denoted as Nerr (every single run on each instance is counted). Then the error rate of an approach on the tested instances is
calculated as Nerr
5×Np .
Recovery cost (Rec.) While an optimization program may encounter compilation error or runtime error, we observe from
our experiments that a certain proportion of the error programs could be repaired and recovered. We provide a metric named
recovery cost to measure the efforts required to repair the generated programs. Concretely, for an optimization program aj,
we denote the number of lines in it as L(j), and the number of lines that need to be repaired as L(j)
err. Then the recovery cost
for aj is rj = L(j)
err
L(j) , and the recovery cost considering all Nerr error programs is calculated as
PNerr
j=1
rj
Nerr
.
Optimization performance (Perf.) We measure the optimization performance of an approach by a min-max normalized
objective value descent. Concretely, we first estimate an optimal objective value f ∗
i for i-th problem instance, which can
be easily obtained from our benchmarking process (achieved best objective value). For the given approach, we denote the
performance on the i-th problem instance in j-th run as a min-max normalized term wi,j =
f ∗
i,j−f ∗
i
f 0
i,j−f ∗
i , where f 0
i,j is the best
objective value of the solutions initialized by the optimizer on solving the i-th problem instance in j-th run, and f ∗
i,j is the
corresponding best objective the optimizer finds. Then the overall average optimization performance of the given approach
on the Np instances can be calculated as follows:
PNp
i=1
P5
j=1 wi,j
5×Np
.
Computational overhead (Comp.) Measuring the computational overhead by the wall-time complexity of an LLM-based
approach is impractical since some of the LLMs only provide API for users. The network communication budget through
calling the APIs would bias the ground results. We instead count the average number of tokens (input+output) consumed by
an approach for solving a problem instance over the test runs.
18


--- Page 19 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
E. Example Input-Output of the Fine-tuned Model
E.1. Synthetic unconstrained example
We showcase the prompt and the generated optimization program (Figure 8) of a synthetic problem instance without
constraints, which has the following formulation:
Minimize :
f(x) =
1
X
i=0
Wifi(z), z = MTx, x ∈RD, M ∈RD×D
Where :
f0(x) = −20 exp

−0.2
v
u
u
t(1/D)
D
X
i=1
x2
i

−exp
 
(1/D)
D
X
i=1
cos (2πxi)
!
+ 20 + e
f1(x) =
D
X
i=1
p
|xi| + 2sin(x3
i )

W0 = 0.6002499789314202
W1 = 0.02117765478091216
Problem Prompt
Given an 16 dimensional optimization problem:
x_in = M@x_in
x=copy.deepcopy(x_in)
temp1 = np.exp(-0.2 * np.sqrt(np.average(x ** 2, -1)))
temp2 = np.exp(np.average(np.cos(2 * np.pi * x),-1))
result = -20 * temp1 - temp2 + 20 + np.e
res0 = result
temp1 = np.power(np.abs(x),0.5)
temp2 = 2*np.sin(x**3)
result = np.sum(temp1+temp2, axis=-1)
res1 = result
result = 0.6002499789314202 * res0 + 0.02117765478091216 * res1
Note that the feasible domain is:
[[-70.0, 70.0], [-70.0, 70.0], [-40.0, 40.0],
 [-40.0, 40.0], [-20.0, 20.0], [-60.0, 60.0], 
[-60.0, 60.0], [-10.0, 10.0], [-70.0, 70.0], 
[-80.0, 80.0], [-70.0, 70.0], [-30.0, 30.0],
 [-30.0, 30.0], [-20.0, 20.0], [-10.0, 10.0], [-50.0, 50.0]].
The problem should be solved within 40000 function evaluations.
Generate a solver for it:
Optimizer Program
from evosax import BIPOP_CMA_ES
import jax
import numpy as np
# Define problem dimension
D =?
# Define the objective function,
# the first parameter is a solution x in one dimensional np.array,
# you can include other parameters for obj_func in args
def obj_func(x: 1D np.array,...) -> float:
    pass   
# Insert other parameters of obj_func into args, empty by default
args = ()
# Assign search ranges
bounds = [[min1, max1],..., [minD, maxD]]
# Assign maximum function evaluations
maxFEs =?
# Assign random seed
seed = 1
# The suggested algorithm configuration
popsize = 100
elite_ratio = 0.5
sigma_init = 1.0
mean_decay = 0.0
min_num_gens = 50
popsize_multiplier = 2
# Construct the optimizer
rng = jax.random.PRNGKey(seed=seed)
optimizer = BIPOP_CMA_ES(popsize=popsize, num_dims=D, elite_ratio=elite_ratio, 
                         sigma_init=sigma_init, mean_decay=mean_decay)
es_params = optimizer.default_params
strategy_params = es_params.strategy_params.replace(init_max=1, init_min=0, clip_max=1, clip_min=0)
restart_params = es_params.restart_params.replace(min_num_gens=min_num_gens, popsize_multiplier=popsize_multiplier)
es_params = es_params.replace(strategy_params=strategy_params, restart_params=restart_params)
state = optimizer.initialize(rng, es_params)
# Optimize the problem
nfev = 0
lb = np.array(bounds)[:,0]
ub = np.array(bounds)[:,1]
while nfev < maxFEs:
    rng, rng_gen, rng_eval = jax.random.split(rng, 3)
    x, state = optimizer.ask(rng_gen, state, es_params)
    fitness = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        xi = np.array(x[i]) * (ub - lb) + lb
        fitness[i] = obj_func(xi, *args)
    nfev += x.shape[0]
    state = optimizer.tell(x, fitness, state, es_params)
# Get the result
x, fun = state.strategy_state.best_member, state.strategy_state.best_fitness
Figure 8. A unconstrained problem prompt (on the left, a Python version), and the optimization program (on the right) output by LLaMoCo-
S. The corresponding 16-dimensional problem is constructed by a composition of two basic functions. Our LLaMoCo-S is prompted to
output a competent optimizer for solving the problem within 40000 function evaluations, which in this case, is BIPOP-CMA-ES.
19


--- Page 20 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
E.2. Synthetic constrained example
We showcase the prompt and the generated optimization program (Figure 9) of a synthetic problem instance with some
constraints, which has the following formulation:
Minimize :
f(x) = z2
1 + 106
D
X
i=2
z2
i , z = x −o, X ∈RD, o ∈RD
s.t. :
h0(x) :
D
X
i=1


i
X
j=1
yj


2
= 0, y = x −o
h1(x) :
D−1
X
i=1
 y2
i −yi+1
2 = 0, y = x −o
Problem Prompt
Given an 23 dimensional optimization problem:
x_in = x_in-o
x=copy.deepcopy(x_in)
def f(x):
    return x[:,0] + np.power(10,6) * np.sum(x[:,1:], axis=-1)
result = f(x**2)
Note that the feasible domain is:
[[-10.0, 10.0], [-20.0, 20.0],[-60.0, 60.0],[-30.0, 30.0], 
[-90.0, 90.0], [-70.0, 70.0],[-10.0, 10.0], [-50.0, 50.0],
[-40.0, 40.0], [-10.0, 10.0],[-70.0, 70.0], [-40.0, 40.0], 
[-20.0, 20.0], [-70.0, 70.0], [-50.0, 50.0],[-20.0, 20.0],
[-60.0, 60.0], [-40.0, 40.0],[-70.0, 70.0], [-30.0, 30.0], 
[-20.0, 20.0], [-50.0, 50.0], [-90.0, 90.0]].
The problem should be solved within 20000 function evaluations.
The constraints for solutions:
equality_1:
x_in = x_in-o
D = x_in.shape[-1]
result = 0
for i in range(1, D+1):
    result += np.square(np.sum(x_in[:,:i], axis=-1))
equality_2:
x_in = x_in-o
result = np.sum(np.square(x_in[:,:-1] - x_in[:,1:]), axis=-1)
Generate a solver for it:
Optimizer Program
from scipy.optimize import minimize
import numpy as np
# Define problem dimension
D =?
# Define the objective function,
# the first parameter is a solution x in one dimensional np.array,
# you can include other parameters for obj_func in args
def obj_func(x: 1D np.array,...) -> float:
    pass
# Insert other parameters of obj_func into args, empty by default
args = ()
# Define the constraints
def h_cons_1(x) -> float:
    pass
...
def g_cons_1(x) -> float:
    pass
...
constraints = [
    {'type': 'eq', 'fun': h_cons_1},
   ...
    {'type': 'ineq', 'fun': g_cons_1},
   ...
]
# Assign search ranges
bounds = [[min1, max1],..., [minD, maxD]]
# Assign maximum function evaluations
maxFEs =?
# The suggested algorithm configuration
method = 'SLSQP'
eps = 0.0001
# Initial solution
x0 = np.random.rand(D)
# Optimize the problem
result = minimize(obj_func, x0, args=args, method=method, bounds=bounds, 
                              constraints = constraints, options={'maxiter': maxFEs, 'eps': eps} )
Figure 9. A constrained problem prompt (on the left, a Python version), and the optimization program (on the right) output by LLaMoCo-S.
The corresponding 23-dimensional problem is one of the basic functions, with two additional quality constraints. Our LLaMoCo-S is
prompted to output a competent optimizer for solving that problem within 20000 function evaluations, which in this case, is SLSQP. We
note that the GPT-4 Turbo attain the same answer on this problem. However, the configurations suggested by LLaMoCo-S achieve higher
optimization performance against GPT-4 Turbo that adopts the default configurations.
20


--- Page 21 ---
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
E.3. Realistic example
We showcase the prompt and the generated optimization program (Figure 10) of a realistic problem instance with a large
number of constraints yet with a relatively simpler objective function, which holds a different problem structure against the
synthetic problems, which has the following formulation:
Minimize :
f(x) = 35x0.6
1
+ 35x0.6
2
s.t. :
h1(x) : 200x1x4 −x3 = 0
h2(x) : 200x2x6 −x5 = 0
h3(x) : x3 −10000(x7 −100) = 0
h4(x) : x5 −10000(300 −x7) = 0
h5(x) = x3 −10000(600 −x8) = 0
h6(x) = x5 −10000(900 −x9) = 0
h7(x)) = x4 ln(x8 −100) −x4 ln(600 −x7) −x8 + x7 + 500 = 0
h8(x) = x6 ln(x9 −x7) −x6 ln(600) −x9 + x7 + 600 = 0
Problem Prompt
Given an 9 dimensional optimization problem:
$\begin{aligned}
Minimize:\quad&f(x)=35x_{1}^{0.6}
+35x_{2}^{0.6} \\s.t.\quad &: \\&
h_1(x):200x_1x_4-x_3=0\\&
h_{2}(x):200x_{2}x_{6}-x_{5}=0\\&
h_{3}(x):x_{3}-10000(x_{7}-100)=0 \\&
h_4(x):x_5-10000(300-x_7)=0\\&
h_{5}(x)=x_{3}-10000(600-x_{8})=0 \\&
h_{6}(x) =x_{5}-10000(900-x_{9})=0  \\&
h_{7}(x))=x_4\ln(x_8-100)-x_4\ln(600-x_7)-x_8+x_7+500=0  \\&
h_{8}(x) =x_6\ln(x_9-x_7)-x_6\ln(600)-x_9+x_7+600=0
\end{aligned}$
Note that the feasible domain is:
[[0.0, 10.0], [0.0, 200.0], [0.0, 100.0], 
[0.0, 200.0], [1000.0, 2000000.0], [0.0, 600.0], 
[100.0, 600.0], [100.0, 600.0], [100.0, 900.0]].
The problem should be solved within 10000 function evaluations.
Generate a solver for it:
Optimizer Program
from advanced_global_optimizers.constrained_solvers import GA_TDX
from advanced_global_optimizers import Problem
import numpy as np
# Define problem dimension
D =?
# Define the objective function,
# the first parameter is a solution x in one dimensional np.array,
# you can include other parameters for obj_func in args
def obj_func(x: 1D np.array,...) -> float:
    pass   
# Insert other parameters of obj_func into args, empty by default
args = ()
# Define the constraints
def h_cons_1(x) -> float:
    pass
...
def g_cons_1(x) -> float:
    pass
...
constraints = [
    {'type': 'eq', 'fun': h_cons_1},
   ...
    {'type': 'ineq', 'fun': g_cons_1},
   ...
]
# Assign search ranges
bounds = [[min1, max1],..., [minD, maxD]]
# Assign maximum function evaluations
maxFEs =?
# The suggested algorithm configuration
param = {
    'beta': 0.1,
    'gamma': 3,
    'NP': 50,
}
# Construct the problem and optimizer
problem = Problem(D, obj_func, bounds, maxFEs, args, constraints)
optimizer = GA_TDX(param)
# Optimize the problem
result = optimizer.optimize(problem)
Figure 10. A realistic problem prompt (on the left, a LaTeX version), and the optimization program (on the right) output by LLaMoCo-S.
The corresponding 9-dimensional problem holds an out-of-distribution structure, with far more constraints than the problem instances
LLaMoCo-S has ever seen. Our LLaMoCo-S is prompted to output a competent optimizer for solving that problem within 10000 function
evaluations, which in this case, is an advanced GA-TDX algorithm specialized in constraints handling. We note that the GPT-4 Turbo
suggests a DE algorithm for this problem, which is hardly adopted for solving constrianed problems.
21
