--- Page 1 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Xinyi Ke 1 Kai Li 1 2 Junliang Xing 3 Yifan Zhang 1 2 Jian Cheng 1 4
Abstract
Large language models (LLMs) have enabled
rapid progress in automatic heuristic discovery
(AHD), yet most existing methods are predomi-
nantly limited by static evaluation against fixed
instance distributions, leading to potential overfit-
ting and poor generalization under distributional
shifts. We propose Algorithm Space Response Or-
acles (ASRO), a game-theoretic framework that
reframes heuristic discovery as a program level co-
evolution between solver and instance generator.
ASRO models their interaction as a two-player
zero-sum game, maintains growing strategy pools
on both sides, and iteratively expands them via
LLM-based best-response oracles against mixed
opponent meta-strategies, thereby replacing static
evaluation with an adaptive, self-generated cur-
riculum. Across multiple combinatorial optimiza-
tion domains, ASRO consistently outperforms
static-training AHD baselines built on the same
program search mechanisms, achieving substan-
tially improved generalization and robustness on
diverse and out-of-distribution instances.
1. Introduction
Combinatorial Optimization (CO) underpins critical appli-
cations across engineering, autonomous systems, and large-
scale resource management, yet its NP-hard nature makes
exact algorithms infeasible at realistic scales (Papadimitriou
& Steiglitz, 1998). Modern solvers therefore rely on hand-
crafted heuristics and metaheuristics (Blum & Roli, 2003;
Burke et al., 2009; 2010a; Drake et al., 2020). This man-
ual design process is slow, brittle, and biased toward the
structural assumptions of human experts, which often leads
to severe performance degradation under distributional or
structural shift (Bengio et al., 2021; Manchanda et al., 2022).
The emergence of Large Language Models (LLMs) capable
1C2DL, Institute of Automation, Chinese Academy of Sciences
2School of Artificial Intelligence, University of Chinese Academy
of Sciences 3Tsinghua University 4AiRiA. Correspondence to: Kai
Li <kai.li@ia.ac.cn>.
of synthesizing executable programs (Austin et al., 2021)
has opened a promising direction for Automatic Heuristic
Design (AHD). Recent work leverages this capability to
automate the synthesis of executable heuristic solvers, en-
abling rapid construction of competitive solvers and partially
automating the manual algorithm-design pipeline.
In practice, existing LLM-AHD pipelines adopt a closed-
loop generate–evaluate–refine workflow: the LLM proposes
heuristic programs, their execution on a fixed evaluator
yields performance and behavioral feedback, and this feed-
back guides subsequent refinement. This iterative loop
forms the backbone of current LLM–AHD methodologies
(Romera-Paredes et al., 2024; Liu et al., 2024a; Ye et al.,
2024; Novikov et al., 2025). Despite their empirical success,
these frameworks are fundamentally tied to static evalua-
tion (Liu et al., 2024a): heuristics are optimized against a
fixed evaluation distribution, e.g., pre-designed datasets.
This static setting raises two core limitations. (i) It invites
overfitting and fragility: discovered heuristics optimized on
a fixed evaluator often generalize poorly under distributional
shift (Sim et al., 2025). (ii) It induces a performance ceiling:
once a heuristic is near-optimal, the evaluator fails to ex-
pose new weaknesses, hindering further improvement. Even
when moving beyond static evaluation, existing approaches
often rely on hand-crafted curricula or ad hoc difficulty
schedules, leaving instance adaptation outside the core opti-
mization objective and structurally retaining a single-agent
training paradigm. Consequently, solver–instance interac-
tions lack a principled, unified optimization framework,
making stable and sustained co-adaptation difficult.
Motivated by this, we propose and formalize a game-
theoretic, co-evolutionary formulation of LLM-based heuris-
tic discovery, termed the Algorithm Space Response Oracle
(ASRO). In ASRO, solvers and instance generators are mod-
eled as two players in a zero-sum game, with strategies rep-
resented as executable programs1. Solvers map instances to
solutions, while generators induce instance distributions that
challenge solver performance. ASRO maintains growing
pools of strategies on both sides, and iteratively (i) evalu-
1Throughout the paper, we study algorithms through their exe-
cutable program realizations. For notational simplicity, we refer to
these realizations as programs, which serve as concrete, executable
solvers or generators synthesized by the LLM.
1
arXiv:2601.22896v2  [cs.AI]  9 Feb 2026


--- Page 2 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
ates all solver–generator pairs to construct a payoff matrix,
(ii) computes mixed meta-strategies (i.e., probability distri-
butions over the current strategy pools), and (iii) invokes
LLM-based best-response oracles to synthesize new solver
or generator programs against the opponent’s meta-strategy.
Conceptually, ASRO draws inspiration from Policy Space
Response Oracles (PSRO) (Lanctot et al., 2017), shar-
ing its population-based, best-response-driven optimization
paradigm. Unlike PSRO, which operates in parametric pol-
icy spaces and approximates best responses via reinforce-
ment learning, ASRO works directly in discrete program
space, where strategies are executable solver and instance
generator programs synthesized by LLMs. A key advantage
of the programmatic formulation is that generators are exe-
cutable code, where a single program induces a structured
stochastic family of instance distributions. By embedding
such generators into the meta-game, ASRO couples instance
difficulty and diversity within a unified co-evolutionary pro-
cess, where generators shift their induced distributions as
solver mixtures improve, yielding solver pools that are ro-
bust beyond specific benchmarks. ASRO induces a self-
generated curriculum: generators adaptively expose weak-
nesses of the current solver mixture, and solvers in turn
adapt to increasingly hard and diverse instance distributions.
Our main contributions are as follows:
• Game-theoretic framework for LLM-based AHD.
We formulate LLM-driven heuristic discovery as a two-
player zero-sum game over executable programs, re-
placing static evaluation with a PSRO-style “solve-and-
expand” loop in discrete program space.
• Meta-game driven co-evolution.
Persistent strat-
egy pools and equilibrium-based meta-strategies in-
duce sustained solver–generator co-evolution, avoiding
short-horizon adversarial dynamics.
• An oracle-agnostic, program-space framework.
ASRO specifies a game-theoretic optimization struc-
ture in program space while leaving the choice of pro-
gram search mechanism open.
• Empirical validation on multiple CO domains.
Across three representative CO domains, ASRO con-
sistently outperforms static-training counterparts built
on identical program search mechanisms, achieving
improved robustness and generalization under distribu-
tional and structural shifts.
2. Related Works
2.1. Automated Heuristic Design
Early AHD relied on hyper-heuristics and genetic program-
ming, evolving heuristics expressed in manually designed
domain-specific languages (DSLs) (Burke et al., 2003; 2013;
2018; Pillay & Qu, 2018). A major line of work devel-
ops heuristic expressions via Genetic Programming (GP),
spanning early program-evolution frameworks (Koza, 1994;
Burke et al., 2009; 2010b) and more recent applications
to routing and packing problems (Hildebrandt et al., 2010;
Duflo et al., 2019). These methods are often paired with
automatic configuration frameworks such as SMAC and
irace (Hutter et al., 2011; L´opez-Ib´a˜nez et al., 2016), but
their low-level search spaces limit scalability and prevent
the exploitation of high-level algorithmic structures.
The integration of LLMs into AHD has transformed the
field, enabling the synthesis of executable heuristics through
semantic reasoning. A prominent paradigm is evaluation-
guided, iterative program evolution, exemplified by a range
of representative works, including early frameworks such
as AEL (Liu et al., 2023b) and FunSearch (Romera-Paredes
et al., 2024), as well as EoH (Liu et al., 2024a) and more
recent systems like AlphaEvolve (Novikov et al., 2025).
Meanwhile, works like (Ye et al., 2024; Liu et al., 2025c)
further enrich this process by incorporating verbal reflection
and historical experience to guide improvement, reflecting
a broader trend toward iterative self-refinement (Madaan
et al., 2023; Shinn et al., 2023). Expanding the scope fur-
ther, research has investigated the automatic generation and
evolution of metaheuristic algorithms and operators using
LLMs across various optimization paradigms (Pluhacek
et al., 2023; van Stein & B¨ack, 2024; Hemberg et al., 2024;
Dat et al., 2025; Yao et al., 2025; Surina et al., 2025).
However, these approaches remain predominantly evaluator-
static, optimizing against predefined task distributions and
overlooking the potential of co-evolutionary adaptation.
2.2. Generalization and Robustness in Combinatorial
Optimization
Generalization has long been a central focus in Neural Com-
binatorial Optimization (NCO). Prior work explores cur-
riculum learning (Lisicki et al., 2020; Zhang et al., 2022;
Iklassov et al., 2023; Liu et al., 2024b), distributional di-
versification (Jiang et al., 2022; Zhou et al., 2023; Luo
et al., 2023), and hardness-aware or adversarial sampling
(Zhang et al., 2022), including game-theoretic adversarial
variants (Wang et al., 2022; 2024). Furthermore, meta-
learning (Manchanda et al., 2022; Qiu et al., 2022; Wang &
Li, 2022; Son et al., 2023; Chen et al., 2023) and knowledge
distillation (Bi et al., 2022; Zhang et al., 2023; Zheng et al.,
2025) have been widely adopted to enhance robustness. No-
tably, these lines of work primarily operate by optimizing
continuous parameters within neural architectures, rather
than evolving symbolic, executable programs.
Recent LLM-AHD studies examine generalization via
heuristic set diversification, cross task transfer, and problem
2


--- Page 3 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
agnostic modeling (Jiang et al., 2024; Shi et al., 2025; Liu
et al., 2025b; Chen et al., 2025). Most, however, rely on
static distributions. While EALG introduces an initial GAN-
style solver–generator loop (Duan et al., 2025), its self-play
formulation does not explicitly maintain a persistent strategy
set on either side. This makes the dynamics more prone
to cycling (Czarnecki et al., 2020) and limits the ability to
form a stable, equilibrium-driven curriculum across diverse
instance families, which is precisely the gap that our ASRO
framework aims to address.
2.3. Game-Theoretic Response Frameworks
Game-theoretic response frameworks comprise a family of
methods that iteratively expand a game’s strategy sets by
solving restricted games and introducing best responses.
Classical approaches like Fictitious Play, Double Oracle,
and empirical game-theoretic analysis established this foun-
dation (Brown, 1951; McMahan et al., 2003; Wellman,
2006; Shoham & Leyton-Brown, 2008), while PSRO later
instantiated these principles in deep multi-agent reinforce-
ment learning by maintaining and updating sets of poli-
cies via iterative best-response training (Lanctot et al.,
2017). This line of work was subsequently refined by im-
proved meta-solvers and ranking mechanisms—exemplified
by Alpha-PSRO (Muller et al., 2019) and Nash averaging
(Balduzzi et al., 2018; 2019)—and scaled to complex envi-
ronments via AlphaStar’s league-based training (Jaderberg
et al., 2017; Vinyals et al., 2019). In contrast to these ap-
proaches, which operate on parametric neural policies, our
work adapts the response-oracle paradigm to the domain of
program synthesis, where solver logic and instance genera-
tors co-evolve as executable programs.
3. ASRO: A Program Space Co-Evolution
Framework for AHD
3.1. Preliminaries: Policy-Space Response Oracles
PSRO (Lanctot et al., 2017) is a response-based framework
for N-player games that operates by iteratively expanding
a restricted set of strategies Π(t)
i
⊂Πi for each player
i = 1, . . . , N. The procedure follows an iterative “solve-
and-expand” workflow. At iteration t, a meta-strategy σ(t)
i
is computed by solving the game restricted to the current
strategy sets, where σ(t)
i
is a probability distribution over
Π(t)
i
and σ(t)
−i denotes the joint meta-strategy of all players
except player i. Subsequently, a Best-Response (BR) oracle
expands the strategy set by finding
πi,new = arg max
πi∈Πi Eπ−i∼σ(t)
−i [ui(πi, π−i)] ,
(1)
where ui(πi, π−i) is the expected utility of player i. In
PSRO, strategies are typically parameterized by neural poli-
cies, with BRs approximated via reinforcement learning.
Details are provided in Appendix A.
3.2. A Program-Space Game Formulation of AHD
Our key observation is that AHD implicitly defines an in-
teraction between two programmatic choices: a solver pro-
gram that determines how solutions are constructed for a
CO instance, and an instance-generation program that de-
termines which instances are used for evaluation. Based on
this observation, we introduce the ASRO framework, which
extends PSRO from policy spaces to the space of symbolic,
executable programs.
Strategy Spaces.
The Solver Space S contains solver
programs s, each of which specifies heuristic decision logic
that is executed within a fixed, domain-specific solver pro-
cedure to construct a feasible solution for a given instance x.
The Generator Space G contains generator programs g that
specify stochastic instance distributions; sampling x ∼g(·)
yields structurally valid problem instances generated from
the program’s specification.
Payoff Function.
We model the interaction between
solver and generator programs as a two-player zero-sum
game defined over a task-specific normalized reference gap,
a standard performance measure in CO (Beasley, 1990). For
a solver program s and an instance x, let
gap(s, x) = V (s, x) −v∗(x)
v∗(x)
(2)
denote the normalized gap with respect to a reference value
v∗(x), where V (s, x) is the objective value produced by
solver s on instance x, and v∗(x) denotes a task-dependent
reference value. Smaller gaps indicate better solver per-
formance. The payoff of a solver–generator pair (s, g) is
defined as the expected gap under the generator-induced
instance distribution:
U(s, g) = Ex∼g

gap(s, x)

.
(3)
The solver seeks to minimize U(s, g), while the generator
seeks to maximize it.
3.3. Meta-Strategy Computation
Given the current solver and generator strategy pools S(t)
and G(t), ASRO constructs a restricted payoff matrix M (t),
where each entry is
M (t)
ij = U(si, gj).
(4)
Since solvers minimize gap while generators maximize it,
the interaction forms a finite zero-sum matrix game. At iter-
ation t, ASRO computes mixed meta-strategies (σ(t)
s , σ(t)
g )
3


--- Page 4 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Current Population & Meta-Game
Solver Strategy Pool 
Generator Strategy Pool 
Evaluate &
 Payoff
Matrix 
Meta-Solver
Meta-Strategy 
Meta-Strategy 
LLM Best-Response Oracles
⋮
…
⋱
⋮
…
   Solver
LLM
     Solver Synthesis 
Evaluate against 
Candidate
Population
Gap Signal
   Generator
LLM
Evaluate against 
Candidates
Population
Generator Synthesis
Gap Signal
Strategy
Pool
Update
New Solver
 
New Generator
Next Iteration 
Figure 1. Overview of ASRO. The iterative process is structured into three phases. (Left) The current solver strategy pool S(t) and
generator strategy pool G(t) are evaluated pairwise to compute a payoff matrix M (t). A meta-solver then computes mixed meta-strategies
(σ(t)
s , σ(t)
g ) over the existing strategy pools. (Middle) Program-space best-response oracles synthesize new solver and generator programs
conditioned on the opponent’s current meta-strategy, instantiated by an underlying program search mechanism (see Figure 2). (Right) The
newly discovered solver st+1 and generator gt+1 are added to their respective strategy pools for the next iteration (t →t + 1), enabling
systematic exploration of the program space.
by approximately solving the minimax equilibrium
min
σs max
σg
Es∼σs, g∼σg

M (t)
sg

.
(5)
For finite zero-sum games, this equilibrium can be obtained
via the standard linear-program formulation of the minimax
problem or, equivalently, via no-regret dynamics such as
multiplicative weights (Freund & Schapire, 1997). The
resulting meta-strategies provide opponent distributions for
the subsequent best-response synthesis.
3.4. Program-Space Best-Response Oracle
To optimize the best-response objective in Eq. (1) over ex-
ecutable programs, ASRO assumes access to a program-
space best-response oracle, implemented by a program
search mechanism. Given the opponent’s meta-strategy, this
oracle approximately optimizes the induced best-response
objective and returns a corresponding program, referred
to as an approximate best response (ABR). When the re-
sponding player is a solver or a generator, we write the
returned programs as ABRs(σg) and ABRg(σs), respec-
tively. ASRO is agnostic to how this search is performed,
enabling modular instantiations. We denote an instantia-
tion using mechanism X as ASRO-X. X may correspond
to a mechanism derived from or inspired by existing LLM-
based algorithm discovery methods, without inheriting their
original optimization objectives or training pipelines.
Instantiation with Evolutionary Search (ASRO-EoH).
In this work, ASRO instantiates the best-response oracle
via a bounded evolutionary program search derived from
Evolution of Heuristics (EoH) (Liu et al., 2024a), referred to
as ASRO-EoH; an overview is shown in Figure 2. The oracle
maintains a population of K heuristic programs, which is
initialized at the first ASRO iteration and warm-started from
previously discovered programs thereafter. Within a single
oracle call, the population is evolved for a fixed number of R
rounds using structured exploration and mutation operators
to generate offspring programs, followed by a selection step
over the current population and newly generated variants
that balances performance under the best-response objective
and population diversity. After the search terminates, the
highest-performing program is returned as an approximate
best response and used to expand the ASRO strategy pool.
Implementation details are provided in Appendix B.
3.5. Training Procedure and Analysis
ASRO (Figure 1) is an iterative framework that alternates
between solving a restricted program-level meta-game and
expanding the strategy pools via LLM-based approximate
best responses. At each iteration, ASRO updates the payoff
matrix over the current solver and generator strategy pools,
computes equilibrium meta-strategies, and invokes best-
response oracles to synthesize new solver and generator
programs. The full procedure is summarized in Algorithm 1.
4. Experiments
4.1. Experimental Setup
Common Settings. We use DeepSeek-V3.2 (Liu et al.,
2025a) as the backbone LLM for both solver and generator
synthesis, with a temperature of 1, promoting exploration in
4


--- Page 5 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Selection: Select parents based on score
Evaluation: Assess Against 
Population Filter and Update
Final Heuristic
Evolution Loop - Repeat R Generations
Initialization
                     Generate heuristic = (Thought, Code)
Code
Thought
  Variantion:
  Generate offspring
LLM
Figure 2. Program-space best-response oracle in ASRO, instan-
tiated via an EoH-style evolutionary program search. Under a
fixed opponent meta-strategy σ−i provided by ASRO, the oracle
performs a bounded evolutionary search over programs in the cor-
responding strategy space. Candidate programs are initialized,
explored, and mutated using structured LLM-based operators, and
evaluated according to the induced best-response objective.
Algorithm 1 ASRO: Program-Level Co-Evolution for AHD
1: Input: Initial pools S(0), G(0); iteration horizon T
2: for t = 0 to T −1 do
3:
(1) Meta-Game Evaluation
4:
Update payoff matrix M (t) over the strategy pools
5:
(2) Meta-Strategy Computation
6:
(σ(t)
s , σ(t)
g ) ←METASOLVER(M (t))
7:
(3) LLM-Based Approximate Best Responses
8:
snew ←ABRs(σ(t)
g )
(solver BR)
9:
gnew ←ABRg(σ(t)
s )
(generator BR)
10:
(4) Strategy Pool Expansion
11:
S(t+1) ←S(t) ∪{snew}
12:
G(t+1) ←G(t) ∪{gnew}
13: end for
14: Output: Final solver and generator strategy pools
program space. The co-evolutionary process runs for T = 8
iterations. During training, a small portion of instances is
always drawn from a fixed, domain-specific base generator
to stabilize early-stage co-evolution and avoid premature
over-specialization
Framework-level comparison with EoH. ASRO-EoH and
EoH use the same evolutionary program search mechanism;
the difference is that EoH is trained on a fixed distribution
induced by the base generator, whereas ASRO-EoH operates
under a game-based framework with an evolving instance
distribution induced by the generator strategy pool.
Evaluation Metric. Across all tasks, we evaluate perfor-
mance using the reference gap defined in Eq. (2).
Convergence Metrics.
We assess convergence using
exploitability-based metrics estimated using approximate
best responses, which quantify the gain from a unilateral
deviation. Given mixed strategies (σ(t)
s , σ(t)
g ) at iteration t,
the solver and generator exploitabilities are
Exps(t) = u(σ(t)
s , σ(t)
g ) −u(ABRs(σ(t)
g ), σ(t)
g ),
Expg(t) = u(σ(t)
s , ABRg(σ(t)
s )) −u(σ(t)
s , σ(t)
g ),
(6)
We report the Approximate NashConv (ANC), ANC(t) =
Exps(t) + Expg(t), which summarizes the exploitability of
the current solver–generator mixture (Timbers et al., 2022).
4.2. Domains and Implementation Details
We evaluate ASRO on three well-studied combinatorial opti-
mization problems: online bin packing (OBP), the traveling
salesman problem (TSP), and the capacitated vehicle routing
problem (CVRP), spanning online and offline optimization
as well as diverse decision structures. Hyperparameter set-
tings and LLM usage are summarized in Appendix D, while
task-specific formulations, prompt templates, and domain
details are provided in Appendix E, F, and G.
4.2.1. ONLINE BIN PACKING (OBP)
Problem Setting. We study online bin packing problem
(Garey & Johnson, 1981; Johnson et al., 1974), where items
arrive sequentially and must be assigned irrevocably to bins
of capacity C, with the objective of minimizing the total
number of bins used (Balogh et al., 2014; Seiden, 2002).
Solver Representation.
Solver programs are executed
within a fixed online packing framework. Each solver pro-
gram specifies an online greedy decision rule in the form
of a numerical priority function that assigns a score to each
feasible bin upon item arrival (Ramanan et al., 1989). The
packing process sequentially handles arriving items and en-
forces feasibility; all other components of the procedure are
kept fixed, with items placed into the feasible bin with the
highest score, without lookahead or post-processing.
Reference Values. v∗(x) are obtained using OR-Tools CP-
SAT when optimality can be certified (Google OR-Tools
Team, 2024); otherwise, we use the Martello–Toth lower
bound (Martello & Toth, 1990).
Benchmarks and Arrival Orders. We evaluate on widely
used public bin packing benchmarks (Delorme et al., 2018):
Falkenauer T and U families (Falkenauer, 1996), the Hard28
benchmark (Delorme et al., 2018), and additional instances
drawn from a range of Weibull distributions (Casti˜neiras
et al., 2012). These benchmarks cover both classical test
cases and structurally challenging settings. The base gen-
erator used during training follows a Weibull distribution.
We evaluate under multiple standard arrival orders (random,
ascending), fixed independently of the algorithms.
5


--- Page 6 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Baselines. We compare ASRO against canonical online
heuristics (First Fit (Ullman, 1971; D´osa & Sgall, 2013)
and Best Fit (Johnson et al., 1974; Albers et al., 2021)) and
EoH (Liu et al., 2024a), an LLM-based solver-only baseline.
4.2.2. TRAVELING SALESMAN PROBLEM (TSP)
Problem Setting. We consider the Euclidean Traveling
Salesman Problem with N cities, where the objective is to
find a Hamiltonian tour of minimum total length.
Solver Representation. Solver programs operate within a
fixed Guided Local Search (GLS) framework (Voudouris
& Tsang, 1999; Liu et al., 2025c). Each solver specifies an
edge-distance update rule, producing a modified distance
matrix for evaluating candidate moves. All other compo-
nents of GLS, including the search loop and standard local
search operators (e.g., 2-opt, relocate), are kept fixed, while
the learned update reshapes the search landscape.
Reference Values. v∗(x) are obtained using the Concorde
solver for all instances considered (Applegate et al., 2011).
Benchmarks. Evaluation is conducted on held-out uni-
formly sampled 100-node instances and TSPLIB (Reinelt,
1991), covering homogeneous Euclidean cases and struc-
tured instances with diverse geometric properties. The base
generator samples instances uniformly.
Baselines.
We compare ASRO against classical con-
structive heuristics (Nearest and Farthest Insertion; NI,
FI) (Rosenkrantz et al., 1977) and EoH (Liu et al., 2024a).
4.2.3. CAPACITATED VEHICLE ROUTING PROBLEM
Problem Setting. We study the CVRP, where a fleet of
identical vehicles departs from and returns to a single depot
to serve customers with known demands. Each customer
must be visited exactly once, and the total demand served
by any vehicle cannot exceed the vehicle capacity Q. The
objective is to minimize the total travel cost over all routes.
Solver Representation.
Solver programs are executed
within a fixed greedy route-construction procedure. Each
solver program specifies a numerical priority function that
assigns a score to each currently feasible customer at a rout-
ing step. The customer with the highest score is selected
and appended to the current route.
Reference Values. Reference values v∗(x) are obtained
using PyVRP (Wouda et al., 2024) under a fixed time budget,
serving as high-quality heuristic reference solutions and not
necessarily global optima.
Benchmarks. We evaluate on CVRP benchmark instances
from CVRPLIB (Augerat et al., 1995; Uchoa et al., 2017),
including the A, B, E, F, M, P, and X families. These in-
stance families span a broad range of problem sizes and
OBP
Exploitability / ANC
TSP
CVRP
Gap (%)
Figure 3. ASRO training dynamics across combinatorial opti-
mization tasks. (Top) Convergence behavior of ASRO on OBP,
TSP, and CVRP, measured by solver exploitability, generator ex-
ploitability, and ANC over iterations, shown for a representative
ASRO run; shaded regions denote confidence intervals via boot-
strap resampling over instances (20 per generator). (Bottom)
Stability across random runs: test gap (%) (mean ± std) over five
independent runs on the hardest benchmark for each task, where
benchmark difficulty is determined by the EoH baseline ranking.
structural characteristics, covering both relatively simple
and highly challenging routing scenarios. The base genera-
tor samples customer coordinates uniformly from [0, 100]2
and customer demands uniformly from [1, Q/3].
Baselines. We compare ASRO with several classical CVRP
heuristics, including nearest-neighbor construction with lo-
cal improvement (NN+2-opt) and parallel insertion heuris-
tics (PI) (Liu et al., 2023a), as well as EoH (Liu et al.,
2024a).
4.3. Results
Figure 3 summarizes the empirical training dynamics and
stability of ASRO across different combinatorial optimiza-
tion tasks. Across domains, ANC exhibits a general down-
ward trend over iterations, while solver and generator ex-
ploitability remain bounded. Meanwhile, performance, mea-
sured by the optimality gap on test benchmarks, improves
consistently across random runs despite stochastic LLM syn-
thesis and approximate best-response updates. Task-specific
differences are evident: OBP exhibits stronger local fluctu-
ations due to its discrete and non-smooth payoff structure;
TSP shows comparatively smoother dynamics consistent
with its geometric formulation and local search procedures;
and CVRP admits greater room for solver–generator adap-
tation, reflecting its intrinsic structure induces a non-trivial
trade-off between minimizing route cost and maintaining
capacity feasibility, which gives rise to more persistent adap-
tive interactions without destabilizing overall convergence.
OBP
Table 1 summarizes the performance on standard
online bin packing benchmarks. Across all settings, ASRO
consistently outperforms the EoH baseline. As the instance
6


--- Page 7 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
 
: Best-fit baseline
 
 Phase-aware residual shaping
 
: Fit-aware scoring
 
: Global-fill leftover penalty
: Weibull distribution
: Size-cluster fragmentation
: Clustered sizes with strategic interleaving
 
: Global-fill adaptive scoring
def score(item, bins):
   r = bins - item                # remaining capacity
   u = global_fill(bins)           # global utilization ∈ [0,1]
   fit = exp(- r / item)                     # local tightness
   s = 1 / (1 + exp(-8 * (u - 0.5)))
   τ = 0.10 + 0.25 * s                # adaptive target fill
   dev = abs(item / bins - τ)      # deviation from target
   risk = (1 + 2.0*u) * dev**1.5 # pressure-aware penalty
   return fit - risk
: Near-capacity stress test    
1. Exact-fit clusters      → perfectly packable groups
2. Near-fit perturbation → break greedy commitments
3. Decoy complements → induce awkward leftovers
4. Structured shuffle     → preserve local traps
Evolution
Evolution
: Mixed adversarial patterns
Figure 4. A snapshot of solver–generator co-evolution in online
bin packing. The figure shows a utility matrix at Round 5, with
meta-strategies computed from Round 4. Side panels summarize
the solver and generator strategies present in the pool at Round 4.
In this snapshot, the generator-side meta-strategy assigns dominant
weight to g4, and the solver s5 is generated as a best response
under this adversarial pressure. The dashed connection indicates
that s5 is generated as a best response to g4. The highlighted entry
(red box) shows the utility of s5 evaluated against g4.
Table 1. Main OBP results (gap % ↓). Gap to the lower bound
on standard OBP benchmarks. F-T and F-U denote the Falkenauer
random-arrival sets. Hard28-R and Hard28-SA correspond to
random and size-ascending arrivals on the Hard28 benchmark.
Method
F-U
F-T
Hard28-R Hard28-SA Weibull
Best Fit
5.39 12.66
8.45
40.02
1.75
First Fit
6.21 12.63
9.88
40.02
2.02
EoH
5.00 12.11
8.40
40.02
1.56
ASRO-EoH 4.53
7.11
8.06
12.45
1.20
structure becomes more challenging—from the simpler
Falkenauer U set, to the more structured Falkenauer T set,
and further to the Hard28 benchmark—the performance gap
between ASRO and EoH tends to increase. This trend sug-
gests that adversarial co-evolution yields greater benefits
on harder and more structured instances. Importantly, even
on the Weibull distribution used to train EoH, ASRO does
not exhibit performance degradation, suggesting improved
robustness without sacrificing in-distribution performance.
Figure 4 provides a view of how ASRO induces solver
adaptation through generator pressure. The generator meta-
strategy concentrates on g4, which constructs item se-
quences that appear easy to pack early on but systemati-
cally result in fragmented bin capacity at later stages. In
this setting, such residual-trap structures are particularly
harmful to locally greedy solvers, whose irrevocable early
decisions leave little flexibility for future items. Facing
this concentrated stress pattern, the solver-side oracle syn-
thesizes s5 as an approximate best response. Compared
to earlier heuristics, s5 appears to trade off immediate fit
against future leftover risk, favoring packing decisions that
preserve global flexibility under near-capacity conditions.
This interaction provides an illustrative example of the core
mechanism of ASRO: generators expose concrete failure
Table 2. Main TSP results (gap % ↓). Average optimality gap on
a synthetic uniform distribution and on TSPLIB instances grouped
by size: LIB-S (n ≤200), LIB-M (201 ≤n ≤500), LIB-L
(501 ≤n ≤1000), and LIB-XL (n > 1000).
Method
LIB-S
LIB-M
LIB-L
LIB-XL
Uniform
NI
19.84
23.51
23.03
21.22
20.88
FI
7.30
10.49
13.30
15.19
8.54
EoH
0.77
1.99
3.40
4.20
0.27
ASRO-EoH
0.21
0.49
1.43
3.00
0.05
Table 3. Main CVRP results (gap % ↓). Average optimality gap
across CVRPLib instance families (A, B, E, F, M, P, and X).
Method
A
B
E
F
M
P
X
NN + 2-opt
35.58 41.30 34.81 46.53 47.14 28.24 22.71
PI
41.82 39.39 63.07 43.73 95.15 51.48 58.34
EoH
20.19 12.22 41.83 59.78 54.22 23.68 25.71
ASRO-EoH 16.85 10.49 20.21 28.71 22.45 19.45 13.65
modes of the solver mixture, and solvers adapt by internal-
izing defenses against these modes in program space.
TSP
Table 2 shows that ASRO consistently outperforms
the EoH baseline on both simple uniform instances and
more complex TSPLIB benchmarks. Moreover, as problem
size increases, the performance gap between ASRO and
EoH widens, indicating stronger scalability and improved
generalization to larger instances, even though both meth-
ods are trained with generators limited to the same problem
size. On several instances, including PR107, PR136, PR144,
TSP225, U159, and KROB100, ASRO attains small nega-
tive optimality gaps relative to reported best-known solution
(BKS) values for TSPLIB instances (Reinelt, 1991), sug-
gesting highly competitive solution quality under the same
evaluation protocol. Overall, these results demonstrate that
ASRO achieves robust and scalable performance across TSP
instances of varying structural complexity and problem size.
CVRP
Table 3 reports average optimality gaps on CVR-
PLib instance families. While A and B consist of relatively
regular and well-structured instances, E, F, M, P, and X
exhibit substantially higher heterogeneity in customer distri-
butions, route lengths, and capacity profiles. ASRO achieves
the lowest gap across all families, performing strongly on
both simpler and more complex instances, indicating robust
performance across diverse CVRP instance structures.
4.4. Ablation Study
Data Augmentation and Self-play.
To examine whether
the gains of ASRO can be attributed to stronger training
data or to adversarial interaction alone, we compare ASRO
7


--- Page 8 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
against two EoH-based baselines: (i) EoH with data augmen-
tation (DA) and (ii) an EoH self-play variant. In both cases,
the program search mechanism is identical to ASRO’s, en-
suring that differences arise from instance distributions or
interaction dynamics rather than from search.
For DA, the solver is trained on a fixed mixture of ge-
ometric instance families, including uniform, clustered,
grid-like, ring-shaped, and multi-scale distributions, provid-
ing substantially richer distributional diversity than single-
distribution training. Full sampling details are provided
in Appendix H.1.
The self-play baseline removes the
game-theoretic structure of ASRO while retaining solver–
generator optimization. At each iteration, the solver and
generator are optimized against each other, but no historical
strategies are retained. In ASRO terminology, this corre-
sponds to discarding the strategy pool and restricting both
players to their most recent strategies, resulting in a greedy,
memoryless adversarial loop.
As shown in Appendix Table 6, both data augmentation
and self-play yield clear improvements over standard EoH,
indicating that increased distributional diversity and direct
adversarial interaction are beneficial for heuristic learning.
However, both variants remain inferior to ASRO, especially
on larger and more structured TSPLIB instances. This sug-
gests that while these factors contribute to stronger solvers,
they are insufficient on their own: the persistent strategy
pool and meta-game dynamics in ASRO play a key role in
maintaining sustained pressure and robust co-evolution.
Effect of LLM Backbones. Appendix Table 7 reports
ASRO-EoH with different LLM backbones for program
search. While stronger backbones achieve modestly lower
gaps on TSPLIB instances, smaller models remain effective.
Notably, ASRO-EoH with LLaMA-3-8B (Grattafiori et al.,
2024) outperforms the EoH baseline trained with a stronger
backbone (DeepSeek) on most evaluation settings, indicat-
ing that ASRO’s gains primarily stem from solver–generator
co-evolution rather than backbone scale.
Oracle-agnosticity of ASRO. All main experiments instan-
tiate the program-space best-response oracle using an EoH-
style program search mechanism; however, ASRO does not
depend on any particular choice of search procedure. Re-
placing this instantiation with a ReEvo-style mechanism (Ye
et al., 2024) consistently improves over the corresponding
ReEvo baseline on TSPLIB (Appendix Table 8), indicating
that ASRO’s gains arise from its response-based evaluation
structure rather than from any particular search mechanism.
Computation Cost Analysis. ASRO incurs additional com-
putational cost in best-response search, since each itera-
tion requires LLM-driven program-space exploration un-
der opponent mixed strategies and repeated evaluations of
candidate programs. However, this cost is largely amor-
tized by parallel and batched evaluation, as detailed in Ap-
pendix C. To better characterize this overhead, we report
an equal-time comparison between ASRO and EoH in Ap-
pendix Table 9, where both methods are evaluated under
the same wall-clock time budget. Under this constraint,
ASRO consistently achieves lower optimality gaps across
all regimes. One key factor is that EoH converges quickly
under a fixed instance distribution, resulting in diminishing
returns, whereas ASRO maintains additional improvement
capacity through alternating solver and generator updates.
5. Discussion
Summary.
This paper proposed ASRO, a game-theoretic
framework for automatic heuristic discovery that replaces
static evaluation with adaptive, interaction-driven assess-
ment by modeling solver–generator interaction as a two-
player zero-sum game with persistent strategy pools. We
evaluated ASRO on three representative combinatorial opti-
mization problems, including online bin packing, the travel-
ing salesman problem, and the capacitated vehicle routing
problem. Experimental results show that ASRO consis-
tently discovers heuristics that outperform the EoH base-
line, which is trained on fixed instance distributions using
the same LLM-based search procedure, and demonstrates
strong robustness across diverse and structured benchmark
instances. In practice, ASRO trades additional computation
for improved robustness, while its evaluation and search pro-
cedures remain naturally amenable to large-scale paralleliza-
tion. Overall, ASRO establishes a principled game-theoretic
framework that formulates heuristic discovery as an interac-
tive process over executable programs, enabling robustness
and generalization beyond fixed evaluation settings.
Limitations and future work. ASRO currently relies on
reliable performance signals to compare generator strate-
gies, yet for many combinatorial optimization instances
exact optima are unavailable; in such cases, optimality gaps
must be computed against strong but incomplete or time-
limited oracles (e.g., CP-SAT bounds (Google OR-Tools
Team, 2024) or feasible solutions from PyVRP (Wouda
et al., 2024)). Such approximations introduce estimation
noise into the meta-game and may obscure fine-grained
distinctions among generators. Looking forward, ASRO
can be extended beyond a strictly zero-sum formulation
to multi-objective or regularized meta-games that explic-
itly trade off hardness, diversity, and realism for more con-
trolled distribution shaping; it can also be extended to sup-
port teacher–student interactions, where generators serve as
adaptive curriculum designers rather than purely adversarial
opponents; and more broadly, the program-space game per-
spective underlying ASRO may generalize beyond combina-
torial optimization to settings such as algorithmic reasoning,
planning, and symbolic decision-making.
8


--- Page 9 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Albers, S., Khan, A., and Ladewig, L. Best fit bin packing
with random order revisited. Algorithmica, 83(9):2833–
2858, 2021.
Alsheddy, A., Voudouris, C., Tsang, E. P. K., and Alhindi,
A. Guided local search. In Handbook of Heuristics, pp.
261–297. Springer, Cham, 2018.
Applegate, D. L., Bixby, R. E., Chv´atal, V., and Cook, W. J.
The traveling salesman problem: a computational study.
In The Traveling Salesman Problem. Princeton university
press, 2011.
Arnold, F. and S¨orensen, K. Knowledge-guided local search
for the vehicle routing problem. Computers & Operations
Research, 105:32–46, 2019.
Augerat, P., Naddef, D., Belenguer, J. M., Benavent, E.,
Corberan, A., and Rinaldi, G. Computational results with
a branch and cut code for the capacitated vehicle routing
problem. Technical report, 1995.
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732, 2021.
Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls,
K., and Graepel, T. The mechanics of n-player differen-
tiable games. In International Conference on Machine
Learning, pp. 354–363. PMLR, 2018.
Balduzzi, D., Garnelo, M., Bachrach, Y., Czarnecki, W.,
Perolat, J., Jaderberg, M., and Graepel, T. Open-ended
learning in symmetric zero-sum games. In International
Conference on Machine Learning, pp. 434–443. PMLR,
2019.
Balogh, J., B´ek´esi, J., D´osa, G., Sgall, J., and Stee, R. v. The
optimal absolute ratio for online bin packing. In Proceed-
ings of the twenty-sixth annual ACM-SIAM symposium
on discrete algorithms, pp. 1425–1438. SIAM, 2014.
Beasley, J. E. Or-library: distributing test problems by elec-
tronic mail. Journal of the operational research society,
41(11):1069–1072, 1990.
Bengio, Y., Lodi, A., and Prouvost, A. Machine learning
for combinatorial optimization: A methodological tour
d’horizon. European Journal of Operational Research,
290(2):405–421, 2021. doi: 10.1016/j.ejor.2020.07.063.
Bi, J., Ma, Y., Wang, J., Cao, Z., Chen, J., Sun, Y., and Chee,
Y. M. Learning generalizable models for vehicle routing
problems via knowledge distillation. Advances in Neural
Information Processing Systems, 35:31226–31238, 2022.
Blum, C. and Roli, A. Metaheuristics in combinatorial opti-
mization: Overview and conceptual comparison. ACM
computing surveys (CSUR), 35(3):268–308, 2003.
Brown, G. W. Iterative solution of games by fictitious play.
Activity analysis of production and allocation, 13(1):374,
1951.
Burke, E., Kendall, G., Newall, J., Hart, E., Ross, P., and
Schulenburg, S. Hyper-heuristics: An emerging direc-
tion in modern search technology.
In Handbook of
Metaheuristics, pp. 457–474. Springer US, 2003. doi:
10.1007/0-306-48056-5 16.
Burke, E. K., Hyde, M. R., Kendall, G., Ochoa, G., Oz-
can, E., and Woodward, J. R. Exploring hyper-heuristic
methodologies with genetic programming. In Computa-
tional intelligence: Collaboration, fusion and emergence,
pp. 177–201. Springer, 2009.
Burke, E. K., Hyde, M., Kendall, G., Ochoa, G., ¨Ozcan, E.,
and Woodward, J. R. A classification of hyper-heuristic
approaches. In Gendreau, M. and Potvin, J.-Y. (eds.),
Handbook of Metaheuristics, pp. 449–468. Springer US,
2010a.
Burke, E. K., Hyde, M., Kendall, G., and Woodward, J. A
genetic programming hyper-heuristic approach for evolv-
ing 2-d strip packing heuristics. IEEE Transactions on
Evolutionary Computation, 14(6):942–958, 2010b.
Burke, E. K., Gendreau, M., Hyde, M., Kendall, G., Ochoa,
G., ¨Ozcan, E., and Qu, R. Hyper-heuristics: A survey of
the state of the art. Journal of the Operational Research
Society, 64(12):1695–1724, 2013.
Burke, E. K., Hyde, M. R., Kendall, G., Ochoa, G., ¨Ozcan,
E., and Woodward, J. R.
A classification of hyper-
heuristic approaches: revisited. In Handbook of meta-
heuristics, pp. 453–477. Springer, 2018.
Casti˜neiras, I., De Cauwer, M., and O’Sullivan, B. Weibull-
based benchmarks for bin packing. In International Con-
ference on Principles and Practice of Constraint Pro-
gramming, pp. 207–222. Springer, 2012.
Chen, J., Wang, J., Zhang, Z., Cao, Z., Ye, T., and Chen, S.
Efficient meta neural heuristic for multi-objective com-
binatorial optimization. Advances in Neural Information
Processing Systems, 36:56825–56837, 2023.
9


--- Page 10 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Chen, Y., Chen, R., Luo, F., and Wang, Z. Improving gener-
alization of neural combinatorial optimization for vehicle
routing problems via test-time projection learning. In The
Thirty-Ninth Annual Conference on Neural Information
Processing Systems, October 2025.
Czarnecki, W. M., Gidel, G., Tracey, B., Tuyls, K., Omid-
shafiei, S., Balduzzi, D., and Jaderberg, M. Real world
games look like spinning tops. Advances in Neural Infor-
mation Processing Systems, 33:17443–17454, 2020.
Dat, P. V. T., Doan, L., and Binh, H. T. T. Hsevo: Elevating
automatic heuristic design with diversity-driven harmony
search and genetic algorithm using llms. In Proceed-
ings of the AAAI Conference on Artificial Intelligence,
volume 39, pp. 26931–26938, 2025.
Delorme, M., Iori, M., and Martello, S. Bpplib: a library
for bin packing and cutting stock problems. Optimization
Letters, 12(2):235–250, 2018.
D´osa, G. and Sgall, J. First fit bin packing: A tight analysis.
In 30th International symposium on theoretical aspects
of computer science (STACS 2013), pp. 538–549. Schloss
Dagstuhl–Leibniz-Zentrum fuer Informatik, 2013.
Drake, J. H., Kheiri, A., ¨Ozcan, E., and Burke, E. K. Recent
advances in selection hyper-heuristics. European Journal
of Operational Research, 285(2):405–428, 2020.
Duan, R., Liu, Y., Dong, X., and Fan, C. Ealg: Evolu-
tionary adversarial generation of language model-guided
generators for combinatorial optimization. arXiv preprint
arXiv:2506.02594, 2025.
Duflo, G., Kieffer, E., Brust, M. R., Danoy, G., and Bou-
vry, P. A gp hyper-heuristic approach for generating tsp
heuristics. In 2019 IEEE International Parallel and Dis-
tributed Processing Symposium Workshops (IPDPSW),
pp. 521–529. IEEE, 2019.
Falkenauer, E. A hybrid grouping genetic algorithm for bin
packing. Journal of heuristics, 2(1):5–30, 1996.
Freund, Y. and Schapire, R. E. A decision-theoretic general-
ization of on-line learning and an application to boosting.
Journal of computer and system sciences, 55(1):119–139,
1997.
Garey, M. R. and Johnson, D. S. Approximation algorithms
for bin packing problems: A survey. In Analysis and
design of algorithms in combinatorial optimization, pp.
147–172. Springer, 1981.
Google OR-Tools Team.
Google or-tools,
2024.
URL
https://developers.google.com/
optimization.
Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian,
A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A.,
Vaughan, A., et al. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783, 2024.
Hemberg, E., Moskal, S., and O’Reilly, U.-M. Evolving
code with a large language model. Genetic Programming
and Evolvable Machines, 25(2):21, 2024.
Hildebrandt, T., Heger, J., and Scholz-Reiter, B. Towards
improved dispatching rules for complex shop floor scenar-
ios: A genetic programming approach. In Proceedings of
the 12th Annual Conference on Genetic and Evolutionary
Computation, pp. 257–264. Association for Computing
Machinery, 2010.
Hutter, F., Hoos, H. H., and Leyton-Brown, K. Sequential
model-based optimization for general algorithm config-
uration. In International conference on learning and
intelligent optimization, pp. 507–523. Springer, 2011.
Iklassov, Z., Medvedev, D., De Retana, R. S. O., and Takac,
M. On the study of curriculum learning for inferring
dispatching policies on the job shop scheduling. In IJCAI,
pp. 5350–5358, 2023.
Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M.,
Donahue, J., Razavi, A., Vinyals, O., Green, T., Dunning,
I., Simonyan, K., et al. Population based training of
neural networks. arXiv preprint arXiv:1711.09846, 2017.
Jiang, C., Shu, X., Qian, H., Lu, X., Zhou, J., Zhou, A.,
and Yu, Y. Llmopt: Learning to define and solve general
optimization problems from scratch. In The Thirteenth
International Conference on Learning Representations,
October 2024.
Jiang, Y., Wu, Y., Cao, Z., and Zhang, J. Learning to solve
routing problems via distributionally robust optimization.
In Proceedings of the AAAI conference on artificial intel-
ligence, volume 36, pp. 9786–9794, 2022.
Johnson, D. S., Demers, A., Ullman, J. D., Garey, M. R., and
Graham, R. L. Worst-case performance bounds for simple
one-dimensional packing algorithms. SIAM Journal on
computing, 3(4):299–325, 1974.
Koza, J. R. Genetic programming as a means for program-
ming computers by natural selection. Statistics and com-
puting, 4(2):87–112, 1994.
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A.,
Tuyls, K., P´erolat, J., Silver, D., and Graepel, T. A uni-
fied game-theoretic approach to multiagent reinforcement
learning. Advances in neural information processing
systems, 30, 2017.
10


--- Page 11 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Lisicki, M., Afkanpour, A., and Taylor, G. W. Evaluating
curriculum learning strategies in neural combinatorial op-
timization. In Learning Meets Combinatorial Algorithms
at NeurIPS2020, November 2020.
Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu,
B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3.
2: Pushing the frontier of open large language models.
arXiv preprint arXiv:2512.02556, 2025a.
Liu, F., Lu, C., Gui, L., Zhang, Q., Tong, X., and Yuan,
M. Heuristics for vehicle routing problem: A survey
and recent advances. arXiv preprint arXiv:2303.04147,
2023a.
Liu, F., Tong, X., Yuan, M., and Zhang, Q. Algorithm
evolution using large language model. arXiv preprint
arXiv:2311.15249, 2023b.
Liu, F., Xialiang, T., Yuan, M., Lin, X., Luo, F., Wang, Z.,
Lu, Z., and Zhang, Q. Evolution of heuristics: Towards
efficient automatic algorithm design using large language
model. In Proceedings of the 41st International Confer-
ence on Machine Learning, pp. 32201–32223. PMLR,
July 2024a.
Liu, F., Liu, Y., Zhang, Q., Tong, X., and Yuan, M. Eoh-s:
Evolution of heuristic set using llms for automated heuris-
tic design. arXiv preprint arXiv:2508.03082, 2025b.
Liu, Y., Zhou, C., Zhang, P., Li, Z., Zhang, S., Lin, X.,
and Wu, X. Cl4co: A curriculum training framework for
graph-based neural combinatorial optimization. In 2024
IEEE International Conference on Data Mining (ICDM),
pp. 779–784. IEEE, 2024b.
Liu, Y., Li, J., Zhao, W. X., Lu, H., and Wen, J.-R.
Experience-guided reflective co-evolution of prompts and
heuristics for automatic algorithm design. arXiv preprint
arXiv:2509.24509, 2025c.
L´opez-Ib´a˜nez, M., Dubois-Lacoste, J., P´erez C´aceres, L.,
Birattari, M., and St¨utzle, T. The irace package: Iterated
racing for automatic algorithm configuration. Operations
Research Perspectives, 3:43–58, 2016.
Luo, F., Lin, X., Liu, F., Zhang, Q., and Wang, Z. Neural
combinatorial optimization with heavy decoder: Toward
large scale generalization. Advances in Neural Informa-
tion Processing Systems, 36:8845–8864, 2023.
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao,
L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S.,
Yang, Y., et al. Self-refine: Iterative refinement with self-
feedback. Advances in Neural Information Processing
Systems, 36:46534–46594, 2023.
Manchanda, S., Michel, S., Drakulic, D., and Andreoli, J.-M.
On the generalization of neural combinatorial optimiza-
tion heuristics. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp.
426–442. Springer, 2022.
Martello, S. and Toth, P.
Lower bounds and reduction
procedures for the bin packing problem. Discrete applied
mathematics, 28(1):59–70, 1990.
McMahan, H. B., Gordon, G. J., and Blum, A. Planning in
the presence of cost functions controlled by an adversary.
In Proceedings of the 20th International Conference on
Machine Learning (ICML-03), pp. 536–543, 2003.
Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Per-
olat, J., Liu, S., Hennes, D., Marris, L., Lanctot, M.,
Hughes, E., Wang, Z., Lever, G., Heess, N., Graepel, T.,
and Munos, R. A generalized training approach for multi-
agent learning. In International Conference on Learning
Representations, 2019.
Novikov, A., V˜u, N., Eisenberger, M., Dupont, E., Huang,
P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz,
F. J., Mehrabian, A., et al. Alphaevolve: A coding agent
for scientific and algorithmic discovery. arXiv preprint
arXiv:2506.13131, 2025.
Papadimitriou, C. H. and Steiglitz, K. Combinatorial Opti-
mization: Algorithms and Complexity. Courier Corpora-
tion, 1998.
Pillay, N. and Qu, R. Hyper-heuristics: theory and applica-
tions. Springer, 2018.
Pluhacek, M., Kazikova, A., Kadavy, T., Viktorin, A., and
Senkerik, R.
Leveraging large language models for
the generation of novel metaheuristic optimization algo-
rithms. In Proceedings of the Companion Conference on
Genetic and Evolutionary Computation, pp. 1812–1820,
2023.
Qiu, R., Sun, Z., and Yang, Y. Dimes: A differentiable
meta solver for combinatorial optimization problems. Ad-
vances in Neural Information Processing Systems, 35:
25531–25546, 2022.
Ramanan, P., Brown, D. J., Lee, C.-C., and Lee, D.-T. On-
line bin packing in linear time. Journal of Algorithms, 10
(3):305–326, 1989.
Reinelt, G. Tsplib—a traveling salesman problem library.
ORSA journal on computing, 3(4):376–384, 1991.
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog,
M., Kumar, M. P., Dupont, E., Ruiz, F. J. R., Ellenberg,
J. S., Wang, P., Fawzi, O., Kohli, P., and Fawzi, A. Math-
ematical discoveries from program search with large lan-
guage models. Nature, 625(7995):468–475, 2024.
11


--- Page 12 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Rosenkrantz, D. J., Stearns, R. E., and Lewis, II, P. M. An
analysis of several heuristics for the traveling salesman
problem. SIAM journal on computing, 6(3):563–581,
1977.
Seiden, S. S. On the online bin packing problem. Journal
of the ACM (JACM), 49(5):640–671, 2002.
Shi, Y., Zhou, J., Song, W., Bi, J., Wu, Y., and Zhang,
J. Generalizable heuristic generation through large lan-
guage models with meta-optimization. arXiv preprint
arXiv:2505.20881, 2025.
Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and
Yao, S. Reflexion: Language agents with verbal rein-
forcement learning. Advances in Neural Information
Processing Systems, 36:8634–8652, 2023.
Shoham, Y. and Leyton-Brown, K. Multiagent systems: Al-
gorithmic, game-theoretic, and logical foundations. Cam-
bridge University Press, 2008.
Sim, K., Renau, Q., and Hart, E. Beyond the hype: Bench-
marking llm-evolved heuristics for bin packing, 2025.
Son, J., Kim, M., Kim, H., and Park, J. Meta-sage: Scale
meta-learning scheduled adaptation with guided explo-
ration for mitigating scale shift on combinatorial opti-
mization. In International Conference on Machine Learn-
ing, pp. 32194–32210. PMLR, 2023.
Surina, A., Mansouri, A., Quaedvlieg, L., Seddas, A., Via-
zovska, M., Abbe, E., and Gulcehre, C. Algorithm discov-
ery with llms: Evolutionary search meets reinforcement
learning. arXiv preprint arXiv:2504.05108, 2025.
Timbers, F., Bard, N., Lockhart, E., Lanctot, M., Schmid,
M., Burch, N., Schrittwieser, J., Hubert, T., and Bowling,
M. Approximate exploitability: Learning a best response.
In Thirty-First International Joint Conference on Artifi-
cial Intelligence, volume 4, pp. 3487–3493, 2022.
Toth, P. and Vigo, D. Vehicle routing: problems, methods,
and applications. SIAM, 2014.
Uchoa, E., Pecin, D., Pessoa, A., Poggi, M., Vidal, T., and
Subramanian, A. New benchmark instances for the ca-
pacitated vehicle routing problem. European Journal of
Operational Research, 257(3):845–858, 2017.
Ullman, J. The performance of a memory allocation algo-
rithm. 1971.
van Stein, N. and B¨ack, T.
Llamea: A large language
model evolutionary algorithm for automatically generat-
ing metaheuristics. IEEE Transactions on Evolutionary
Computation, 2024.
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,
Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,
T., Georgiev, P., et al. Grandmaster level in starcraft ii
using multi-agent reinforcement learning. nature, 575
(7782):350–354, 2019.
von Neumann, J. On the theory of games of strategy. Con-
tributions to the Theory of Games, 4(13-42):41, 1959.
Voudouris, C. and Tsang, E. Guided local search and its
application to the traveling salesman problem. European
journal of operational research, 113(2):469–499, 1999.
Wang, C., Yang, Y., Slumbers, O., Han, C., Guo, T., Zhang,
H., and Wang, J. A game-theoretic approach for im-
proving generalization ability of tsp solvers. In ICLR
2022 Workshop on Gamification and Multiagent Solu-
tions, 2022.
Wang, C., Yu, Z., McAleer, S., Yu, T., and Yang, Y. Asp:
Learn a universal neural solver! IEEE Transactions on
Pattern Analysis and Machine Intelligence, 46(6):4102–
4114, 2024.
Wang, H. P. and Li, P. Unsupervised learning for combina-
torial optimization needs meta learning. In The Eleventh
International Conference on Learning Representations,
2022.
Wellman, M. P. Methods for empirical game-theoretic anal-
ysis. In AAAI, volume 980, pp. 1552–1556, 2006.
Wouda, N. A., Lan, L., and Kool, W.
Pyvrp: A high-
performance vrp solver package.
INFORMS Journal
on Computing, 36(4):943–955, 2024.
Yao, S., Liu, F., Lin, X., Lu, Z., Wang, Z., and Zhang,
Q.
Multi-objective evolution of heuristic using large
language model. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 39, pp. 27144–27152,
2025.
Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park,
J., and Song, G. Reevo: Large language models as hyper-
heuristics with reflective evolution. Advances in neural
information processing systems, 37:43571–43608, 2024.
Zhang, D., Xiao, Z., Wang, Y., Song, M., and Chen, G. Neu-
ral tsp solver with progressive distillation. In Proceed-
ings of the AAAI Conference on Artificial Intelligence,
volume 37, pp. 12147–12154, 2023.
Zhang, Z., Zhang, Z., Wang, X., and Zhu, W. Learning to
solve travelling salesman problem with hardness-adaptive
curriculum. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 36, pp. 9136–9144, 2022.
12


--- Page 13 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Zheng, Y., Luo, F., Wang, Z., Wu, Y., and Zhou, Y. Mtl-
kd: Multi-task learning via knowledge distillation for
generalizable neural vehicle routing solver. arXiv preprint
arXiv:2506.02935, 2025.
Zhou, J., Wu, Y., Song, W., Cao, Z., and Zhang, J. Towards
omni-generalizable neural methods for vehicle routing
problems. In International conference on machine learn-
ing, pp. 42769–42789. PMLR, 2023.
13


--- Page 14 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
A. Policy-Space Response Oracle
A.1. Game-Theoretic Setup
We consider a general N-player normal-form game. Each player i ∈{1, . . . , N} chooses a strategy πi from a (possibly
infinite) strategy space Πi. The joint strategy profile is denoted by π = (π1, . . . , πN), and π−i refers to the strategies of all
players except i. Player i receives utility ui(πi, π−i) ∈R.
A.2. Restricted Games and Meta-Strategies
Directly solving games over the full strategy spaces Πi is generally intractable. PSRO (Lanctot et al., 2017) addresses this by
iteratively constructing a restricted game. At iteration t, each player i maintains a finite set of candidate strategies Π(t)
i
⊂Πi.
The restricted game is defined by evaluating all joint strategy profiles drawn from Π(t)
1 × · · · × Π(t)
N . A meta-strategy σ(t)
i
is
a mixed strategy over Π(t)
i , i.e., a probability distribution satisfying
σ(t)
i
∈∆(Π(t)
i ).
Here, ∆(Π(t)
i ) denotes the probability simplex over the finite set Π(t)
i . In practical implementations, a meta-strategy often
samples a strategy πi ∼σ(t)
i
and uses it for evaluation within that iteration.
When the restricted game is finite, computing meta-strategies reduces to solving a standard equilibrium problem over a
payoff matrix. For clarity, we illustrate meta-strategy computation in a two-player zero-sum setting. In this case, the game
is fully characterized by the utility of player 1, with u2(π1, π2) = −u1(π1, π2). The restricted game can therefore be
represented by a payoff matrix M (t) ∈Rm×n with entries
M (t)
ij = u1(πi
1, πj
2),
where πi
1 ∈Π(t)
1
and πj
2 ∈Π(t)
2 .
The meta-strategy for player 1 can then be obtained by solving the following linear program, as implied by the minimax
theorem for two-player zero-sum games (von Neumann, 1959; Shoham & Leyton-Brown, 2008):
max
x, v
v
s.t.
x⊤M (t) ≥v1,
x ≥0,
1⊤x = 1,
where x corresponds to the mixed strategy σ(t)
1
over Π(t)
1 . An analogous linear program yields the meta-strategy for player 2.
A.3. Best-Response Oracles
Given opponents’ meta-strategies σ(t)
−i, a best-response (BR) for player i is defined as a strategy that maximizes expected
utility against the induced distribution over opponent strategies:
πi,BR ∈arg max
π∈Πi Eπ−i∼σ(t)
−i [ui(π, π−i)] .
In practice, computing exact best responses is rarely tractable. PSRO therefore relies on approximate BR oracles (Lanctot
et al., 2017), whose role is to return a high-performing strategy under the current opponent meta-strategy, rather than a
provably optimal one. The above definition assumes utility maximization; minimization objectives can be handled by
negating the payoff without loss of generality.
A.4. PSRO Algorithm
PSRO proceeds in an iterative “solve-and-expand” loop:
14


--- Page 15 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
1. Initialization. Initialize each population Π(0)
i
with a small set of strategies.
2. Meta-game solving. At iteration t, construct the restricted game induced by {Π(t)
i }N
i=1 and compute a meta-strategy
σ(t) = (σ(t)
1 , . . . , σ(t)
N ) using a suitable equilibrium solver.
3. Response-based expansion. For one or more players, invoke a BR oracle against σ(t)
−i to obtain a new strategy, which
is added to the population:
Π(t+1)
i
←Π(t)
i
∪{πi,new}.
By iteratively expanding the restricted game with targeted best responses, PSRO refines the approximation of the underlying
full game.
B. EoH-style Program Search Mechanism
Within ASRO, we instantiate the program-space best-response oracle using an EoH-style evolutionary program search
mechanism (Liu et al., 2024a). Conditioned on the opponent’s meta-strategy σ(t)
−i, this mechanism evolves a population of
solver or generator programs via LLM-driven variation and selection to approximately optimize the induced best-response
objective.
In this mechanism, each program is represented through a natural-language “thought” that guides LLM reasoning and an
executable “code” component that is directly evaluated. This dual representation enables both conceptual and syntactic
variation during evolution in practice.
B.1. Evolutionary Operators and Prompt Strategies
The EoH-style search mechanism includes one initialization operator and five variation operators, grouped into exploration
and modification categories to generate diverse and progressively refined heuristics.
Initialization (I1). At the first ASRO iteration, I1 constructs the initial population by prompting the LLM to produce
heuristics entirely from scratch. This provides a starting population for subsequent evolution.
Exploration and modification operators follow a unified usage pattern: in each evolutionary round, a parent set of size p is
sampled from the current population; an operator-specific prompt is applied; and the LLM outputs a new heuristic derived
from these parents.
Exploration Operator E1 (Diverse Exploration). E1 encourages large jumps in program space by asking the LLM to
design a heuristic that differs substantially from all selected parents. Its role is to introduce large, conceptually novel
variations that sustain population diversity.
Exploration Operator E2 (Shared-Idea Recombination). E2 performs semantic recombination. The LLM is instructed to
identify the common idea shared by the parents and then construct a new heuristic that preserves this idea while introducing
new structural elements. This operator encourages abstraction and meaningful recombination of successful patterns.
Modification Operator M1 (Semantic Refinement). M1 improves a single parent by refining its underlying idea. The
LLM analyzes the heuristic’s logic and proposes targeted adjustments that strengthen its conceptual soundness or operational
behavior.
Modification Operator M2 (Parameterized Adjustment). M2 performs local numerical tuning. It directs the LLM
to adjust constants, thresholds, or coefficients without altering the heuristic’s overall structure, enabling fine-grained
performance improvements.
Modification Operator M3 (Simplification and Pruning). M3 simplifies a parent heuristic by identifying and removing
redundant or overly complex components while preserving the core idea. This promotes parsimony and often yields
heuristics with stronger generalization.
B.2. Mechanism Workflow and Iterative Search Process
The EoH-style search mechanism performs program-space search through an iterative evolutionary process that combines
LLM-generated program variations with fitness-based population updates. Algorithm 2 provides the procedural view of a
15


--- Page 16 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
EoH Prompt Template Structure
1. Task Description
Defines the optimization problem and the required I/O format.
2. Operator-Specific Prompt
Instruction for the chosen operator (I1, E1, E2, M1, M2, M3). Specifies how the LLM should reason over parent
heuristics and produce a variant.
3. Expected Output
LLM must first describe the heuristic idea, then provide a full Python function with a fixed signature (name, inputs,
outputs).
4. Note
Additional constraints: determinism, no extra imports, concise response, and strict adherence to the required code
format.
5. Parent Heuristics
Includes the thought and code of p parent heuristics (empty only in I1). Serves as in-context examples for reasoning
and variation.
Figure 5. Unified structure of all prompt templates used by EoH evolutionary operators. Each operator (I1, E1, E2, M1, M2, M3) uses this
five-part template with different operator-specific instructions.
single best-response search, while this section summarizes the conceptual workflow. Here, p, K, and R denote user-defined
hyperparameters controlling parent selection, population size, and evolutionary depth, respectively.
At the beginning of iteration t in ASRO, the search mechanism initializes a population P0 of candidate programs. If t = 0,
the population is created via prompt-based synthesis; otherwise, it is inherited from the previous ASRO iteration. Each
program is represented by a thought–code pair and evaluated against the fixed opponent meta-strategy σ(t)
−i.
The mechanism then runs R evolutionary rounds, where R is a user-defined parameter controlling the search depth. In each
round, a subset of parents Psel is selected according to fitness and diversity. LLM operators (I1, E1, E2, M1, M2, M3)
are instantiated as prompt templates (Figure 5), and each operator generates new program candidates by transforming the
selected parents. These candidates are evaluated under σ(t)
−i, merged with the existing population, and truncated to maintain
a fixed size K. After R rounds, the highest-performing program in PR is returned as the approximate best response.
This iterative scheme enables the search mechanism to explore both large conceptual variations (via exploration operators)
and fine-grained improvements (via modification operators), while maintaining diversity across evolution. The parameter R
trades optimization depth for computational cost: larger R allows richer program refinement but increases total LLM calls.
In all experiments, R is kept moderate to balance performance and efficiency.
B.3. Evaluation under Mixed Strategies
At each ASRO iteration, candidate programs are evaluated as approximate best responses to a fixed opponent meta-strategy
σ(t)
−i. This requires estimating expected performance under a mixed strategy rather than against a single opponent.
Solver Evaluation. When evolving solver programs, the opponent strategy is the generator mixture σ(t)
g . For a solver
candidate s, evaluation proceeds by first considering each generator gj in the current population. A fixed set of nI instances
is sampled from gj, and the solver is applied to all sampled instances. Here, nI denotes the number of problem instances
sampled per generator for evaluation. Gaps are averaged within each generator to obtain a per-generator estimate
d
gap(s, gj) = 1
nI
nI
X
k=1
gap(s, xj,k),
xj,k ∼gj.
where gap denotes the normalized reference gap defined in Eq. 2. The overall performance of s under the mixed strategy is
then computed by aggregating across generators according to σ(t)
g :
16


--- Page 17 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Algorithm 2 Program-Space Best-Response Search (LLM-Based)
1: Input: Opponent meta-strategy σ−i; LLM operators O; population size K; iterations R
2: Output: Best-performing program p∗
3: (0) Initialization
4: if first iteration then
5:
P(0) ←Prompt-based synthesis
6: else
7:
P(0) ←Inherit from previous population
8: end if
9: for r = 1 to R do
10:
(1) Selection
11:
Select parent set Psel ⊂P(r−1) based on fitness
12:
(2) LLM-Based Variation
13:
C(r) ←{LLM(p, o) | p ∈Psel, o ∈O}
14:
(3) Evaluation Against Meta-Strategy
15:
Evaluate all p ∈C(r) ∪P(r−1) against σ−i
16:
(4) Population Update
17:
P(r) ←TOP-K(C(r) ∪P(r−1))
18: end for
19: p∗←arg maxp∈P(R) f(p)
d
gap(s | σ(t)
g ) =
X
j
σ(t)
g (gj) d
gap(s, gj).
Generator Evaluation.
Generator candidates are evaluated analogously under the solver mixture σ(t)
s . For a generator g,
all solver programs sℓare applied to instances sampled from g. Gaps are first averaged within each solver,
d
gap(sℓ, g) = 1
nI
nI
X
k=1
gap(sℓ, xk),
and final performance is obtained by aggregating across solvers according to σ(t)
s :
d
gap(g | σ(t)
s ) =
X
ℓ
σ(t)
s (sℓ) d
gap(sℓ, g).
For generators, higher d
gap(g | σ(t)
s ) indicates a stronger adversary and is therefore preferred. In both cases, evaluation
reduces to computing weighted empirical averages over solver–generator–instance interactions, where the weights are given
by the opponent meta-strategy.
C. Batch Evaluation Engine
At each meta-game iteration of ASRO, both best-response search and payoff matrix construction require evaluating a large
number of solver–generator interactions under fixed opponent mixed strategies, as defined in Section B.3. These evaluations
correspond to computing the mixed-strategy expectations introduced above.
Concretely, given ns solver programs, ng generator programs, and nI instances sampled per generator, a single meta-game
iteration involves
O(ns × ng × nI)
solver executions.
17


--- Page 18 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Table 4. Hyperparameter settings used in ASRO experiments across domains.
Task
K
Rs
Rg
nI
min-ratio
OBP
20
4
2
2
0.3
TSP
10
2
2
3
0.4
CVRP
10
2
2
5
0.3
To implement this evaluation workload in a unified and computationally efficient manner, we adopt a batch evaluation
engine that materializes all required evaluation tasks and executes them in parallel.
C.1. Unified Two-Stage Evaluation Pipeline
The batch evaluation engine follows a unified two-stage pipeline consisting of task preparation and batch execution.
Stage I: Task Preparation. The preparation stage constructs the complete set of evaluation tasks corresponding to solver–
generator–instance triples. First, each generator samples a fixed set of nI problem instances. For a given generator, the same
instance set is shared across all solvers to ensure consistent comparison. If required by the gap definition, reference values
(e.g., optimal solutions or bounds) are computed for all generated instances. All solver programs are then pre-processed
(e.g., compilation or validation) once prior to execution, avoiding redundant overhead during evaluation and isolating
failures of individual solver programs. Finally, evaluation tasks are constructed by enumerating all combinations. Each task
encapsulates the solver identity, generator identity, instance data, the reference value (if applicable), and the corresponding
opponent meta-strategy weight.
Stage II: Batch Task Execution. All prepared tasks are executed in batch. Each task corresponds to one solver execution
on a single instance and returns the observed normalized gap. Results are grouped by solver–generator pairs and averaged
across instances to produce empirical estimates of meta-game payoffs.
C.2. Parallelization and Robustness
Parallelism is applied internally within both preparation and execution stages to accelerate evaluation at scale. Independent
resource budgets are used to prevent CPU oversubscription, and all evaluation tasks are protected by timeouts. Failures of
individual solver executions are isolated and penalized without interrupting the overall evaluation process.
D. Additional Experimental Details
Hyperparameters.
Table 4 summarizes the key hyperparameters used in ASRO across domains. The outer iteration
count T specifies the number of solver–generator meta-game expansions. At each iteration, fixed-size solver and generator
populations of size K are maintained. The program-space best-response oracle is implemented via an EoH-style evolutionary
search, where Rs and Rg denote the numbers of evolutionary rounds for synthesizing solver and generator candidates,
respectively. Each candidate program is evaluated on nI instances sampled from the corresponding generator program.
To stabilize early-stage co-evolution, a fixed fraction of instances (min-ratio) is always drawn from a task-specific base
generator.
In terms of LLM usage, each best-response oracle invocation synthesizes R × K candidate programs for each EoH operator
in a fixed operator set, with an additional one-time initialization at the first ASRO iteration; this procedure is applied
symmetrically to both solver and generator at every iteration.
E. Online Bin Packing (OBP)
Problem Definition. We consider the online bin packing problem, where items arrive sequentially and must be assigned
irrevocably to bins of fixed capacity C (Seiden, 2002). At each time step t, an item of size wt ∈(0, C] arrives and must be
placed into an existing bin with sufficient remaining capacity or into a newly opened bin. Once placed, items cannot be
moved or reassigned. The objective is to minimize the total number of bins used.
An instance is defined by an item sequence
x = (w1, w2, . . . , wT ),
and the solver must make decisions online, without lookahead or reordering.
18


--- Page 19 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Figure 6. Base prompt examples for solver and generator synthesis in online bin packing (OBP).
Prompt for Generator
Task.
Write a novel Python function to generate sequences of items
for the online bin packing problem. Goal: Create instances
that are as difficult as possible for heuristic solvers to pack
efficiently. The difficulty must arise from the item arrival order
and the strategic relationship between sequential items.
I/O.
Implement generate instances(seeds, capacity,
num items) and return instances.
Input/Output Info.
seeds: iterable of integers, one per instance.
capacity: bin capacity (int > 1).
num items: number of items in each instance (int > 0).
instances: Python list with the same length as seeds; each
element is a NumPy integer array named items of shape
(num items, )with values strictly in [1, capacity −1].
Other Info.
Avoid simple random noise; instead, design structured se-
quences that force poor decision-making in real-time assign-
ments. Avoid trivial or degenerate streams that make the objec-
tive uninformative. Output ONLY the function code.
Prompt for Solver
Task.
Write a novel scoring Python function for the online bin
packing problem using a greedy strategy. Goal: Minimize
the total number of bins by making each assignment facil-
itate future packing. Strategy Hint: Consider not just the
tightest fit, but the potential usability of the remaining space.
A good score should balance between filling a bin and avoid-
ing leaving behind tiny, unusable gaps that might reject
future items.
I/O.
Implement score(item, bins) and return scores.
Input/Output Info.
item: current item size.
bins: NumPy array of remaining capacities for feasible
bins only (each entry satisfies bins[i] >= item).
scores: NumPy array with the same shape as bins.
Higher is better.
Other Info.
Be scale-invariant (use ratios such as (bins −
item)/max(bins) or statistics of bins instead of hard-
coded thresholds). Output ONLY the function code.
E.1. Solver Program Interface.
Each solver program defines a numerical priority function
score(w, r),
which takes the current item size w and a vector r of remaining capacities of all feasible bins, and returns a score vector
aligned with r (one score per feasible bin). Only the priority function varies across solvers; the decoding logic and online
constraints are fixed.
E.2. Prompt Configuration
We first specify task-level base prompts for online bin packing, as illustrated in Figure 6. These prompts define the OBP
task, including the task description, the required function interfaces, and task-specific constraints for solver and generator
synthesis.
During evolution, these task-level prompts are embedded into a unified prompt template (Figure 5). Specifically, the Task
section of the OBP prompt is placed in the Task Description slot, defining the optimization objective and problem setting.
The function signatures specified under I/O are enforced in the Expected Output slot. Detailed input/output specifications
and additional constraints provided under Input/Output Info and Other Info are incorporated into the Note section. All
remaining components of the prompt, including operator-specific instructions and the inclusion of parent programs, follow
the standard template of the search mechanism without modification.
E.3. Evolution Outcomes
Figure 7 provides a qualitative snapshot of the solver–generator strategy space shaped by co-evolution in ASRO. The figure
visualizes representative executable programs selected from the final solver and generator pools, highlighting characteristic
structural patterns induced by sustained solver–generator interaction. On the solver side, co-evolution with adversarial
instance generators gives rise to scoring rules that extend beyond purely local heuristics by incorporating global packing
statistics and adaptive penalty mechanisms. On the generator side, pressure from increasingly capable solvers leads to
structured instance constructions that induce systematic pairing conflicts and amplify online misallocation. Together, these
examples illustrate how solver and generator strategies mutually adapt under ASRO, resulting in a diverse and interdependent
19


--- Page 20 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Best-Fit-like baseline
def score(item, bins):
    remaining = bins - item 
    valid_mask = remaining >= 0
    scores = np.where(valid_mask, -remaining, -np.inf)
    return scores
Procedural adversarial generator
def generate_instances(seeds, capacity, num_items):
    out = []
    for seed in seeds:
        rng = np.random.default_rng(seed), items = []
        for _ in range(randint(2,4)):   # perfect clusters (sum exactly to capacity) 
            items += make_exact_fit_cluster(capacity, k=2..4)
        for _ in range(randint(2,4)):   # near-fit clusters (slightly under, then perturb)
            items += make_near_fit_cluster(capacity, k=2..3, waste=1..3, perturb=+1..3)
        for _ in range(randint(2,5)):    # decoy pairs (tempting but suboptimal)
            items += make_decoy_pair(capacity, jitter=±5)
        items += sample_connectors(capacity, n=3..7) # connector items 
        items += sample_bimodal_filler(capacity, until=num_items) # realistic filler
        items = clip_len_clip(items, num_items, 1, capacity-1)
        items = sort(items)   # structured ordering to amplify online fragmentation 
        items = block_shuffle(items, block=4..7, reverse_each_block=50%)
        out.append(items)
    return out
Modular size-class generator
def generate_instances(seeds, capacity, num_items):
    out = []
    for seed in seeds:
        rng = np.random.default_rng(seed)  
        K = randint(3, 6)                         # size classes
        cores = make_cores(capacity, K, offsets)  # class centers
        items = []
        for k in classes:
            items += sample_tight(cores[k], 70%)  # near-fit pressure
            items += sample_wide (cores[k], 30%)  # disruptors
        cls = assign_by_nearest_core(items, cores)
        buckets = [sort_or_reverse(items[cls==k]) for k in classes]
        out.append(interleave_chunks(buckets, chunk=2..4)[:num_items])
    return out
Gated two-regime scoring with global curvature
def score(item, bins):
    scores = -inf_like(bins)
    r = bins - item
    C = max(bins)                 # capacity proxy
    x = r / (C + eps)
    A = exp(-5 * r / (item + eps))  #  A: waste-avoid (prefer small residual vs item)
    B = exp(-2 * (C - r) / (C + eps))  # B: consolidation (shape by distance to max)
    gate = sigmoid(-10 * (x - 0.15))  # gated blend: small residual -> A, large -> B
    blended = gate*A + (1-gate)*B
    avg = mean(b) / (C + eps)  # global curvature from packing state
    var = var(bins / (C + eps))
    kappa = 1.5 + 2*avg - 1.5*var
    t = 0.2 * C    # piecewise shaping around transition t
    shaped = exp(-kappa * r/(t+eps))         if r<t
             exp(-0.5*kappa*(r-t)/(C-t+eps)) otherwise
    # mild synergy on item/bin ratio + used-bin pressure
    syn = 1 / (1 + 10*abs(item/(b+eps) - 0.5))
    used = mean(bins < C)                    # used-bin proxy
    pressure = 0.3 + 0.7*used
    scores = pressure*shaped + 0.4*blended + 0.3*syn
    return scores
Global-fill adaptive scoring
def score(item, bins):                 
    feasible = bins >= item
    scores = -inf_like(bins)
    r = bins[feasible] - item           # local residual
    fit = exp(- r / (item + eps))       # best-fit bias
    fill = clip(global_fill_proxy(bins), 0, 1)
    target = 0.10 + 0.25 * sigmoid(8*(fill - 0.5))
    dev = abs(item/(bins[feasible] + eps) - target)
    risk = (1 + 2*fill) * dev**1.5      # stronger penalty when system is tight
    scores[feasible] = fit - risk
    return scores
Weibull distribution
def generate_instances(seeds, capacity, num_items):
    out = []
    for seed in seeds:
        rng = np.random.default_rng(seed)   
        items = (rng.weibull(1.4, num_items) * 30.0).clip(1, capacity-1).astype(int)
        out.append(items)
    return out
Figure 7. Representative solver and generator programs discovered by ASRO on online bin packing. The figure shows representative
solver scoring rules (left) and instance generators (right) selected from the final solver and generator pools. Code is schematized to
emphasize algorithmic structure rather than implementation details.
strategy space.
E.4. Datasets and Arrival Orders.
We evaluate OBP solvers on a collection of widely used public benchmarks as well as synthetic distributions. Specifically,
we use the Falkenauer T and U benchmark families (Falkenauer, 1996), the Hard28 benchmark (Delorme et al., 2018). We
additionally evaluate on synthetic instances generated from Weibull distributions, as commonly used in online bin packing
studies (Casti˜neiras et al., 2012). These datasets cover both classical test cases and structurally challenging scenarios
commonly studied in the online bin packing literature.
Since online bin packing performance is sensitive to item arrival order (Albers et al., 2021), we evaluate all methods under
multiple arrival models. In particular, we consider random order and size-based orders (ascending). Arrival orders are fixed
independently of the evaluated algorithms and applied consistently across all methods to ensure fair comparison.
E.5. Baselines.
We compare ASRO-generated solvers against classical online bin packing heuristics and the EoH baseline. The classical
baselines include First Fit (Ullman, 1971; D´osa & Sgall, 2013) and Best Fit (Johnson et al., 1974; Albers et al., 2021), which
represent standard greedy strategies widely used in practice. In addition, we include the solver evolved by Evolution of
Heuristics (EoH) (Liu et al., 2024a), which follows a static, solver-centric LLM-based heuristic discovery pipeline without
adversarial instance generation. All baselines operate under the same online constraints and decoding procedure as ASRO
solvers, without lookahead, reordering, or post-processing.
20


--- Page 21 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
F. Traveling Salesman Problem (TSP)
F.1. Problem Definition.
We consider the Euclidean Traveling Salesman Problem (TSP) with N cities. Given city coordinates {ci}N
i=1 ⊂R2, the
objective is to find a Hamiltonian tour of minimum total length. Equivalently, for a permutation π of {1, . . . , N}, the tour
cost is
V (π) =
N
X
i=1
d
 cπi, cπi+1

,
with πN+1 = π1,
where d(·, ·) is the Euclidean distance. The solver aims to minimize V (π).
F.2. Solver Procedure and Program Interface (TSP)
Solver Procedure (Guided Local Search).
For the Traveling Salesman Problem (TSP), we adopt Guided Local Search
(GLS) (Voudouris & Tsang, 1999; Alsheddy et al., 2018; Arnold & S¨orensen, 2019) as the fixed solver procedure. GLS is a
local-search-based algorithm that iteratively improves a candidate tour using a predefined set of neighborhood operators,
specifically 2-opt and relocate. At any point in the search, candidate local moves are evaluated under an augmented edge-
distance matrix D = (dij), which is initialized from the Euclidean distances and subsequently modified by solver-defined
update rules to incorporate adaptive edge penalties.
Starting from an initial tour, GLS repeatedly applies local search operators under D until a locally optimal tour πloc is
reached, i.e., no improving local move exists under D. Based on πloc, GLS updates an edge-usage matrix U = (uij), where
uij records how frequently edge (i, j) has been selected in locally optimal tours during the search history. The search then
continues by updating the augmented edge-distance matrix via the solver-defined update rule and resuming local search
under the updated distances.
Solver Program Interface.
Within this fixed GLS procedure, solver programs specify an edge-distance update rule that
modifies the augmented edge-distance matrix. Given the current augmented edge-distance matrix D, the current locally
optimal tour πloc, and the edge-usage matrix U, the solver program outputs an updated augmented edge-distance matrix:
˜D = update edge distance(D, πloc, U).
The updated matrix ˜D is then used by GLS to evaluate subsequent local moves. All other components of the solver procedure,
including the local search operators and the GLS control logic, are fixed across methods; only the distance-update rule varies
across solver programs.
F.3. Prompt Configuration
We specify task-level base prompts for the traveling salesman problem, as illustrated in Figure 8. These prompts define the
TSP task, including the optimization objective, required function interfaces, and task-specific constraints for solver and
generator synthesis. Prompt construction for TSP follows the same procedure as in online bin packing. Specifically, the
TSP base prompts are embedded into the unified prompt template structure shown in Figure 5, with task-specific content
populated accordingly and all remaining components following the standard template of the search mechanism.
F.4. Evolution Outcomes
Figure 9 shows representative solver and generator programs learned by ASRO on the Traveling Salesman Problem. Solver
programs evolve toward more structured distance-update mechanisms, while instance generators progressively induce
structured geometries that interact with these updates.
F.5. Benchmark Datasets
We evaluate the learned TSP solvers on benchmark instances from TSPLIB (Reinelt, 1991), a standard library of Traveling
Salesman Problem instances widely used in the combinatorial optimization literature. TSPLIB instances exhibit substantial
diversity in both scale and geometric structure, including uniform and clustered point sets, grid-like layouts, and instances
with irregular spatial patterns.
21


--- Page 22 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Figure 8. Base prompt examples for solver and generator synthesis in Traveling Salesman Problem (TSP).
Prompt for Generator
Task.
Write a novel Python function to generate Euclidean
Traveling Salesman Problem instances. The goal
is to create instances that are as difficult as possi-
ble for heuristic solvers, measured by the resulting
tour length. The difficulty should arise from the geo-
metric structure of city coordinates and their spatial
arrangement, rather than from simple random noise.
Design instances that induce misleading local struc-
tures, long-range dependencies, or deceptive spatial
patterns that make greedy or local-search-based
solvers struggle.
I/O.
Implement generate instances(seeds,
n cities) and return instances.
Input/Output Info.
seeds: iterable of integers, one per instance.
n cities: number of cities in each TSP instance.
instances: Python list with the same length as
seeds; each element is a NumPy array of shape
(n cities, 2), representing city coordinates in [0, 1]2.
Other Info.
Output ONLY the function code.
Prompt for Solver
Task.
Write a GLS-style edge-distance update rule as a Python function that
helps the solver minimize total tour length. The update rule should
modify edge penalties based on the structure of a locally optimized
tour, encouraging exploration of alternative edges and discouraging
repeatedly used suboptimal edges. The goal is to improve solution
quality across the sampled instance distribution, not just on a single
instance.
I/O.
Implement update edge distance(edge distance,
local opt tour, edge n used) and return
updated edge distance.
Input/Output Info.
edge distance: NumPy array of shape (N, N), symmetric.
local opt tour: list or 1D NumPy array of city indices represent-
ing a tour.
edge n used: NumPy array of shape (N, N), symmetric, storing
edge usage counts.
updated edge distance: NumPy array of shape (N, N), sym-
metric.
Other Info.
Do NOT modify input arrays in-place; operate on copies and return
updated copies. Output ONLY the function code.
The benchmark covers a broad range of problem sizes, from small instances with fewer than 200 cities to large-scale
instances exceeding 1,000 cities. All TSPLIB instances considered are evaluated against reference tour lengths (e.g.,
best-known solutions), enabling standardized reporting via normalized optimality gaps.
Importantly, TSPLIB instances are not drawn from the distributions used during training. Evaluating on TSPLIB therefore
provides a stringent test of generalization under distributional and structural shift.
F.6. More Results: Per-instance TSPLIB Performance
Table 5 reports per-instance results on TSPLIB benchmarks, comparing the final ASRO solver against the EoH baseline.
Rather than aggregating by instance size or family, this table exposes instance-level behavior and highlights where
improvements are consistently realized.
F.7. Baselines
Classical constructive heuristics include Nearest Insertion (NI) and Farthest Insertion (FI) (Applegate et al., 2011;
Rosenkrantz et al., 1977), which build tours incrementally according to distance-based criteria. These methods are
simple, deterministic, and widely used as standard non-learning baselines.
G. Capacitated Vehicle Routing Problem (CVRP)
G.1. Problem Definition.
We consider the Capacitated Vehicle Routing Problem (CVRP) (Toth & Vigo, 2014). A fleet of identical vehicles departs
from and returns to a single depot to serve a set of customers with known demands. Each customer must be visited exactly
once, and the total demand served by any vehicle cannot exceed the vehicle capacity Q. The objective is to minimize the
total travel cost over all routes. An instance is defined by a depot location, customer locations {ci}N
i=1, customer demands
{di}N
i=1, and a vehicle capacity Q. Feasibility with respect to capacity constraints must be satisfied at all times.
22


--- Page 23 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Baseline (No Diversification)
def update_edge_distance(edge_distance, local_opt_tour, edge_n_used):
    return edge_distance.copy()
Uniformly sample cities in the unit square.
def generate_instances(seeds, n_cities):
    instances = []
    for seed in seeds:
        rng = np.random.default_rng(seed)
        instances.append(rng.random((n_cities, 2)))
    return instances
Place cities on a perturbed grid to enforce medium-scale
geometric regularity.
def generate_instances(seeds, n_cities):
    instances = []
    for seed in seeds:
        rng = np.random.default_rng(seed)
        side = ceil(sqrt(n_cities))
        spacing = 1.0 / (side + 1)
        base = [(spacing*(i+1), spacing*(j+1)) for i in range(side) for j in
range(side)]   # regular grid geometry
        pts  = array(base)[:n_cities]   # bounded geometric perturbation
        jitter = rng.uniform(-0.15*spacing, 0.15*spacing, size=pts.shape)
        pts = clip(pts + jitter, 0, 1)
        rng.shuffle(pts)
        instances.append(pts)
    return instances
Arrange cities along a noisy circular structure to induce long
cross-ring edges.
def generate_instances(seeds, n_cities):
    instances = []
    for seed in seeds:
        rng = np.random.default_rng(seed)
        # angular ordering on a circle
        angles = linspace(0, 2*pi, n_cities, endpoint=False)
        angles += rng.uniform(-0.2, 0.2, size=n_cities)
        radii = 0.3 + 0.2 * rng.random(n_cities)
        x = 0.5 + radii * cos(angles)
        y = 0.5 + radii * sin(angles)
        pts = clip(stack([x, y], axis=1), 0, 1)  # ring-shaped city layout
        instances.append(pts)
    return instances
Generate multiple concentric rings to create competing geometric
scales.
def generate_instances(seeds, n_cities):
    instances = []
    for seed in seeds:
        rng = np.random.default_rng(seed)
        n_rings = max(2, int(sqrt(n_cities / 2)))
        per_ring = ceil(n_cities / n_rings)
        pts = []
        for r in range(n_rings):
            radius = 0.1 + 0.4 * r / (n_rings - 1)  # multi-scale radial layers
            angles = linspace(0, 2*pi, per_ring, endpoint=False)
            angles += rng.uniform(0, 2*pi/(n_rings+1), size=per_ring)
            x = 0.5 + radius * cos(angles)
            y = 0.5 + radius * sin(angles)
            pts.append(stack([x, y], axis=1))
        pts = clip(concat(pts, axis=0)[:n_cities], 0, 1)
        rng.shuffle(pts)
        instances.append(pts)
    return instances
Multiply tour-edge distances by a saturated function of relative usage
frequency.
def update_edge_distance(edge_distance, local_opt_tour, edge_usage):
    # Common skeleton (omitted in other solvers below):
    updated = edge_distance.copy()
    # edges appearing in the current local-opt tour
    tour_edges = edges_of(local_opt_tour)
    for (i, j) in tour_edges:   # update edge usage counts
        edge_usage[i, j] += 1
        edge_usage[j, i] += 1
    avg_usage = mean(edge_usage)
    # penalize frequently used tour edges
    for (i, j) in tour_edges:   
        u = edge_usage[i, j] / (avg_usage + eps)
        penalty = log1p(u)
        updated[i, j] *= (1 + penalty)
        updated[j, i] = updated[i, j]
    return updated
Penalize tour edges based on recent usage acceleration and global
usage rank.
def update_edge_distance(edge_distance, local_opt_tour, edge_n_used):
    # (common skeleton omitted)
    # rank edges by global usage (0 = most used)
    rank = rank_edges(edge_usage)
    # recent usage acceleration
    accel = (edge_usage - prev_usage) / (prev_usage + 1)
    for (i, j) in tour_edges:
        a = accel[i, j]
        r = rank[i, j]
        base  = 1.0 + 0.3 * tanh(a)
        decay = 1.0 / (1.0 + 0.05 * prev_usage[i, j])
        updated[i, j] *= (1.0 + decay * base * (1.0 - r))
        updated[j, i] = updated[i, j]
    return updated
def update_edge_distance(edge_distance, local_opt_tour, edge_usage):
    # (common skeleton omitted)
    avg_usage = mean(edge_usage)
    max_usage = max(edge_usage)
    # global temperature reflecting search stagnation
    temperature = 1.0 + 2.0 * avg_usage / (max_usage + eps)
    for (i, j) in tour_edges:
        u = edge_usage[i, j]
        growth = tanh(u / (avg_usage + eps))
        decay  = 1.0 / (1.0 + log1p(u))
        penalty = temperature * growth * decay * edge_distance[i, j]
        updated[i, j] = edge_distance[i, j] + penalty
        updated[j, i] = updated[i, j]
    return updated
Add usage-dependent penalties scaled by a global usage-derived
temperature.
Figure 9. Representative solver and generator programs discovered by ASRO on the Traveling Salesman Problem. We show
distilled solver distance-update rules (left) and instance generators (right) drawn from the final solver and generator pools. Code is
schematized for clarity to emphasize algorithmic structure; auxiliary implementation details are omitted.
G.2. Solver Program Interface.
Solver programs for CVRP are implemented as step-wise selection functions invoked within a fixed greedy route-construction
procedure. At each decision step, given the current node v ∈{0, 1, . . . , N} (with v = 0 denoting the depot), the depot index
v0 = 0, the feasible unvisited customer index collection U (a subset of {1, . . . , N}), the remaining vehicle capacity q, the
customer demand vector d ∈RN+1 (with d0 = 0), and the distance matrix D ∈R(N+1)×(N+1), the solver program selects
the next node to visit:
i⋆= select(v, v0, U, q, d, D).
23


--- Page 24 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Table 5. Per-instance TSPLIB results. Lower is better.
Instance
ASRO-EoH
EoH
Instance
ASRO-EoH
EoH
Instance
ASRO-EoH
EoH
Instance
ASRO-EoH
EoH
A280
0.301
2.195
FNL4461
4.635
4.635
PCB442
0.462
0.990
RD400
0.163
2.717
BERLIN52
0.031
2.279
GIL262
0.414
0.955
PR1002
2.245
3.051
RL1304
3.780
6.771
BIER127
0.318
0.224
KROA100
0.016
0.119
PR107
-0.003
0.614
RL1323
2.582
4.764
CH130
0.012
0.381
KROA150
0.003
0.639
PR124
0.001
0.096
RL1889
2.784
3.879
CH150
0.044
1.282
KROA200
0.005
0.907
PR136
-0.001
0.459
ST70
0.313
0.313
D1291
3.649
5.484
KROB100
-0.009
-0.009
PR144
-0.003
0.353
TS225
2.582
4.764
D1655
4.006
4.144
KROB150
-0.009
1.367
PR152
0.002
0.907
TSP225
-0.901
0.561
D198
0.301
1.056
KROB200
0.015
0.819
PR226
0.002
0.877
U1060
1.850
4.132
D2103
1.068
1.839
KROC100
0.008
0.008
PR2392
4.186
4.186
U1432
2.645
4.069
D493
1.249
2.998
KROD100
0.001
0.317
PR264
0.000
0.231
U159
-0.010
-0.010
D657
1.012
3.023
KROE100
0.024
0.438
PR299
0.039
0.791
U1817
4.072
4.168
EIL101
1.782
4.144
LIN105
0.028
0.028
PR439
2.255
5.300
U2152
4.025
4.491
EIL51
0.674
0.700
LIN318
0.342
3.738
PR76
0.000
1.467
U2319
2.366
2.366
EIL76
1.184
1.732
LINHP318
2.002
5.454
RAT195
0.582
0.982
U574
1.174
5.071
FL1400
1.824
5.096
NRW1379
2.829
3.961
RAT575
2.183
3.152
U724
1.411
3.161
FL1577
2.409
5.031
P654
0.404
2.080
RAT783
2.363
3.907
VM1084
1.158
3.552
FL3795
4.382
4.382
PCB1173
3.246
5.004
RAT99
0.681
0.681
VM1748
2.133
3.280
FL417
0.509
0.984
PCB3038
4.132
4.132
RD100
0.005
0.016
The decoding procedure enforces feasibility by restricting U to customers whose demand does not exceed the remaining
capacity q. If no feasible customer exists, the current route is terminated and the vehicle returns to the depot. Only the
selection function varies across solver programs; all other components of the route-construction procedure are fixed.
G.3. Prompt Configuration
We specify task-level base prompts for the capacitated vehicle routing problem, as illustrated in Figure 10. These prompts
define the CVRP task, including the optimization objective, required function interfaces, and task-specific constraints
for solver and generator synthesis. Prompt construction for CVRP follows the same procedure as in online bin packing.
Specifically, the CVRP base prompts are embedded into the unified prompt template structure shown in Figure 5, with
task-specific content populated accordingly and all remaining components following the standard template of the search
mechanism.
G.4. Evolution Outcomes
Figure 11 visualizes representative solver and generator programs discovered by ASRO on CVRP. On the solver side, the
learned rules incorporate savings- and regret-shaped customer selection under explicit capacity pressure. On the generator
side, instance constructions evolve from simple sampling to spatially clustered geometries with heterogeneous demand
profiles that stress routing decisions.
G.5. Benchmarks (CVRPLIB).
We evaluate CVRP solvers on benchmark instances from CVRPLIB (Uchoa et al., 2017), a widely used public repository of
capacitated vehicle routing problems. CVRPLIB contains instances with diverse spatial distributions, demand patterns, and
capacity constraints, and is commonly adopted for evaluating both heuristic and learning-based routing algorithms.
We consider instance families A, B, E, F, M, P, and X. These families span a broad range of problem sizes and structural
characteristics (Uchoa et al., 2017). In particular, A and B instances are relatively regular and homogeneous, while E, F, M,
P, and X exhibit increasing heterogeneity in customer distributions, route lengths, and demand structures.
All CVRPLIB instances used for evaluation are distinct from the synthetic distributions employed during training. Per-
formance is measured using normalized optimality gaps with respect to reference solutions, providing a standardized and
widely accepted evaluation protocol.
H. Additional Ablation Results
This appendix reports detailed ablation results that support the analysis in Section 4.4. Due to space constraints, these results
are omitted from the main text.
24


--- Page 25 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Figure 10. Base prompt examples for solver and generator synthesis in the Capacitated Vehicle Routing Problem (CVRP).
Prompt for Generator
Task.
Write a novel Python function that produces realistic, well-structured
CVRP instances designed to reduce the expected solution quality achieved
by a mixed pool of heuristic solvers. The difficulty should arise from
general structural properties of customer distributions, demand patterns,
and capacity constraints, rather than solver-specific tricks.
The generation process may follow a structured protocol: (A) choose a
sampling plan that induces hard solution landscapes (e.g., clustered cus-
tomers, uneven demands, tight capacity); (B) sample realistic customer
locations, avoiding extreme clustering or purely uniform layouts; (C) as-
sign demands that create non-trivial capacity challenges via a mix of small
and large demands; (D) perform lightweight self-checks (e.g., demand
sum, capacity feasibility, spatial spread), and resample with bounded at-
tempts if degenerate cases arise.
I/O.
Implement generate instances(seeds, num customers,
vehicle capacity) and return instances.
Input/Output Info.
seeds: iterable of integers, one per instance.
num customers: number of customers per instance (int).
vehicle capacity: vehicle capacity (int).
instances: Python list with the same length as seeds; each ele-
ment is a dict with exactly the following keys: depot: list [x, y]
representing depot coordinates; customers: list of dicts, each with
’coords’ [x, y] in [0, 100] × [0, 100] and ’demand’ (positive inte-
ger); vehicle capacity: integer.
Other Info.
Avoid pathological or degenerate instances; prefer bounded resampling
with quick checks over heavy computation. Output ONLY the Python
function code.
Prompt for Solver
Task.
Write a step-by-step Python construction heuris-
tic for the CVRP that selects the next node at
each step to minimize total travel distance while
respecting vehicle capacity constraints. The
heuristic should aim to perform well across
instances drawn from a mixture of generators,
rather than overfitting to a single instance distri-
bution.
I/O.
Implement select(current node,
depot, unvisited nodes,
rest capacity, demands,
distance matrix) and return next node.
Input/Output Info.
current node: int, current location (0 de-
notes the depot; 1..n are customers).
depot: int, always 0.
unvisited nodes: NumPy array of feasible
customer IDs (subset of {1, . . . , n}).
rest capacity: remaining vehicle capacity
(float).
demands: NumPy array of length (n + 1) with
demands[0]=0.
distance matrix: NumPy array of shape
(n + 1, n + 1) with symmetric distances.
next node: int, either 0 (return to depot) or an
element of unvisited nodes.
Other Info.
Output ONLY the Python function code.
H.1. Data Augmentation and Self-play
TSP Data augmentation distribution.
The data augmentation (DA) training distribution is constructed as a fixed mixture
of five geometric families with weights: uniform square (30%), clustered Gaussian (25%), grid-jitter (15%), annulus/ring
(15%), and two-scale mixture (15%). For each instance, a family is sampled according to these weights, followed by
family-specific coordinate generation. Full specifications are given below.
(A) Uniform square (30%). Each city is sampled independently as (x, y) ∼Unif([0, 1]2).
(B) Clustered Gaussian (25%). The number of clusters is sampled as k ∼Unif{3, . . . , 8}, with cluster centers drawn
uniformly in [0, 1]2. Each city is assigned to a random cluster and sampled as (x, y) = cj + ϵ, where ϵ ∼N(0, σ2I) and
σ ∼Unif[0.02, 0.08]. Coordinates are clipped to [0, 1]2.
(C) Grid-jitter (15%). An approximately ⌈√n⌉× ⌈√n⌉regular grid is constructed over [0, 1]2, where n denotes the
number of cities. Grid points are perturbed by i.i.d. noise δx, δy ∼Unif[−0.05, 0.05], and n points are selected; if necessary,
remaining points are sampled uniformly.
(D) Annulus / ring (15%). Angles are sampled uniformly in [0, 2π] and radii are sampled uniformly from [r0, r1], where
r0 ∼Unif[0.2, 0.4] and r1 ∼Unif[0.5, 0.7]. Polar coordinates are converted to Cartesian coordinates, translated to the
unit-square center, and clipped to [0, 1]2.
(E) Two-scale mixture (15%). Seventy percent of cities are sampled uniformly over [0, 1]2, while the remaining 30% are
sampled within a small square of side length sampled from [0.05, 0.15] at a random corner. City order is randomly permuted
after generation.
25


--- Page 26 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Nearest-feasible neighbor
def select(cur, depot, U, cap, dem, dist):
    if not U: 
        return depot
    F = [u for u in U if dem[u] <= cap]
    if not F: return depot
    return argmin(F, key=lambda u: dist[cur,u])
Uniform square + moderate demands
def generate_instances(seeds, N, C):
    instances = []
    for seed in seeds:
        rng = np.random.default_rng(seed)
        depot  = uniform_square(rng)
        coords = [uniform_square(rng) for _ in range(N)]
        dem    = [randint(rng, 1, floor(C/3)) for _ in range(N)]
        instances.append(instance(depot, coords, dem, C))
    return instances
Inner ring + outer clusters + heavy-tail demands
def generate_instances(seeds, N, C):
    inst = []
    for seed in seeds:
        rng = np.random.default_rng(seed), depot = (50, 50), coords = [] 
        n_in = round(uniform(rng, 0.3, 0.6) * N)
        for _ in range(n_in):    # inner ring
            a = uniform(rng, 0, 2π); r = uniform(rng, 10, 30)
            coords.append(depot + r*(cos(a), sin(a)))
        K = randint(rng, 2, 3)    # outer clusters
        centers = [depot + uniform(rng, 40, 70)*(cos(a), sin(a)) for a in uniform_vec(rng, K, 0, 2π)]
        for _ in range(N - n_in):
            c = choice(rng, centers)
            coords.append(normal2d(rng, mean=c, sigma=uniform(rng, 5, 20)))
        # heavy-tailed demands (lognormal → rescale → clip)
        raw = lognormal(rng, sigma=uniform(rng, 1.5, 2.5), size=N)
        dem = clip(rescale_to_int(raw, lo=1, hi=⌊0.9C⌋), 1, C-1)
        inst.append(instance(depot, clip2d(coords), dem, C))
    return inst
Skewed clusters + rare large demands + high-utilization filtering
def generate_instances(seeds, N, C):
    inst = []
    for seed in seeds:
        rng = np.random.default_rng(seed), depot = (50, 50)
        for _ in range(tries=20):
            # skewed cluster masses → uneven spatial pressure
            K = randint(rng, 3, 6)
            centers = sample_ring_centers(rng, depot, K, r_range=(15, 65))
            mass = normalize([1/(k+1) for k in range(K)])  # skew
            cid  = categorical(rng, mass, size=N)
            coords = [normal2d(rng, centers[c], sigma=uniform(rng, 5, 20)) for c in cid]
            # demand traps: mostly small/medium, inject rare large demands
            dem = []
            for i in range(N):
                if bernoulli(rng, p=0.10): dem.append(randint(rng, ⌊C/2⌋, C-1))
                else:  dem.append(randint(rng, 1, ⌊C/3⌋))
            # enforce tight utilization band by rejection sampling
            m = ceil(sum(dem)/C), util = sum(dem)/(m*C)
            if 0.75 <= util <= 0.90 and spread_ok(coords):
                inst.append(instance(depot, clip2d(coords), dem, C)), break
    return inst
Savings − Capacity pressure
def select(cur, depot, U, cap, dem, dist, eps=1e-9):
    if not U:
        return depot
    score = {}
    for u in U:
        # depot-based savings: how much shorter than going back to depot
        saving_u = dist[cur, depot] + dist[u, depot] - dist[cur, u]
        # capacity pressure: penalize customers consuming large fraction of remaining cap
        pressure_u = dem[u] / (cap + eps)
        score[u] = saving_u - pressure_u
    return argmax(U, key=lambda u: score[u])
Regret + isolation penalty + depot gate
def select(cur, depot, U, cap, dem, dist, eps=1e-9):
    if not U:
        return depot
    # regret(u): second-best continuation much worse than best (local uncertainty)
    def regret(u):
        V = [v for v in U if v != u]
        if len(V) < 2:
            return 0.0
        nn = sorted([dist[u, v] for v in V])[:2]
        return nn[1] - nn[0]
    rmax = max(regret(u) for u in U) + eps
    dmax = max(dist[depot, u] for u in U) + eps
    # combine local lookahead, regret bonus, and isolation penalty
    score = {}
    for u in U:
        lookahead = dist[cur, u] + 0.5 * dist[u, depot]
        iso = dist[depot, u] / dmax
        cap_pressure = dem[u] / (cap + eps)
        score[u] = (2.0 - cap_pressure) / (lookahead + eps) * (1.0 + 0.5 * regret(u) / rmax) *
(1.2 - iso)
    best = argmax(U, key=lambda u: score[u])
    # depot-return gate under tight capacity or risky detour
    risky_detour = (dist[cur, best] > 1.5 * dist[cur, depot]) and (dist[depot, best] > 0.9 *
dmax)
    if cur != depot and (cap < mean(dem[1:]) * 1.2 or risky_detour):
        return depot
    return best
Figure 11. Representative solver and generator programs discovered by ASRO on the capacitated vehicle routing problem (CVRP).
We show distilled solver customer-selection rules (left) and instance generators (right) drawn from the final solver and generator pools.
Code is schematized to highlight the core algorithmic logic induced by adversarial pressure; auxiliary implementation details (e.g.,
batching, feasibility filtering, and acceptance criteria) are omitted for clarity.
Self-play baseline.
The self-play variant removes the meta-game structure of ASRO while retaining alternating solver–
generator optimization. At each iteration, the solver and generator are updated only against their most recent counterparts,
without maintaining a strategy pool or computing mixed meta-strategies. Concretely, self-play is implemented by restricting
both the solver and generator strategy pools to size one, so that only the most recent program is retained on each side. This
results in a greedy, memoryless adversarial loop rather than equilibrium-based co-evolution.
Table 6 reports the corresponding ablation results.
H.2. Effect of LLM Backbones
Backbone specification.
We evaluate ASRO with multiple LLM backbones for program-space search: DeepSeek-
V3.2 (Liu et al., 2025a), Gemini-3-Flash-Preview, GPT-5-Nano, GPT-5-Mini, and LLaMA-3-8B-Instruct (Grattafiori et al.,
2024). All models are used with identical prompts and decoding configurations. Table 7 reports the corresponding ablation
results.
H.3. Oracle-agnosticity
ReEvo (Ye et al., 2024) is an evolutionary program-synthesis framework that integrates LLMs into evolutionary search by
using LLMs both to generate new heuristic candidates and to produce reflective feedback that guides evolution. Compared
26


--- Page 27 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Table 6. Ablation on training strategies for TSP. Comparison between EoH with data augmentation (DA) or self-play (SP) and
ASRO-EoH. Results are reported as average optimality gaps (%↓).
Method
LIB-S
LIB-M
LIB-L
LIB-XL
Uniform
EoH
0.77
1.99
3.40
4.20
0.27
EoH + SP
0.30
1.06
2.22
3.50
0.22
EoH + DA
0.44
1.17
2.03
3.66
0.28
ASRO-EoH
0.23
0.67
1.73
2.82
0.06
Table 7. Effect of different LLM backbones in ASRO (TSP). All variants use the same ASRO-EoH framework and differ only in the
LLM used for program-space search; the EoH baseline (DeepSeek) is included for reference. Results are reported as average optimality
gaps (%↓).
LLM
LIB-S
LIB-M
LIB-L
LIB-XL
Uniform
DeepSeek
0.23
0.67
1.73
2.82
0.06
Gemini
0.23
0.45
0.96
2.45
0.06
GPT-5 nano
0.26
0.69
2.38
3.68
0.10
GPT-5 mini
0.21
0.47
1.39
2.53
0.10
LLaMA-3-8B
0.31
1.01
3.00
4.33
0.20
EoH (DeepSeek)
0.77
1.99
3.40
4.20
0.27
1
2
3
4
5
Generator Best-Response Search Rounds Rg
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Gap (%)
(a) Varying generator best-response rounds Rg with solver rounds
fixed (Rs = 2).
1
2
3
4
5
Solver Best-Response Search Rounds Rs
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Gap (%)
libs
libm
lib
libxl
uniform
(b) Varying solver best-response rounds Rs with generator rounds
fixed (Rg = 2).
Figure 12. Ablation of best-response search rounds in ASRO on Euclidean TSP. Each subplot reports the optimality gap (%) across
TSP benchmark instances under different configurations of solver-side and generator-side best-response rounds.
to the EoH-style program search mechanism, which relies on implicit feedback from heuristic evaluations, ReEvo explicitly
incorporates reflective LLM feedback into the evolutionary loop via short- and long-term reflections that guide crossover
and mutation. In this ablation, the EoH-style program search mechanism used in the main experiments is replaced with
a ReEvo-style mechanism to instantiate ASRO’s program-space best-response oracle. Table 8 reports the corresponding
ablation results.
H.4. Computation Cost Details
This section provides additional details for the equal-time comparison. Both ASRO and the EoH baseline are evaluated
under an identical wall-clock time budget of 2200 seconds using the same batch evaluation engine described in Appendix C.
Table 9 reports the resulting optimality gaps under this evaluation protocol.
I. Additional Ablation Studies
Ablation on Best-Response Search Depth
We observe heterogeneous sensitivity across benchmarks. Simpler instance
distributions (e.g., uniform instances) are largely insensitive to best-response depth, while more structured benchmarks
(e.g., TSPLIB-XL) exhibit clearer responses. Increasing solver- or generator-side rounds generally improves best-response
27


--- Page 28 ---
Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
Table 8. ReEvo as the program-space best-response oracle. Comparison with ReEvo under the same ASRO framework.
Method
LIB-S
LIB-M
LIB-L
LIB-XL
Uniform
ReEvo
0.38
1.35
2.25
3.65
0.38
ASRO-ReEvo
0.30
1.19
2.06
3.53
0.07
Table 9. Equal-time comparison between EoH and ASRO-EoH on TSP. Both methods are evaluated under the same wall-clock time
budget (2200s).
Method
LIB-S LIB-M LIB-L LIB-XL Uniform
EoH
0.769
1.985
3.399
4.201
0.270
ASRO-EoH 0.319
0.936
2.417
3.993
0.117
0.0
0.2
0.4
0.6
0.8
Minimum Ratio
0
1
2
3
4
Gap (%)
TSPLIB-S
TSPLIB-M
TSPLIB-L
TSPLIB-XL
Uniform
Figure 13. Ablation of the minimum base-generator ratio in ASRO on Euclidean TSP. Results report the average optimality gap (%) across
TSP benchmarks under different minimum ratio settings.
accuracy, but with diminishing returns. On the solver side, performance largely saturates at Rs = 2. On the generator side,
moderate depth is sufficient; isolated fluctuations may arise due to stochastic effects and conditioning on a fixed counterpart
during ablation. Overall, we adopt Rs = 2, Rg = 2 as a balanced setting that achieves strong performance with substantially
lower computational cost.
Ablation on Base-Generator Mixing Ratio.
The minimum ratio specifies a lower bound on the probability mass assigned
to the base generator, with the remaining mass allocated to the learned generator mixture during solver-side best-response
search. Its effect varies with instance difficulty. For simpler distributions, performance is largely insensitive to this parameter,
as the solver can reliably optimize against the base generator. In contrast, more structured benchmarks exhibit a clearer
dependence: very small ratios may lead to instability due to overfitting to the learned generator mixture, while excessively
large ratios bias the opponent distribution toward the base generator and limit the effectiveness of co-evolution.
28
