--- Page 1 ---
1
Hyperion: Hierarchical Scheduling for Parallel LLM
Acceleration in Multi-tier Networks
Mulei Ma, Graduate Student Member, IEEE, Xinyi Xu, Minrui Xu, Member, IEEE, Zihan Chen, Member, IEEE,
Tony Q. S. Quek, Fellow, IEEE, and Yang Yang, Fellow, IEEE
Abstract‚ÄîLLMs are increasingly executed in edge where
limited GPU memory and heterogeneous computation jointly
constrain deployment which motivates model partitioning and
request scheduling. In this setting, minimizing latency requires
addressing the tight coupling between model placement and
request scheduling across heterogeneous nodes, as suboptimal
decisions in one domain can negate benefits in the other. In this
paper, we propose Hyperion, a hierarchical two-stage framework
that jointly optimizes partitioning and scheduling for pipelined
LLM inference. Hyperion minimizes latency by balancing re-
sources across tiers without requiring model retraining or in-
curring significant runtime overhead. Leveraging the timescale
difference between partitioning and request arrivals, Stage 1
performs offline, inter-tier partitioning via a Hyperion Split
with Dynamic Programming (HypSplit-DP) procedure to produce
balanced stage times under tier capacity and memory constraints;
to adapt to time-varying load, Stage 2 performs online, intra-tier
scheduling with a lightweight Hyperion Scheduling for Real-Time
(HypSched-RT) that maps each request to the best available node
using real-time estimates of queue length and effective capacity.
Experiments with Phi-3-medium demonstrate that Hyperion
reduces latency by up to 52.1% (vs. GPipe) and 31.2% (vs.
HEFT). Furthermore, Hyperion exhibits superior scalability for
long-sequence generation, maintaining 44.5% lower latency and
higher GPU utilization.
Index Terms‚ÄîLarge Language Models (LLMs), Distributed
Inference, Edge Computing, Hierarchical Scheduling
I. INTRODUCTION
Large language models (LLMs) based on the Transformer
architecture have achieved revolutionary breakthroughs in the
field of natural language processing (NLP) [1]. The rise
of LLMs is transforming cloud/data-center workloads and
Mulei Ma is with the IoT Thrust, The Hong Kong University of Sci-
ence and Technology (Guangzhou), China. (e-mail: mma085@connect.hkust-
gz.edu.cn).
Xinyi Xu is with the National Key Laboratory of Wireless Communications,
University of Electronic Science and Technology of China, China. (e-mail:
xinyixu@std.uestc.edu.cn).
Minrui Xu is with the School of Computer Science and Engineering,
Nanyang Technological University, Singapore 639798, Singapore (e-mail:
minrui001@e.ntu.edu.sg).
Zihan Chen is with the Information System Technology and Design Pillar,
Singapore University of Technology and Design, Singapore 487372 (e-mail:
zihan chen@sutd.edu.sg).
Tony Q. S. Quek is currently the Full Professor with the Singapore
University of Technology and Design (SUTD), Singapore 487372, where he
also serves as the Head of the ISTD Pillar, the Sector Lead of the SUTD
AI Program, and the Deputy Director of the SUTD-ZJU IDEA. (e-mail:
tonyquek@sutd.edu.sg)
Yang Yang is currently the Dean with the Shanghai Center, Hong Kong
University of Science and Technology, China, also with Peng Cheng Lab-
oratory, Shenzhen 518055, China, and also with Terminus Group, Beijing
100027, China. (e-mail: yyiot@hkust-gz.edu.cn).
sparking strong interest at the edge [2]. According to a forecast
by the International Data Corporation (IDC), by 2025, over
75% of data will be generated at the edge [3]. Rapid Internet
of Things (IoT) and 5G development is driving strong demand
for efficient on-device AI across smartphones, wearables, etc.
In these scenarios, low latency and real time processing are
crucial; deploying LLMs on edge devices can help provide
more intelligent and personalized services [4].
However, edge devices still face severe limitations in com-
puting and memory resources [5], making it challenging to
run LLMs directly [6]. For example, NVIDIA Jetson Nano
typically have memory ranging from 4 GB to 12 GB and com-
puting capabilities around 472 gigaflops (GFLOPS), whereas
GPT-3 has a model size exceeding 350 GB and requires
approximately 314 GFLOPS of computation per token during
inference [7]. This limitation extends beyond single devices,
rendering flat, single-tier networks similarly impractical for
efficient LLM inference. A single-layer topology precludes
proximity-aware, sensitivity-driven deployment of inference
stages, resulting in poor load balancing and diminished fault
tolerance. Therefore, adopting a multi-tier network architecture
offers an effective path for deploying LLMs in resource-
constrained Scenarios [8].
To facilitate LLM deployment, we adopt a multi-tier net-
work architecture. As shown in Fig. 1, this multi-tier ar-
chitecture comprises heterogeneous computing nodes dis-
tributed across three tiers under the central cloud [9]. Through
Network AI Management and Orchestration (NAMO), the
cloud coordinates cross-domain resources, and orchestrates
task placement [10]; tier-level Network AI Logic and Control
(NALC) modules execute real-time, region-local optimization.
The multi-tier design also offers practical deployability, pro-
viding enhanced privacy and flexibility while aligning with
geographic or organizational boundaries [11]. For example,
in manufacturing, Tier 1 sits at each production line, Tier 2
spans the campus across multiple lines, and Tier 3 resides
in the company‚Äôs headquarters, connected via secure wireless
backhaul.
Despite the potential of this multi-tier architecture, ef-
ficiently deploying LLMs within such a framework faces
two hurdles. First, the edge environment is characterized by
device heterogeneity, with hardware ranging significantly in
performance from high-performance edge servers and routers
to low-power sensors [12]. Compounding this issue, statistics
indicate that edge device idle rates can be as high as 60%,
leading to systemic resource underutilization [13]. Second,
there is a research gap in optimizing LLMs for these specific
arXiv:2511.14450v2  [cs.DC]  1 Dec 2025


--- Page 2 ---
2
First Tier
Second Tier
Third Tier
Central Cloud
Edge
Cloud Server
Devices
Network AI Management and Orchestration 
(NAMO)
Management and 
orchestration
Cross-domain resource 
coordination
Service quality 
assurance
Multi-vendor resource 
integration
Task distribution 
and management
Prompt
Network AI Logic and 
Control (NALC)
Task-oriented 
management
Real-time 
optimization 
and control
Local resource coordination
Hierarchical 
structure 
management
Edge 
computing 
capabilities
LLM Weight 
Transmission
Prompt & 
Activation 
Prompt
Model 
partitioning
Prompt 
scheduling
Fig. 1: Multi-tier Network Architecture Incorporating three Edge Tiers for Optimized LLM Inference and Resource Management
network structures. Current research primarily focuses on
techniques like model compression, knowledge distillation,
and model pruning to adapt to the resource constraints of
edge devices [14]. Moreover, existing distributed deep learning
strategies are poorly optimized to exploit resources across
heterogeneous, multi-tier computing networks, leading to low
system efficiency [15]. Most existing scheduling algorithms
fail to consider the specific topology characteristics of multi-
tier networks, leaving a gap in cross-tier optimization [16].
Bridging this research gap is complex, as optimizing in-
ference performance in such multi-tier architectures hinges
on two tightly coupled decisions. The first is inter-tier model
partitioning: segmenting the model across heterogeneous tiers
to enable pipeline parallelism. The second is intra-tier task
scheduling: dispatching incoming requests to the most suit-
able nodes within a tier under resource constraints. Partition-
ing determines compute and communication balance, while
scheduling governs queueing delay and resource contention.
Optimizing them in isolation yields imbalanced pipelines and
head-of-line blocking that dominate end-to-end latency [17].
Therefore, to achieve efficient inference across multi-tier
networks, we propose Hyperion, a framework that introduces
a two-stage strategy. Crucially, our work moves beyond the
isolated, single-tier scheduling common in current research by
creating a unified, holistic optimization strategy that considers
the entire network hierarchy. To this end, we first formulate a
comprehensive Min-Max problem aimed at minimizing the in-
ference latency, denoted as (P0), and then introduce Hierarchi-
cal Parallel Execution and resource-aware scheduling for LLM
Inference on multi-tier Networks (Hyperion), which decouples
the complex joint optimization problem into two less complex,
sequential sub-problems. First Stage: Model Partitioning and
inter-tier Allocation Strategy (P1), which is an offline static
process that partitions the LLM‚Äôs blocks across tiers. By
considering each tier‚Äôs computing and memory resources to
optimize load balancing, this stage leverages a Hyperion
Split with Dynamic Programming (HypSplit-DP) algorithm to
determine the optimal partition strategy. Second Stage: intra-
tier Task Scheduling Strategy (P2). Taking the fixed partition
from first stage as a given, this stage leverages the Hyperion
Scheduling for Real-Time (HypSched-RT) algorithm, which
achieves optimal O(Kj) time complexity via a single linear
scan, performing online scheduling of prompts/activations to
identify the most suitable intra-layer nodes for pipeline paral-
lel. This hierarchical decoupling strategy significantly reduces
the overall computing complexity, providing a scalable and
efficient solution. The main contributions of this study are
summarized as follows:
1) We formulate the inference latency challenge in heteroge-
neous, memory-constrained multi-tier networks as a Min-
Max orchestration problem (P0). We propose Hyperion,
a framework that pioneers a dual-driven (model & task
aware), two-stage strategy, decoupling offline model par-
titioning (P1) from online task scheduling (P2) to achieve
holistic optimization.
2) For the offline stage, we design a HypSplit-DP algorithm
that delivers a provably optimal, resource-aware partition-
ing of the LLM‚Äôs blocks across heterogeneous tiers by
balancing computing and memory constraints.
3) For the online stage, we introduce the lightweight
HypSched-RT approach, which dynamically assigns in-


--- Page 3 ---
3
coming inference requests to the optimal node within
each tier by considering real-time device status and avail-
ability, guaranteeing a stable performance upper bound
with an linear O(Kj) complexity and negligible runtime
overhead.
4) We evaluate the Hyperion framework through exten-
sive experiments. Results demonstrate Hyperion‚Äôs per-
formance, reducing end-to-end latency by up to 31.2%
against the HEFT baseline and 52.1% against GPipe
(specifically 44.5% in long-sequence generation), validat-
ing its efficiency and scalability.
The remainder of this paper is organized as follows. Sec-
tion II reviews prior work in distributed LLM inference and
resource scheduling. Section III details our system model and
the optimization problem formulation. Section IV introduces
the Hyperion framework, detailing its two-stage scheduling
algorithms. Section V provides an extensive experimental
evaluation against several baselines. Finally, Section VI we
conclude the paper.
II. RELATED WORK
In recent years, as LLMs have continued to grow in
scale, efficiently deploying and inferring LLMs in resource
constrained edge scenarios has become an important research
area [18]. This paper focuses on the following four aspects
of related work: distributed accelerated inference of LLMs
in edge, resource scheduling, parallel computing (pipeline
parallelism, tensor parallelism, etc.), and the scheduling of
LLMs in multi-tier computing networks [19].
A. Distributed Accelerated Inference of LLMs in Edge Com-
puting
In edge scenarios, the computing and memory resources
of devices are limited, making it difficult to directly deploy
LLMs [20]. To address this issue, researchers have proposed
various distributed inference acceleration methods. Teerapit-
tayanon et al. [21] introduced the concept of Distributed Deep
Neural Networks (DDNN), which partitions the model into
multiple segments that are executed across the cloud, edge,
and end devices, reducing latency and bandwidth consumption.
This work demonstrated that collaborative execution across
different tiers significantly reduces inference time. To reduce
computing load, BranchyNet [22] introduced an early exit
mechanism that allows the model to output results early when
accuracy requirements are met. F. Dong et al. [23] further
proposed Adaptive Early Exit, which dynamically adjusts early
exit points to improve inference efficiency on edge devices.
Model compression are widely used on edge devices. Han
et al. [24] proposed Deep Compression, which uses pruning,
quantization, and Huffman coding to reduce model size by 35
times and computing load by 49 times. Li et al. [25] designed
a lightweight model architecture for Transformer models to
enable efficient inference on mobile devices. Hinton et al. [26]
introduced Knowledge Distillation, where a smaller model
learns to mimic the behavior of a larger model, achieving
model simplification. Kim et al. [27] applied knowledge
distillation to Transformer models, significantly improving
the performance of smaller models. However, these methods
still face limitations when dealing with LLMs that have over
tens of billions of parameters. Achieving efficient distributed
inference for LLMs in edge computing while maintaining
model performance remains a challenge.
B. Resource Scheduling for LLMs in Edge Computing
Efficient resource scheduling is a hotspot in LLM de-
ployment research. Wang et al. [28] proposed a deep re-
inforcement learning based computation offloading strategy
that dynamically allocates resources in multi user and multi
task edge environments. Their model considers factors such
as communication delay, energy consumption, and device
resources to minimize task completion time. Gu et al. [29]
designed a collaborative computing framework that allows
multiple edge devices to jointly execute tasks. By decompos-
ing and scheduling tasks, they significantly improved system
throughput and resource utilization. Lin et al. [30] proposed
a joint optimization strategy for task offloading and resource
allocation using game theory for multi device collaboration.
In edge environments, device heterogeneity poses a challenge
for resource scheduling. Xiao et al. [31] introduced a resource
management framework for heterogeneous edge environments
that considers device computing capacity, energy consumption,
and reliability to achieve efficient LLM deployment. During
the process of computation offloading, data security and pri-
vacy protection are also crucial. Zhou et al. [32] proposed a
secure computation offloading framework that uses differential
privacy and encryption techniques to safeguard user data
security. While these methods provide a foundational basis,
the device heterogeneity and fragmented resources inherent
in multi-tier networks pose distinct challenges. An effective
resource scheduling strategy is thus critical for the optimal
allocation of LLM partitions, which is a central aspect of our
work.
C. Parallel Computing of LLMs in Edge Computing
Parallel computation play a vital role in accelerating the
training and inference of LLMs.Huang et al. [33] proposed
GPipe, which uses pipeline parallelism to assign different
tiers of a model to different devices, significantly improving
training efficiency. Goyal et al.‚Äôs PipeDream framework [34]
further optimized the communication overhead of pipeline
parallelism, achieving efficient training in heterogeneous envi-
ronments. In edge computing, these methods can enable col-
laborative inference by mapping model blocks onto different
edge devices. Megatron-LM [35] utilizes tensor parallelism to
split large model parameters across multiple GPUs, enabling
efficient model training. In edge environments, Laskaridis et
al. [36] proposed the SPINN framework, which uses tensor
parallelism to distribute model parameters across multiple edge
devices, thereby enhancing inference performance. Narayanan
et al. [37] introduced Pipeline Model Parallelism, combining
data parallelism, model parallelism, and pipeline parallelism
to achieve better scalability. In edge scenarios, this hybrid
strategy can flexibly select parallel methods based on device
resources and network conditions, enhancing overall system


--- Page 4 ---
4
performance. In edge environments, network bandwidth and
communication latency are bottlenecks for parallel comput-
ing. Shi et al. [38] proposed a communication-aware par-
allel training method that reduces communication overhead
using techniques such as compression and pruning. Zheng
et al. [39] designed an efficient parameter synchronization
mechanism that reduces communication costs in distributed
training. However, despite these advances, applying these tech-
niques in edge requires overcoming challenges such as device
heterogeneity and network limitations. Also, these methods
primarily focus on single-tier optimizations and often overlook
the unique topological characteristics of multi-tier networks,
leaving a research gap in holistic, cross tier scheduling. Our
work addresses this by proposing a hierarchical framework
that cohesively optimizes LLM deployment across the entire
network hierarchy, accounting for its structural properties to
enhance overall system efficiency.
D. Research on LLMs Scheduling in Multi-tier Computing
Networks
Multi-tier Computing Networks integrate cloud, edge, and
end devices, providing abundant and heterogeneous computing
resources, thus bringing new opportunities and challenges for
deploying LLMs. Li et al. [40] proposed CoEdge, which dis-
tributes different layers of LLMs across cloud, edge, and end
devices, achieving hierarchical collaborative computing. Their
experiments showed that this method reduces latency by 30%
while maintaining model accuracy. Zhou et al. [41] designed
a dynamic task decomposition and scheduling framework that
dynamically adjusts task partitioning and scheduling strategies
of LLMs based on real time network and device conditions.
By predicting task execution time and communication delays,
system performance optimization is achieved. Cen and Zhu
[42] proposed NP-LLM, a unified framework that leverages
large language models for 6G network-layer planning. This
approach utilizes LLM generality to conduct scalable traf-
fic optimization and resource allocation. Wang et al. [43]
introduced a joint optimization scheduling strategy that si-
multaneously considers computing resources, communication
resources, and energy consumption. Using multi objective op-
timization algorithms, they achieved a balance between system
performance and energy efficiency. To address the challenges
of high communication overhead for LLM serving in multi-
tier networks, Wu et al. [44] proposed RecServe, a recursive
offloading framework that employs hierarchical confidence
evaluation and dynamic threshold adjustments. Their exper-
iments demonstrated that RecServe enhances service quality
and communication efficiency. Although extensive research
has been conducted at different levels on LLM deployment
and scheduling in edge and multi-tier computing networks,
a comprehensive optimization approach that simultaneously
considers model partitioning, resource scheduling, and task
scheduling is still lacking. This study aims to fill this gap by
proposing a two-stage task scheduling strategy to achieve effi-
cient deployment of LLMs in multi-tier computing networks.
Decoder Transformer 
Block
Decoder Transformer 
Block
Decoder Transformer 
Block
SoftMax
Classifier
Linear
Embedding
Text Embedding
Tokenization
‚Ä¶
‚Ä¶
Grouped Query 
Attention
RMS Norm
Feed Forward Layer
(SwiGLU)
‚Ä¶
Linear
Linear
SiLU
Attention Output
Q
K
V
RMS Norm
Grouped Query 
Attention
RMS Norm
‚äï
‚äï
Linear
Query
Key
Value
KV Cache
Rotary Positional 
Encoding
Fig. 2: Hierarchical Architecture of a Llama 3, from Decoder
Blocks to Detailed GQA and SwiGLU Component Views.
III. SYSTEM MODEL AND PROBLEM FORMULATION
A. Modeling of LLM Structure
The inference process of the decoder only based LLMs can
be represented as a linear Directed Acyclic Graph (DAG).
As shown in Fig. 2, we use the Llama 3 as an example to
illustrate the structure between these decoder blocks. Here,
the attention mechanism is implemented as Grouped Query
Attention (GQA), which incorporates Rotary Positional En-
coding (RoPE). The Feed Forward Network utilizes a SwiGLU
gating mechanism, and Root Mean Square Normalization
(RMS Norm) is applied before both the attention and feed
forward sub layers. These sub layers are sequentially arranged
to form a decoder transformer block, and the overall LLM is
constructed by stacking a series of these blocks. Despite this
intricate internal structure, we maintain the abstraction of the
entire decoder block as a single, atomic computing unit.
We abstract an LLM M as a computation graph composed
of N sequential decoder blocks. Let B = {B1, B2, . . . , BN}
be the ordered set of decoder blocks that constitute the model,
where N ‚ààN+ denotes the depth of the model. During the
inference process, a strict sequential dependency exists among
these blocks, which can be formally expressed as:
‚àÄi ‚àà{1, 2, . . . , N ‚àí1},
Bi ‚ÜíBi+1.
(1)
This implies that the output of block Bi serves as the exclu-
sive input to block Bi+1. The execution of each decoder block
Bi is accompanied by quantifiable computing and memory
resource requirements. We define the following two functions
to characterize its resource:
‚Ä¢ Computing Workload: We define a function f : B ‚ÜíR+,
where f (Bi) = fi denotes the total number of Floating
Point Operations (FLOPs) required to perform a single
forward propagation of module Bi.
‚Ä¢ Memory Requirements: We define a function m : B ‚Üí
R+, where m (Bi) = mi denotes the memory space,
measured in Gigabytes (GB), required on a computing


--- Page 5 ---
5
node to load the weight parameters of module Bi and
store its intermediate activation values during the forward
propagation process.
We aggregate the computing workload and memory require-
ments of the entire model into N-dimensional column vectors,
respectively:
f = [f1, f2, . . . , fN]T ‚ààRN√ó1.
(2)
m = [m1, m2, . . . , mN]T ‚ààRN√ó1.
(3)
B. Multi-tier Heterogeneous Network Architecture
We consider a multi-tier network composed of T tiers,
designed to host the pipelined inference tasks of a LLM. Let
T = {1, 2, . . . , T} be the index set of the network tiers. Each
tier j ‚ààT consists of a cluster containing Kj nodes. We
represent the set of nodes in tier j as Kj = {1, 2, . . . , Kj},
where Kj = |Kj| is the cardinality of nodes in that tier. The
set of all nodes across the entire network can thus be expressed
as K = S
j‚ààT {(j, k) | k ‚ààKj}.
Each node (j, k) in the network (the k-th node within
tier j) is equipped with independent computing and memory
resources. In this work, device heterogeneity is inter-tier. That
is, within any given tier j, all nodes have the same hardware
specification and are treated as homogeneous. The processing
speed of a node is defined by its computing capacity, denoted
as Cj,k and measured in FLOPs/s (Floating Point Operations
per second). Correspondingly, the available memory size of a
node is defined by its memory capacity, Mj,k, measured in
GB. For the purpose of making model partitioning decisions,
we need a metric to assess the overall service potential of each
tier. To this end, we define the ‚Äúeffective capacity‚Äù of a tier
based on the optimal performance achievable within that tier,
represented by the resources of its most capable node:
Ceff
j
‚âúmax
k‚ààKj {Cj,k} ,
‚àÄj ‚ààT ,
(4)
M eff
j
‚âúmax
k‚ààKj {Mj,k} ,
‚àÄj ‚ààT .
(5)
Here, Ceff
j
and M eff
j
represent the effective computing
capacity and effective memory capacity of a tier, respectively.
This definition characterizes the performance upper bound that
each tier can achieve under ideal scheduling conditions. When
different parts of the model are deployed across various tiers,
the transmission of intermediate data introduces communica-
tion latency.
We define an inter-tier communication latency, denoted as
œÑj,l, to be the time required to transmit an activation tensor
from any node in tier j to any node in tier l. Given the linear
pipeline structure of the model, the primary communication
overhead occurs between adjacent tiers, we have œÑj,j+1 =
Sact/Rj,j+1. Where, Sact is the size of the activation tensor
and Rj,j+1 is the achievable data rate. This data rate can be
expressed as Rj,j+1 = Bj,j+1 log2(1 + SINRj,j+1), where
Bj,j+1 is the allocated channel bandwidth, and SINRj,j+1
is the Signal-to-Interference-plus-Noise Ratio at the receiving
node [45].
C. Overall Optimization Problem
System Model & Problem Formulation
Overall 
Optimization Min-
Max Problem (P0) 
Hyperion: 
Hierarchical Parallel 
and Resource-aware 
scheduling for LLM 
Inferencing
Sub-problem 1 
(P1‚Äã):
Inter-Tier Model 
Partitioning
Sub-problem 2 
(P2‚Äã):
Intra-Tier Task 
Scheduling
Hyperion Two-Stage Scheduling 
Framework
Hyperion Split 
with Dynamic 
Programming
(HypSplit-DP)
Cross-layer 
Resource 
Awareness
Tier-Wide 
Partition 
Deployment
Offline Static 
Optimal 
Partitioning
Online Low-
Complexity Fast 
Scheduling
P1 solves ùíë
P2 solves ùíÄ
Algorithm
Problem 
Decoupling
Hyperion 
Scheduling for 
Real-Time 
(HypSched-RT)
Fig. 3: Framework for Problem Decomposition in Hyperion,
Detailing how the Overall Optimization Problem (P0) is
Decoupled into two Sub-problems, Inter-tier Partitioning (P1)
and Intra-tier Scheduling (P2), and Solved Respectively.
As illustrated in Fig. 3, we defines the core challenge as
an overall optimization Min-Max problem, denoted as P0.
The figure outlines how this primary problem is decoupled
into two manageable sub-problems: inter-tier Model Partition-
ing (P1) and intra-tier Task Scheduling (P2). The objective
is to minimize the end-to-end latency for inference, which
comprises two primary components: computing latency and
communication latency.
To precisely formulate the model partitioning and task
scheduling problems, we define two sets of decision variables.
We employ a partitioning vector p = (p1, p2, . . . , pT ‚àí1) ‚àà
NT ‚àí1 to define the partitioning scheme of the model. Herein,
pj denotes the global index of the last decoder block allocated
to the j-th tier. For convenience, we introduce the auxiliary
variables p0 ‚âú0 and pT ‚âúN. Consequently, the index set of
the decoder blocks assigned to tier j ‚ààT can be expressed
as:
Ij(p) = {i ‚ààN | pj‚àí1 < i ‚â§pj} .
(6)
We
define
a
set
of
binary
variables
Y
=
{yj,k | j ‚ààT , k ‚ààKj}
to
represent
the
task
scheduling
decisions. These variables are defined as follows:
yj,k =
(
1,
task of tier j scheduled to node k,
0,
otherwise .
(7)
The total latency, denoted as Ltotal(p, Y), is composed
of the computing latency Lcomp(p, Y) and the communica-
tion latency Lcomm(p). For a given partitioning scheme p,
the computing workload assigned to tier j is defined as
Wj(p) = P
i‚ààIj(p) fi. Should this workload be executed
on a specific node (j, k), the computation time would be
Wj(p)/Cj,k. Consequently, factoring in the scheduling de-
cision matrix Y, the actual computation time for tier j is
expressed as P
k‚ààKj yj,k
Wj(p)
Cj,k . Given that the tiers operate
in a pipeline fashion, the overall computing latency is dictated
by the performance of the slowest stage (i.e., the bottleneck).
Therefore:


--- Page 6 ---
6
Lcomp (p, Y) = max
j‚ààT
Ô£±
Ô£≤
Ô£≥
X
k‚ààKj
yj,k
Ppj
i=pj‚àí1+1 fi
Cj,k
Ô£º
Ô£Ω
Ô£æ.
(8)
The total communication latency is the cumulative sum
of the communication latency between all adjacent pipeline
stages.
Lcomm (p) =
T ‚àí1
X
j=1
œÑj,j+1.
(9)
Our goal is to minimize the total latency, which is the sum
of communication and computing latency, denoted as P0:
P0 : min
p,Y
Ô£´
Ô£≠max
j‚ààT
Ô£±
Ô£≤
Ô£≥
X
k‚ààKj
yj,k
Ppj
i=pj‚àí1+1 fi
Cj,k
Ô£º
Ô£Ω
Ô£æ
+
T ‚àí1
X
j=1
œÑj,j+1
Ô£∂
Ô£∏,
(10a)
s.t.
1 ‚â§p1 < p2 < ¬∑ ¬∑ ¬∑ < pT ‚àí1 < N,
(10b)
X
k‚ààKj
yj,k = 1, ‚àÄj ‚ààT ,
(10c)
pj
X
i=pj‚àí1+1
mi ‚â§
X
k‚ààKj
yj,kMj,k, ‚àÄj ‚ààT ,
(10d)
pj ‚àà{1, 2, . . . , N ‚àí1}, ‚àÄj ‚ààT ,
(10e)
yj,k ‚àà{0, 1}, ‚àÄj ‚ààT , ‚àÄk ‚ààKj.
(10f)
The core objective of P0 is to minimize the end-to-end
total inference latency, which is formulated as the sum of
two components. The first term in Eq. (10a) is the computing
latency, which corresponds to the computation time of the
longest running stage (i.e., the bottleneck) among all tiers. The
second term in Eq. (10a) is the communication latency, defined
as the aggregate time for transmitting intermediate activations
between all adjacent tiers. This optimization problem is subject
to a series of constraints. Constraint Eq. (10b) ensures the
validity of the model partition vector p, mandating that the
partition must be strictly increasing and fall within a valid
range, thereby partitioning the N decoder blocks into T non
empty and contiguous subsequences. Constraint Eq. (10c)
is the scheduling uniqueness constraint, which dictates that
for each tier j, its task must be assigned to one and only
one node within its node cluster Kj. Constraint Eq. (10d) is
the memory capacity constraint, which requires that the total
memory footprint of all modules assigned to any given tier
j must not exceed the available memory capacity Mj,k of
the specific node selected by the scheduling decision yj,k.
Constraints Eq. (10e) and Eq. (10f) define the domains of
the decision variables, specifying that the partition points pj
are integers, while the scheduling decisions yj,k are binary
variables.
D. Problem Decoupling and Hierarchical Solution
The overall optimization problem, P0, as previously for-
mulated, can be formulated as a Mixed Integer Nonlinear
Programming (MINLP) problem [46]. Its decision variables
include integer types (the model partition points p) and binary
types (the task scheduling decisions Y), and its objective
function contains a nonlinear max operator. Such problems are
typically NP hard, and the computing complexity of solving
them directly grows dramatically with the model depth N and
the number of network nodes, making direct solutions difficult
to apply.
To address this complexity, we propose a hierarchical frame-
work that decouples the problem into two more tractable
sub-problems. This two-stage approach serves as a heuristic
strategy designed to find an efficient solution. We leverage
the difference in the time scales of the decisions to decouple
the problem: model partitioning is generally a static, one time
offline decision, whereas task scheduling is a dynamic, online
decision that requires real time responsiveness. Based on this,
we decouple P0 into the following two sub-problems:
‚Ä¢ Sub problem 1: Cross Hierarchy Model Partitioning (P1):
This is an inter-tier, static planning problem. Its objective
is to find the globally optimal model partition point vector
p, under the assumption that each hierarchy can find the
optimal node.
‚Ä¢ Sub problem 2: Intra Hierarchy Task Scheduling (P2):
This is a intra-tier, dynamic decision problem. Given a
fixed model partitioning scheme, when the input data for
an inference request (i.e., the tokenized prompt for Tier
1, or intermediate activations for subsequent tiers) arrives
at a specific hierarchy, an optimal execution node must
be selected to execute the corresponding computational
task based on the real time status of all nodes within that
hierarchy.
The core of P1 is to determine the optimal model partition
vector, denoted as p‚àó. To render the partitioning decision
independent of the dynamic and instantaneous states of the
nodes, we introduce a key assumption. When making the
macroscopic partitioning decision, we replace the node level
computing capability Cj,k and memory capacity Mj,k in
the original problem with the tier level effective computing
capability Ceff
j
and tier level effective memory capacity M eff
j
,
respectively. Consequently, the original objective function and
its associated constraints are simplified, transforming it into
an integer programming problem, P1, that depends solely on
the partition variable p.


--- Page 7 ---
7
P1 : min
p
Ô£´
Ô£≠max
j‚ààT
(Ppj
i=pj‚àí1+1 fi
Ceff
j
)
+
T ‚àí1
X
j=1
œÑj,j+1
Ô£∂
Ô£∏,
(11a)
s.t.
1 ‚â§p1 < p2 < ¬∑ ¬∑ ¬∑ < pT ‚àí1 < N,
(11b)
pj
X
i=pj‚àí1+1
mi ‚â§M eff
j
, ‚àÄj ‚ààT ,
(11c)
pj ‚àà{1, 2, . . . , N ‚àí1}, ‚àÄj ‚àà{1, . . . , T ‚àí1}.
(11d)
Here, P1 is to minimize the total system latency based on
the estimated optimal performance of each tier. Constraints
(Eq.(11b)) and (Eq.(11c)) ensure the validity of the partition,
while constraint (Eq.(11d)) guarantees that the total memory
required for the model chunks assigned to any tier does not
exceed its effective memory capacity.
After obtaining the optimal partition p‚àó, which is computed
offline, the task for each tier j is determined. The subproblem
P2 is an online scheduling problem. When the intermediate
activations for an inference task arrive at tier j from tier j-1
(let the arrival time be t), the intra-tier scheduler must select an
execution node for it. This decision corresponds to determining
the scheduling vector for tier j, denoted as Yj = {yj,k}k‚ààKj,
which is a component of the overall decision matrix Y defined
in P0.
Unlike the offline static partitioning problem P1, P2 is a
dynamic online decision that must be made in real time as
tasks arrive. A high complexity scheduling algorithm would
introduce scheduling overhead, creating a new latency bottle-
neck that would undermine the optimization goals. Therefore,
the algorithm used to solve P2 (i.e., HypSched-RT) must be
lightweight and low complexity to ensure minimal overhead
and meet these real time requirements.
The optimization objective here is to minimize the task‚Äôs
completion time at the current tier, which is composed of the
node‚Äôs queuing latency and the task‚Äôs actual execution time.
Let Qj,k(t) be the task queue of node (j, k) at time t, and
let T wait
j,k
(t) be the expected waiting time to process all tasks
currently in that queue.
P2 : min
Yj
X
k‚ààKj
yj,k
Ô£´
Ô£≠T wait
j,k
(t) +
Pp‚àó
j
i=p‚àó
j‚àí1+1 fi
Cj,k
Ô£∂
Ô£∏,
(12a)
s.t.
X
k‚ààKj
yj,k = 1,
yj,k ‚àà{0, 1},
(12b)
p‚àó
j
X
i=p‚àó
j‚àí1+1
mi ‚â§M avail
j,k (t),
(12c)
IV. HYPERION: A TWO-STAGE HIERARCHICAL
PARTITIONING AND SCHEDULING FRAMEWORK
As illustrated in Fig. 4, the Hyperion framework operates
through a two-stage process to manage LLM inference across
the network. The first stage is an offline, inter-tier partitioning
phase where the central cloud strategically divides the LLM
based on resource awareness of the multi-tier network. This
partitioning results in a sequential deployment of the sub
models, creating a pipeline across the network tiers. The
second stage consists of online, intra-tier task scheduling.
Here, incoming prompts are tokenized and dispatched in real
time, with the HypSched-RT algorithm dynamically mapping
inference tasks to the optimal nodes within each tier for
execution, from activation input to output forwarding. This
scheduling process selects a single, most suitable device from
each tier‚Äôs node cluster to handle its portion of the inference
task.
A. Stage 1: Offline inter-tier LLM Partitioning via HypSplit-
DP
We aim to determine the optimal model partition vector
p‚àó=
 p‚àó
1, p‚àó
2, . . . , p‚àó
T ‚àí1

, which can be expressed as P1.
For notational convenience, we define the computing load of
the j-th tier as Lj(p) =
1
Cj
Ppj
i=pj‚àí1+1 fi, where p0 = 0
and pT = N. Therefore, the objective function can be con-
cisely expressed as minp maxj‚ààT {Lj(p)}. It is noteworthy
that the volume of data transmitted between adjacent tiers
is batch size √ó sequence length √ó hidden dim, which is a
constant value independent of the partitioning strategy p. Con-
sequently, the total inter-tier transmission time, represented
by PT ‚àí1
j=1 œÑj,j+1, can be disregarded during the optimization
process as it does not affect the determination of the optimal
solution.
This problem is formulated as a Min-Max combinatorial
optimization problem. A brute force approach of enumerating
all possible combinations of partitioning points, denoted by p,
would lead to a combinatorial explosion. To address this, we
have designed an algorithm that named Hyperion Split with
Dynamic Programming, which we refer to as HypSplit-DP.
We transform the original problem into a decision problem:
given a target maximum latency œÑ, does a valid partitioning
scheme p exist such that the computing latency of every tier
does not exceed œÑ. Specifically, we need to determine if there
exists a partitioning vector p = (p1, . . . , pT ‚àí1) that satisfies
the constraints of P1 while also fulfilling the condition:
Lj(p) =
Ppj
i=pj‚àí1+1 fi
Cj
‚â§œÑ,
‚àÄj ‚ààT .
(13)
Solving this efficiently allows us to then perform a binary
search for œÑ to find its minimum possible value, which corre-
sponds to the optimal solution of the original problem. We em-
ploy dynamic programming to solve the decision subproblem
Pcheck(œÑ). We define a boolean state variable, DP(j, n), which
is true if and only if there exists a feasible assignment of the
first n decoder blocks (B1, . . . , Bn) to the first j computing
tiers (Tier 1, . . ., Tier j) that satisfies both the latency and
memory constraints.
Definition 1. The state DP(j, n) is true if and only if there
exists a partition point k (where j ‚àí1 ‚â§k < n) such that the
following two conditions are satisfied:
‚Ä¢ The first k modules can be feasibly partitioned among
the first j ‚àí1 tiers; that is, DP(j ‚àí1, k) is true.


--- Page 8 ---
8
1st             2nd             3rd  
Multi-tier 
Network
Stage 1: Offline Inter-Tier LLM Partitioning
LLM
Resource 
Awareness
Cloud
Partitioning 
Strategy
Stage 2: Online Intra-Tier Task Scheduling
Prompt
Local
Processing
Activation 
Activation 
Activation 
Activation 
‚Ä¶
Deployment
Scheduling
Dispatch
Execution
Intra-Tier 
Model 
Replication
Map Optimal 
Node Path
Dispatch 
Prompt/ 
Activation 
Activation/
Output 
Forward 
Tokenization
Tokenization
via
HypSched-RT
via
HypSplit-DP
Partition 
Vector 
ùëù‚àó
Fixed 
Block 
Assignment
Resource 
Awareness
Fig. 4: System Architecture and Operational Workflow of the Hyperion Framework, Illustrating the end-to-end Process from
Offline LLM Partitioning in the Cloud to Online, Real Time Task Execution Across the Multi-tier Network.
‚Ä¢ The modules from k + 1 to n are assigned as a single
group to the j-th tier, and this assignment satisfies the
latency and memory constraints of the given decision
problem.
This logical relationship can be described by the following
state transition:
DP(j, n) =
n‚àí1
_
k=j‚àí1

DP(j ‚àí1, k) ‚àß
"
1
Cj
n
X
i=k+1
fi ‚â§œÑ
#
‚àß
"
n
X
i=k+1
mi ‚â§Mj
# 
,
(14)
in which ‚à®denotes the logical OR operation, ‚àßdenotes the
logical AND operation, and [¬∑] is the Iverson bracket, which
evaluates to 1 if the condition inside is true, and 0 otherwise.
To improve computing efficiency, we precompute the prefix
sums of fi and mi:
Sf(n) =
n
X
i=1
fi,
(15)
Sm(n) =
n
X
i=1
mi.
(16)
Hence, the interval sums can be computed efficiently:
Pn
i=k+1 fi = Sf(n) ‚àíSf(k) and Pn
i=k+1 mi = Sm(n) ‚àí
Sm(k). For the base case, at the first tier (j = 1), the state
DP(1, n) is true if and only if placing all of the first n modules
on the first tier satisfies the constraints:
DP(1, n) =
Sf(n)
C1
‚â§œÑ

‚àß[Sm(n) ‚â§M1] .
(17)
The solution to the entire decision problem Pcheck(œÑ) is
determined by the value of DP(T, N). If DP(T, N) is true,
it indicates that a feasible scheme exists with a latency not
exceeding œÑ; otherwise, no such scheme exists.
As illustrated in Algorithm 1, we first determine the initial
lower and upper bounds of the latency, œÑlow and œÑhigh. In each
iteration, we select a midpoint value œÑmid and solve a feasibility
subproblem, Pcheck(œÑmid), to ascertain whether the latency œÑmid
is achievable. This feasibility subproblem is efficiently solved
using dynamic programming (DP). Specifically, we construct
a DP table where the state DP(j, n) represents the feasibility
of partitioning the first n tasks among the first j nodes.
Based on the final state of the DP table, DP(T, N), we
adjust the boundaries for the binary search. If the state is true,
it implies that a better solution may exist; thus, we update
the upper bound to œÑhigh = œÑmid and record the currently
feasible partitioning scheme. If it is false, it indicates that
the current latency is too stringent and must be relaxed, so
we update the lower bound to œÑlow = œÑmid. This process
iterates until the search interval is smaller than a predefined
precision threshold œµ. Finally, the algorithm converges to the
optimal objective value, œÑhigh, and the optimal task partition-
ing vector, p‚àó, is retrieved by backtracking through the last
successful DP table. The overall complexity of this algorithm
is O(T ¬∑ N 2 ¬∑ log( œÑhigh‚àíœÑlow
œµ
)), which demonstrates excellent
computing efficiency for practical problem sizes.
The HypSplit-DP algorithm guarantees convergence to the
optimal solution of subproblem P1. It exploits the problem‚Äôs
monotonicity, that is, a feasible maximum delay œÑ implies any
œÑ ‚Ä≤ > œÑ is also feasible. In each search iteration, a DP check,
Pcheck(œÑ), serves as an exhaustive feasibility checker. Its state
DP(j, n) and transition function systematically exhaust all
valid combinations for partitioning N blocks into T tiers by
iterating through all possible preceding split points k.


--- Page 9 ---
9
Algorithm 1 HypSplit-DP for Optimal Inter-tier Model Par-
titioning
1: Input: Number of blocks N, tiers T, block costs
{fi}, {mi}, tier properties {Cj}, {Mj}, and precision œµ.
2: Output: Optimal partition vector p‚àóand minimized max-
imum latency œÑ ‚àó.
3: Compute prefix sums: Sf[n] ‚ÜêPn
i=1 fi and Sm[n] ‚Üê
Pn
i=1 mi.
4: Initialize binary search bounds œÑlow and œÑhigh.
5: Initialize p‚àó‚Üênull, œÑ ‚àó‚ÜêœÑhigh.
6: while (œÑhigh ‚àíœÑlow) > œµ do
7:
Set current target latency œÑmid ‚Üê(œÑlow + œÑhigh)/2.
8:
Initialize DP table DP[j][n] to false and predecessor
table P[j][n].
9:
Set DP[0][0] ‚Üêtrue.
10:
for j ‚Üê1 to T do
11:
for n ‚Üêj to N do
12:
for k ‚Üêj ‚àí1 to n ‚àí1 do
13:
if DP[j ‚àí1][k] is true then
14:
Calculate computing load Lj ‚Üê(Sf[n] ‚àí
Sf[k])/Cj.
15:
Calculate memory usage Uj
‚ÜêSm[n] ‚àí
Sm[k].
16:
if Lj ‚â§œÑmid and Uj ‚â§Mj then
17:
Set DP[j][n] ‚Üêtrue.
18:
Record predecessor P[j][n] ‚Üêk.
19:
Break.
20:
end if
21:
end if
22:
end for
23:
end for
24:
end for
25:
if DP[T][N] is true then
26:
A feasible partition found, update optimal latency
œÑ ‚àó‚ÜêœÑmid.
27:
Adjust search upper bound œÑhigh ‚ÜêœÑmid.
28:
Reconstruct partition vector p‚àóby backtracking
through P.
29:
else
30:
No feasible partition found, adjust search lower
bound œÑlow ‚ÜêœÑmid.
31:
end if
32: end while
33: Return p‚àóand œÑ ‚àó.
B. Stage 2: Online intra-tier Task Scheduling via HypSched-
RT
Once the optimal model partition p‚àóis determined, the
task assigned to tier j becomes fixed, with a total computing
workload of F ‚àó
j
= Pp‚àó
j
i=p‚àó
j‚àí1+1 fi. When an inference task
arrives at tier j at time t, the scheduler must select a node k
from the set of nodes at that tier, Kj, to execute the task. This
selection process constitutes the scheduling problem, Psched.
We reformulate the P2 problem as follows:
k‚àó= arg min
k‚ààKj
Ô£´
Ô£≠T wait
j,k
(t) +
Pp‚àó
j
i=p‚àó
j‚àí1+1 fi
Cj,k
Ô£∂
Ô£∏,
(18a)
s.t.
Node k is available at time t,
(18b)
p‚àó
j
X
i=p‚àó
j‚àí1+1
mi ‚â§M avail
j,k (t),
(18c)
We
propose
the
Hyperion
Scheduling
for
Real-Time
(HypSched-RT) algorithm to handle the online challenge of
arriving inference tasks. HypSched-RT enables intra-tier task
parallelism by scheduling inference tasks across a tier‚Äôs nodes
(Kj). We maintain a task queue Qj,k, for each node (j, k),
which enables the estimation of the waiting time T wait
j,k
(t).
The task queue Qj,k is essential because individual edge
nodes, such as the Jetson, possess limited parallel inference
capabilities. When a new task arrives, we assume it is placed
at the end of this queue, following a First In First Out (FIFO)
strategy. At a given time t, the expected waiting time for node
(j, k) is the sum of the remaining processing times for all tasks
currently executing and those waiting in its queue. Let qrun
j,k (t)
be the task being executed on node (j, k) at time t, and let
Qwait
j,k (t) be the set of tasks waiting in the queue of node (j, k)
at time t. Let Fq denote the total computing workload of a task
q, and let Fq,rem(t) be the remaining computing workload of
task q at time t. The expected waiting time can be calculated
as:
T wait
j,k
(t) =
Fqrum
j,k
(t), rem (t) + P
q‚Ä≤‚ààQwait
j,k (t) Fq‚Ä≤
Cj,k
.
(19)
If no task is currently being executed, then Fqrun
j,k,rem(t) = 0.
For a new task with a computing workload of F ‚àó
j arriving
at tier j at time t, the optimal scheduling decision k‚àóis
to select the node that yields the earliest completion time.
The completion time of the task on node k, denoted as
Tcomplete (j, k, t), is:
Tcomplete(j, k, t) = t + T wait
j,k
(t) + F ‚àó
j
Cj,k
.
(20)
Given that the current time t is a constant. Let Kavail
j
(t)
denote the set of all nodes in tier j that satisfy the availability
and real time memory constraints at time t. The optimal
scheduling decision, k‚àó, is given by:
k‚àó= arg
min
k‚ààKavail
j
(t)
Fqrun
j,k(t),rem(t) + P
q‚Ä≤‚ààQwait
j,k(t) Fq‚Ä≤
Cj,k
+ F ‚àó
j
Cj,k

.
(21)
As illustrated in Algorithm 2, when a task arrives at any
tier j of the network at time t, the scheduler immediately
gathers real time status from all nodes k ‚ààKj within that
tier. This information includes each node‚Äôs available memory,
M avail
j,k (t), and the current load of its task queue, encompassing


--- Page 10 ---
10
Algorithm 2 HypSched-RT for Intra-tier Scheduling
1: Input: A new task with workload F ‚àó
j ; the set of nodes
Kj in tier j; the real time state for each node k ‚ààKj
(including Cj,k, M avail
j,k (t), Fqrun
j,k(t),rem(t), and Qwait
j,k (t));
required memory mact.
2: Output: The optimal node for execution, k‚àó.
3: Initialize minimum completion cost min cost ‚Üê‚àû, and
optimal node k‚àó‚Üênull.
4: for each node k ‚ààKj do
5:
if node k is available and satisfies memory constraint
M avail
j,k (t) ‚â•mact then
6:
Calculate the total workload of tasks already in its
queue: Fk,queued ‚ÜêFqrun
j,k(t),rem(t) + P
q‚Ä≤‚ààQwait
j,k(t) Fq‚Ä≤.
7:
Compute the completion cost for the new task on
node k: costk ‚Üê(Fk,queued + F ‚àó
j )/Cj,k.
8:
if costk < min cost then
9:
Update the minimum cost: min cost ‚Üêcostk.
10:
Designate node k as the current optimal node:
k‚àó‚Üêk.
11:
end if
12:
end if
13: end for
14: Return the determined optimal node k‚àó.
the remaining workload of the running task, F run
qj,k(t), rem(t),
and the tasks in its waiting queue, Qwait
j,k (t). Subsequently,
the algorithm filters these nodes based on availability and
memory constraints to establish a set of candidate nodes,
Kavail
j
(t). Finally, by calculating the expected completion time
for the task on each candidate node k as Tcomplete(j, k, t) =
t + T wait
j,k
(t) + F ‚àó
j /Cj,k, the scheduler assigns the task to
the node k‚àóthat offers the minimum expected completion
time. The HypSched-RT algorithm is lightweight by design,
achieving an optimal computational complexity upper bound
of O(Kj). This efficiency stems from its deterministic, single-
pass linear scan, which eschews complex search-based heuris-
tics. As detailed in Algorithm 2, the scheduler iterates through
the Kj nodes exactly once. Within this pass, it executes only
constant-time operations (e.g., status checks) for each node.
This direct approach ensures rapid, real-time decision-making
with negligible computation overhead.
V. EXPERIMENT AND MODEL EVALUATION
A. Experimental Setup and Baselines
To evaluate the performance of the Hyperion framework, we
designed the following network. The setup comprised three
computing tiers, each consisting of multiple heterogeneous
NVIDIA Jetson nodes. Following the pipeline parallelism
model, the inference tasks flow sequentially from Tier 1 to
Tier 3. For clarity, the heterogeneity considered in our study
is across tiers: within each tier, all nodes are of the same device
model. The specific configurations for each tier, including the
device type, quantity, computing, and memory, are detailed
in Table I. To evaluate the generality of our scheduling
framework, we also evaluate Hyperion on the Llama3-8B
and Phi-3-medium models, with their architectural differences
TABLE I: Specifications of the Multi-tier Network and LLM
Architectures
Hardware Specifications
Tier
Device
Qty.
TOPS
Mem (GB)
Tier 1
J. Orin Nano
3
67
8
Tier 2
J. Orin NX
3
157
16
Tier 3
J. AGX Orin
2
200
32
Model Architectures
Model
Params
Blocks
Hid. Dim.
Attention
Llama3-8B
8B
32
4096
GQA
Phi-3-medium
14B
40
5120
GQA
detailed in Table I. The input size for all tasks was standardized
to 64 tokens, and the generation length was set to 128 tokens
for each task.
To evaluate the system under a dynamic workload, we
model the arrival of inference tasks as a Poisson process
with an average arrival rate of Œª = 0.2 tasks per second
[47]. We used the Linux TC traffic control tool to emulate
communication bandwidths of 1 Gbps and 100 Mbps. For the
offline model partitioning stage, the precision threshold for
the HypSplit-DP algorithm‚Äôs binary search is set to œµ = 10‚àí3,
ensuring millisecond level accuracy in latency optimization.
To benchmark the performance of the Hyperion algorithm,
comparisons were drawn against following baselines.
‚Ä¢ Heterogeneous Earliest Finish Time (HEFT): To provide
a robust heuristic based benchmark, we implement the
HEFT algorithm. This approach first employs a memory
aware greedy strategy to partition the LLM into contigu-
ous blocks and assign them to different computing tiers
[48]. Subsequently, for intra-tier scheduling, the classic
HEFT algorithm prioritizes tasks based on their upward
rank and maps each task to the node offering the earliest
finish time.
‚Ä¢ GPipe style Partitioning with GNN based Scheduling
(GPipe): To benchmark against a modern learning based
method, we developed a second baseline. For inter-tier
partitioning, it adopts the classic static, load balanced par-
titioning strategy of the GPipe framework, which divides
the model into contiguous segments [33]. For intra-tier
scheduling, it then leverages a Graph Neural Network
(GNN) to generate an optimal policy for mapping the
assigned model segments to specific nodes [17].
‚Ä¢ Hierarchical resource aware Scheduling for Parallel LLM
Inference (Hyperion): Central to this study, Hyperion
optimizes LLM inference in multi-tier networks through a
two-stage scheduling framework. Hyperion first performs
offline model partitioning using the HypSplit-DP algo-
rithm, followed by online, real time task scheduling with
the HypSched-RT algorithm.
B. Comparative Analysis of end-to-end Latency
We conduct experiments within the three tier architecture,
and the results are shown in Fig. 5. Using the LLaMA3 model,
we test at bandwidths of 1 Gbps, as presented in Fig. 5(a). The
x axis represents the number of tasks from 1 to 14, while the
y axis shows the average latency required to complete these


--- Page 11 ---
11
2
4
6
8
10
12
14
Number of tasks
0
200
400
600
Latency (s)
Hyperion
GPipe
HEFT
(a) 1 Gbps
2
4
6
8
10
12
14
Number of tasks
0
200
400
600
Latency (s)
Hyperion
GPipe
HEFT
(b) 100 Mbps
Fig. 5: Comparative Analysis of end-to-end Latency for Hy-
perion using the Llama 3 Model under Increasing Task Loads
at (a) 1 Gbps and (b) 100 Mbps Bandwidths.
tasks. The performance of GPipe, HEFT, and Hyperion are
represented by the grey dashed line, brown dash dotted line,
and blue solid line, respectively. Clearly, the latency of GPipe
and HEFT is significantly higher than that of Hyperion, with
GPipe consistently showing the highest latency. Specifically,
when the task count reaches 14, GPipe and HEFT take ap-
proximately 637s and 451s, respectively, while Hyperion takes
about 312s. At this point, the latency for GPipe and HEFT
are 51.0% and 30.8% higher than Hyperion‚Äôs, respectively.
This demonstrates that Hyperion effectively implements model
partitioning and scheduling, showing its efficiency in the multi-
tier network.
In Fig. 5(b), we test at bandwidths of 100 Mbps. As
the number of tasks increased from 1 to 14, the latency
for GPipe and HEFT increased from approximately 52.7s to
641.5s and from 38.5s to 471.1s, respectively. This shows total
latency increases of 588.8s for GPipe and 432.6s for HEFT.
In contrast, Hyperion‚Äôs latency increased from about 27.5s to
368.8s, a more modest increase of only 341.3s. This represents
a reduction in latency growth of 42.0% compared to GPipe and
21.1% compared to HEFT. This indicates that when the task
number surges, Hyperion can optimize task distribution across
tiers via its model partitioning algorithm, thereby utilizing
resources more efficiently and reducing task latency.
2
4
6
8
10
12
14
Number of tasks
0
200
400
600
800
1000
Latency (s)
Hyperion
GPipe
HEFT
(a) 1 Gbps
2
4
6
8
10
12
14
Number of tasks
0
200
400
600
800
1000
Latency (s)
Hyperion
GPipe
HEFT
(b) 100 Mbps
Fig. 6: Comparative Analysis of end-to-end Latency for Hy-
perion using the Phi-3-medium Model under Increasing Task
Loads at (a) 1 Gbps and (b) 100 Mbps Bandwidths.
To further validate the performance of Hyperion under
different models, we replaced the larger Phi-3-medium model
for testing. In Fig. 6(a), the curve corresponding to Hyperion
still remains at the bottom, indicating the shortest latency
across all task counts. The latency for GPipe and HEFT are
both significantly higher than that of Hyperion, with GPipe
consistently demonstrating the highest latency. For example,
when the number of tasks reaches 10, the latency for Hyperion
is approximately 355s, whereas HEFT and GPipe require about
516s and 742s, respectively. At this point, the latency for
HEFT is approximately 31.2% higher than Hyperion, while
GPipe‚Äôs latency is 52.1% higher. This suggests that for the
new Phi-3 model, Hyperion still achieves the lowest latency
through superior model scheduling, outperforming the other
algorithms. When the bandwidth decreases from 1Gbps to
100Mbps, the results shown in Fig. 6(b). Hyperion demon-
strated a clear advantage in both latency and the rate of
increase in reference time as task grew. The first two sets of
experiments shown that, the efficient scheduling of Hyperion
enables faster completion of inference under various model
and bandwidth.
Table II presents a performance of Hyperion with the
Llama3 and Phi-3 models on three-tier network in more detail.
Our analysis reveals several key findings. First, the Phi-3-


--- Page 12 ---
12
TABLE II: Performance Breakdown of the Hyperion Framework: A Detailed Look at Latency, Resource Utilization, and Block
Allocation Across Tiers for Llama 3 and Phi-3 Models at Varied Bandwidths.
Jetson Device
Model
Condition
Metric
AGX Orin
Orin NX
Orin Nano
Llama3-8B
1 Gbps Bandwidth
Avg. GPU Util. (%)
45.9%
47.2%
52.1%
Avg. Mem. Util. (%)
42.7%
45.2%
53.1%
Alloc. Blocks
18
9
5
end-to-end Latency (s)
24.8
100 Mbps Bandwidth
Avg. GPU Util. (%)
43.3%
45.3%
50.2%
Avg. Mem. Util. (%)
42.7%
45.2%
53.2%
Alloc. Blocks
18
9
5
end-to-end Latency (s)
27.2
Phi-3-medium
1 Gbps Bandwidth
Avg. GPU Util. (%)
53.6%
55.3%
59.3%
Avg. Mem. Util. (%)
69.2%
78.3%
83.1%
Alloc. Blocks
20
13
7
end-to-end Latency (s)
38.7
100 Mbps Bandwidth
Avg. GPU Util. (%)
49.7%
51.8%
55.7%
Avg. Mem. Util. (%)
69.2%
78.4%
83.3%
Alloc. Blocks
20
13
7
end-to-end Latency (s)
42.0
medium model is demonstrably more resource intensive than
Llama3-8B, exhibiting higher end-to-end latency and consum-
ing more GPU and memory resources across all configurations.
For instance, under the 1 Gbps condition, Phi-3-medium‚Äôs
latency was 35.9% higher than Llama3‚Äôs (38.7s vs. 24.8s),
with its memory utilization on the AGX Orin being notably
higher (69.2% vs. 42.7%).
Second, a clear trend is observed regarding device capabil-
ity: as computing decreases from the AGX Orin to the Orin
Nano, both average GPU and memory utilization consistently
increase for any given model and bandwidth. This indicates
that less powerful devices must operate at a higher capacity
to handle the workload. For example, with Llama3 at 1 Gbps,
GPU utilization rises from 45.9% on AGX Orin to 52.1% on
Orin Nano. Finally, network bandwidth has a direct impact
on performance. Reducing the bandwidth from 1 Gbps to
100 Mbps increased end-to-end latency by 9.7% for Llama3
and 8.5% for Phi-3-medium. Interestingly, the number of
allocated blocks for each device remained constant regardless
of bandwidth, suggesting a static allocation strategy, whereas
performance metrics like latency and GPU utilization are
dynamically affected by network conditions.
C. Resource Utilization Efficiency
We evaluated the GPU utilization of three algorithms,
exemplified on the AGX Orin nodes. With results illustrated
in Fig. 7, at 3 tasks, Hyperion exhibited the highest median
GPU utilization at approximately 47.4%. HEFT showed a
slightly lower median usage of about 46.3%, while GPipe
had the lowest at around 44.9%. When scaled to 13 tasks,
all algorithms demonstrated elevated GPU utilization. Hype-
rion‚Äôs median utilization rose to approximately 56.1%. At
this point, the median GPU utilization of HEFT and GPipe
also increased, to about 55.0% and 53.2% respectively, but
remained lower than that of Hyperion. This means that by
1
3
5
7
9
11
13
Task Range
42
44
46
48
50
52
54
56
GPU Utilization (%)
Hyperion
HEFT
GPipe
Fig. 7: GPU Utilization Efficiency on the AGX Orin: A
Comparison of Hyperion, HEFT, and GPipe with Increasing
Task Range.
J. Orin Nano
67TOPS, 8GB
J. Orin NX
157TOPS, 16GB
J. AGX Orin
200TOPS, 32GB
(a) Llama 3
J. Orin Nano
67TOPS, 8GB
J. Orin NX
157TOPS, 16GB
J. AGX Orin
200TOPS, 32GB
(b) Phi-3
Fig. 8: Tier Wise Resource Utilization under the Hyperion
Framework: A Scatter Plot Analysis of GPU vs. Memory
Consumption for (a) Llama 3 and (b) Phi-3 Models.
optimizing task scheduling in stage two, Hyperion achieves
superior utilization of network resources, minimizing device
idle time and reducing latency.


--- Page 13 ---
13
Next, we record the resource utilization of each tier device
when running the LLama3. As shown in Fig. 8 (a), devices in
Tier 1 demonstrated the most intensive resource usage, with
GPU utilization clustered between approximately 51.2% and
53.5% and corresponding memory utilization ranging from
51.6% to 53.8%. Following this, Tier 2 nodes operated at
an intermediate level, exhibiting GPU utilization in the range
of 46.1% to 47.5% and memory utilization between 45.1%
and 46.5%. Tier 3 nodes showed the lowest consumption,
with GPU utilization around 45% and memory utilization
consistently around 42%.
Analysis of the Phi model‚Äôs tier wise resource utilization
also reveals a clear hierarchical structure in Fig. 8 (b). GPU
utilization was distinctly stratified, with Tier 1 averaging
approximately 59.0%, followed by Tier 2 at 55.3-56.0% and
Tier 3 at 53.5-54.0%. This hierarchical pattern was even more
pronounced in memory consumption, where Tier 1 reached
roughly 83.0%, Tier 2 clustered around 78.0-79.0%, and Tier
3 used the least at approximately 69.0%. This confirms that the
resource utilization of nodes within each tier remained stable,
indicating that Hyperion achieves effective task scheduling.
Furthermore, devices at lower tiers exhibited higher resource
utilization, which can likely be attributed to their more limited
resources. This observation also underscores the efficacy of
the HypSplit-DP algorithm‚Äôs load balancing strategy. By allo-
cating a computing load to more capable higher tier devices,
the framework successfully balances system wide throughput,
compelling the less resourced devices to operate near their
capacity to maintain pace.
D. Scalability with Increasing Output Tokens
Next, we test the performance difference between Hyperion
and the baseline algorithm under different output tokens. As
shown in Fig. 9 (a), GPipe, consistently demonstrates the
highest latency, starting at 50.1s for 128 tokens and rising
steeply to about 66.2s at 256 tokens. The HEFT algorithm,
performs slightly better but follows a similar steep trajectory,
with its latency increasing from 35.0s to 48.1s over the
same token range. In contrast, Hyperion proves to be the
most efficient algorithm. It begins with the lowest latency
of approximately 25.2s at 128 tokens and concludes at a
significantly lower 35.8s at 256 tokens.
Next we test Hyperion‚Äôs performance on the Phi in Fig.
9 (b), which reveals a widening performance gap as output
tokens increase. While the initial latencies at 128 tokens were
closely clustered between 40.1s (Hyperion), 53.4s (HEFT)
and 78.7s (GPipe), their different scaling efficiencies led to
a significant divergence. By the 256 token, Hyperion‚Äôs latency
reached only 57.6s, whereas HEFT and GPipe climbed to
74.5s and 103.7s, respectively. This demonstrates Hyperion‚Äôs
superior scalability, making its end-to-end latency approx-
imately 22.7% lower than HEFT‚Äôs and 44.5% lower than
GPipe‚Äôs for longer generative tasks. The consistent outper-
formance of Hyperion over HEFT and GPipe, suggesting
that its resource aware mechanism prevents the creation of
‚Äúbottleneck‚Äù nodes in stage one, thereby optimizing overall
latency.
135
150
165
180
195
210
225
240
255
Output Tokens
30
40
50
60
Latency (s)
Hyperion
Gpipe
HEFT
(a) Llama 3
135
150
165
180
195
210
225
240
255
Output Tokens
40
60
80
100
Latency (s)
Hyperion
Gpipe
HEFT
(b) Phi-3
Fig. 9: end-to-end Latency Comparison of Hyperion, HEFT,
and GPipe across Different Token Output for (a) Llama 3 and
(b) Phi-3.
135
150
165
180
195
210
225
240
255
Output Tokens
30
40
50
60
Latency (s)
LLama-1Gbps/s
LLama-100Mbps/s
Phi-1Gbps/s
Phi-100Mbps/s
Fig. 10: Impact of Bandwidth Variation on the Performance
of the Hyperion across Different Output Token.
Fig. 10 shows the performance of Hyperion under different
models and bandwidths. As shown in the figure, there is
an positive correlation between latency and the number of
tokens. Specifically, under a 1Gbps bandwidth condition (solid
line), its latency for Phi-3 grows from approximately 40.1s at
128 tokens to 57.6s at 256 tokens. The 100Mbps condition
(dashed line) for Phi-3 follows a nearly identical trajectory,
starting slightly higher at 41.8s and reaching 60.8s. In stark
contrast, the Llama 3 model, represented by lines with blue,
demonstrates substantially lower latency. For the 1Gbps case


--- Page 14 ---
14
135
150
165
180
195
210
225
240
255
Output Tokens
42
44
46
48
50
Utilization (%)
LLama-Comp
LLama-Mem
(a) Llama 3
135
150
165
180
195
210
225
240
255
Output Tokens
55
60
65
70
Utilization (%)
Phi-Comp
Phi-Mem
(b) Phi-3
Fig. 11: Analysis of Llama 3 and Phi-3‚Äôs Resource Utilization
on AGX Orin with Increasing Token.
(solid line), Llama 3‚Äôs latency increases only modestly from
25.2s to 36.1s across the same token range. The performance at
100Mbps (dashed line) is almost indistinguishable, rising from
28.4s to 38.3s. Overall, Fig. 10 indicates that the latency of
Phi-3 is more sensitive to the output length, suggesting inferior
performance scalability. In contrast, Llama 3 exhibits a more
flat latency growth curve, demonstrating its stability in gener-
ating long texts. Also, the system‚Äôs performance bottleneck is
compute bound rather than constrained by network bandwidth.
Despite a tenfold difference in bandwidth, the impact on total
latency is negligible, typically resulting in a disparity of only
2-3 seconds.
The resource utilization on the AGX Orin under the Hyper-
ion was statistically analyzed. It was observed that computing
and memory resources exhibit divergent trends as the number
of output tokens increases. As shown in Fig. 11 (a), the
computing utilization, represented by a solid line with light
blue, exhibits a positive and linear correlation with the token
count. It begins at approximately 42.9% utilization for 128
tokens and steadily climbs to a peak of around 48.8% at
256 tokens, clearly indicating that the computing load scales
directly with the length of the generation tokens. In contrast,
the memory utilization, depicted by a dashed line with dark
blue, remains almost entirely static across the entire observed
range. It maintains a constant utilization level of approximately
43%, irrespective of the number of tokens being generated.
TABLE III: Definition of Heterogeneous Network Topologies
for Scalability Evaluation.
Network Config.
Tier
Device
Quantity
Two-Tier
Tier 1
J. Orin NX
3
Tier 2
J. AGX Orin
2
Three-Tier
Tier 1
J. Orin Nano
3
Tier 2
J. Orin NX
3
Tier 3
J. AGX Orin
2
Four-Tier
Tier 1
J. Orin Nano
2
Tier 2
J. Orin Nano
2
Tier 3
J. Orin NX
3
Tier 4
J. AGX Orin
3
Next we use the larger Phi-3 for statistics shown in Fig.
11 (b). Memory utilization, demonstrates remarkable stability
across the entire token generation process. It hovering in
a narrow band between approximately 54% and 55%. In
contrast, the computing load, is both significantly higher and
more dynamic, exhibiting a gentle upward trend. Starting from
an already high utilization of around 69.0% at 128 tokens, it
fluctuates as the token count increases, eventually reaching its
peak of approximately 72.6% at 256 tokens. This opposing
behavior in two models suggests that while the computing
demand is dynamic and dependent on the workload, the
memory footprint is largely fixed once the model is loaded for
inference. This points to a stable memory allocation for the
model‚Äôs weights and associated caches during the generation
process.
E. Performance across Different Network Architectures
Next, we evaluate the performance of Hyperion on different
network topologies. The network configuration for each case is
shown in Table III. The network structure becomes progres-
sively more complex, with the number of layers increasing
from two to four, and the total amount of resources gradually
expands. It is important to note that within each network,
nodes at lower layers have weaker capabilities.
Fig. 12 (a) illustrates the average latency across different
network architectures as a function of the number of tasks.
The x axis represents the number of tasks, and the y axis
represents the latency in seconds. The figure presents three
latency curves: the two tier network (dash dot brown line),
the three tier network (solid blue line), and the four tier
network (dashed grey line). At 6 tasks, the latencies for the
four tier, three tier, and two tier networks were approximately
138.3s, 146.1s, and 164.0s, respectively. As the number of
tasks increased to 14, the four tier network‚Äôs latency reached
approximately 261.5s, while the three tier and two tier net-
works experienced higher latencies of approximately 303.8s
and 347.8s, respectively. This indicates that the three tier and
two tier networks exhibited latencies approximately 16.2% and
33.0% higher than the four tier network, respectively.
We then deployed the larger Phi-3 in the Hyperion frame-
work. A clear trend in Fig. 12 (b) of performance divergence is
evident as the task load intensifies. Initially, under a light load
of four tasks, the three tier and four tier networks demonstrate
comparable latency at approximately 150.0s, while the two tier
network lags slightly at around 178.2s. This disparity is further
magnified at the maximum tested load of 14 tasks, where the


--- Page 15 ---
15
2
4
6
8
10
12
14
Number of tasks
0
100
200
300
Latency (s)
Two Tier Network
Three Tier Network
Four Tier Network
(a) Llama 3
2
4
6
8
10
12
14
Number of tasks
0
100
200
300
400
500
600
Latency (s)
Two Tier Network
Three Tier Network
Four Tier Network
(b) Phi-3
Fig. 12: Performance of Hyperion for Llama 3 and Phi-3
Deployment Across Various Network Architectures.
latency for the two tier, three tier, and four tier systems reaches
approximately 573.3, 501.5, and 427.2s, respectively. Fig. 12
reveals that the benefits of a multi-tier network architecture
are significantly amplified when deploying larger, more com-
putingly intensive models. While the four tier network is the
optimal configuration for both models, the performance degra-
dation in two and three tier systems is far more pronounced
with the larger Phi-3 model.
VI. CONCLUSION
This paper addressed the challenge of the end-to-end latency
optimization for LLM inference within resource-constrained,
heterogeneous multi-tier networks. We define this as a cross-
tier resource orchestration problem, where device heterogene-
ity and strict memory/GPU limits are dominant, coupled
constraints. To solve this, we pioneer Hyperion, a hierar-
chical framework Implementing this dual-driven (model &
task aware) optimization. It first determines a global aware,
resource-balanced model partition via the offline HypSplit-DP
algorithm. Subsequently, the lightweight, online HypSched-RT
algorithm leverages this fixed partition to perform real-time,
dynamic task dispatch, adapting instantly to device status to
minimize queuing delays.
Our evaluation validates Hyperion‚Äôs superiority, demonstrat-
ing performance gains over established baselines across mul-
tiple models and scenarios. For instance, with the Llama3-8B
model under a heavy load of 14 tasks, Hyperion achieved an
average latency of approximately 312s, outperforming HEFT
by 31.2% and GPipe by 52.1%. The framework‚Äôs scalability
was particularly evident in long sequence generation tasks.
Specifically, when generating 256 output tokens with the
Phi-3 model, GPipe‚Äôs latency of 103.7s was approximately
44.5% higher than Hyperion‚Äôs 57.6s. Moreover, Hyperion‚Äôs
architecture inherently bolsters data security; by processing
data locally at lower tiers, it minimizes network exposure
and mitigates interception risks. These findings prove that a
hierarchical, resource aware strategy is critical for the practical
deployment of LLM at the network edge. This work, therefore,
paves the way for more advanced, low latency services, with
future research poised to explore additional optimizations such
as energy efficiency.
REFERENCES
[1] A. Vaswani, et al., ‚ÄùAttention is All You Need,‚Äù Advances in Neural
Information Processing Systems, 2017.
[2] Y. He, J. Fang, F. R. Yu, and V. C. Leung, ‚ÄùLarge Language Models
(LLMs) Inference Offloading and Resource Allocation in Cloud-Edge
Computing: An Active Inference Approach,‚Äù IEEE Transactions on
Mobile Computing, vol. 23, no. 12, pp. 11253-11264, Dec. 2024.
[3] IDC, ‚ÄùWorldwide Global DataSphere IoT Device and Data Forecast,
2021‚Äì2025,‚Äù 2021.
[4] W. Shi, et al., ‚ÄùEdge Computing: Vision and Challenges,‚Äù IEEE Internet
of Things Journal, vol. 3, no. 5, pp. 637-646, 2016.
[5] J. Huang, et al., ‚ÄùDynamic UAV-Assisted Cooperative Edge AI Inference,‚Äù
IEEE Transactions on Wireless Communications, vol. 24, no. 1, pp. 615-
628, Jan. 2025.
[6] X. Li and S. Bi, ‚ÄùOptimal AI Model Splitting and Resource Allocation
for Device-Edge Co-Inference in Multi-User Wireless Sensing Systems,‚Äù
IEEE Transactions on Wireless Communications, vol. 23, no. 9, pp.
11094-11108, Sept. 2024.
[7] NVIDIA, ‚ÄùJetson Nano Developer Kit,‚Äù 2019. [Online]. Available:
https://developer.nvidia.com/embedded/jetson-nanodeveloper-kit.
[8] N. Singh and M. Adhikari, ‚ÄùA Hybrid Semi-Asynchronous Federated
Learning and Split Learning Strategy in Edge Networks,‚Äù IEEE Transac-
tions on Network Science and Engineering, vol. 12, no. 2, pp. 1429-1439,
March-April 2025.
[9] M. Ma, C. Gong, L. Zeng, Y. Yang and L. Wu, ‚ÄùFlocOff: Data Hetero-
geneity Resilient Federated Learning With Communication-Efficient Edge
Offloading,‚Äù in IEEE Journal on Selected Areas in Communications, vol.
42, no. 11, pp. 3262-3277, Nov. 2024.
[10] Y. Yang, M. Ma, Y. Huang, H. Chai, C. Gong, H. Geng, Y. Zhou, et al.,
‚ÄùAgentic Web: Weaving the Next Web With AI Agents,‚Äù arXiv preprint
arXiv:2507.21206, 2025.
[11] Y. Yang, et al., ‚Äù6G Network AI Architecture for Everyone-Centric
Customized Services,‚Äù IEEE Network, vol. 37, no. 5, pp. 71-80, Sept.
2023.
[12] M. Ma, C. Gong, L. Zeng and Y. Yang, ‚ÄùMOGR: Multi-task Offloading
via Graph Representation in Heterogeneous Computing Network,‚Äù ICC
2024 - IEEE International Conference on Communications, Denver, CO,
USA, 2024, pp. 1237-1242.
[13] P. Mach and Z. Becvar, ‚ÄùMobile Edge Computing: A Survey on Archi-
tecture and Computation Offloading,‚Äù IEEE Communications Surveys &
Tutorials, vol. 19, no. 3, pp. 1628-1656, 2017.
[14] Y. Cheng, et al., ‚ÄùA Survey of Model Compression and Acceleration for
Deep Neural Networks,‚Äù arXiv preprint arXiv:1710.09282, 2017.
[15] K. Liu, J. Wang, Z. Huang, and J. Pan, ‚ÄùSampling-Based Multi-Job
Placement for Heterogeneous Deep Learning Clusters,‚Äù IEEE Transac-
tions on Parallel and Distributed Systems, vol. 35, no. 6, pp. 1029-1043,
June 2024.
[16] Z. Zhang, Y. Zhao, H. Li, C. Lin, and J. Liu, ‚ÄùDVFO: Learning-Based
DVFS for Energy-Efficient Edge-Cloud Collaborative Inference,‚Äù IEEE
Transactions on Mobile Computing, vol. 23, no. 10, pp. 9042-9059, Oct.
2024.
[17] M. Ma, C. Gong, L. Zeng and Y. Yang, ‚Äùmulti-tier Multi-Node
Scheduling of LLM for Collaborative AI Computing,‚Äù IEEE INFOCOM
2025 - IEEE Conference on Computer Communications, London, United
Kingdom, 2025, pp. 1-10.


--- Page 16 ---
16
[18] X. Zhang, et al., ‚ÄùBeyond the Cloud: Edge Inference for Generative
Large Language Models in Wireless Networks,‚Äù IEEE Transactions on
Wireless Communications, vol. 24, no. 1, pp. 643-658, Jan. 2025.
[19] X. Xu, G. Feng, Y. Liu, S. Qin, J. Wang, and Y. Wang, ‚ÄùJoint Inference
Offloading and Model Caching for Small and Large Language Model
Collaboration,‚Äù IEEE Transactions on Mobile Computing, 2025.
[20] D. Xu, et al., ‚ÄùEdgeLLM: Fast On-Device LLM Inference With Specu-
lative Decoding,‚Äù IEEE Transactions on Mobile Computing, vol. 24, no.
4, pp. 3256-3273, April 2025.
[21] S. Teerapittayanon, B. McDanel, and H. T. Kung, ‚ÄùDistributed Deep
Neural Networks Over the Cloud, the Edge and End Devices,‚Äù Pro-
ceedings of the 2017 IEEE 37th International Conference on Distributed
Computing Systems (ICDCS), 2017, pp. 328-339.
[22] S. Teerapittayanon, B. McDanel, and H. T. Kung, ‚ÄùBranchyNet: Fast
Inference via Early Exiting From Deep Neural Networks,‚Äù Proceedings
of the 2016 23rd International Conference on Pattern Recognition (ICPR),
2016, pp. 2464-2469.
[23] F. Dong et al., ‚ÄùMulti-Exit DNN Inference Acceleration Based on Multi-
Dimensional Optimization for Edge Intelligence,‚Äù IEEE Transactions on
Mobile Computing, vol. 22, no. 9, pp. 5389-5405, 1 Sept. 2023.
[24] S. Han, H. Mao, and W. J. Dally, ‚ÄùDeep Compression: Compressing
Deep Neural Networks With Pruning, Trained Quantization and Huff-
man Coding,‚Äù Proceedings of the International Conference on Learning
Representations (ICLR), 2016.
[25] W. Li, et al., ‚ÄùMobileBERT: A Compact Task-Agnostic BERT for
Resource-Limited Devices,‚Äù Proceedings of the Annual Meeting of the
Association for computing Linguistics (ACL), 2020, pp. 2158-2170.
[26] G. Hinton, O. Vinyals, and J. Dean, ‚ÄùDistilling the Knowledge in a
Neural Network,‚Äù arXiv preprint arXiv:1503.02531, 2015.
[27] S. Kim, et al., ‚ÄùSequence-Level Knowledge Distillation,‚Äù Proceedings
of the Conference on Empirical Methods in Natural Language Processing
(EMNLP), 2016, pp. 1317-1327.
[28] S. Wang, et al., ‚ÄùAdaptive Federated Learning in Resource Constrained
Edge Computing Systems,‚Äù IEEE Journal on Selected Areas in Commu-
nications, vol. 37, no. 6, pp. 1205-1221, 2019.
[29] L. Gu, D. Zeng, S. Guo, and A. Barnawi, ‚ÄùCost Efficient Resource Man-
agement in Fog Computing Supported Medical Cyber-Physical System,‚Äù
IEEE Transactions on Emerging Topics in Computing, vol. 7, no. 2, pp.
131-143, 2018.
[30] W. Lin, et al., ‚ÄùTask Offloading and Resource Allocation for Mobile
Edge Computing With Deep Reinforcement Learning,‚Äù IEEE Transac-
tions on Vehicular Technology, vol. 69, no. 1, pp. 431-445, 2020.
[31] Y. Xiao, et al., ‚ÄùEdge Computing Security: State of the Art and
Challenges,‚Äù Proceedings of the IEEE, vol. 107, no. 8, pp. 1608-1631,
2019.
[32] Z. Zhou, et al., ‚ÄùSecure and Efficient Vehicle-to-Grid Energy Trading in
Cyber Physical Systems: Integration of Blockchain and Edge Computing,‚Äù
IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 50,
no. 1, pp. 43-57, 2019.
[33] Y. Huang, et al., ‚ÄùGPipe: Efficient Training of Giant Neural Networks
Using Pipeline Parallelism,‚Äù Advances in Neural Information Processing
Systems (NeurIPS), 2019, pp. 103-112.
[34] N. Goyal, et al., ‚ÄùPipeDream: Generalized Pipeline Parallelism for DNN
Training,‚Äù Proceedings of Machine Learning and Systems, vol. 1, 2021,
pp. 1-13.
[35] M. Shoeybi, et al., ‚ÄùMegatron-LM: Training Multi-Billion Param-
eter
Language
Models
Using
Model
Parallelism,‚Äù
arXiv
preprint
arXiv:1909.08053, 2019.
[36] S. Laskaridis, et al., ‚ÄùSPINN: Synergistic Progressive Inference of
Neural Networks Over Device and Cloud,‚Äù Proceedings of the 26th
Annual International Conference on Mobile Computing and Networking,
2020, pp. 1-15.
[37] D. Narayanan, et al., ‚ÄùMemory-Efficient Pipeline-Parallel DNN Train-
ing,‚Äù arXiv preprint arXiv:2010.13038, 2021.
[38] R. Shi, et al., ‚ÄùCommunication-Efficient Distributed Deep Learning With
Merged Gradient Sparsification on GPUs,‚Äù IEEE Transactions on Parallel
and Distributed Systems, vol. 32, no. 5, pp. 1091-1104, 2021.
[39] S. Zheng, et al., ‚ÄùAsynchronous Stochastic Gradient Descent With Delay
Compensation,‚Äù Proceedings of the International Conference on Machine
Learning (ICML), 2017, pp. 4120-4129.
[40] Y. Li, et al., ‚ÄùEdge-Oriented Computing Paradigms: A Survey on
Architecture Design and System Management,‚Äù ACM Computing Surveys,
vol. 51, no. 2, pp. 1-34, 2019.
[41] Z. Zhou, et al., ‚ÄùEdge Intelligence: Paving the Last Mile of Artificial
Intelligence With Edge Computing,‚Äù Proceedings of the IEEE, vol. 107,
no. 8, pp. 1738-1762, 2020.
[42] S. Cen and Y. Zhu, ‚ÄùNP-LLM: A Unified Large-Language-Model-
Assisted Framework of 6G Network-Layer Planning,‚Äù IEEE Communi-
cations Magazine, vol. 63, no. 8, pp. 92-98, August 2025.
[43] X. Wang, Y. Han, V. C. Leung, D. Niyato, X. Yan, and X. Chen,
‚ÄùConvergence of Edge Computing and Deep Learning: A Comprehensive
Survey,‚Äù IEEE Communications Surveys & Tutorials, vol. 22, no. 2, pp.
869-904, 2020.
[44] Z. Wu, et al., ‚ÄùRecursive Offloading for LLM Serving in Multi-tier
Networks,‚Äù arXiv preprint arXiv:2505.16502, 2025.
[45] P. Gupta and P. R. Kumar, ‚ÄùThe capacity of wireless networks,‚Äù IEEE
Transactions on Information Theory, vol. 46, no. 2, pp. 388-404, Mar.
2002.
[46] P. Belotti, et al., ‚ÄùMixed-integer nonlinear optimization,‚Äù Acta Numerica,
vol. 22, pp. 1-131, 2013.
[47] Y. Li, Z. Li, W. Yang, and C. Liu, ‚ÄùRT-LM: Uncertainty-Aware Re-
source Management for Real-Time Inference of Language Models,‚Äù arXiv
preprint arXiv:2309.06619, 2023.
[48] H. Topcuoglu, S. Hariri, and M. Y. Wu, ‚ÄùPerformance-Effective and
Low-Complexity Task Scheduling for Heterogeneous Computing,‚Äù IEEE
Transactions on Parallel and Distributed Systems, vol. 13, no. 3, pp. 260-
274, March 2002.
