--- Page 1 ---
R-ConstraintBench:
Evaluating LLMs on NP-Complete Scheduling
Raj Jain
rjain@labelbox.com
Marc Wetter
mwetter@labelbox.com
Labelbox
Abstract
Effective scheduling under tight resource, timing, and operational constraints underpins large-scale planning across sectors
such as capital projects, manufacturing, logistics, and IT fleet transitions. However, the reliability of large language models
(LLMs) when reasoning under high-constraint regimes is insufficiently characterized. To address this gap, we present R-
ConstraintBench, a scalable framework that evaluates models on Resource-Constrained Project Scheduling Problems (RCPSP),
an NP-Complete feasibility class, while difficulty increases via linear growth in constraints. R-ConstraintBench incrementally
increases non-redundant precedence constraints in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal
windows, and disjunctive constraints. As an illustrative example, we instantiate the benchmark in a data center migration setting
and evaluate multiple LLMs using feasibility and error analysis, identifying degradation thresholds and constraint types most
associated with failure. Empirically, strong models are near-ceiling on precedence-only DAGs, but feasibility performance
collapses when downtime, temporal windows, and disjunctive constraints interactâ€”implicating constraint interaction, not
graph depth, as the principal bottleneck. Performance on clean synthetic ramps also does not guarantee transfer to domain-
grounded scenarios, underscoring limited generalization.
1. INTRODUCTION
1.1. Motivation
Accurate scheduling under resource, timing, and opera-
tional constraints is critical across construction, manufactur-
ing turnarounds, airline and crew planning, supply-chain co-
ordination, clinical trial operations, satellite tasking, and large
IT transitions. As organizations explore LLMs for planning
and coordination, small reliability lapses can propagate into
cost, safety, and service risks.
As a result, ensuring that
LLM-generated schedules are feasible is therefore a prerequi-
site for deployment. Beyond feasibility, systematic evaluation
is needed to understand the precise conditions under which
LLMs remain reliable, where they break down, and when al-
ternative solution strategies should be employed. To isolate
modelsâ€™ core latent reasoning ability on this task, our eval-
uation does not permit specialized tool use; tool-augmented
pipelines are left to future work.
1.2. LLM Reliability on Complex RCPSPs
LLMs show competence on small scheduling puzzles and
short dependency chains [1], but generalization to regimes
with interacting precedence, resource, and temporal rules is
unclear. We adopt RCPSP feasibility as a focused testbed be-
cause it concentrates the reasoning burden on satisfying het-
erogeneous constraints. We ask whether models scale their
reasoning to deep, structured hierarchies with resource calen-
dars and mutual-exclusion requirements, conditions common
in multi-site data center migrations, capital builds, and airline
crew pairing.
1.3. Our Contributions
In this work, we develop R-ConstraintBench (Resource-
Constraint Bench), a controlled benchmarking framework that
reveals when and why leading LLMs succeed or fail on RCPSP
feasibility tasks. Our key contributions are:
â€¢ Synthetic Constraint Progression, via a layered DAG
generator that incrementally increases non-redundant
precedence constraints by one per level, followed by re-
source downtimes, temporal windows, and disjunctive
exclusivity at controlled rates.
â€¢ Domain Instantiation (Illustrative), mapping the same
RCPSP structure to a data center migration scenario, pre-
serving layered dependencies while placing tasks into
representative operational phases and constraints.
â€¢ Comprehensive Evaluation and Infeasibility Analysis,
assessing feasibility and failure modes while identifying
breakpoints where reliability degrades.
arXiv:2508.15204v1  [cs.AI]  21 Aug 2025


--- Page 2 ---
2. RELATED WORK
Recent efforts connect LLMs with operations research (OR)
by training models to translate natural language into opti-
mization models.
OR-Instruct develops a semi-automated
data synthesis pipeline and an IndustryOR benchmark to train
ORLMs for optimization modeling, with competitive results
on NL4OPT [2] and MAMO when models are finetuned for
OR tasks [3]. OptiMUS similarly targets scalable modeling
via LLMs, emphasizing end-to-end pipelines that produce ex-
ecutable code for MIP/CP solvers [4, 5]. MAMO frames math-
ematical modeling as a benchmark tied to solver correctness,
moving evaluation beyond free-form text to executable out-
puts [6]. Our scope differs: rather than broad modeling trans-
lation, R-ConstraintBench isolates feasibility on NP-Complete
RCPSP instances, varies the number of active constraints sys-
tematically, and provides an infeasibility analysis exposing
failure modes.
Work on LLMs for Scheduling shows that models can be
prompted to generate valid CP models for Job-Shop Schedul-
ing and solve them with CP solvers [7]. Starjob contributes a
modern dataset tailored to LLM-driven JSSP, enabling finetun-
ing and prompting comparisons [8]. By contrast, our bench-
mark does not require models to construct CP-SAT solvers;
the focus of this study is to understand inherent reasoning abil-
ity rather than optimizing for a particular task where tool use
would be employed.
A complementary thread studies text-to-formalism and
verification-in-the-loop for planning and constraint reason-
ing [9]. Planetarium evaluates translation from natural lan-
guage into planning languages [10]. Other work augments
LLM outputs with satisfiability/SMT solvers to catch incon-
sistencies in decision pipelines [11]. Relatedly, streamlining
constraints use auxiliary constraints to ease solving without
sacrificing correctness [12]. R-ConstraintBench keeps a for-
mal verifier in the loop but focuses on RCPSP feasibility and
structured failure attribution.
3. PROBLEM FORMULATION
We formalize RCPSP feasibility and the axes of complexity
we vary.
3.1. RCPSP as a Feasibility Problem
A
Resource-Constrained
Project
Scheduling
Problem
(RCPSP) asks: â€œGiven a set of tasks, resources, and con-
straints, can we assign start and end times such that the
entire schedule is valid?â€
In our formulation, we do not
minimize makespan; instead, we require complete constraint
satisfaction. This choice isolates feasibility reasoning from
optimization objectives, enabling binary verification and
attributing errors to constraint handling rather than objective
trade-offs, consistent with our focus on latent reasoning
Figure 1: Precedence activity with duration on a node graph
ability and comparable difficulty across instances as constraint
density increases. Formally, an RCPSP instance comprises a
collection of tasks ğ‘‡= {1, . . . , ğ‘›}, each with fixed duration
ğ‘ğ‘–; a set of resources ğ‘…= {1, . . . , ğ‘š}, each with time-varying
capacity ğ‘ğ‘Ÿ(ğ‘¡); resource demands ğ‘ğ‘–,ğ‘Ÿdenoting how much of
resource ğ‘Ÿtask ğ‘–consumes; and precedence relations ğ‘–â†’ğ‘—
that forbid task ğ‘—from starting before task ğ‘–completes.
A schedule assigns start times ğ‘ ğ‘–â‰¥0 so that:
Precedence:
ğ‘ ğ‘—â‰¥ğ‘ ğ‘–+ ğ‘ğ‘–,
âˆ€(ğ‘–â†’ğ‘—).
(1)
Resource feasibility:
âˆ‘ï¸
ğ‘–: ğ‘ ğ‘–â‰¤ğ‘¡<ğ‘ ğ‘–+ğ‘ğ‘–
ğ‘ğ‘–,ğ‘Ÿâ‰¤ğ‘ğ‘Ÿ(ğ‘¡),
âˆ€ğ‘Ÿâˆˆğ‘…, âˆ€ğ‘¡â‰¥0.
(2)
3.2. Representing RCPSP as DAGs
Any set of precedence constraints prohibiting cycles forms a
Directed Acyclic Graph (DAG). Beyond tautology, using lay-
ered DAGs to represent RCPSPs gives three key benefits:
1. Natural modeling of phases. Large programs (e.g., con-
struction sequences, release trains, data center reloca-
tions) decompose into stages; layers mirror these stages.
2. Guaranteed acyclicity and clarity. Tasks with no cir-
cular dependencies are fundamental to feasibility; DAG
structure enforces this at origin.
Layers prevent inad-
vertent back-edges and clearly separate â€œearlierâ€ from
â€œlaterâ€ tasks.
3. Controlled complexity progression. We add one new
cross-layer precedence edge at a time; at level ğ‘˜, the DAG
has exactly ğ‘˜non-redundant constraints, isolating the ef-
fect of increasing dependency depth.
2


--- Page 3 ---
3.3. DAG Generation
To operationalize layered generation over ğ‘šlayers, we parti-
tion tasks into layers ğ¿1, ğ¿2, . . . , ğ¿ğ‘šand initialize an empty
reachability matrix. Then, for each of ğ‘˜desired precedence-
constraint levels, we sample a candidate edge (ğ‘–, ğ‘—) where
ğ‘–âˆˆğ¿ğ‘and ğ‘—âˆˆğ¿ğ‘with ğ‘> ğ‘. We accept this edge only
if no path ğ‘–â†’ğ‘—already exists in the transitive closure, and
subsequently update the reachability matrix so that later lev-
els cannot re-introduce any implied edges. This guarantees
acyclic, non-redundant structures. Parameters ğ‘šand ğ‘˜allow
fine-grained control over the DAGâ€™s hierarchy and density.
3.4. Realism via Additional Constraint Axes
A pure-precedence DAG is the skeleton. We layer on three
more constraint types:
â€¢ Resource Downtime: Certain resources become unavail-
able (ğ‘ğ‘Ÿ(ğ‘¡) = 0).
â€¢ Temporal Windows: Release times ğ‘Ÿğ‘–(ğ‘ ğ‘–â‰¥ğ‘Ÿğ‘–); dead-
lines ğ‘‘ğ‘–(ğ‘ ğ‘–+ ğ‘ğ‘–â‰¤ğ‘‘ğ‘–).
â€¢ Disjunctive (No-Overlap): For some (ğ‘–, ğ‘—), enforce ğ‘ ğ‘–+
ğ‘ğ‘–â‰¤ğ‘ ğ‘—or ğ‘ ğ‘—+ ğ‘ğ‘—â‰¤ğ‘ ğ‘–.
Each axis is sampled at controlled probabilities (e.g., 75%).
The result is a parameterized, multi-axis challenge suit-
able for systematic evaluation, forming the core of our R-
ConstraintBench evaluation.
4. SYSTEM DESIGN AND EXPERIMENTAL
METHODOLOGY
R-ConstraintBenchâ€™s evaluation pipeline consists of three in-
tegrated components: (1) structured instance generation with
guaranteed solvability, (2) controlled LLM scheduling, and
(3) exact automated verification of each candidate schedule.
Separating generation, model interaction, and validation helps
ensure measured performance reflects model reasoning rather
than data noise or human intervention.
4.1. Structured Instance Generation
We synthesize RCPSP problem instances that are realistic and
tunable in difficulty. Leveraging an RCPSP solver program
(built on OR-Toolsâ€™ CP-SAT), we enforce a two-step process:
1. Layered-DAG Construction.
Partition tasks into ğ‘šlayers (e.g., 2â€“5); initialize reach-
ability. For each of ğ‘˜levels, add exactly one cross-layer
edge if it is non-redundant, updating reachability immedi-
ately. The result is a DAG with precisely ğ‘˜non-redundant
dependencies.
Figure 2: An overview of R-ConstraintBench system design
2. Constraint Injection and Feasibility Check.
Randomly assign each task a duration drawn from
[1, ğ·max], ensure every task demands at least one of three
resource types, impose temporal windows on a subset
of tasks, introduce downtime intervals during which cer-
tain resources are fully unavailable, and add disjunctive
constraints forbidding simultaneous execution of specific
task pairs.
Immediately after generation, we invoke the solver to
check feasibility.
Any instance that fails to admit a
valid schedule is discarded and regenerated, guaranteeing
that every instance entering evaluation admits a feasible
schedule.
4.2. Controlled LLM Scheduling
Once instances are generated, we evaluate each LLM under
identical conditions:
â€¢ Prompt Design.
A concise natural-language descrip-
tion lists every taskâ€™s duration, resource needs, and all
constraints, followed by a JSON-schema template defin-
ing the expected â€˜scheduleâ€™ output format (Appendix).
To keep evaluation focused on inherent model capabil-
ity rather than prompt-engineering technique, we adopt
a single-shot format that requires the model to integrate
all constraints and produce a complete schedule in one
pass [13, 14, 15, 16].
â€¢ Response Capture.
We test a representative suite of
models.
Each instance is presented in a single-shot
prompt, and model outputs are parsed into structured can-
didate schedules. This setup requires the model to inter-
pret the entire scheduling problem holistically, integrat-
ing all constraints into a coherent plan without intermedi-
ate guidance or iterative feedback loops.
4.3. Exact Automated Verification
To measure correctness, every candidate schedule is fed into a
verifier program, which checks:
â€¢ Precedence Compliance. No task starts before all its
predecessors finish.
3


--- Page 4 ---
â€¢ Resource and Downtime Adherence. At each time step,
resource usage never exceeds capacity, and resources re-
main idle during downtime windows.
â€¢ Temporal Window Satisfaction. All start times respect
release dates and deadlines.
â€¢ Disjunctive Enforcement. For each forbidden pair (ğ‘–, ğ‘—),
tasks do not overlap in time.
Violations are automatically classified by type (precedence,
resource/downtime, temporal, disjunctive), enabling a fine-
grained infeasibility analysis. Together, this three-stage de-
signâ€”generation, scheduling, verificationâ€”forms the back-
bone of R-ConstraintBench, providing a controlled evaluation
of LLM reliability on increasingly complex RCPSP feasibility
tasks.
5. BENCHMARK CONSTRUCTION
To rigorously evaluate LLM scheduling reliability, we assem-
bled a suite of synthetic RCPSP benchmarks whose structure
and operational complexity can be dialed. Every instance is
generated by our solver-driven pipeline (Section 5) and passed
through a CP-SAT solver to guarantee feasibility before model
evaluation. We organize our evaluation into two phases â€“ first
isolating pure precedence reasoning, then layering on full op-
erational constraints â€“ culminating in an illustrative data center
migration scenario.
5.1. Phase I: Pure-Precedence DAGs
Phase I strips the problem to its dependency graph, so any
model failure stems from mis-reasoning about precedence.
Because Phase I is pure-precedence DAGs, the identification
of any valid topological sort is admitted as a solution. Finding
a topological sort of a precedence DAG can be done in linear
time ğ‘‚(ğ‘‰+ ğ¸). Therefore, these are not NP-Complete prob-
lems.
1. Two-Layer Bipartite DAGs:
(a) Levels 1â€“200: level ğ‘˜has exactly ğ‘˜edges.
(b) Zero transitive/redundant edges.
(c) 10 instances per level â†’2,000 total.
2. Multi-Layer DAGs:
(a) Layer counts: 3-, 4-, and 5-layer configurations.
(b) Levels 1â€“200: exactly ğ‘˜non-redundant cross-layer
edges at level ğ‘˜.
(c) 10 instances per level, per layer count â†’2,000Ã—3 =
6,000 total.
5.2. Phase IIa: Multi-Constraint Interaction
Phase IIa fixes a 5-layer backbone and injects three addi-
tional constraint types at 75% probability to emulate the struc-
ture of complex, NP-Complete RCPSP problems. The 75%
probability was selected to balance realism and difficulty â€”
high enough to meaningfully stress models with frequent con-
straint interactions, yet low enough to avoid near-unsolvable
instances at modest precedence levels. These cannot be solved
in linear time because checking feasibility is no longer a pure
ordering problem.
1. 5-Layer DAGs: Levels 1â€“200 each feature exactly ğ‘˜
non-redundant left â†’right edges (level ğ‘˜has ğ‘˜edges)
with zero transitive dependencies. Temporal, downtime,
and disjunctive constraints are each applied indepen-
dently at 75% probability, and 10 instances are generated
per levelâ€”2,000 total.
5.3. Phase IIb: Data Center Migration
We map Phase IIa (75/75/75) onto Phase IIb, an illustrative
data center migration narrative:
â€¢ Five-stage rack workflow: Each server rack follows
Shutdown â†’Unrack â†’Transport â†’Install â†’Test, mir-
roring the 5-layer DAG backbone.
â€¢ Specialized
Resources:
IT Team,
DC Crew,
Network Engineers,
Forklift
and
Convoy
appear with real capacities (e.g. single-Forklift, 3-person
crew).
â€¢ Downtime windows (75%): Periods during which spe-
cific resources are entirely unavailable (e.g., network
maintenance, forklift inspection) are introduced.
â€¢ Temporal windows (75%): Key steps carry earliest-start
or deadline constraints to model regulatory cut-over win-
dows.
â€¢ Disjunctive conflicts (75%): Tasks sharing unique as-
sets (e.g. Forklift) cannot overlap, enforcing mutual ex-
clusion.
10 instances are generated per levelâ€”2,000 total. By map-
ping exactly ğ‘˜non-redundant cross-rack precedence plus these
three constraint axes onto a rack migration, we obtain RCPSP
instances that both reflect operational complexity and retain
the NP-Complete hardness of Phase II. Phase II datasets are
used to calculate model performance on R-ConstraintBench.
6. EVALUATION SETUP
6.1. Research Questions
We evaluate the performance of state-of-the-art (SOTA) LLMs
on RCPSP problem solving with R-ConstraintBench. Specif-
4


--- Page 5 ---
Figure 3: Smoothed feasibility rates vs. constraint levels using 10-point moving average windows
ically, we aim to address the following research questions
(RQs):
1. RQ1: Constraint Scalability. At what level of added
precedence constraints does each modelâ€™s feasibility rate
drop below a usable threshold?
2. RQ2: Constraint-Type Sensitivity. How do resource
downtime, temporal windows, and disjunctive constraints
collectively impact model reliability when layered onto a
fixed DAG?
3. RQ3: Domain Transferability. Can performance pat-
terns observed on synthetic benchmarks predict reliabil-
ity in an illustrative, business scenario?
6.2. Model Selection
Solving an RCPSP instance in R-ConstraintBench demands
deep combinatorial reasoning, the ability to juggle long con-
textual descriptions, and careful handling of heterogeneous
constraint types. To probe these capabilities, we select nine
SOTA LLMs for evaluation. Two of them are open-source
models (DeepSeek-R1-0528 and Qwen3-235B-A22B, where
DeepSeek-R1 is a reasoning model and Qwen3-235B-A22B is
used in â€œthinkingâ€ mode). The other seven are closed-source
models (Claude-4-Sonnet, GPT-4o-2024-11-20, Gemini-2.5-
Pro, o4-mini-2025-04-16, o3-pro-2025-06-10, GPT-5-2025-
08-07, and Grok-4-0709). For brevity, version numbers will
be omitted when referencing model names throughout the pa-
per.
6.3. Evaluation Metrics
We measure model performance along three complementary
dimensionsâ€”accuracy, robustness, and failure-mode break-
down. All metrics are computed per model ğ‘€and per dataset
ğ·, where each dataset spans ğ¾levels (e.g., ğ¾=200) with 10
instances per level (ğ‘˜= 1, . . . , ğ¾).
1. Feasibility Rate (Feasibility %): For each level ğ‘˜,
ğ¹ğ‘€
ğ·(ğ‘˜) = #{instances at level ğ‘˜solved correctly}
10
, (3)
i.e., the fraction of the 10 instances at level ğ‘˜that the
model schedules feasibly.
5


--- Page 6 ---
2. Weighted Area Under the Curve (WAUC): Because
difficulty increases monotonically with ğ‘˜(one additional
constraint per level), later levels are weighted more heav-
ily via linear weights
ğ‘¤ğ‘˜=
2ğ‘˜
ğ¾(ğ¾+1)
with
ğ¾
âˆ‘ï¸
ğ‘˜=1
ğ‘¤ğ‘˜= 1 .
(4)
The dataset-level robustness is then
WAUCğ‘€
ğ·=
ğ¾
âˆ‘ï¸
ğ‘˜=1
ğ‘¤ğ‘˜ğ¹ğ‘€
ğ·(ğ‘˜) ,
(5)
which rewards models that remain feasible at higher con-
straint levels.
3. Breakpoint Level (BP-Level): The smallest level ğ‘˜at
which
ğ¹ğ‘€
ğ·(ğ‘˜) < ğœ,
(6)
for a high-accuracy threshold ğœ(e.g., 70%).
4. Infeasibility Analysis: Every failed schedule is auto-
matically classified by the verifier into one or more of
four violation categories: Precedence, Resource/Down-
time, Temporal, and/or Disjunctive.
Official R-ConstraintBench Score. Benchmarkâ€™s headline
score for model ğ‘€averages WAUC over the Phase-II datasets:
Scoreğ‘€=
1
|Dphase2|
âˆ‘ï¸
ğ·âˆˆDphase2
WAUCğ‘€
ğ·.
(7)
This definition aligns the score with practical difficulty, em-
phasizing high-constraint regimes while remaining compara-
ble across datasets.
7. RESULTS
We report results across the full 200-level range using per-level
feasibility (Fig. 3), weighted area under the curve (WAUC;
Fig. 4), breakpoints at ğœ=70% (Table 1), and per-dataset
comparisons including the real-world domain (Fig. 5, Ta-
ble 2). We set ğœ=70% to reflect an operational-majority cri-
terionâ€”requiring at least 7 of 10 instances per level to be fea-
sibleâ€”which is stringent enough to flag fragile behavior while
avoiding over-penalization for occasional parsing or format-
ting errors.
7.1. Phase I: Pure-Precedence Baselines
Across two- to five-layer DAGs, top models sustain high feasi-
bility across nearly all levels (Fig. 3), reflected in near-ceiling
WAUCs (Fig. 4). Grok 4 is strongest overall on pure prece-
dence (Phase I WAUC = 0.975), followed closely by GPT-5
Model
2-Layer
3-Layer
4-Layer
5-Layer
MCI
Real-World
Claude 4 Sonnet
4
28
32
39
6
1
DeepSeek R1
40
34
37
41
12
13
GPT-4o
10
9
18
18
1
1
GPT-5
> 200
> 200
> 200
> 200
> 200
95
Gemini 2.5 Pro
> 200
> 200
> 200
199
50
50
Grok 4
> 200
> 200
> 200
> 200
40
61
Qwen3
6
18
17
22
6
4
o3
> 200
> 200
> 200
> 200
> 200
49
o4-Mini
195
120
129
100
25
16
Table 1: Breakpoint level (BP; lowest ğ‘˜where Feasibility%
< ğœ) by model and dataset.
(0.971), o3 (0.933), and Gemini 2.5 Pro (0.919). Mid-tier
models trail (o4-Mini: 0.732), and smaller/general models
are far below (Table 2). Breakpoints mirror this pattern (Ta-
ble 1): for example, Grok 4, GPT-5, and o3 exhibit > 200 on
2-, 3-, 4-, and 5-layer datasets. These baselines confirm that
certain SOTA LLMs can reliably produce feasible schedules
when constraints are purely precedence-based.
7.2. Phase IIa: Multi-Constraint Interaction
Introducing downtime, temporal windows, and disjunctive
no-overlap constraints (Multi-Constraint Interaction; MCI)
sharply separates models (Fig. 3, Fig. 4). o3 dominates the
synthetic MCI constraints set with WAUC = 0.931, with
GPT-5 competitive just behind the leader (0.924) and above
both Gemini 2.5 Pro (0.289) and Grok 4 (0.183) (Table 2).
Breakpoints align with this picture (Table 1): o3 and GPT-
5 never drop below ğœacross the full 200-level ramp (> 200),
while Grok 4 and Gemini cross at 40 and 50 respectively; o4-
Mini crosses at 25, and the remaining models much earlier.
For weaker systems, feasibility collapses soon after constraint
interactions become frequent (levels â‰ˆ10â€“30), whereas o3 and
GPT-5 maintain high feasibility deep into the ramp (Fig. 3).
7.3. Phase IIb: Data Center Migration
On the data center migration domain, the top of the rank-
ing changes (Fig. 4).
GPT-5 achieves the best operational
performance with WAUC = 0.661 and the latest real-world
breakpoint (BP = 95), ahead of Grok 4 (WAUC = 0.483, BP
= 61) and Gemini 2.5 Pro (WAUC = 0.282, BP = 50); o3
drops to 0.214 with BP = 49 (Table 2). These results indicate
that domain-specific coupling (e.g., single-capacity equipment
overlapping with maintenance windows and deadlines) alters
the failure surface in ways not fully captured by synthetic MCI
instances.
7.4. R-ConstraintBench Score
The benchmarkâ€™s headline score averages Phase-II WAUC
across the synthetic MCI constraints set and the data center
migration set (Sec. 4.3). The top tier consists of GPT-5 and
6


--- Page 7 ---
Figure 4: WAUC calculations vs. model per dataset.
Model
PI WAUC
PI AUC
PI Succ.%
PII
WAUC
PII AUC
PII
Succ.%
Bench.
Score
5-L
WAUC
MCI
WAUC
Real-
World
WAUC
GPT-5
0.971
0.979
97.9%
0.792
0.847
84.7%
0.792
0.956
0.924
0.661
o3
0.933
0.952
95.2%
0.572
0.661
66.0%
0.572
0.886
0.931
0.214
Grok 4
0.975
0.981
98.1%
0.333
0.487
48.7%
0.333
0.982
0.183
0.483
Gemini 2.5 Pro
0.919
0.948
94.8%
0.286
0.443
44.3%
0.286
0.899
0.289
0.282
o4-Mini
0.732
0.807
80.8%
0.144
0.231
23.1%
0.144
0.608
0.259
0.028
Deepseek R1
0.071
0.234
23.6%
0.020
0.100
10.0%
0.020
0.064
0.017
0.023
Claude 4 Sonnet
0.238
0.334
33.4%
0.019
0.037
3.7%
0.019
0.333
0.028
0.010
Qwen3
0.028
0.123
12.3%
0.004
0.046
4.6%
0.004
0.041
0.003
0.006
GPT-4o
0.011
0.088
8.8%
0.003
0.005
0.5%
0.003
0.014
0.006
0.000
Table 2: Complete performance metrics for all models across all benchmark phases and datasets.
o3, followed by Grok 4 and Gemini 2.5 Pro (Table 2). GPT-
5 attains the highest overall score (0.792) due to its leading
real-world WAUC (0.661) and competitive synthetic stand-
ing. Together with breakpoints (Table 1) and feasibility curves
(Fig. 3), these results delineate the current reliability frontier
under increasing constraint density and domain realism.
8. ANALYSIS
8.1. Structure vs. Constraint Interaction
Phase I shows that the strongest models (GPT-5, Grok 4,
o3, Gemini 2.5 Pro) reliably handle precedence-only schedul-
ing. Phase II reveals, however, that constraint interactionâ€”
overlapping downtime windows, deadlines, and disjunctive
pairsâ€”rapidly erodes feasibility for most models (Fig. 3,
Fig. 4). The dispersion in breakpoints (Table 1) suggests that
â€œgraph reasoningâ€ competence is necessary but insufficient for
resourceâ€“temporal coupling, which demands globally consis-
tent reasoning across heterogeneous rules.
8.2. Domain Transfer and the Real-World Gap
Across models, Phase II shows that complex, operational
RCPSPs remain beyond current LLM reasoning. Several mod-
els score highly on the MCI constraints track, yet performance
shifts on the data center migration domain in ways dependent
on the model (Fig. 5, Table 2). GPT-5 attains the best real-
world score (WAUC = 0.661), ahead of Grok 4 (0.483), Gem-
ini 2.5 Pro (0.282), and o3 (0.214). The gap view in Fig. 5
shows contrasting transfer patterns: Grok 4 improves when
constraints are embedded in realistic couplings (positive real-
world gap), while o3 exhibits the largest negative gap; GPT-
5 also shows a negative gap relative to its synthetic standing
but still leads operationally. These shifts indicate that domain-
specific context changes the failure surface in ways not cap-
tured by synthetic stress alone. Moreover, the real-world feasi-
bility curves (Fig. 3) reveal a pronounced late-level decline for
all modelsâ€”including GPT-5â€”which remain strong through
the mid band and then erode as constraint density approaches
the tail. Even when a model leads on average, the upper end
of the difficulty ramp is a bottleneck for sustained feasibility.
7


--- Page 8 ---
Figure 5: Performance gap between real world and MCI constraints
8.3. Reliability Thresholds and Practicality
Taken togetherâ€”feasibility curves (Fig. 3), breakpoints at
ğœ=70% (Table 1), and WAUCs (Table 2)â€”the evidence is con-
sistent: even the strongest LLMs struggle to sustain feasibility
at the highest difficulty bands. GPT-5 lifts the real-world ceil-
ing (best WAUC 0.661, latest real-world BP = 95) yet still de-
clines before upper levels; Grok 4 transfers well but likewise
degrades (BP = 61); Gemini 2.5 Pro sits between extremes
(BP = 50); and o3 excels synthetically but suffers the largest
operational drop (BP = 49). The downward trajectories across
all curves, notably in the 150â€“200 band, make the unreliability
explicit: every model ultimately bends toward low feasibility
as constraints accumulate.
9. INFEASIBILITY ANALYSIS
To better understand the nature of model failures, we classify
every invalid schedule into violation categories.
â€¢ Precedence: At least one task begins before all of its pre-
requisite tasks have completed.
â€¢ Resource/Downtime: Resource capacities are exceeded
at one or more time steps, or tasks are scheduled during
periods when the required resource is unavailable.
â€¢ Temporal: Violations of earliest-start or latest-finish re-
quirements, where a task begins too early or finishes late.
â€¢ Disjunctive: Two tasks that must not overlap in time (due
to exclusivity constraints) are scheduled concurrently.
â€¢ Other: Failures that do not fall neatly into the above cat-
egories, such as malformed outputs that cannot be parsed,
conflicting minor violations, or combinations of multiple
small errors.
Model
Precedence
Resource/
Downtime
Temporal
Disjunctive
Other
Claude 4 Sonnet
41.6%
11.0%
12.1%
23.2%
12.1%
Deepseek R1
41.8%
7.6%
7.5%
17.5%
25.6%
GPT-4o
64.8%
6.5%
11.2%
10.2%
7.3%
GPT-5
68.4%
9.1%
8.9%
7.0%
6.6%
Gemini 2.5 Pro
42.9%
6.1%
3.9%
45.4%
1.7%
Grok 4
38.5%
8.7%
3.1%
48.4%
1.2%
Qwen3
56.8%
11.2%
6.8%
19.7%
5.4%
o3
93.7%
0.6%
3.7%
1.0%
1.0%
o4-Mini
70.7%
5.2%
4.9%
18.8%
0.5%
Table 3: Infeasibility percentage breakdown for invalid
schedules.
9.1. Observed Patterns Across Models
Table 3 summarizes the percentage breakdown of these viola-
tion types for each evaluated model. Several notable patterns
emerge:
â€¢ Precedence violations dominate for certain models. o3
exhibits an exceptionally high rate of precedence errors
(93.7%), indicating that once it fails, it almost always
does so by misordering tasks. Other models such as o4-
Mini (70.7%), GPT-5 (68.4%) and GPT-4o (64.8%) also
show a strong bias toward precedence failures.
â€¢ Disjunctive constraint violations are a major weak-
ness for Grok 4 and Gemini 2.5 Pro. Grok 4 (48.4%)
and Gemini 2.5 Pro (45.4%) both fail almost half the time
due to overlapping mutually exclusive tasks, suggesting
difficulty reasoning over exclusivity constraints.
â€¢ Temporal errors are non-trivial across most mod-
els. Claude 4 Sonnet (12.1%), GPT-4o (11.2%), GPT-5
(8.9%) and Qwen3 (6.8%) show a meaningful share of
temporal window violations, pointing to challenges in en-
forcing deadlines and release times.
â€¢ Mixed failure profiles in â€œOther.â€ DeepSeek R1 has the
8


--- Page 9 ---
highest proportion of â€œOtherâ€ errors (25.6%), suggesting
frequent compound or non-standard violations. Claude 4
Sonnet also shows a relatively high â€œOtherâ€ rate (12.1%).
â€¢ Resource/Downtime is rarely the dominant issue. Al-
though present in every modelâ€™s error profile, this cate-
gory generally remains under 12%, implying that most
models can respect downtime constraints when they fail
elsewhere.
Overall, these patterns reveal that while some models ex-
hibit concentrated weaknesses (e.g., precedence ordering for
o3), others distribute their errors across multiple constraint
types. This breakdown is valuable for guiding targeted im-
provements.
10. DISCUSSION AND LIMITATIONS
The analysis reveals that even the strongest modelsâ€”o3,
Grok 4, and GPT-5â€”exhibit sharply divergent strengths de-
pending on the constraint profile and domain. o3 dominates on
the synthetic MCI constraints track, sustaining feasibility deep
into the difficulty ramp, yet it suffers the largest negative trans-
fer in the Data Center Migration setting. Grok 4 shows the re-
verse pattern: modest synthetic performance but substantially
better real-world transfer. GPT-5 lifts the real-world ceiling
further (best observed WAUC and latest real-world BP), while
remaining competitive synthetically; however, its real-world
feasibility still declines in the tail, indicating that late-level
constraint density remains a bottleneck even for the strongest
model. Taken together, these outcomes show that success in
purely synthetic RCPSP benchmarks is necessary but not suf-
ficient for robustness in constraint-rich operations, and that do-
main grounding can materially alter the failure surface.
The infeasibility analysis further clarifies these differences.
Models with high synthetic scores often fail predominantly
on precedence violations, while those with stronger domain
transfer suffer more from disjunctive or temporal window vi-
olations. These patterns indicate that constraint-specific rea-
soning weaknesses, rather than generalized â€œhardness,â€ drive
many failures. This has implications for model selection: prac-
titioners should match model strengths to the constraint types
most prevalent in their operational context.
Interpreting these findings also requires recognizing the de-
sign boundaries of our study. First, we evaluate nine models
under a single-shot prompting protocol; while this controls for
prompt variance, it does not capture the effects of multi-step
reasoning, tool integration, or self-verification strategies that
could yield higher feasibility rates [13, 14, 15, 16]. Second,
the instance generator emphasizes five-layer DAGs with sam-
pled temporal, downtime, and disjunctive constraints at fixed
probabilitiesâ€”ideal for controlled scaling but narrower than
the correlated, irregular patterns found in real deployments.
Third, we frame scheduling as a feasibility task, using CP-SAT
for verification; this design isolates reasoning accuracy from
optimization, yet leaves open the question of whether feasi-
ble outputs can also meet makespan or cost objectives with-
out solver assistance. Future work could complement pass/fail
feasibility with a feasibility distance metric that quantifies how
far a schedule is from validityâ€”e.g., minimal slack required to
satisfy constraints, or minimal start-time adjustmentsâ€”to en-
able graded scoring. Fourth, our real-world translation focuses
on data center migration; while representative of a demonstra-
tive scheduling domain, other areas such as manufacturing,
construction, or satellite operations may exhibit different con-
straint interactions and failure distributions.
By integrating these limitations into the interpretation of re-
sults, we position R-ConstraintBench as both a reliable mea-
sure of current prompt-only LLM scheduling capability and
a foundation for future exploration into constraint-aware and
domain-adapted reasoning approaches.
11. CONCLUSION
Constraint-rich scheduling underpins operations across indus-
tries, yet prevailing evaluations rarely expose where LLM rea-
soning fails when heterogeneous rules must be satisfied simul-
taneously. R-ConstraintBench addresses this gap by isolating
latent feasibility reasoning, ramping constraint interaction and
density (MCI), and testing transfer in a domain-grounded data-
center instantiation.
Two results are decisive. First, constraint interactionâ€”not
graph depthâ€”is the bottleneck:
strong models are near-
ceiling on precedence-only DAGs, but feasibility collapses
once downtime, temporal windows, and disjunctive exclusiv-
ity interact. Second, synthetic success does not guarantee do-
main transfer: models that excel on clean synthetic ramps can
degrade when constraints are coupled as they are in practice.
Across nine models, we observe steep drops when con-
straint types interact. While o3 leads on the synthetic MCI
track, GPT-5 attains the strongest data center score (and latest
breakpoint), and all systems decline in the highest-constraint
band. Failure profiles are diagnostic: precedence and disjunc-
tive violations dominate many errors, with temporal and re-
source/downtime violations appearing less frequently.
We speculate that training and evaluation regimes empha-
sizing constraint interactionâ€”rather than depth aloneâ€”will
improve performance on RCPSP-style tasks and may support
broader reasoning generalization; testing this explicitly is left
to future work.
12. FUTURE WORK
Several directions can extend and deepen the impact of R-
ConstraintBench:
â€¢ Finetuning for Scheduling Competence:
Explore
whether finetuning models on RCPSP-style problems,
9


--- Page 10 ---
with either synthetic or domain-specific schedules, im-
proves constraint satisfaction rates and robustness under
high-density conditions.
â€¢ Ablations on Prompting and Instance Factors: Sys-
tematically vary prompt formulation (instruction framing,
schema strictness, verbosity, ordering, units), constraint
mix (proportions and correlation of downtime/tempo-
ral/disjunctive), and structural scale (tasks per layer,
number of layers, resource types, calendar granular-
ity). Quantify sensitivity of WAUC, BP-level, feasibility-
distance, and error taxonomy to these factors.
â€¢ Expanding to Additional NP-Complete Problems: Ex-
tend the benchmark to include other combinatorial opti-
mization problems such as the Traveling Salesman Prob-
lem, Job-Shop Scheduling, and Vehicle Routing Prob-
lems, enabling broader assessment of reasoning and opti-
mization capabilities.
â€¢ Multi-Domain Evaluation: Introduce new operational
domains beyond data center migration (e.g., construc-
tion sequencing, supply chain coordination, satellite task
scheduling) to evaluate transfer learning and domain
adaptation.
â€¢ Prompting Strategy Variations: Systematically study
the impact of chain-of-thought prompting, step-by-step
reasoning scaffolds, verification prompts, and retrieval-
augmented generation on schedule feasibility and con-
straint adherence.
â€¢ Code-Generation for Solvers: Given the growing abil-
ity of modern LLMs to produce and execute code, eval-
uate model performance in generating complete, correct
CP-SAT solver programs for given scheduling problems,
comparing direct-scheduling outputs with solver-assisted
outputs.
13. REFERENCES
[1] K. Valmeekam, M. Marquez, A. Olmo, S. Sreed-
haran, and S. Kambhampati, â€œPlanBench:
An Ex-
tensible Benchmark for Evaluating Large Language
Models on Planning and Reasoning about Change,â€
arXiv:2206.10498, 2022.
[2] R. Ramamonjison, T. T. Yu, R. Li, et al., â€œNL4Opt
Competition:
Formulating
Optimization
Problems
Based
on
Their
Natural
Language
Descriptions,â€
arXiv:2303.08233, 2023.
[3] C. Huang, Z. Tang, S. Hu, R. Jiang, X. Zheng, D. Ge,
B. Wang, and Z. Wang, â€œORLM: A Customizable Frame-
work in Training Large Models for Automated Optimiza-
tion Modeling,â€ arXiv:2405.17743, 2024 (v5, 2025).
[4] M. AhmadiTeshnizi, P. Wang, J. Liu, et al., â€œOptiMUS:
Scalable Optimization Modeling with Solvers and Large
Language Models,â€ arXiv:2407.19633, 2024.
[5] T. Ahmed and S. Choudhury, â€œLM4OPT: Unveiling the
Potential of Large Language Models for Optimization
Modeling,â€ arXiv:2403.01342, 2024.
[6] C. Huang, Y. Jin, W. Yu, et al., â€œMAMO: A Mathematical
Modeling Benchmark with Solvers,â€ arXiv:2405.13144,
2024.
[7] H. Abgaryan, A. Harutyunyan, T. Cazenave, et al.,
â€œLLMs can Schedule,â€ arXiv:2408.06993, 2024.
[8] H. Abgaryan, A. Harutyunyan, T. Cazenave, et al., â€œStar-
job: A Dataset for LLM-Driven Job Shop Scheduling,â€
arXiv:2503.01877, 2025.
[9] T. Zhang, Y. Shen, W. Luo, Y. Zhang, H. Liang, F. Yang,
M. Lin, Y. Qiao, W. Chen, B. Cui, W. Zhang, and
Z. Zhou, â€œCFBench:
A Comprehensive Constraints-
Following Benchmark for LLMs,â€ arXiv:2408.01122,
2024.
[10] M. Zuo, A. F. Velez, X. Li, et al., â€œPlanetarium: A Rigor-
ous Benchmark for Translating Text to Structured Plan-
ning Languages,â€ arXiv:2407.03321, 2025.
[11] B. Chen, A. A. Babikian, S. Feng, et al., â€œLLM-
based
Satisfiability
Checking
of
String
Require-
ments by Consistent Data and Checker Generation,â€
arXiv:2506.16639, 2025.
[12] F. Voboril, V. Ramaswamy, S. Szeider, et al., â€œGenerating
Streamlining Constraints with Large Language Models,â€
arXiv:2408.10268, 2025.
[13] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter,
F. Xia, E. H. Chi, Q. V. Le, and D. Zhou, â€œChain-of-
Thought Prompting Elicits Reasoning in Large Language
Models,â€ arXiv:2201.11903, 2022.
[14] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths,
Y. Cao, and K. Narasimhan, â€œTree of Thoughts: De-
liberate Problem Solving with Large Language Models,â€
arXiv:2305.10601, 2023.
[15] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan,
and Y. Cao, â€œReAct: Synergizing Reasoning and Acting
in Language Models,â€ arXiv:2210.03629, 2022.
[16] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang,
J. Callan, and G. Neubig, â€œPAL: Program-Aided Lan-
guage Models,â€ arXiv:2211.10435, 2022.
10


--- Page 11 ---
APPENDIX
Phase IIa: Multi-Constraint Interaction Sample Prompt
You are a project scheduler tasked with creating a feasible schedule for a resource-
constrained project scheduling problem.
- Task_5: Duration 5 time units, Requires: Resource_2, Resource_3
- Task_2: Duration 8 time units, Requires: Resource_2, Resource_1, Cannot start before
time 14
- Task_7: Duration 10 time units, Cannot start before time 3
- Task_4: Duration 4 time units, Depends on: Task_7, Cannot start before time 17, Must
finish by time 53
- Task_6: Duration 7 time units, Depends on: Task_2, Task_5, Cannot start before time 4
- Task_8: Duration 3 time units, Depends on: Task_7, Task_5, Task_2, Cannot start before
time 10
- Task_10: Duration 1 time units, Depends on: Task_2, Task_7, Task_4, Cannot start before
time 15, Must finish by time 58
- Task_11: Duration 3 time units, Requires: Resource_1, Resource_3, Depends on: Task_7,
Task_5, Task_8, Task_6, Cannot start before time 6
- Task_3: Duration 4 time units, Requires: Resource_2, Depends on: Task_7, Task_6, Task_5,
Must finish by time 73
- Task_1: Duration 5 time units, Depends on: Task_10, Task_6, Task_8, Cannot start before
time 8, Must finish by time 47
- Task_9: Duration 2 time units, Requires: Resource_1, Depends on: Task_6, Task_1, Must
finish by time 63
- Task_12: Duration 4 time units, Requires: Resource_3, Depends on: Task_3, Task_4, Task_5
, Task_6, Task_10
- Resource_1: Capacity 3 (can handle 3 tasks simultaneously), Downtime windows:
unavailable 8-10
- Resource_2: Capacity 1 (can handle 1 tasks simultaneously), Downtime windows:
unavailable 18-20
- Resource_3: Capacity 1 (can handle 1 tasks simultaneously), Downtime windows:
unavailable 33-37
- Task_2 and Task_7 cannot run simultaneously
- Task_4 and Task_9 cannot run simultaneously
- Task_10 and Task_9 cannot run simultaneously
- Task_10 and Task_12 cannot run simultaneously
- Task_11 and Task_12 cannot run simultaneously
- Task_7 and Task_11 cannot run simultaneously
- Task_7 and Task_6 cannot run simultaneously
- Task_9 and Task_12 cannot run simultaneously
- Task_2 and Task_4 cannot run simultaneously
- Task_2 and Task_6 cannot run simultaneously
- Task_8 and Task_3 cannot run simultaneously
- Task_6 and Task_1 cannot run simultaneously
- Task_2 and Task_11 cannot run simultaneously
- Task_2 and Task_10 cannot run simultaneously
- Task_10 and Task_11 cannot run simultaneously
- Task_8 and Task_9 cannot run simultaneously
- Task_2 and Task_1 cannot run simultaneously
- Task_4 and Task_12 cannot run simultaneously
- Task_6 and Task_3 cannot run simultaneously
- Task_5 and Task_7 cannot run simultaneously
11


--- Page 12 ---
- Task_4 and Task_8 cannot run simultaneously
- Task_10 and Task_1 cannot run simultaneously
- Task_2 and Task_8 cannot run simultaneously
- Task_4 and Task_11 cannot run simultaneously
- Task_5 and Task_3 cannot run simultaneously
- Task_7 and Task_9 cannot run simultaneously
- Task_7 and Task_3 cannot run simultaneously
- Task_6 and Task_10 cannot run simultaneously
- Task_8 and Task_12 cannot run simultaneously
- Task_11 and Task_1 cannot run simultaneously
- Task_7 and Task_10 cannot run simultaneously
- Task_7 and Task_1 cannot run simultaneously
- Task_8 and Task_10 cannot run simultaneously
- Task_3 and Task_9 cannot run simultaneously
- Task_5 and Task_1 cannot run simultaneously
- Task_7 and Task_8 cannot run simultaneously
- Task_8 and Task_1 cannot run simultaneously
- Task_5 and Task_6 cannot run simultaneously
- Task_5 and Task_10 cannot run simultaneously
- Task_5 and Task_8 cannot run simultaneously
- Task_4 and Task_1 cannot run simultaneously
- Task_5 and Task_4 cannot run simultaneously
- Task_6 and Task_9 cannot run simultaneously
- Task_3 and Task_12 cannot run simultaneously
- Task_4 and Task_3 cannot run simultaneously
- Task_1 and Task_12 cannot run simultaneously
- Task_11 and Task_3 cannot run simultaneously
- Task_4 and Task_6 cannot run simultaneously
- Task_5 and Task_9 cannot run simultaneously
- Task_3 must finish before Task_12 can start
- Task_2 must finish before Task_10 can start
- Task_2 must finish before Task_6 can start
- Task_7 must finish before Task_3 can start
- Task_4 must finish before Task_12 can start
- Task_6 must finish before Task_9 can start
- Task_7 must finish before Task_8 can start
- Task_10 must finish before Task_1 can start
- Task_7 must finish before Task_11 can start
- Task_5 must finish before Task_12 can start
- Task_6 must finish before Task_1 can start
- Task_8 must finish before Task_1 can start
- Task_1 must finish before Task_9 can start
- Task_7 must finish before Task_10 can start
- Task_6 must finish before Task_12 can start
- Task_5 must finish before Task_8 can start
- Task_7 must finish before Task_4 can start
- Task_4 must finish before Task_10 can start
- Task_6 must finish before Task_3 can start
- Task_10 must finish before Task_12 can start
- Task_2 must finish before Task_8 can start
- Task_5 must finish before Task_11 can start
- Task_8 must finish before Task_11 can start
- Task_5 must finish before Task_3 can start
- Task_6 must finish before Task_11 can start
- Task_5 must finish before Task_6 can start
12


--- Page 13 ---
Create a feasible schedule that minimizes the total project duration (makespan) while
respecting all constraints.
Please provide your solution in JSON format with the following structure:
{
"makespan": <total_duration>,
"schedule": [
{
"task": "<task_id>",
"start_time": <start_time>,
"end_time": <end_time>,
"resources": ["<resource_id>"]
}
]
}
13


--- Page 14 ---
Phase IIb: Real-World Data Migration Sample Prompt
You are managing a critical data center migration from Facility 1 to Facility 2. This
migration involves 2 server racks that must be carefully moved while minimizing total
downtime.
MIGRATION OVERVIEW:
- Number of racks: 2
- Total tasks: 10
- Migration phases per rack: Shutdown -> Unrack -> Transport -> Install -> Test
- Complexity level: 1
MIGRATION TASKS:
Each rack must go through the following sequence:
Rack_1:
- Rack_1_shutdown: 27 minutes, Requires: IT_Team
- Rack_1_unrack: 40 minutes, Requires: DC_Crew, Forklift, Cannot start before minute 4
- Rack_1_transport: 33 minutes, Requires: Convoy
- Rack_1_install: 45 minutes, Requires: DC_Crew, Forklift
- Rack_1_test: 32 minutes, Requires: IT_Team, Network_Engineers
Rack_2:
- Rack_2_shutdown: 26 minutes, Requires: IT_Team
- Rack_2_unrack: 20 minutes, Requires: DC_Crew, Forklift, Must complete by minute 52
- Rack_2_transport: 58 minutes, Requires: Convoy, Cannot start before minute 32
- Rack_2_install: 41 minutes, Requires: DC_Crew, Forklift, Cannot start before minute
43, Must complete by minute 300
- Rack_2_test: 34 minutes, Requires: IT_Team, Network_Engineers
AVAILABLE RESOURCES:
- IT_Team: Can handle 2 tasks simultaneously
- DC_Crew: Can handle 3 tasks simultaneously
- Network_Engineers: Can handle 2 tasks simultaneously
- Forklift: Can handle 1 task at a time
- Convoy: Can handle 1 task at a time (Downtime: unavailable minutes 169-203, unavailable
minutes 41-88)
CRITICAL INTER-RACK DEPENDENCIES:
The following tasks have dependencies across racks:
- Rack_2_shutdown must complete before Rack_1_test
EQUIPMENT CONFLICTS:
The following task pairs cannot run simultaneously due to shared equipment:
- Rack_1_install and Rack_2_unrack (both need Forklift, DC_Crew)
- Rack_1_install and Rack_2_install (both need Forklift, DC_Crew)
- Rack_1_install and Rack_1_unrack (both need Forklift, DC_Crew)
- Rack_1_unrack and Rack_2_install (both need Forklift, DC_Crew)
- Rack_1_unrack and Rack_2_unrack (both need Forklift, DC_Crew)
PRECEDENCE CONSTRAINTS:
- Rack_1_shutdown must finish before Rack_1_unrack can start
- Rack_1_unrack must finish before Rack_1_transport can start
- Rack_1_transport must finish before Rack_1_install can start
- Rack_1_install must finish before Rack_1_test can start
- Rack_2_shutdown must finish before Rack_2_unrack can start
- Rack_2_unrack must finish before Rack_2_transport can start
14


--- Page 15 ---
- Rack_2_transport must finish before Rack_2_install can start
- Rack_2_install must finish before Rack_2_test can start
- Rack_2_shutdown must finish before Rack_1_test can start
CONSTRAINTS:
1. Each rackâ€™s tasks must follow the sequence: Shutdown -> Unrack -> Transport -> Install
-> Test
2. Tasks require specific crews and equipment with limited capacity
3. Resources have scheduled downtime windows (breaks, maintenance)
4. Some tasks have time windows (earliest start, latest finish)
5. Tasks sharing limited equipment cannot run simultaneously
6. Inter-rack dependencies must be respected
OBJECTIVE:
Create a migration schedule that completes all tasks in a feasible time while respecting
all constraints.
Please provide your solution in JSON format with the following structure:
{
"makespan": <total_duration>,
"schedule": [
{
"task": "<task_id>",
"start_time": <start_time>,
"end_time": <end_time>,
"resources": ["<resource_id>"]
}
]
}
15


--- Page 16 ---
Error Analysis on Model Output Sample
Prompt:
You are a project scheduler tasked with creating a feasible schedule for a resource-
constrained project scheduling problem.
- Task_5: Duration 3 time units
- Task_3: Duration 7 time units, Requires: Resource_1, Cannot start before time 5, Must
finish by time 16
- Task_1: Duration 2 time units, Requires: Resource_2, Depends on: Task_3, Cannot start
before time 10
- Task_2: Duration 5 time units, Requires: Resource_3, Resource_1, Depends on: Task_5,
Cannot start before time 6
- Task_4: Duration 6 time units, Depends on: Task_2, Task_1, Must finish by time 27
- Task_6: Duration 6 time units, Depends on: Task_3, Cannot start before time 3, Must
finish by time 20
- Task_7: Duration 8 time units, Depends on: Task_5, Task_1, Cannot start before time 9
- Task_8: Duration 2 time units, Requires: Resource_2, Depends on: Task_5, Task_1, Task_4,
Cannot start before time 4
- Resource_1: Capacity 1 (can handle 1 tasks simultaneously)
- Resource_2: Capacity 1 (can handle 1 tasks simultaneously), Downtime windows:
unavailable 4-9
- Resource_3: Capacity 1 (can handle 1 tasks simultaneously), Downtime windows:
unavailable 6-9
- Task_5 and Task_2 cannot run simultaneously
- Task_5 and Task_1 cannot run simultaneously
- Task_3 and Task_1 cannot run simultaneously
- Task_5 and Task_6 cannot run simultaneously
- Task_3 and Task_8 cannot run simultaneously
- Task_3 and Task_7 cannot run simultaneously
- Task_1 and Task_7 cannot run simultaneously
- Task_3 and Task_4 cannot run simultaneously
- Task_5 and Task_4 cannot run simultaneously
- Task_4 and Task_6 cannot run simultaneously
- Task_4 and Task_8 cannot run simultaneously
- Task_2 and Task_8 cannot run simultaneously
- Task_6 and Task_7 cannot run simultaneously
- Task_2 and Task_7 cannot run simultaneously
- Task_1 and Task_4 cannot run simultaneously
- Task_7 and Task_8 cannot run simultaneously
- Task_1 and Task_2 cannot run simultaneously
- Task_4 and Task_7 cannot run simultaneously
- Task_1 and Task_6 cannot run simultaneously
- Task_5 and Task_7 cannot run simultaneously
- Task_6 and Task_8 cannot run simultaneously
- Task_3 must finish before Task_1 can start
- Task_5 must finish before Task_2 can start
- Task_2 must finish before Task_4 can start
- Task_5 must finish before Task_7 can start
- Task_3 must finish before Task_6 can start
- Task_1 must finish before Task_4 can start
- Task_1 must finish before Task_7 can start
- Task_5 must finish before Task_8 can start
- Task_1 must finish before Task_8 can start
16


--- Page 17 ---
- Task_4 must finish before Task_8 can start
Create a feasible schedule that minimizes the total project duration (makespan) while
respecting all constraints.
Please provide your solution in JSON format with the following structure:
{
"makespan": <total_duration>,
"schedule": [
{
"task": "<task_id>",
"start_time": <start_time>,
"end_time": <end_time>,
"resources": ["<resource_id>"]
}
]
}
Model Solution Attempt:
{
"makespan": 23,
"schedule": [
{"task": "Task_5", "start_time": 0, "end_time": 3, "resources": []},
{"task": "Task_3", "start_time": 5, "end_time": 12, "resources": ["Resource_1"]},
{"task": "Task_1", "start_time": 12, "end_time": 14, "resources": ["Resource_2"]},
{"task": "Task_2", "start_time": 14, "end_time": 19, "resources": ["Resource_1", "
Resource_3"]},
{"task": "Task_6", "start_time": 12, "end_time": 18, "resources": ["Resource_1"]},
{"task": "Task_4", "start_time": 19, "end_time": 25, "resources": []},
{"task": "Task_7", "start_time": 14, "end_time": 22, "resources": []},
{"task": "Task_8", "start_time": 25, "end_time": 27, "resources": ["Resource_2"]}
]
}
Infeasibility Analysis:
- Resource capacity violation: Resource_1 has 2 tasks at time 14, capacity is 1
- Resource capacity violation: Resource_1 has 2 tasks at time 15, capacity is 1
- Resource capacity violation: Resource_1 has 2 tasks at time 16, capacity is 1
- Resource capacity violation: Resource_1 has 2 tasks at time 17, capacity is 1
- Disjunctive constraint violation: Task_6 and Task_7 overlap
- Disjunctive constraint violation: Task_2 and Task_7 overlap
- Disjunctive constraint violation: Task_4 and Task_7 overlap
- Disjunctive constraint violation: Task_1 and Task_6 overlap
17
