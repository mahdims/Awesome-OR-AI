--- Page 1 ---
TIDE: TUNING-INTEGRATED DYNAMIC EVOLUTION FOR
LLM-BASED AUTOMATED HEURISTIC DESIGN
Chentong Chen1∗, Mengyuan Zhong1∗, Ye Fan2, Jialong Shi1†, Jianyong Sun1†
1School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China
2School of Electronics and Information, Northwest Polytechnical University, Xi’an, China
{chengtong.chen, my.zhong}@stu.xjtu.edu.cn, fanye@nwpu.edu.cn,
{jialong.shi, jy.sun}@xjtu.edu.cn
ABSTRACT
Although Large Language Models have advanced Automated Heuristic Design, treating algorithm
evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic
structures and continuous numerical parameters. Consequently, existing methods often discard
promising algorithms due to uncalibrated constants and suffer from premature convergence resulting
from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated
Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization.
TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit
Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation
with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler
dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive
experiments across nine combinatorial optimization problems demonstrate that TIDE discovers
heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving
improved search efficiency and reduced computational costs.
Keywords Automated Heuristic Design · Large Language Models · Evolutionary Computation · Combinatorial
Optimization · Program Synthesis
1
Introduction
The design of heuristic algorithms is crucial for solving NP-hard Combinatorial Optimization Problems (COPs) across
various domains [Sánchez et al., 2020, Peres and Castelli, 2021]. Traditionally, this process was a laborious pursuit
constrained by the limits of human intuition. Recently, the advent of Large Language Models (LLMs) has catalyzed a
paradigm shift, giving rise to Language Hyper-Heuristics (LHHs) [Zhang et al., 2024]. Groundbreaking studies, such as
FunSearch [Romera-Paredes et al., 2024], EoH [Liu et al., 2024a], and ReEvo [Ye et al., 2024], have demonstrated
that LLMs can act as intelligent designers [Liu et al., 2024b] within evolutionary frameworks, automatically designing
heuristics that surpass human-crafted baselines. By treating algorithm generation as a search problem within the code
space, these methods have opened new frontiers in Automated Heuristic Design (AHD) [Liu et al., 2024b, Wu et al.,
2024].
However, treating algorithm evolution purely as a monolithic text generation task implies an oversimplified heuristic
representation, obscuring the intrinsic nature of heuristics: the coupling of a discrete algorithmic structure (i.e., the
symbolic logic flow) with a set of continuous-valued parameters. Although LLMs excel at symbolic reasoning, they
suffer from numerical blindness [Feng et al., 2024], often hallucinating suboptimal constants that undermine the validity
of the logic itself. The preceding approaches bifurcate the evolution of algorithmic structure and parameter configuration.
For instance, AutoEP [Xu et al., 2025] serves as a hyperparameter controller for static algorithmic frameworks, limiting
optimization to incremental refinement of pre-defined logic rather than to structural innovation. Conversely, HSEvo [Dat
et al., 2025] relegates parameter tuning to a post-hoc refinement stage applied only to pre-selected elites, creating
∗Equal contribution.
†Corresponding author.
arXiv:2601.21239v2  [cs.AI]  7 Feb 2026


--- Page 2 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Heuristic
Representation
Search 
Strategy
Explicit
Implicit
Stochastic
Systematic
EoH 
(ICML, 2024)
ReEvo
(NeurIPS, 2024)
HSEvo
(AAAI, 2025)
MCTS-AHD
(ICML, 2025)
PartEvo
(NeurIPS, 2025)
FunSearch 
(Nature, 2023)
TIDE (Ours)
(b) Search Strategy
Stochastic Search
Systematic Search
Search Direction
Candidate
Systematic
Guidance
Mechanism
Optimal
Direction
(a) Heuristic Representation
Code
Code
Thought
Parameter
Logic
…
Implicit
Explicit
Insight
Reflection
Figure 1: Left: Comparison of representative LLM-based AHD methods along Search Strategy (Stochastic →
Systematic) and Heuristic Representation (Implicit →Explicit). Right: (a) Heuristic Representation: implicit heuristics
lack explicit structure, whereas explicit heuristics define modular components for interpretability and structure. (b)
Search Strategy: stochastic search uses random sampling, while systematic search employs guided mechanisms for
efficient exploration.
a sequential dependency that overlooks the intrinsic coupling between algorithmic logic and numerical constants.
Consequently, these methods bias the search towards designs that are merely robust to initialization, rather than
unlocking the full potential of complex heuristics through joint optimization.
In addition, current methods struggle with both search direction control and diversity maintenance. Frameworks like
EoH [Liu et al., 2024a] and MCTS-AHD [Zheng et al., 2025] often use random or static strategies to schedule prompts,
wasting tokens on unpromising directions. Conversely, reflection-based methods [Ye et al., 2024, Dat et al., 2025, Qi
et al., 2025, Guo et al., 2025] tend to converge rapidly into a single dominant logic, sacrificing population diversity.
PartEvo [Hu and Zhang, 2025] employs niching to enhance diversity. However, it relies on textual statistics (e.g.,
CodeBLEU [Ren et al., 2020]) or semantic embeddings [Chandrasekaran and Mago, 2021]. These metrics cannot
distinguish between real logical changes and simple code rephrasing, leading to a misaligned estimation of population
diversity.
Thus, as illustrated in Figure 1, many LLM-based AHD methods exhibit limitations in both heuristic representation and
search strategy. To address these challenges, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework
that refines heuristic representation by decoupling discrete algorithmic structures from continuous numerical parameters,
and optimizes search strategy through dynamic mechanisms.
TIDE adopts a nested architecture initiated by an outer TSED-Guided Island Model. To quantify algorithm structure
diversity, we employ the Tree Similarity Edit Distance (TSED) [Song et al., 2024] to govern the adaptive migration and
selective reset mechanisms. Within each island, the search is driven by a Co-Evolutionary Inner Loop that synergizes
two specialized modules: a Upper Confidence Bound(UCB)-based scheduler that adaptively selects high-yield prompt
strategies for LLM-based logic generation, and a differential mutation operator specifically dedicated to calibrating
continuous parameters. This division of labor mitigates the LLM’s numerical blindness, ensuring that evaluation fidelity
reflects the true upper bound of a heuristic’s performance, thereby enabling the search to accurately identify and retain
high-potential algorithmic backbones.
Overall, we make the following contributions:
• We propose TIDE, a nested evolutionary framework that strictly decouples algorithmic logic generation from
continuous parameter optimization, thereby mitigating the numerical limitations of LLMs and uncovering the
performance potential of algorithmic backbones via dedicated calibration.
• We introduce a TSED-guided Island Model that leverages a scale-invariant structural metric to govern adaptive
migration and selective reset mechanisms, effectively maintaining global algorithm structure diversity and
preventing premature convergence.
• We formulate the scheduling of prompting strategies as a non-stationary Multi-Armed Bandit problem and
employ a UCB-based policy to dynamically prioritize high-yield prompt strategies, maximizing search
efficiency under a constrained function evaluation budget.
2


--- Page 3 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
2
Preliminary
2.1
Problem Definition: Automatic Heuristic Design
Let Q denote a specific Combinatorial Optimization Problem. Associated with Q is an instance space X and a solution
space Y. We assume problem instances x ∈X are drawn from a specific distribution DQ. For any given instance x, a
valid heuristic h : X →Y constructs a feasible solution y = h(x) ∈Y. The quality of this solution is evaluated by an
objective function f : X × Y →R, where f(x, y) represents the cost (or negative reward) to be minimized.
The goal of AHD is to search for an optimal heuristic h∗within a vast, open-ended heuristic space H that minimizes
the expected cost over the problem distribution [Ye et al., 2024, Zheng et al., 2025]:
h∗= arg min
h∈H
Ex∼DQ [f(x, h(x))] .
(1)
In practice, since the true distribution DQ is typically unknown or continuous, the expectation is approximated by the
empirical average performance on a finite training set of instances {x1, . . . , xN} sampled from DQ.
2.2
Program Representation and Structural Distance
Unlike neural policies operating in latent spaces, heuristics in AHD are symbolic programs where functionality is
coupled with syntax. To robustly quantify phenotypic diversity, we utilize a structural analysis pipeline that decouples
algorithmic logic from surface-level lexical variations.
Abstract Syntax Tree (AST) and Tree Edit Distance (TED).
Let C denote the space of source code. We define a
parsing and normalization function Φ : C →T that maps a heuristic c to an Abstract Syntax Tree (AST) T = (V, E, λ).
Here, V is the set of nodes, E represents hierarchical parent-child dependencies, and λ : V →Σ assigns labels from a
simplified vocabulary Σ. To measure the topological divergence between two heuristics, we employ Tree Edit Distance
(TED) [Zhang and Shasha, 1989], computed via the state-of-the-art APTED algorithm [Pawlik and Augsten, 2015].
TED is defined as the minimum cost of a sequence of elementary operations, comprising insertion, deletion, and
renaming, that transforms a source tree Ta into a target tree Tb. Formally, let Sa→b be the set of all valid edit scripts
transforming Ta to Tb. The edit distance ∆is the minimization objective:
∆(Ta, Tb) =
min
S∈Sa→b
X
op∈S
γ(op),
(2)
where γ(op) represents the cost function associated with operation op (typically unit cost).
3
TIDE-AHD
We propose TIDE, a framework instantiated as a nested evolutionary architecture (Figure 2). As implied by the
nomenclature, the system orchestrates two distinct levels of optimization: the Dynamic Evolution corresponds to an
Outer Loop that navigates the global topological landscape through a TSED-guided Island Model, while the Tuning-
Integrated component refers to an Inner Loop that synergizes algorithmic structure reasoning with precise parameter
calibration.
3.1
TSED-Guided Island Model
3.1.1
Structural Similarity Quantification using TSED
Current AHD methods predominantly follow a Thought-to-Code paradigm, often approximating diversity via the textual
similarity of intermediate thoughts or superficial lexical metrics like Jaccard distance. We contend that these proxies are
ill-suited for heuristic evolution, as they fail to distinguish between lexical variations and genuine algorithmic novelty.
To quantify phenotypic diversity, we employ the Tree Similarity Edit Distance (TSED) [Song et al., 2024], a metric that
operates on the AST level to isolate control flow from syntactic noise.
The computation of TSED follows a deterministic pipeline designed to ensure evaluation fidelity. First, the generated
heuristic code c is parsed into a raw AST. To mitigate the stochasticity of LLM-generated identifiers, we apply a
structural normalization function Φ(·) that removes non-functional elements such as comments and imports, and
maps specific variable names to uniform placeholders. This yields a normalized tree ˆT = Φ(c) representing the
algorithmic backbone. Subsequently, we employ the APTED algorithm to compute the TED, denoted as ∆( ˆTA, ˆTB),
3


--- Page 4 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
def select_next_node(cur, dest, U, D):
    c1, c2, c3, c4 = 0.4, 0.3, 0.2, 0.1
    score_map = {}
    for v in U:
        d_v = [D[v][i] for i in U if i != v]
        avg_d = np.mean(d_v)
        std_d = np.std(d_v)
        score = c1 * D[cur][v] - c2 * avg_d
                    + c3 * std_d - c4 * D[dest][v]
        score_map[v] = score
    next_node = min(score_map, key=score_map.get)
    return next_node
def select_next_node(cur, dest, U, D):
    c1, c2, c3, c4 =      ,      ,      , 
    score_map = {}
    for v in U:
        d_v = [D[v][i] for i in U if i != v]
        avg_d = np.mean(d_v)
        std_d = np.std(d_v)
        score = c1 * D[cur][v] - c2 * avg_d
                    + c3 * std_d - c4 * D[dest][v]
        score_map[v] = score
    next_node = min(score_map, key=score_map.get)
    return next_node
Search 
Range
0.45
0.3
0.2
0.05
Population
0.4
0.3
0.2
0.1
…
LLM
Extract 
e1 e2 m1 m2 m3
LLM
m2
Heuristic Evolution
TSED
Insight
Elite Code
Evaluation
fitness
parameter vector
Parameter Adjustment
0.4
0.3
0.2
0.1
0.45 0.2 0.15 0.05
0.45
0.3
0.2
0.05
Target Vector
Crossover
Mutant Vector
Trial Vector
Differential 
Mutation
…
Fusion Reset
Distance between Islands
Migration of Insight
Migration of Code
Island
Tournament Selection
Parameter
Logic
Figure 2: The pipeline of TIDE-AHD. Left:The outer loop functions as a TSED-guided Island Model, regulating global
diversity via adaptive migration and fusion resets. Right: The inner loop executes co-evolutionary search, synergizing
UCB-scheduled algorithmic logic generation with parameter refinement to mitigate numerical blindness.
which quantifies the minimum cumulative cost of elementary operations, including insertion, deletion, and renaming,
required to align two normalized trees. Finally, to ensure scale invariance across heuristics of varying complexity, we
derive the normalized similarity score STSED ∈[0, 1] as:
STSED( ˆTA, ˆTB) = max
 
0, 1 −
∆( ˆTA, ˆTB)
max(| ˆTA|, | ˆTB|)
!
,
(3)
where | ˆT | denotes the node count of the normalized tree. This formulation effectively filters out superficial textual
differences, providing a robust basis for the island model’s migration decisions.
3.1.2
Adaptive Migration and Reset Strategies
TSED-based Topology Construction. In our Island Model, TSED serves as the definitive metric for defining the
interaction topology among subpopulations. Let I = {I1, I2, . . . , IK} denote the set of K independent islands. Within
each island Ik, the population consists of a set of individuals, where each individual is defined as a heuristic algorithm
represented by Python code. At each migration interval, we construct a pairwise code similarity matrix M ∈[0, 1]K×K
to quantify structural relationships across the global population. The entry Mij is computed as the arithmetic mean of
the TSED similarity scores between the individuals of island i and island j:
Mij =
1
|Ii||Ij|
X
c∈Ii
X
c′∈Ij
STSED(c, c′)
(4)
This matrix provides a comprehensive topological map of the search space, distinguishing between islands that have
collectively converged into similar algorithmic basins and those that explore distinct structural regions.
Dual-Mode Migration Strategy. Migration is triggered adaptively to counteract stagnation, utilizing a dual-mode
transfer strategy governed by the structural topology matrix M. For island pairs exhibiting high structural convergence
(high Mij), which signals a shared basin of attraction, maintaining independence becomes computationally redundant.
In such cases, we enforce exploitation by directly overwriting the target island’s lowest-performing individual with the
4


--- Page 5 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
source island’s current best-performing individual (the elite). This synchronizes fine-grained parameter refinements
across structurally similar populations. Conversely, for pairs located in distinct structural basins (low Mij), direct
code transfer risks disrupting local evolutionary trajectories. Therefore, we prioritize exploration through conceptual
knowledge transfer: the LLM distills the pivotal algorithmic mechanisms of the source elite into a natural language
insight [Chen et al., 2025], injecting it into the target’s context to assimilate superior design principles without
compromising its unique implementation architecture.
Constructive Fusion Reset. Reset serves as a rigorous intervention reserved for deep stagnation where an island
fails to improve over an extended timeframe. Rather than random re-initialization, we employ a Constructive Fusion
Mechanism. We prompt the LLM to synthesize a hybrid candidate by integrating the robust structural backbone of
the current global elite with the specialized local mechanisms of the Stagnated Incumbent. This strategy generates
a high-quality seed that preserves accumulated domain knowledge while injecting sufficient structural novelty to
re-energize the search.
3.2
Co-Evolution of Algorithm Structure and Parameters
We formalize the heuristic space as H = S
l∈L{Φ(l, p) | p ∈P(l) ⊆Rd(l)}, where each heuristic combines a discrete
algorithmic structure l(encoding the symbolic logic) with continuous parameters p. Consequently, the AHD objective
decomposes into a bi-level optimization problem:
h∗= argmin
l∈L

min
p∈P(l) Ex∼DQ[f(x, Φ(l, p)(x))]

.
(5)
This structure motivates our framework’s division into two complementary components: Structural Search to traverse
the logic space L, and Parameter Refinement to optimize p within the selected logic.
3.2.1
Structural Search with Adaptive Prompting Strategy
Prompting Action Space. In LLM-based AHD, the structural quality of generated offspring is intrinsically linked to
the specific prompting strategy employed. We formally define a discrete action space A = {a1, a2, . . . , aK}, where
each action ak constitutes a distinct prompting strategy, serving as an evolutionary operator instantiated through a
specific prompt template to elicit targeted algorithmic transformations. Drawing upon the paradigms established in
EoH Liu et al. [2024a], our action space is categorized into crossover and mutation strategies. The crossover strategies
include Exploratory Generation (e1), which synthesizes novel architectures to maximize diversity, and Backbone
Consensus (e2), designed to extract and preserve shared high-performing substructures. The mutation strategies consist
of Topology Perturbation (m1) for modifying control flows, Scoring Logic Refinement (m2) for adjusting internal
weighting schemes, and Parsimony Enforcement (m3) for simplifying logic to enhance generalization capabilities.
Stochastic MAB Formulation. Selecting the optimal strategy is non-trivial due to two intrinsic properties of the
generative process. First, LLM-based generation is stochastic: invoking a strategy ak yields samples from a high-
variance distribution rather than deterministic improvements, resulting in noisy reward signals. Second, the process
is non-stationary: the marginal utility of a specific strategy is not constant but evolves dynamically depending on the
optimization state.
Consequently, we formulate this sequential decision-making process as a Multi-Armed Bandit (MAB) problem [Chen
et al., 2013, Besbes et al., 2014]. In this setting, the prompting strategies in A correspond to the arms, and the
normalized fitness improvement serves as a stochastic reward signal. The central challenge constitutes an exploration-
exploitation dilemma: given a constrained budget of function evaluations, the search process must strategically balance
the exploitation of strategies with proven efficacy against the exploration of under-sampled prompt strategies.
UCB Selection Policy. We address this dilemma by implementing the Upper Confidence Bound (UCB) algorithm [Auer
et al., 2002, DaCosta et al., 2008, Candan et al., 2013, Barto, 2021]. This deterministic policy selects the prompting
strategy at at iteration t by maximizing a score that accounts for both the empirical reward estimate and the uncertainty
of that estimate:
at = argmax
a∈A
 
Qt(a) + C
s
2 ln Nt
nt(a)
!
(6)
where Qt(a) denotes the average reward obtained by strategy a up to iteration t, quantified as the relative fitness gain
over the current optimum; Nt and nt(a) represent the global count of strategy invocations and the specific selection
frequency of strategy a, respectively; and C is the exploration constant that modulates the confidence interval width
to control the propensity for testing uncertain strategies. By dynamically updating these statistics, UCB effectively
5


--- Page 6 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
mitigates the impact of noisy rewards and adapts the prompt strategies distribution to the evolving optimization
landscape.
Evaluation and Update. For crossover strategies, we utilize Tournament Selection to sample parents from the local
subpopulation. Conversely, mutation strategies prioritize the incumbent elite to leverage the existing algorithmic
backbone. Following code generation and evaluation, the valid offspring are merged into the subpopulation. We
then enforce a rank-based survival strategy where the pool is sorted and truncated to retain only the top-performing
individuals. The cycle concludes by calculating the relative fitness gain of the offspring, which serves as the reward
signal to update the value estimates of the UCB policy.
3.2.2
Parameter Refinement with Differential Mutation Operator
Effective heuristic algorithms rely on the synergy between discrete symbolic logic and continuous hyperparameters,
where performance is often critically sensitive to tightly coupled numerical values. Although LLMs excel at semantic
reasoning for structural discovery, their nature as probabilistic token predictors limits their arithmetic precision,
rendering them inefficient for iterative numerical fine-tuning. To resolve this, we draw upon the design philosophy of
Memetic Algorithms (MA) [Krasnogor and Smith, 2005, Neri et al., 2011], which advocates coupling global structural
search with intensive local refinement.
Specifically, we employ a Differential Mutation Operator adapted from standard Differential Evolution (DE)[Storn
and Price, 1997]. Given the gradient-free nature of the heuristic optimization landscape, this operator effectively steers
the search direction using scaled vector differences between population members. This mechanism provides an adaptive
means of navigating the complex terrain of coupled parameters, enabling efficient fine-tuning of the novel heuristic
logic proposed by the LLM.
Conditional Activation and Trust Region Construction. We restrict intensive local refinement to offspring competitive
with the island’s current elite. This protocol filters out structurally inferior candidates early, concentrating gradient-free
optimization resources solely on high-potential algorithm structures.
Once activated, we employ the LLM to identify tunable parameters in the code and instantiate the initial parameter vector
xinit. To ensure subsequent variations remain semantically valid, we construct a dynamic hyper-rectangular trust region
Ωcentered on xinit. Since heuristic parameters often vary across multiple orders of magnitude, we adaptively define
the search boundaries based on the initial value of each dimension. For standard coefficients, we determine the upper
and lower bounds proportional to the parameter’s magnitude, allowing for sufficient directional exploration without
destabilizing the logic. In contrast, for near-zero parameters where proportional bounds would become negligible,
we apply a constant value range to maintain an effective search volume. Finally, to bootstrap the local search, we
initialize a micro-population P by applying Gaussian perturbations [Sun et al., 2019] to xinit, thereby exploring the
local neighborhood while preserving the original algorithmic intent.
Differential Mutation Mechanism. Operating on this initialized micro-population, we execute the canonical rand/1
differential mutation scheme [Storn and Price, 1997]. For a target vector xi, a mutant vector vi is generated as :
vi = xr1 + F · (xr2 −xr3)
(7)
where xr1, xr2, xr3 are three distinct vectors sampled uniformly from P. The difference vector (xr2−xr3) establishes a
stochastic search direction derived from the local neighborhood geometry, while the scaling factor F ∈(0, 1) moderates
the mutation strength.
We then apply binomial crossover to produce the final trial vector ui:
ui,j =
vi,j
if Rj ≤CR or j = jrand
xi,j
otherwise
(8)
where, for each dimension j, Rj ∼U(0, 1). The crossover rate CR ∈[0, 1] controls the probability of inheriting
components from the mutant vector, ensuring that at least one dimension (jrand) is updated to drive the search forward.
Evaluation and Update. We generate and evaluate a candidate batch in parallel by mapping trial vectors back into
the executable structure to obtain their objective costs g(·). Adopting a greedy selection strategy, we update the
current parameter vector xi with the optimal candidate u∗= argmink g(u(k)) only if it improves the solution quality
(i.e., g(u∗) < g(xi)). This update mechanism ensures that novel algorithmic architectures are paired with optimized
numerical configurations independent of LLM inference. Furthermore, these optimized constants are preserved in the
global elite population, propagating fine-grained numerical traits to accelerate convergence across the outer Island
Model.
6


--- Page 7 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Table 1: Performance of constructive and improvement heuristics on multiple combinatorial optimization tasks. Obj.↓
/ ↑indicate minimization / maximization objectives, and Gap reports deviation from the corresponding optimal or
best-known reference. Optimal results are obtained using LKH3 (TSP) and OR-Tools (KP), and optimal results for
online BPP and ASP are taken from MCTS-AHD. All results of LLM-based methods are averaged over three runs. The
best results among LLM-based AHD methods are highlighted in bold.
Constructive Heuristic
Task
TSP
KP
Scale
N=50
N=100
N=200
N=50, W=12.5
N=100, W=25
N=200, W=25
Methods
Obj.↓
Gap
Obj.↓
Gap
Obj.↓
Gap
Obj.↑
Gap
Obj.↑
Gap
Obj.↑
Gap
Optimal
5.68
0.00%
7.75
0.00%
10.72
0.00%
20.04
0.00%
40.27
0.00%
57.45
0.00%
Greedy
6.96
22.62%
9.71
25.32%
13.46
25.60%
19.99
0.26%
40.23
0.12%
57.40
0.09%
POMO
5.70
0.38%
8.00
3.30%
12.90
20.33%
19.61
2.12%
39.68
1.48%
57.27
0.31%
EoH
6.36
12.11%
8.89
14.72%
12.45
16.14%
19.92
0.60%
40.05
0.55%
56.44
1.76%
ReEvo
6.15
8.27%
8.52
10.06%
11.87
10.75%
20.00
0.21%
40.23
0.10%
57.40
0.08%
HSEvo
6.13
8.04%
8.52
10.01%
11.88
10.80%
20.00
0.17%
40.24
0.08%
57.41
0.07%
MCTS-AHD
6.14
8.19%
8.53
10.18%
11.91
11.09%
20.00
0.17%
40.24
0.08%
57.41
0.07%
TIDE(ours)
5.95
4.76%
8.30
7.15%
11.86
10.65%
20.03
0.04%
40.26
0.02%
57.44
0.02%
Task
BPP online
ASP
Scale
N=1k, W=100
N=5k, W=100
N=10k, W=100
n=12, w=7
n=15, w=10
n=21, w=15
Methods
Obj.↓
Gap
Obj.↓
Gap
Obj.↓
Gap
Obj.↑
Gap
Obj.↑
Gap
Obj.↑
Gap
Optimal
402.4
0.00%
2019.4
0.00%
4010.6
0.00%
792
0.00%
3003
0.00%
43596
0.00%
EoH
418.2
3.93%
2043.6
1.20%
4035.9
0.63%
775
2.15%
2732
9.02%
32217
26.10%
ReEvo
418.5
4.01%
2088.1
3.40%
4139.9
3.22%
767
3.16%
2781
7.39%
29743
31.78%
HSEvo
421.5
4.74%
2104.7
4.22%
4172.1
4.03%
760
4.04%
2747
8.52%
30291
30.52%
MCTS-AHD
417.5
3.76%
2069.7
2.49%
4095.9
2.13%
777
1.89%
2801
6.73%
33334
23.54%
TIDE(ours)
415.8
3.33%
2034.2
0.73%
4025.1
0.36%
780
1.52%
2812
6.36%
33549
23.04%
Improvement Heuristic
Task
TSP GLS
TSP KGLS
Scale
N=100
N=200
N=500
N=100
N=200
N=500
Methods
Obj.↓
Gap
Obj.↓
Gap
Obj.↓
Gap
Obj.↓
Gap
Obj.↓
Gap
Obj.↓
Gap
Optimal
7.76
0.000%
10.70
0.00%
16.52
0.00%
7.768
0.000%
10.71
0.00%
16.50
0.00%
NeuOpt
7.78
0.304%
10.82
1.08%
-
-
7.796
0.362%
10.82
1.05%
-
-
EoH
7.76
0.034%
10.74
0.37%
16.75
1.37%
7.768
0.003%
10.73
0.23%
16.66
0.96%
ReEvo
7.82
0.862%
10.98
2.56%
17.14
3.74%
7.769
0.010%
10.75
0.36%
16.72
1.36%
HSEvo
7.76
0.064%
10.79
0.76%
16.86
2.04%
7.769
0.008%
10.73
0.22%
16.66
0.97%
MCTS-AHD
7.76
0.081%
10.76
0.49%
16.79
1.62%
7.769
0.011%
10.73
0.23%
16.66
0.98%
TIDE(ours)
7.76
0.026%
10.74
0.34%
16.75
1.37%
7.768
0.006%
10.73
0.20%
16.65
0.90%
4
Experiments
In this section, we evaluate the heuristics designed by our method across 9 problem types and 5 algorithmic frameworks.
The problem types include several well-known NP-hard combinatorial optimization problems and more challenging
complex optimization problems. The problem formulations are detailed in Appendix B.1. We consider 5 general
algorithmic frameworks in our experiments, including Constructive Heuristic, Improvement Heuristic, Ant Colony
Optimization (ACO), Genetic Algorithm (GA), and Reinforcement Learning (RL). These frameworks are introduced in
Appendix B.2.
4.1
Experimental Setup
Setting.
For fair comparison, all experiments were conducted on an Intel Core i7-12700 CPU, employing
Qwen3-Max-2025-09-23 accessed via its official API as the unified LLM for all methods. The outer framework
employs 6 islands with a TSED diversity threshold of 0.7. In the inner framework, each island has a population size of
8 and runs for 800 generations. For the adaptive operator selection strategy, the UCB exploration constant is set to
√
2,
following UCB1 [Auer et al., 2002]. A more detailed description is provided in Appendix C.1.
Baseline. To provide an intuitive comparison of the performance of heuristics designed by our method, we introduce
several methods from different domains as baselines. (1) Handcrafted heuristics, e.g., LKH3 [Lin and Kernighan, 1973]
7


--- Page 8 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Table 2:
Performance of different heuristics with different frameworks on various tasks. The ACO framework is
evaluated on TSP, CVRP, OP, MKP, and offline BPP; the GA framework on DPP; RL methods on Mountain Car. The
results in the table represent objective function values, and arrows (↓/ ↑) indicate minimization/maximization objectives.
The results of ACO and DeepACO are taken from MCTS-AHD and marked with an asterisk (*). LLM-based results are
averaged over three runs. And, the best results are highlighted in bold.
Frameworks
ACO
GA
Others
Task
TSP↓
CVRP↓
OP↑
MKP↑
BPP offline↓
DPP↑
Mountain Car↓
Scale
50
100
50
100
100
200
100
200
500
1000
-
-
ACO
5.99
8.95
11.36
18.78
24.11
36.83
22.74
40.67
208.83
417.94
-
-
DeepACO
5.84
8.28
8.89
14.93
30.36
51.35
23.09
41.99
203.13
405.17
-
-
EoH
5.85
8.35
9.25
15.96
29.94
53.17
23.22
42.30
204.59
408.48
-
103.3
ReEvo
5.84
8.32
9.80
16.68
30.05
52.79
23.23
42.36
204.70
408.73
12.74
106.3
HSEvo
5.84
8.28
9.43
16.01
30.79
54.70
23.26
42.45
204.50
408.36
-
116.7
MCTS-AHD
5.82
8.20
9.12
15.85
30.73
54.74
23.30
42.77
203.36
405.63
-
105.7
TIDE(ours)
5.80
8.15
9.01
15.37
30.90
55.43
23.33
42.90
203.20
405.20
12.80
98.3
for TSP, OR-Tools for KP. (2) Neural Combinatorial Optimization (NCO) models, e.g., POMO [Kwon et al., 2020]
for TSP and KP in a constructive heuristic framework, NeuOpt [Ma et al., 2023] for TSP in an improvement heuristic
framework, DeepACO [Ye et al., 2023] for TSP, CVRP, OP, MKP and BPP offline. (3) LLM-based AHD methods, e.g.,
EoH [Liu et al., 2024a], ReEvo [Ye et al., 2024], HSEvo [Dat et al., 2025] and MCTS-AHD [Zheng et al., 2025].
4.2
Empirical Result
Constructive Heuristic.
Constructive heuristics generate solutions incrementally by making a sequence of decisions,
where each decision determines how a partial solution is extended. In this framework, we evaluate heuristics on TSP,
KP, online BPP, and ASP. Our method is to automatically design construction rules that guide the selection of the next
element to be added to the partial solution. Table 1 reports the performance of different heuristic design methods in
constructive frameworks. Overall, the heuristic designed by our method consistently achieves the best performance in
LLM-based methods across all evaluated tasks and problem scales. Notably, on the KP task, our automatically designed
heuristics outperform end-to-end NCO models.
Improvement Heuristic.
Improvement heuristics start from an initial feasible solution and iteratively refine it through
local search operations. Among them, Guided Local Search (GLS) and its variants, like Knowledge-Guided Local
Search (KGLS), are widely adopted for large-scale combinatorial optimization due to their strong empirical performance
and clear separation between search operators and guidance mechanisms. We consider TSP under the GLS and KGLS
frameworks. Table 1 reports the results on TSP under both the GLS and KGLS frameworks. The heuristics designed by
our method exhibit competitive performance, outperforming heuristics generated by existing LLM-based AHD methods
and achieving substantial performance advantages over the improvement-based NCO model.
Ant Colony Optimization.
Ant Colony Optimization (ACO) is a population-based search framework that generates
solutions by simulating the collective behavior of ants guided by pheromone trails and heuristic information. This setup
allows us to assess the generality of our method across multiple problem domains under a unified search framework.
So, we evaluate our method on TSP, CVRP, OP, and MKP within the ACO framework. Our method automatically
designs the heuristic information components that influence state transitions during solution construction. Results
within the ACO framework across different methods are shown in Table 2. The heuristics designed by our method
achieve substantial improvements over manually designed heuristics in classical ACO. It also surpasses NCO methods
that use neural networks to predict heuristic information on most problems. Notably, our method exhibits competitive
performance compared to existing LLM-based AHD approaches.
Genetic Algorithm.
Genetic Algorithms (GA) evolve a population of solutions via selection, crossover, and mutation,
making them suitable for complex optimization problems. Following experiments in ReEvo, we evaluate our method
on the Decap Placement Problem (DPP), automatically designing problem-specific crossover operators to improve
crossover strategies for generating new offspring in our experiment. In Table 2, we compare our results with heuristics
designed by ReEvo.
Reinforcement Learning.
Thanks to the strong language understanding and generality of large models, we are able
to design heuristics for more complex frameworks. Following experiments in MCTS-AHD, we evaluate our method on
8


--- Page 9 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Table 3: Ablation study of TIDE on TSP, KP, and online BPP. Variants remove 3 key components to evaluate their
contribution, with results reported as relative gaps (%). For KP, W = 12.5 for 50-item instances and W = 25 for 100-
and 200-item instances; for online BPP, the capacity is 100. All results are averaged over three runs. The best results
are highlighted in bold.
TSP
Scale
50
100
200
w/o Migration Strategy
8.00%
9.97%
10.73%
w/o UCB Selection Policy
5.87%
8.91%
12.64%
w/o Parameter Refinement
8.01%
9.74%
10.85%
TIDE
4.76%
7.15%
10.65%
KP
Scale
50
100
200
w/o Migration Strategy
0.19%
0.09%
0.08%
w/o UCB Selection Policy
0.13%
0.06%
0.05%
w/o Parameter Refinement
0.17%
0.07%
0.07%
TIDE
0.04%
0.02%
0.02%
Online BPP
Scale
1k
5k
10k
w/o Migration Strategy
3.58%
2.63%
2.47%
w/o UCB Selection Policy
3.64%
1.70%
1.34%
w/o Parameter Refinement
4.26%
1.83%
1.51%
TIDE
3.33%
0.73%
0.36%
a policy optimization task Mountain Car-v0, with a reinforcement learning framework. We compare our method with
LLM-based AHD methods and demonstrate clear advantages, as shown in Table 2.
Ablation Study.
To better understand the contribution of each component in the proposed method, we conduct a
comprehensive ablation study by systematically removing or modifying key modules, with the results reported in Table 3.
All ablation experiments are performed under the same experimental settings as the full model to ensure fair comparison.
Specifically, the first variant removes the island migration mechanism and instead performs random migration. The
second variant eliminates the adaptive operator selection mechanism and resorts to random selection. The third variant
removes the hyperparameter tuning module. The results demonstrate that removing any single component leads to a
noticeable degradation in solution quality, indicating that each module plays a complementary role in the proposed
framework.
Comparative Experiment.
We compare the convergence behavior of different LLM-based AHD methods in terms
of objective value versus the number of evaluations. As shown in Figure 3, our method achieves better results under
the same number of evaluations. Specifically, methods initialized with seed functions, such as ReEvo and HSEvo,
tend to converge prematurely with limited improvement, while our method continues to improve and consistently
achieves better objective values. Specifically, ReEvo, HSEvo, and our method all adopt a warm-start strategy with
reflection, i.e., they are initialized with seed functions. However, since our method is island-based, it maintains richer
population diversity, and the adaptive operator selection facilitates escaping local optima, thereby avoiding the premature
convergence observed in other methods.
5
Conclusion
In this paper, we introduced TIDE to overcome the limitations of monolithic code generation in Automated Heuristic
Design by employing a nested architecture that decouples algorithmic logic from continuous parameters. To achieve
this, TIDE synergizes an outer TSED-guided island model for structural diversity with an inner loop that combines LLM
reasoning and gradient-free tuning to mitigate numerical blindness. Extensive experiments across nine combinatorial
optimization tasks demonstrate that TIDE not only discovers superior heuristics compared to state-of-the-art baselines
but also improves search efficiency by minimizing token consumption on parameter tuning. Future work will explore
9


--- Page 10 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
0
100
200
300
400
500
600
700
800
Number of Evaluations
5.9
6.0
6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
Objective Value
Convergence Analysis
EOH
ReEvo
HSEvo
MCTS-AHD
TIDE-AHD
Figure 3: A key example on TSP50 under constructive framework: Convergence comparison of different LLM-based
AHD methods. The x-axis denotes the number of evaluations, and the y-axis represents the objective value.
applying TIDE to multi-objective optimization scenarios and investigating the transferability of learned structural
insights across disparate problem domains.
References
Melissa Sánchez, Jorge M Cruz-Duarte, José carlos Ortíz-Bayliss, Hector Ceballos, Hugo Terashima-Marin, and
Ivan Amaya. A systematic review of hyper-heuristics on combinatorial optimization problems. IEEE Access, 8:
128068–128095, 2020.
Fernando Peres and Mauro Castelli. Combinatorial optimization problems and metaheuristics: Review, challenges,
design, and development. Applied sciences, 11(14):6449, 2021.
Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Understanding the importance of
evolutionary search in automated heuristic design with large language models. In International Conference on
Parallel Problem Solving from Nature, pages 185–202. Springer, 2024.
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien
Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from
program search with large language models. Nature, 625(7995):468–475, 2024.
Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evo-
lution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint
arXiv:2401.02051, 2024a.
Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, and Guojie
Song. Reevo: Large language models as hyper-heuristics with reflective evolution. Advances in neural information
processing systems, 37:43571–43608, 2024.
Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Xi Lin, Zhe Zhao, Xialiang Tong, Kun Mao, Zhichao Lu, Zhenkun
Wang, et al. A systematic survey on large language models for algorithm design. ACM Computing Surveys, 2024b.
Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, and Kay Chen Tan. Evolutionary computation in the era of large
language model: Survey and roadmap. IEEE Transactions on Evolutionary Computation, 2024.
Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, and Liwei Wang.
How numerical precision affects mathematical reasoning capabilities of llms. arXiv preprint arXiv:2410.13857,
2024.
Zhenxing Xu, Yizhe Zhang, Weidong Bao, Hao Wang, Ming Chen, Haoran Ye, Wenzheng Jiang, Hui Yan, and
Ji Wang. Autoep: Llms-driven automation of hyperparameter evolution for metaheuristic algorithms. arXiv preprint
arXiv:2509.23189, 2025.
10


--- Page 11 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Pham Vu Tuan Dat, Long Doan, and Huynh Thi Thanh Binh. Hsevo: Elevating automatic heuristic design with
diversity-driven harmony search and genetic algorithm using llms. In Proceedings of the AAAI Conference on
Artificial Intelligence, pages 26931–26938, 2025.
Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, and Bryan Hooi. Monte carlo tree search for comprehensive exploration in
llm-based automatic heuristic design. arXiv preprint arXiv:2501.08603, 2025.
Fubo Qi, Tianyu Wang, Ruixiang Zheng, and Mian Li. A memetic and reflective evolution framework for automatic
heuristic design using large language models. Applied Sciences, 15(15):8735, 2025.
Shuhan Guo, Nan Yin, James Kwok, and Quanming Yao. Nested-refinement metamorphosis: Reflective evolution for
efficient optimization of networking problems. In Findings of the Association for Computational Linguistics: ACL
2025, pages 17398–17429, 2025.
Qinglong Hu and Qingfu Zhang. Partition to evolve: Niching-enhanced evolution with llms for automated algorithm
discovery. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025.
Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,
and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297,
2020.
Dhivya Chandrasekaran and Vijay Mago. Evolution of semantic similarity—a survey. Acm Computing Surveys (Csur),
54(2):1–37, 2021.
Yewei Song, Cedric Lothritz, Xunzhu Tang, Tegawendé Bissyandé, and Jacques Klein. Revisiting code similarity
evaluation with abstract syntax tree edit distance. In Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pages 38–46, 2024.
Kaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance between trees and related problems.
SIAM journal on computing, 18(6):1245–1262, 1989.
Mateusz Pawlik and Nikolaus Augsten. Efficient computation of the tree edit distance. ACM Transactions on Database
Systems (TODS), 40(1):1–40, 2015.
Chentong Chen, Mengyuan Zhong, Jianyong Sun, Ye Fan, and Jialong Shi. Hifo-prompt: Prompting with hindsight and
foresight for llm-based automatic heuristic design. arXiv preprint arXiv:2508.13333, 2025.
Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and applications. In
International conference on machine learning, pages 151–159. PMLR, 2013.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary rewards.
Advances in neural information processing systems, 27, 2014.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine
learning, 47(2):235–256, 2002.
Luis DaCosta, Alvaro Fialho, Marc Schoenauer, and Michèle Sebag. Adaptive operator selection with dynamic
multi-armed bandits. In Proceedings of the 10th annual conference on Genetic and evolutionary computation, pages
913–920, 2008.
Caner Candan, Adrien Goëffon, Frédéric Lardeux, and Frédéric Saubion. Non stationary operator selection with island
models. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 1509–1516,
2013.
Andrew G Barto. Reinforcement learning: An introduction. by richard’s sutton. SIAM Rev, 6(2):423, 2021.
Natalio Krasnogor and James Smith. A tutorial for competent memetic algorithms: model, taxonomy, and design issues.
IEEE transactions on Evolutionary Computation, 9(5):474–488, 2005.
Ferrante Neri, Carlos Cotta, and Pablo Moscato. Handbook of memetic algorithms, volume 379. Springer, 2011.
Rainer Storn and Kenneth Price. Differential evolution–a simple and efficient heuristic for global optimization over
continuous spaces. Journal of global optimization, 11(4):341–359, 1997.
Gaoji Sun, Yanfei Lan, and Ruiqing Zhao. Differential evolution with gaussian mutation and dynamic parameter
adjustment. Soft Computing, 23(5):1615–1642, 2019.
Shen Lin and Brian W Kernighan. An effective heuristic algorithm for the traveling-salesman problem. Operations
research, 21(2):498–516, 1973.
Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min. Pomo: Policy
optimization with multiple optima for reinforcement learning. Advances in Neural Information Processing Systems,
33:21188–21198, 2020.
11


--- Page 12 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Yining Ma, Zhiguang Cao, and Yeow Meng Chee. Learning to search feasible and infeasible regions of routing problems
with flexible neural k-opt. Advances in Neural Information Processing Systems, 36:49555–49578, 2023.
Haoran Ye, Jiarui Wang, Zhiguang Cao, Helan Liang, and Yong Li. Deepaco: Neural-enhanced ant systems for
combinatorial optimization. Advances in neural information processing systems, 36:43706–43728, 2023.
Nasser R Sabar, Masri Ayob, Graham Kendall, and Rong Qu. Automatic design of a hyper-heuristic framework
with gene expression programming for combinatorial optimization problems. IEEE Transactions on Evolutionary
Computation, 19(3):309–325, 2014.
He Yu and Jing Liu. Deep insights into automated optimization with large language models and evolutionary algorithms.
arXiv preprint arXiv:2410.20848, 2024.
Fei Liu, Yilu Liu, Qingfu Zhang, Xialiang Tong, and Mingxuan Yuan. Eoh-s: Evolution of heuristic set using llms for
automated heuristic design. arXiv preprint arXiv:2508.03082, 2025.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language
models as optimizers. In The Twelfth International Conference on Learning Representations, 2023.
Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, and Yue-Jiao Gong. Llamoco:
Instruction tuning of large language models for optimization code generation. arXiv preprint arXiv:2403.01131,
2024.
Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, and Caglar
Gulcehre. Algorithm discovery with llms: Evolutionary search meets reinforcement learning. arXiv preprint
arXiv:2504.05108, 2025.
Darrell Whitley, Soraya Rana, and Robert B Heckendorn. The island model genetic algorithm: On separability,
population size and convergence. Journal of computing and information technology, 7(1):33–47, 1999.
Zbigniew Skolicki. An analysis of island models in evolutionary computation. In Proceedings of the 7th annual
workshop on Genetic and evolutionary computation, pages 386–389, 2005.
Marek Ruci´nski, Dario Izzo, and Francesco Biscani. On the impact of the migration topology on the island model.
Parallel Computing, 36(10-11):555–571, 2010.
Jun-ichi Kushida, Akira Hara, Tetsuyuki Takahama, and Ayumi Kido. Island-based differential evolution with varying
subpopulation size. In 2013 IEEE 6th international workshop on computational intelligence and applications
(IWCIA), pages 119–124. IEEE, 2013.
Amr Abdelhafez, Enrique Alba, and Gabriel Luque. Performance analysis of synchronous and asynchronous distributed
genetic algorithms on multiprocessors. Swarm and Evolutionary Computation, 49:147–157, 2019.
Guohua Wu, Rammohan Mallipeddi, and Ponnuthurai Nagaratnam Suganthan. Ensemble strategies for population-based
optimization algorithms–a survey. Swarm and evolutionary computation, 44:695–711, 2019.
Guoxia Fu, Chaoli Sun, Ying Tan, Guochen Zhang, and Yaochu Jin. A surrogate-assisted evolutionary algorithm with
random feature selection for large-scale expensive problems. In International conference on parallel problem solving
from nature, pages 125–139. Springer, 2020.
Mohammed A Awadallah, Mohammed Azmi Al-Betar, Asaju La’aro Bolaji, Iyad Abu Doush, Abdelaziz I Hammouri,
and Majdi Mafarja. Island artificial bee colony for global optimization. Soft Computing, 24(17):13461–13487, 2020.
Grasiele Regina Duarte, Afonso Celso de Castro Lemonge, Leonardo Goliatt da Fonseca, and Beatriz Souza Leite Pires
de Lima. An island model based on stigmergy to solve optimization problems. Natural Computing, 20(3), 2021.
Grasiele Duarte, Afonso Lemonge, and Leonardo Goliatt. A dynamic migration policy to the island model. In 2017
IEEE congress on evolutionary computation (CEC), pages 1135–1142. IEEE, 2017.
Alexander Novikov, Ngân V˜u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey
Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: A coding agent for
scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025.
Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen.
Evolving deeper llm thinking. arXiv preprint arXiv:2501.09891, 2025.
Chaiyong Ragkhitwetsagul, Jens Krinke, and David Clark. A comparison of code similarity analysers. Empirical
Software Engineering, 23(4):2464–2519, 2018.
Matija Novak, Mike Joy, and Dragutin Kermek. Source-code similarity detection and detection tools used in academia:
a systematic review. ACM Transactions on Computing Education (TOCE), 19(3):1–37, 2019.
Qurat Ul Ain, Wasi Haider Butt, Muhammad Waseem Anwar, Farooque Azam, and Bilal Maqbool. A systematic review
on code clone detection. IEEE access, 7:86121–86144, 2019.
12


--- Page 13 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Shengnan Zhang, Yan Hu, and Guangrong Bian. Research on string similarity algorithm based on levenshtein distance.
In 2017 IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), pages
2247–2251. IEEE, 2017.
Pengfei Xu and Jiaheng Lu. Efficient taxonomic similarity joins with adaptive overlap constraint. In Proceedings of the
27th ACM International Conference on Information and Knowledge Management, pages 1563–1566, 2018.
Tobias Christiani, Rasmus Pagh, and Johan Sivertsen. Scalable and robust set similarity join. In 2018 IEEE 34th
international conference on data engineering (ICDE), pages 1240–1243. IEEE, 2018.
Goutam Majumder, Partha Pakray, Alexander Gelbukh, and David Pinto. Semantic textual similarity methods, tools,
and applications: A survey. Computación y Sistemas, 20(4):647–665, 2016.
Z Feng. Codebert: A pre-trained model for program-ming and natural languages. arXiv preprint arXiv:2002.08155,
2020.
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy,
Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366,
2020.
Aryaz Eghbali and Michael Pradel. Crystalbleu: precisely and efficiently measuring the similarity of code. In
Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, pages 1–12, 2022.
Iulian Neamtiu, Jeffrey S Foster, and Michael Hicks. Understanding source code evolution using abstract syntax tree
matching. In Proceedings of the 2005 international workshop on Mining software repositories, pages 1–5, 2005.
Kevin Sendjaja, Satrio Adi Rukmono, and Riza Satria Perdana. Evaluating control-flow graph similarity for grading
programming exercises. In 2021 International Conference on Data and Software Engineering (ICoDSE), pages 1–6.
IEEE, 2021.
Grigori Sidorov, Helena Gómez-Adorno, Ilia Markov, David Pinto, and Nahun Loya. Computing text similarity using
tree edit distance. In 2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS)
held jointly with 2015 5th World Conference on Soft Computing (WConSC), pages 1–4. IEEE, 2015.
Stefan Schwarz, Mateusz Pawlik, and Nikolaus Augsten. A new perspective on the tree edit distance. In International
Conference on Similarity Search and Applications, pages 156–170. Springer, 2017.
Yusuf Kartal, E Kaan Akdeniz, and Kemal Özkan. Automating modern code review processes with code similarity
measurement. Information and Software Technology, 173:107490, 2024.
Álvaro Fialho. Adaptive operator selection for optimization. PhD thesis, Université Paris Sud-Paris XI, 2010.
Jiyuan Pei, Yi Mei, Jialin Liu, Mengjie Zhang, and Xin Yao. Adaptive operator selection for meta-heuristics: A survey.
IEEE Transactions on Artificial Intelligence, 2025.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathemat-
ics, 6(1):4–22, 1985.
Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for non-stationary bandit problems. arXiv
preprint arXiv:0805.3415, 2008.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article
recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670, 2010.
Álvaro Fialho, Luis Da Costa, Marc Schoenauer, and Michele Sebag. Analyzing bandit-based adaptive operator
selection mechanisms. Annals of Mathematics and Artificial Intelligence, 60(1):25–64, 2010.
Navikkumar Modi, Philippe Mary, and Christophe Moy. Qos driven channel selection algorithm for cognitive radio
network: Multi-user multi-armed bandit approach. IEEE Transactions on Cognitive Communications and Networking,
3(1):49–66, 2017.
Yisong Zhang and Guoxing Yi. Laos: Large language model-driven adaptive operator selection for evolutionary
algorithms. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 517–526, 2025.
Pablo Moscato et al. On evolution, search, optimization, genetic algorithms and martial arts: Towards memetic
algorithms. Caltech concurrent computation program, C3P Report, 826(1989):37, 1989.
Manuel López-Ibáñez, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres, Mauro Birattari, and Thomas Stützle. The irace
package: Iterated racing for automatic algorithm configuration. Operations Research Perspectives, 3:43–58, 2016.
Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm
configuration. In International conference on learning and intelligent optimization, pages 507–523. Springer, 2011.
13


--- Page 14 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Qiang Ma, Suwen Ge, Danyang He, Darshan Thaker, and Iddo Drori. Combinatorial optimization by graph pointer
networks and hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936, 2019.
Eugene L Lawler. The traveling salesman problem: a guided tour of combinatorial optimization. Wiley-Interscience
Series in Discrete Mathematics, 1985.
Karla L Hoffman, Manfred Padberg, and Giovanni Rinaldi. Traveling salesman problem. In Encyclopedia of operations
research and management science, pages 1573–1578. Springer, 2013.
Paolo Toth and Daniele Vigo. Models, relaxations and exact approaches for the capacitated vehicle routing problem.
Discrete Applied Mathematics, 123(1-3):487–512, 2002.
Pieter Vansteenwegen, Wouter Souffriau, and Dirk Van Oudheusden. The orienteering problem: A survey. European
Journal of Operational Research, 209(1):1–10, 2011.
Yongquan Zhou, Yan Shi, Yuanfei Wei, Qifang Luo, and Zhonghua Tang. Nature-inspired algorithms for 0-1 knapsack
problem: A survey. Neurocomputing, 554:126630, 2023.
Jakob Puchinger, Günther R Raidl, and Ulrich Pferschy. The multidimensional knapsack problem: Structure and
algorithms. INFORMS Journal on Computing, 22(2):250–265, 2010.
Chanaleä Munien and Absalom E Ezugwu. Metaheuristic algorithms for one-dimensional bin-packing problems: A
survey of recent advances and applications. Journal of Intelligent Systems, 30(1):636–663, 2021.
Istvan Borgulya. A hybrid evolutionary algorithm for the offline bin packing problem. Central European Journal of
Operations Research, 29(2):425–445, 2021.
Steven S Seiden. On the online bin packing problem. Journal of the ACM (JACM), 49(5):640–671, 2002.
Jon Barwise. Admissible sets and structures, volume 7. Cambridge University Press, 2017.
Larry D Smith, Raymond E Anderson, Douglas W Forehand, Thomas J Pelc, and Tanmoy Roy. Power distribution
system design methodology and capacitor selection for modern cmos technology. IEEE Transactions on Advanced
Packaging, 22(3):284–291, 1999.
Rafael Martí and Gerhard Reinelt. Heuristic methods. In The linear ordering problem: exact and heuristic methods in
combinatorial optimization, pages 17–40. Springer, 2010.
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to algorithms. MIT press,
2022.
Daniel J Rosenkrantz, Richard E Stearns, and Philip M Lewis, II. An analysis of several heuristics for the traveling
salesman problem. SIAM journal on computing, 6(3):563–581, 1977.
David L Applegate, Robert E Bixby, Vašek Chvátal, and William J Cook. The traveling salesman problem: a
computational study. In The Traveling Salesman Problem. Princeton university press, 2011.
Maram Assi and Ramzi A Haraty. A survey of the knapsack problem. In 2018 International Arab Conference on
Information Technology (ACIT), pages 1–6. IEEE, 2018.
Jordan S Ellenberg and Dion Gijswijt. On large subsets of with no three-term arithmetic progression. Annals of
Mathematics, pages 339–343, 2017.
Edward G Coffman Jr, János Csirik, Gábor Galambos, Silvano Martello, and Daniele Vigo. Bin packing approximation
algorithms: survey and classification. In Handbook of combinatorial optimization, pages 455–531. Springer, 2013.
Chun-Wei Tsai and Ming-Chao Chiang. Handbook of metaheuristic algorithms: from fundamental theories to advanced
applications. Elsevier, 2023.
Christos Voudouris, Abdullah Alsheddy, and Ahmad Alhindi. Guided local search. In Handbook of Heuristics, pages
427–467. Springer, 2025.
Florian Arnold and Kenneth Sörensen. Knowledge-guided local search for the vehicle routing problem. Computers &
Operations Research, 105:32–46, 2019.
Helena Ramalhinho Lourenço, Olivier C Martin, and Thomas Stützle. Iterated local search: Framework and applications.
In Handbook of metaheuristics, pages 129–168. Springer, 2018.
Kenan Karagul, Erdal Aydemir, and Sezai Tokat. Using 2-opt based evolution strategy for travelling salesman problem.
An International Journal of Optimization and Control: Theories & Applications (IJOCTA), 6(2):103–113, 2016.
Marco Dorigo and Thomas Stützle. Ant colony optimization: overview and recent advances. Handbook of metaheuristics,
pages 311–351, 2018.
Marco Dorigo and Krzysztof Socha. An introduction to ant colony optimization. In Handbook of approximation
algorithms and metaheuristics, pages 395–408. Chapman and Hall/CRC, 2018.
14


--- Page 15 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Paolo Toth and Daniele Vigo. Vehicle routing: problems, methods, and applications. SIAM, 2014.
Aldy Gunawan, Hoong Chuin Lau, and Pieter Vansteenwegen. Orienteering problem: A survey of recent variants,
solution approaches and applications. European Journal of Operational Research, 255(2):315–332, 2016.
John H Holland. Adaptation in natural and artificial systems: an introductory analysis with applications to biology,
control, and artificial intelligence. MIT press, 1992.
Agoston E Eiben and James E Smith. Introduction to evolutionary computing. Springer, 2015.
Nelishia Pillay and Rong Qu. Hyper-heuristics: theory and applications. Springer, 2018.
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as
policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.
Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University of Cambridge,
Computer Laboratory, 1990.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.
Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
learning. nature, 518(7540):529–533, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J Lim. Learning to synthesize programs as interpretable and
generalizable policies. Advances in neural information processing systems, 34:25146–25163, 2021.
15


--- Page 16 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
A
Related Work
A.1
LLM-Driven Automated Heuristic Design
The paradigm of Automated Heuristic Design (AHD) has fundamentally evolved from syntax-oriented Genetic
Programming (GP) to semantic-aware generation driven by Large Language Models (LLMs) [Sabar et al., 2014, Yu
and Liu, 2024]. This transition capitalizes on the ability of LLMs to act as intelligent variation operators, generating
executable code from natural language prompts rather than performing random symbolic mutations. Foundational
frameworks such as FunSearch [Romera-Paredes et al., 2024] and EoH [Liu et al., 2024a] established the efficacy of
this approach by integrating LLMs within evolutionary loops to iteratively refine algorithms for combinatorial problems.
Building on this, ReEvo [Ye et al., 2024] introduced reflective mechanisms, utilizing short-term and long-term memory
to guide the heuristic search process explicitly.
To overcome the limitations of standard evolutionary search, recent research has diversified into more sophisticated
exploration strategies. MCTS-AHD [Zheng et al., 2025] adapts Monte Carlo Tree Search to the space of programs,
enabling a more comprehensive traversal of algorithmic possibilities compared to greedy or local search methods.
Parallel efforts have focused on the critical challenge of population diversity. PartEvo [Hu and Zhang, 2025] introduces
feature-assisted partitioning to maintain distinct niches of solutions, thereby managing the exploration-exploitation
trade-off. Similarly, HSEvo [Dat et al., 2025] incorporates entropy-based diversity metrics within a Harmony Search
framework to prevent premature convergence toward homogeneous code structures.
Beyond single-objective optimization, the field has expanded toward robustness and dynamic adaptation. Acknowl-
edging that a single heuristic rarely generalizes across all problem distributions, EoH-S [Liu et al., 2025] shifts the
design objective from optimizing individual algorithms to evolving complementary heuristic sets. Furthermore, moving
beyond fixed LLM generators, recent methodologies have integrated learning signals directly into the generation process.
Methods such as OPRO [Yang et al., 2023] leverage optimization trajectories as meta-prompts, while LLaMoCo [Ma
et al., 2024] and EvoTune [Surina et al., 2025] employ instruction tuning and Reinforcement Learning (RL) to fine-tune
the backbone model’s internal weights for code generation tasks. NeRM [Guo et al., 2025] further extends this by
coupling generation with a predictor-assisted evaluation model in a nested co-evolutionary cycle.
A.2
Island Models in Evolutionary Computation
The Island Model represents a canonical paradigm in parallel evolutionary computation designed to mitigate premature
convergence by decomposing the global population into semi-isolated subpopulations [Whitley et al., 1999, Skolicki,
2005]. This spatial structuring maintains genotypic diversity by restricting interaction to periodic migration events,
thereby allowing distinct evolutionary trajectories to develop concurrently [Ruci´nski et al., 2010]. Foundational research
has extensively characterized the impact of communication topologies, including ring [Kushida et al., 2013, Abdelhafez
et al., 2019, Wu et al., 2019, Fu et al., 2020] and torus configurations [Awadallah et al., 2020, Duarte et al., 2021], on
the propagation of genetic material [Wu et al., 2019]. These studies establish that the connectivity structure critically
regulates the trade-off between the diffusion speed of superior traits and the preservation of local diversity. While
static topologies have been widely adopted, the field has also advanced towards dynamic topology adaptation where
connectivity evolves in response to search stagnation or population states [Duarte et al., 2017]. In the emerging domain
of Language Hyper-Heuristics, frameworks like FunSearch [Romera-Paredes et al., 2024] have integrated multi-island
architectures to scale candidate generation [Novikov et al., 2025, Lee et al., 2025]. However, current implementations in
this specific domain predominantly rely on static isolation or randomized migration, often bypassing the sophisticated
adaptive mechanisms developed in the broader evolutionary computation literature.
A.3
Code Representation and Similarity
The quantification of code similarity [Ragkhitwetsagul et al., 2018, Novak et al., 2019] is a foundational challenge
in Software Engineering and AHD, critical for tasks ranging from clone detection [Ain et al., 2019] to evolutionary
diversity management. Methodologies in this domain have historically evolved through three distinct paradigms. Lexical
metrics, such as Levenshtein distance [Zhang et al., 2017] and Jaccard similarity [Xu and Lu, 2018, Christiani et al.,
2018], operate on surface-level text or token sequences [Majumder et al., 2016]. While computationally efficient, these
metrics are notoriously sensitive to non-functional variations like formatting style and variable naming. With the advent
of large-scale representation learning, semantic approaches employing pre-trained models (e.g., CodeBERT [Feng,
2020], GraphCodeBERT [Guo et al., 2020], CrystalBLEU [Eghbali and Pradel, 2022]) have gained prominence. These
methods map code snippets into dense vector spaces to capture functional intent, a technique recently adapted in
heuristic generation frameworks like PartEvo [Hu and Zhang, 2025] for niche construction. Parallel to these, structural
metrics leverage static analysis representations, particularly Abstract Syntax Trees (AST) [Neamtiu et al., 2005] and
16


--- Page 17 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Control Flow Graphs (CFG) [Sendjaja et al., 2021]. Measures such as Tree Edit Distance (TED) [Sidorov et al., 2015,
Pawlik and Augsten, 2015, Schwarz et al., 2017] quantify topological discrepancies by calculating the minimum edit
operations between tree structures [Kartal et al., 2024]. Unlike probabilistic embeddings, structural metrics provide a
deterministic basis for identifying algorithmic isomorphism and logical divergence, independent of syntactic sugar.
A.4
Adaptive Operator Selection Strategies
In evolutionary computation, the performance of variation operators exhibits intrinsic non-stationarity, fluctuating
significantly across different stages of the search process. Adaptive Operator Selection (AOS) [Fialho, 2010, Pei
et al., 2025] has thus emerged as a critical methodology to dynamically allocate computational resources by learning
from historical performance. Early research focused on empirical heuristics, such as Probability Matching (PM)
and Adaptive Pursuit (AP), which adjust selection probabilities proportional to recent rewards. To provide rigorous
theoretical guarantees for the exploration-exploitation trade-off, the field subsequently adopted the Multi-Armed Bandit
(MAB) formulation [Lai and Robbins, 1985, Garivier and Moulines, 2008, Li et al., 2010]. Algorithms like the Upper
Confidence Bound (UCB) [Auer et al., 2002] provide deterministic bounds on regret, ensuring optimal asymptotic
behavior [DaCosta et al., 2008, Fialho et al., 2010, Modi et al., 2017]. These mechanisms have become foundational not
only in operator scheduling for discrete optimization but also in tree-search algorithms [Pei et al., 2025], exemplified
by the Upper Confidence bounds for Trees (UCT) used in Monte Carlo Tree Search (MCTS) [Zheng et al., 2025]. In
the emerging domain of LLM-based optimization, where inference incurs substantial cost, recent studies have begun
investigating such adaptive mechanisms [Zhang and Yi, 2025] to navigate the expansive space of prompting strategies
efficiently.
A.5
Memetic Algorithms and Hybridization
Heuristic algorithms are inherently dualistic, comprising a discrete symbolic structure for logic flow and continuous
numerical parameters for coefficients. Historically, the Memetic Algorithm (MA) framework addressed this duality
by hybridizing global evolutionary search (for structural exploration) with intensive local search (for individual
refinement) [Moscato et al., 1989, Neri et al., 2011]. This separation of concerns is also evident in the field of Automated
Algorithm Configuration (AAC), where specialized tools like Irace [López-Ibáñez et al., 2016] and SMAC [Hutter et al.,
2011] were developed specifically to tune continuous parameters for fixed algorithmic skeletons. Recent advancements
in Neural Combinatorial Optimization (NCO) have similarly adopted hybrid strategies, integrating neural policy
learning with gradient-free local search to enhance solution quality [Ma et al., 2019]. In the emerging domain of
LLM-based algorithm design, while LLMs demonstrate strong symbolic reasoning capabilities, recent studies have
begun to characterize their limitations in precise numerical regression [Feng et al., 2024]. Consequently, exploring
hybrid architectures that synergize language-based logic generation with specialized numerical optimizers represents a
growing frontier in the literature.
B
Task and Framework Details.
B.1
Problem Formulations
We evaluate our method on a diverse set of representative NP-hard combinatorial optimization problems and control
tasks. We formally define the objective functions and constraints for each problem below.
Traveling Salesman Problem (TSP)
Given a complete graph G = (V, E) with node set V = {1, . . . , n} representing
n cities, and a cost matrix where cij ≥0 denotes the travel cost between city i and city j, the Traveling Salesman
Problem (TSP) aims to find a minimum-cost Hamiltonian cycle, i.e., a tour that visits every city exactly once before
returning to the origin [Lawler, 1985]. We define binary variables xij ∈{0, 1} to indicate whether the edge from city i
17


--- Page 18 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
to city j is included in the tour. The TSP can be formulated as the following integer program [Hoffman et al., 2013]:
min
X
i∈V
X
j∈V
cijxij
(9)
s.t.
X
j∈V
xij = 1,
∀i ∈V
(10)
X
i∈V
xij = 1,
∀j ∈V
(11)
X
i∈S
X
j∈S
xij ≤|S| −1,
∀S ⊂V, 2 ≤|S| ≤n −1
(12)
xij ∈{0, 1},
∀i, j ∈V
(13)
Constraints (10) and (11) enforce that each city has exactly one outgoing and one incoming edge, respectively.
Constraint (12) eliminates subtours by ensuring that no proper subset of cities forms a disconnected cycle. The TSP is
known to be NP-hard, making it computationally intractable to find optimal solutions for large-scale instances.
Capacitated Vehicle Routing Problem (CVRP)
CVRP generalizes TSP to multiple routes serving a set of customers
using a homogeneous fleet with capacity Q. Let V = {0} ∪Vc be the node set, where 0 represents the depot and
Vc = {1, . . . , N} represents N customer nodes. Each customer i ∈Vc has a deterministic demand δi. A solution is
a set of routes R = {r1, . . . , rK}, where each route rk = (vk
0, vk
1, . . . , vk
mk, vk
mk+1) starts and ends at the depot (i.e.,
vk
0 = vk
mk+1 = 0) [Toth and Vigo, 2002]. The objective is to minimize the total travel distance subject to capacity
constraints:
min
R
K
X
k=1
mk
X
t=0
dvk
t ,vk
t+1
s.t.
mk
X
t=1
δvk
t ≤Q,
∀k ∈{1, . . . , K},
(14)
ensuring that every customer i ∈Vc is visited exactly once across all routes.
Orienteering Problem (OP)
The Orienteering Problem [Vansteenwegen et al., 2011] combines elements of Knapsack
and TSP. Given a set of nodes V = {0, 1, . . . , N} in a metric space, where node 0 is the depot. Each node i is associated
with a prize pi ≥0 (with p0 = 0). Let dij denote the travel cost between node i and j. The objective is to find a tour
τ = (v1, . . . , vk) starting and ending at the depot (v1 = vk = 0) that maximizes the total collected prize, subject to a
maximum travel budget Tmax:
max
τ
X
i∈set(τ)
pi
s.t.
k−1
X
t=1
dvt,vt+1 ≤Tmax,
(15)
where set(τ) denotes the set of unique nodes visited in the tour. Each node can be visited at most once.
0-1 Knapsack Problem (KP)
Consider a set of N items, where item i is associated with a value vi and a weight wi.
Given a knapsack with capacity W, the problem seeks a binary selection vector x ∈{0, 1}N to maximize the total
value without exceeding the weight limit [Zhou et al., 2023]:
max
x
N
X
i=1
vixi
s.t.
N
X
i=1
wixi ≤W,
xi ∈{0, 1}.
(16)
Multidimensional Knapsack Problem (MKP)
MKP extends KP by introducing M distinct resource con-
straints [Puchinger et al., 2010]. Each item j ∈{1, . . . , N} has a value vj and consumes wij units of the i-th
resource (i ∈{1, . . . , M}). Let Ci denote the capacity of the i-th dimension. The objective is to maximize total value
subject to all resource constraints:
max
x
N
X
j=1
vjxj
s.t.
N
X
j=1
wijxj ≤Ci,
∀i ∈{1, . . . , M},
(17)
where xj ∈{0, 1} is the binary decision variable for item j.
18


--- Page 19 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Bin Packing Problem (BPP)
Given a set of items I = {1, . . . , N} with sizes si ∈(0, C], the goal is to partition I
into a minimum number of disjoint subsets (bins) B1, . . . , BK such that the sum of sizes in each bin does not exceed
the capacity C [Munien and Ezugwu, 2021].
• Offline BPP: The solver has full access to the set of items I and their sizes a priori [Borgulya, 2021].
• Online BPP: Items arrive sequentially. At step t, item it must be irreversibly assigned to a bin before observing
it+1 [Seiden, 2002].
Mathematically, we minimize K subject to P
i∈Bk si ≤C for all k ∈{1, . . . , K} and S
k Bk = I.
Admissible Set Problem (ASP)
ASP is a combinatorial design problem that constructs a set of vectors A ⊂{0, 1, 2}n
with maximal cardinality [Barwise, 2017]. Each vector a ∈A must have a fixed Hamming weight ∥a∥0 = w.
Furthermore, for any distinct triplet a, b, c ∈A, there must exist a coordinate index k such that the values {ak, bk, ck}
form a valid configuration set Svalid = {{0, 0, 1}, {0, 0, 2}, {0, 1, 2}} (modulo permutation). The formulation is:
max
A |A|
s.t.
∀a ∈A, ∥a∥0 = w;
∀distinct a, b, c ∈A, ∃k : {ak, bk, ck} ∈Svalid.
(18)
Decap Placement Problem (DPP)
DPP addresses the optimization of Power Distribution Networks (PDNs) [Smith
et al., 1999]. It is formulated as a black-box combinatorial optimization problem on a 2D discrete grid G = {(i, j) |
1 ≤i, j ≤L}. A problem instance is defined by a context c = (p, K), where p ∈G is the probing port location and
K ⊂G denotes keep-out regions. The objective is to select a binary placement matrix x ∈{0, 1}L×L to minimize the
PDN impedance, subject to a maximum capacitor count Nmax:
min
x Φ(x; c)
s.t.
X
(i,j)∈G
xij ≤Nmax,
and
xij = 0, ∀(i, j) ∈K ∪{p},
(19)
where Φ(·) is a black-box oracle (e.g., SPICE simulation) evaluating the maximum impedance over a target frequency
range.
Mountain Car Control (MountainCar-v0)
MountainCar-v0 is a classic continuous control benchmark involving an
underpowered car driving up a steep hill. The state st = (xt, vt) ∈S represents position and velocity. The discrete
action space is A = {0, 1, 2}. The dynamics follow a deterministic transition rule st+1 = f(st, at). The objective is to
synthesize a heuristic policy π(·) that minimizes the trajectory length L required to reach the goal state xgoal ≥0.5:
min
π L
s.t.
xL ≥0.5,
st+1 = f(st, π(st)),
s0 ∈Sinit,
(20)
where Sinit is the distribution of initial states (typically x0 ∈[−0.6, −0.4], v0 = 0), and L is bounded by a maximum
horizon Tmax = 200. [Zheng et al., 2025]
B.2
General Optimization Frameworks
We evaluate our method by designing key heuristic functions within three established algorithmic frameworks: Construc-
tive Heuristics, Ant Colony Optimization (ACO), and Local Search variants. This demonstrates the framework-agnostic
capability of our approach. In each setting, the framework provides the high-level search logic (the skeleton), while our
method optimizes a specific parameterized function or heuristic component (the brain).
B.2.1
Constructive Heuristic
Constructive heuristics generate a solution by sequentially selecting discrete decision variables until a valid complete
solution is formed [Martí and Reinelt, 2010, Cormen et al., 2022]. Let St be the partial solution at step t, and At be the
set of available candidate actions (e.g., unvisited cities or unpacked items). A policy function h : At × St →R assigns
a priority score to each candidate. The algorithm deterministically selects the action with the highest score:
a∗= argmaxa∈At h(a, St).
(21)
We employ this framework for the following problems:
• TSP: The heuristic selects the next node to visit. The input state includes the current node location, the set of
unvisited nodes, and the distance matrix [Rosenkrantz et al., 1977, Applegate et al., 2011].
• KP: The heuristic ranks items to be added to the knapsack. Inputs include the value and weight of remaining
items and the residual capacity [Assi and Haraty, 2018].
19


--- Page 20 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
• ASP: The heuristic evaluates an n-dimensional vector’s suitability for inclusion in the admissible set A,
ensuring constraint satisfaction [Ellenberg and Gijswijt, 2017, Romera-Paredes et al., 2024].
• Online BPP: Upon the arrival of a new item, the heuristic assigns a preference score to each existing bin (or a
new bin) based on the item’s size and the bins’ residual capacities [Coffman Jr et al., 2013].
B.2.2
Improvement Heuristic
Local search algorithms iteratively improve a candidate solution by exploring its neighborhood. To overcome the
limitation of getting trapped in local optima, we employ meta-heuristic frameworks that incorporate guidance mecha-
nisms [Peres and Castelli, 2021, Tsai and Chiang, 2023]. We apply our method to automate the core heuristic functions
within two distinct frameworks: Guided Local Search (GLS) and Knowledge-Guided Local Search (KGLS).
Guided Local Search
Standard GLS [Voudouris et al., 2025] escapes local optima by dynamically augmenting the
objective function with penalty terms associated with solution features (e.g., edges in a tour). Let f(s) be the original
cost of a solution s. GLS minimizes an augmented cost function g(s):
g(s) = f(s) + λ ·
X
(i,j)∈E
pij · Iij(s),
(22)
where E is the set of all edges, pij represents the accumulated penalty count for edge (i, j), and Iij(s) is an indicator
function that equals 1 if edge (i, j) is present in solution s, and 0 otherwise. λ is a regularization parameter.
The core heuristic logic lies in determining which feature to penalize when the search is trapped in a local optimum
s∗. This is governed by a utility function µ(i, j). Standard GLS defines this utility as µ(i, j) = dij/(1 + pij). In our
framework, we task the LLM to evolve a more effective utility function ΨGLS:
µ(i, j) = ΨGLS(dij, pij, . . . ).
(23)
The edge with the maximum utility in s∗is penalized (pij ←pij + 1), altering the landscape to guide the solver away
from the current basin of attraction.
Knowledge-Guided Local Search
We also evaluate the Knowledge-Guided Local Search framework [Arnold and
Sörensen, 2019], which operates as a specialized Iterated Local Search (ILS) [Lourenço et al., 2018]. Unlike GLS,
KGLS does not modify the objective function during the local descent. Instead, it utilizes a pre-computed guidance
matrix to bias the perturbation phase.
The algorithm alternates between two stages:
1. Local Descent: A standard descent algorithm (e.g., 2-opt [Karagul et al., 2016], Relocate) minimizes the
original cost f(s) based on the distance matrix D until a local optimum is reached.
2. Guided Perturbation: To escape the local optimum, the solution is perturbed. This perturbation is biased by
a guidance matrix (or heatmap) Ω∈RN×N.
Our method automates the design of the mapping function ΨKGLS that constructs this matrix from the raw problem
instance I (e.g., coordinates and distances):
Ω= ΨKGLS(I).
(24)
The matrix Ωencodes the global desirability of edges. During perturbation, edges with higher values in Ωare
preferentially inserted, while edges with lower values are removed, guiding the solver toward globally promising regions
of the search space.
B.2.3
Ant Colony Optimization
ACO is a meta-heuristic that constructs solutions probabilistically based on dynamic pheromone trails (τ) and static
heuristic information (η) [Dorigo and Stützle, 2018, Dorigo and Socha, 2018]. The probability of selecting a component
eij (e.g., an edge moving from node i to j) is typically proportional to τ α
ij · ηβ
ij. While τ evolves dynamically during
the search based on solution quality, the heuristic information η is traditionally a fixed matrix derived from problem
features (e.g., ηij = 1/dij in standard TSP).
Our method aims to automate the design of the function that generates this heuristic matrix η. We adopt the DeepACO
framework [Ye et al., 2023], where the learned function maps problem instances to an N × N matrix η. We apply this
framework to the following problem settings:
20


--- Page 21 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
• Routing Problems (TSP, CVRP, OP): For TSP and CVRP, the heuristic function maps spatial features
(distances) and node constraints (demands) to an edge desirability matrix [Applegate et al., 2011, Toth and
Vigo, 2014]. For the Orienteering Problem (OP), the heuristic explicitly incorporates node prizes to balance
the trade-off between collecting rewards and conserving the travel budget [Gunawan et al., 2016].
• Packing Problems (MKP, Offline BPP): The function maps item-bin compatibility features to a preference
matrix, prioritizing efficient packing configurations to minimize resource usage [Coffman Jr et al., 2013].
B.2.4
Genetic Algorithm
Genetic Algorithms (GA) are population-based meta-heuristics that evolve a set of candidate solutions over multiple
generations, inspired by natural selection [Holland, 1992]. Unlike the constructive or local search policies described
above, which operate on a single solution trajectory, GAs maintain a population Pt = {x1, . . . , xP } at generation t.
In the context of the Decap Placement Problem (DPP) [Ye et al., 2024], each xi represents a discrete set of placement
locations. The evolutionary cycle typically involves three stages [Eiben and Smith, 2015]:
1. Selection: A subset of high-performing individuals (elites) is preserved based on the objective function value
(e.g., power integrity reward). Parents are selected from Pt to produce offspring.
2. Variation (Crossover & Mutation): New solutions are generated by recombining information from parents
and introducing stochastic perturbations.
3. Repair/Validation: Domain-specific constraints (e.g., keep-out regions, non-overlapping indices) are enforced
to ensure solution feasibility.
While the high-level GA loop (initialization, selection, replacement) is fixed, our method automates the design of the
core variation operators, aligning with the principles of generative hyper-heuristics [Pillay and Qu, 2018]. We define
two heuristic functions to be evolved:
• Crossover Operator (Ψcross): A function that takes a set of parent solutions and produces offspring by
combining their substructures.
xchild = Ψcross(xparent1, xparent2)
• Mutation Operator (Ψmut): A function that introduces diversity by modifying an individual with a specific
probability.
xnew = Ψmut(xchild)
Our method searches for the optimal logic within Ψcross and Ψmut that balances exploration (diversity) and exploitation
(preserving good placement patterns) specifically for the EDA landscape.
B.2.5
Reinforcement Learning
Reinforcement Learning (RL) addresses sequential decision-making problems where an agent interacts with a dynamic
environment to maximize cumulative rewards [Barto, 2021]. Unlike the static combinatorial problems discussed
previously, RL tasks involve continuous state spaces and long-term temporal dependencies. In this framework, the goal
of Automated Heuristic Design is to synthesize a Policy Function π : S →A, which maps the current state st directly
to an optimal action at [Liang et al., 2022].
We evaluate this framework on the classic control task MountainCar-v0, originally formulated by [Moore, 1990] and
standardized in the OpenAI Gym suite [Brockman et al., 2016]. The problem is characterized by an underpowered
vehicle positioned between two mountains. The engine lacks sufficient power to climb the slope directly in a single
pass. Consequently, the optimal policy requires a momentum-based strategy, where the agent must oscillate back and
forth to build up sufficient potential energy to reach the target on the right hilltop.
In our experiments, the LLM is tasked with designing an explicit, interpretable heuristic function choose_action to
serve as the agent’s policy.
• State Space: The input is a continuous vector s = (x, ˙x), representing the car’s horizontal position and current
velocity.
• Action Space: The output is a discrete action a ∈{0, 1, 2}, corresponding to pushing left, applying no force,
or pushing right.
21


--- Page 22 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
• Design Objective: The generated heuristic must implement logical rules (e.g., threshold-based decisions or
phase-space analysis) to determine the action that minimizes the number of steps required to reach the goal
state (x ≥0.5).
Unlike Deep Reinforcement Learning methods that encode policies in opaque neural networks [Mnih et al., 2015,
Schulman et al., 2017], this approach aims to discover symbolic control laws that are both effective and human-
readable [Trivedi et al., 2021].
C
Experimental Configuration
C.1
Hyperparameter Settings
To ensure reproducibility, we detail the other specific hyperparameter configurations used in our main experiments.
Table 4 summarizes the default settings.
Table 4: Default Hyperparameter Configuration for TIDE.
Hyperparameter
Symbol
Value
Outer Loop (Island Model)
Island Count
Nisland
6
Population Size per Island
Npop
8
TSED Diversity Threshold
τ
0.7
Stagnation Tolerance Iteration (Reset)
Istag
8
Migration Cooldown Interval
Icool
2
Inner Loop (Co-Evolution)
UCB Exploration Constant
C
√
2
Parameter Tuning Candidates
Ntune
3
LLM Generation
Sampling Temperature
TLLM
1.0
where Nisland = 6 and Npop = 8 define the global topology, selected to maximize parallel evolutionary trajectories
while constraining the per-generation token cost. Regarding migration dynamics, consistent with the asynchronous
triggering mechanism described in Section 3.1.2, TIDE does not employ a fixed migration frequency. Instead, the
operational cycle is regulated by Icool = 2, which establishes a minimum temporal bound between consecutive events.
This mandatory cooldown prevents oscillatory behavior and ensures local populations have sufficient generations to
stabilize and assimilate new elite code or insights before further migration can occur.
τ = 0.7 serves as the critical TSED threshold governing Dual-Mode Migration: pairs with similarity STSED > τ
execute Code Transfer for local exploitation, while those with STSED ≤τ trigger Insight Transfer for global exploration.
Istag = 8 sets the tolerance for the Selective Reset mechanism to resolve deep stagnation. C =
√
2 is adopted from
standard UCB1 [Auer et al., 2002] literature to balance operator exploration with exploitation. Ntune = 3 is set as
the minimal viable batch size for the parameter refinement module; this value corresponds to the theoretical lower
bound required to construct a valid difference vector for mutation (i.e., one target vector plus two difference vectors),
ensuring gradient-free tuning with minimal computational overhead. Finally, TLLM = 1.0 is maintained to encourage
stochastic diversity in structural reasoning, deviating from the lower temperatures typically used for precision-oriented
code generation.
C.2
Parameter Sensitivity Analysis
We conduct a parameter sensitivity analysis to study the impact of key hyperparameters in our framework, including the
exploration coefficient c in UCB, the island configuration, and the diversity threshold. The analysis is performed on two
representative tasks: online BPP and constructive KP, covering different problem scales.
As shown in Table 5, we systematically evaluate three critical hyperparameters to validate the framework’s robustness.
First, the UCB exploration constant c determines the weight of the uncertainty term in the bandit selection policy,
directly controlling the trade-off between exploring under-sampled prompt strategies and exploiting high-yield ones.
The results indicate that deviating from the theoretical standard of
√
2 degrades search efficiency: a lower value
22


--- Page 23 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
(c = 0.5) causes the scheduler to greedily lock onto local optima, while a higher value (c = 2.0) results in inefficient
random search behavior.
Second, the island topology configuration defines the granularity of the global population structure. We observe that
extreme fragmentation (12 × 4) is detrimental because the insufficient genetic material within small local populations
leads to rapid genetic drift, rendering LLM-based crossover operators ineffective. Conversely, a coarse-grained topology
(3 × 16) limits the global search breadth, covering fewer distinct basins of attraction.
Third, the diversity threshold τ acts as the decision boundary for the dual-mode migration strategy. Setting τ too
low (0.5) triggers excessive code overwriting that destroys global structural diversity, whereas setting it too high (0.9)
hinders the necessary exploitation of elite solutions.
Ultimately, these experiments demonstrate that TIDE maintains robust performance across reasonable parameter ranges,
confirming that the default configuration effectively captures the intended architectural balance without requiring
intensive user-side tuning.
C.3
Details of Evaluation Datasets
For all LLM-based AHD methods considered in our experiments, we use the same training and test datasets to ensure a
fair and controlled comparison. All methods are trained and evaluated on identical instance sets, eliminating potential
performance differences caused by data variations rather than heuristic design. The detailed numbers of instances in the
training and test sets for each problem are summarized in Table 6.
D
More Experimental Results
D.1
Results on TSPLib
TSPLIB is a widely used benchmark suite for the Traveling Salesman Problem, comprising instances with diverse sizes
and heterogeneous spatial distributions, including random, clustered, and structured layouts derived from real-world
scenarios.
To further demonstrate the robustness and generalization capability of the heuristics designed by our method, we
conduct additional experiments on TSPLIB instances. Compared to synthetically generated data, TSPLIB provides a
more challenging and diverse testbed, enabling a comprehensive evaluation of heuristic performance across varying
instance scales and distribution patterns. These experiments allow us to assess whether the proposed heuristics can
consistently deliver improvements under different structural characteristics of problem instances.
We conduct experiments on TSPLIB under the constructive framework, with results reported in Table 7. Our method
outperforms existing LLM-based AHD approaches on most instances, indicating its effectiveness in constructing
high-quality solutions across diverse problem settings.
Table 5: Parameter Sensitivity Analysis on Online BPP and Constructive KP. This table reports the performance
under varying hyperparameter settings. For the island configuration, values are reported in the form of number of
islands×population size per island. The default configuration is marked in the table. Best results under each setting are
highlighted in bold.
Parameters
BPP online
KP constructive
1k
5k
10k
50
100
200
C in UCB
0.5
419.8
2063.5
4084.1
20.00
40.23
57.37
√
2 (default)
415.8
2034.2
4025.1
20.03
40.26
57.44
2
415.2
2047.3
4052.1
20.01
40.25
57.42
Nisland × Npop
3×16
417.2
2036.9
4032.8
20.00
40.24
57.41
6×8 (default)
415.8
2034.2
4025.1
20.03
40.26
57.44
12×4
415.2
2037.7
4034.8
20.02
40.25
57.42
TSED diversity threshold (τ)
0.5
413.5
2045.4
4050.7
20.02
40.25
57.42
0.7 (default)
415.8
2034.2
4025.1
20.03
40.26
57.44
0.9
418.7
2084.4
4129.9
20.00
40.23
57.40
23


--- Page 24 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Table 6: Summary of training and test datasets used across different problems and algorithmic frameworks. For
each entry, the number of instances is reported, followed by key problem parameters in parentheses (e.g., number of
nodes/items, capacity constraints).
Framework
Constructive Heuristic
Problems
TSP
KP
BPP online (WeiBull)
ASP
Training Datasets
64 (50 nodes)
64 (100 items, W=25)
1 (1k items, W=100)
1 (1k items, W=500)
1 (5k items, W=100)
1 (5k items, W=500)
1 (n=15, w=10)
Test Datasets
1000 (50 nodes)
128 (100 nodes)
64 (200 nodes)
1000 (50 items, W=12.5)
1000 (100 items, W=25)
1000 (200 items, W=25)
5 (1k items, W=100)
5 (1k items, W=500)
5 (5k items, W=100)
5 (5k items, W=500)
5 (10k items, W=100)
5 (10k items, W=500)
1 (n=12, w=7)
1 (n=15, w=10)
1 (n=21, w=15)
Framework
Improvement Heuristic
ACO
Problems
TSP GLS
TSP KGLS
TSP
CVRP
Training Datasets
64 (100 nodes)
10 (200 nodes)
5 (50 nodes)
10 (50 nodes, C=50)
Test Datasets
1000 (100 nodes)
128 (200 nodes)
128 (500 nodes)
1000 (100 nodes)
1000 (200 nodes)
64 (500 nodes)
64 (50 nodes)
64 (100 nodes)
64 (50 nodes, C=50)
64 (100 nodes, C=50)
Framework
ACO
GA
Problems
OP
MKP
BPP offline
DPP
Training Datasets
5 (50 nodes)
5 (100 items, m=5)
5 (500 items, W=150)
2
Test Datasets
64 (50 nodes)
64 (100 nodes)
64 (100 items, m=5)
64 (200 items, m=5)
64 (500 items, W=150)
64 (1000 items, W=150)
64
D.2
Results on Weibull BPP Instances
In addition to the results on online BPP presented in the main paper, we further evaluate our method on broader
Weibull-distributed BPP instances. Compared with uniformly distributed item sizes, Weibull distributions more closely
resemble real-world packing scenarios, and these instances are generated following the standard protocols adopted in
prior LLM-based AHD studies.
We construct a richer benchmark by considering a wider range of bin capacities and problem scales to comprehensively
assess the robustness of the proposed heuristics. This design introduces greater diversity and difficulty into the evaluation
setting, enabling a more thorough examination of performance across heterogeneous conditions.
As reported in Table 8, our method consistently achieves lower average optimality gaps with respect to the optimal
solutions across all evaluated datasets when compared to other LLM-based AHD methods. Notably, the heuristics
produced by our approach also outperform traditional expert-designed heuristics, demonstrating the effectiveness of
LLM-guided heuristic discovery in capturing complex packing patterns beyond manually crafted rules.
D.3
Results with Different LLMs
We further investigate the robustness of our method with respect to different large language model backbones on the
Knapsack Problem (KP) under the constructive framework. Specifically, we conduct controlled experiments using
multiple representative LLMs while keeping all other components and hyperparameters fixed to ensure a fair comparison.
All results are averaged over three independent runs to mitigate the impact of stochasticity. The complete results are
summarized in Table 9.
Overall, the results indicate that AHD methods exhibit consistently stable performance across a wide range of LLM
backbones, demonstrating strong model-level generalization.
24


--- Page 25 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Table 7: Results on various TSPLib instances. The best results are highlighted in bold.
name
EoH
ReEvo
HSEvo
MCTS-AHD
TIDE
eil51
14.07%
8.76%
9.25%
13.36%
5.16%
st70
13.19%
12.77%
10.48%
9.81%
6.44%
eil76
15.58%
10.52%
9.82%
11.33%
9.78%
pr76
17.71%
14.25%
11.46%
16.61%
9.76%
rat99
18.30%
13.37%
12.77%
10.49%
8.01%
kroA100
15.20%
11.79%
10.12%
11.38%
7.05%
kroB100
16.33%
10.70%
12.05%
10.50%
5.89%
kroC100
14.47%
12.64%
12.36%
8.49%
7.97%
kroD100
19.67%
11.08%
15.38%
12.15%
9.39%
kroE100
18.88%
12.34%
13.00%
8.59%
6.59%
rd100
17.40%
12.63%
14.00%
9.88%
9.27%
eil101
20.96%
11.77%
13.12%
14.60%
9.69%
pr107
4.41%
6.14%
6.58%
2.49%
1.61%
pr124
14.53%
15.93%
13.27%
5.50%
5.11%
pr144
6.58%
7.46%
9.18%
3.76%
3.65%
ch150
11.28%
10.35%
10.35%
7.66%
6.71%
kroA150
17.69%
12.00%
13.36%
10.71%
10.30%
kroB150
16.91%
11.95%
11.05%
10.84%
8.46%
pr152
12.01%
12.50%
11.11%
6.19%
8.20%
u159
18.41%
10.46%
13.03%
15.03%
11.96%
rat195
15.56%
8.07%
9.64%
8.02%
8.43%
kroA200
20.41%
12.88%
12.80%
11.30%
8.04%
kroB200
18.14%
14.61%
15.16%
13.03%
12.93%
ts225
15.03%
7.89%
7.08%
11.83%
11.67%
tsp225
18.33%
11.55%
12.62%
13.12%
9.05%
pr226
14.73%
14.38%
15.49%
9.78%
8.90%
lin318
19.35%
14.13%
15.04%
14.44%
13.22%
rd400
17.65%
12.88%
14.36%
12.42%
14.07%
fl417
17.96%
19.58%
18.15%
13.35%
11.98%
p654
24.19%
17.56%
19.26%
17.10%
15.71%
avg.
16.16%
12.10%
12.38%
10.79%
8.83%
Table 8: Results on Online BPP in Weibull instances with various capacities and problem sizes. Results marked with *
denote values taken from MCTS-AHD Zheng et al. [2025].
Capacity
Size
First Fit*
Best Fit*
EoH
ReEvo
HSEvo
MCTS-AHD
TIDE(ours)
100
1k
4.77%
5.02%
3.93%
4.01%
4.74%
3.76%
3.33%
5k
4.31%
4.65%
1.20%
3.40%
4.22%
2.49%
0.73%
10k
4.05%
4.36%
0.63%
3.22%
4.03%
2.13%
0.36%
500
1k
0.25%
0.25%
0.41%
0.17%
0.25%
0.41%
0.33%
5k
0.55%
0.55%
0.48%
0.46%
0.55%
0.41%
0.51%
10k
0.47%
0.50%
0.46%
0.41%
0.47%
0.37%
0.49%
avg.
2.40%
2.56%
1.18%
1.95%
2.38%
1.60%
0.96%
Table 9: Performance comparison of our method with different LLMs backbones on KP under constructive framework.
Scale
50
100
200
500
DeepSeek-v3
20.013
40.250
57.421
90.939
DeepSeekr1
20.003
40.238
57.405
90.918
GLM-4.7
19.998
40.236
57.405
90.924
Qwen3-30b
19.996
40.236
57.399
90.918
Qwen3-max
20.029
40.264
57.437
90.954
25


--- Page 26 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
D.4
Results with Complex Optimization Tasks
Owing to the generality of large language models, our approach can automatically design heuristics for more complex
problems and algorithmic frameworks, substantially reducing the reliance on expert knowledge and manual effort. In
this section, we provide more detailed experimental results for the DPP under the GA framework and for Mountain Car
under RL framework.
DPP under GA Framework
We evaluate the DPP under the GA framework, which is an iterative evolutionary
optimization approach that progressively refines solutions through selection, crossover, and mutation operations. This
framework is particularly suitable for complex optimization tasks (e.g., DPP) due to its ability to explore a large and
complex search space and to continuously improve solution quality over successive generations.
To provide clearer insights into the optimization behavior during the evolutionary process, we explicitly report the
intermediate performance of LLM-based AHD methods. Specifically, Table 10 presents the results of ReEvo and TIDE,
each executed for three independent runs with a total of 10 evolutionary generations. This evaluation protocol allows
us to better observe how solution quality evolves over time and facilitates a transparent comparison of evolutionary
dynamics across different methods.
Table 10: Evolutionary Performance of ReEvo and TIDE on DPP under GA framework. Results are reported over
three runs with 10 evolutionary generations. Generation 0 denotes the initial population before evolution. We show the
performance at each generation and the average across runs. Higher values indicate better solution quality. The best
average performance is highlighted in bold.
Generation
0
1
2
3
4
5
6
7
8
9
10
ReEvo
run1
9.92
12.16
12.40
12.52
12.60
12.63
12.67
12.69
12.70
12.73
12.75
run2
9.93
12.20
12.38
12.52
12.60
12.64
12.68
12.70
12.72
12.73
12.74
run3
9.94
12.16
12.38
12.56
12.63
12.67
12.70
12.71
12.73
12.73
12.74
avg.
9.93
12.17
12.39
12.53
12.61
12.65
12.68
12.70
12.72
12.73
12.74
TIDE
run1
9.94
12.24
12.44
12.59
12.77
12.82
12.85
12.90
12.97
13.00
13.01
run2
9.90
12.11
12.32
12.52
12.58
12.64
12.67
12.70
12.72
12.73
12.75
run3
9.93
12.15
12.34
12.46
12.51
12.55
12.61
12.62
12.63
12.65
12.65
avg.
9.92
12.16
12.37
12.53
12.62
12.67
12.71
12.74
12.78
12.79
12.80
Mountain Car under RL Framework
Mountain Car-v0 is a classic control task in the reinforcement learning setting,
where an underpowered car must learn a policy to drive up a steep hill by leveraging momentum through oscillatory
movements. The objective is to reach the goal position with as few steps as possible, making the task a standard
benchmark for evaluating policy optimization methods.
In this setting, we evaluate our method under the reinforcement learning framework, and Table 11 reports a detailed
comparison with existing LLM-based AHD methods, highlighting the effectiveness of the heuristics designed by our
approach in policy optimization tasks.
Table 11: Performance Comparison of LLM-based AHD methods on Mountain Car-v0 under RL framework. Results
are averaged over three runs. We report the number of steps required to reach the goal in Mountain Car-v0 for each run
and their average. Lower values indicate better policy performance. The best average result is highlighted in bold.
Methods
run1
run2
run3
avg.
EoH
118
92
100
103.3
ReEvo
85
117
117
106.3
HSEvo
114
117
119
116.7
MCTS-AHD
124
103
90
105.7
TIDE
85
109
101
98.3
26


--- Page 27 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
D.5
Comparative Results
In this section, we conduct a comprehensive analysis to validate the internal mechanisms of our framework, specifically
focusing on optimization efficiency, evolutionary topology, and adaptive strategy selection. Unless otherwise stated,
all analyses presented below are based on the training results of the TSP-Constructive with problem size 50. The
experimental configurations and hyperparameter settings are strictly aligned with those used in the main experiments.
Synergy Analysis: Structural Search vs. Parameter Tuning
The optimization process visualized in Figure 4
reveals three critical insights regarding the internal dynamics of our framework. First, the trajectory highlights the
cost-efficiency of the parameter tuning module. The blue dots represent the structural search performed by the LLM,
which incurs a substantial token cost. In contrast, the parameter tuning module optimizes numerical constants within
these structures without consuming tokens. Consequently, the distinct vertical drops (black lines) demonstrate that
the tuning module contributes significant performance gains at zero marginal token cost. Second, the plot evidences
a synergistic co-evolution where structural search and parameter tuning mutually reinforce each other. The tuned
solutions (red stars) often surpass the subsequent raw LLM-generated candidates, indicating that parameter refinement
is essential for realizing the full potential of a code structure. This collaboration leads to substantial performance leaps
which would be inefficient to achieve via structural sampling alone. Third, the overall trajectory exhibits a characteristic
staircase pattern. This morphological feature aligns with our algorithmic design, where the LLM conducts exploration in
the discrete code space (the horizontal or gentle slopes) while the tuning module performs exploitation in the continuous
parameter space (the vertical drops). This pattern confirms that our framework effectively balances global structural
exploration with local numerical exploitation.
0k
200k
400k
600k
800k
Accumulated Token Usage
5.85
5.90
5.95
6.00
6.05
6.10
6.15
6.20
6.25
Global Best Objective (Min)
Optimization Efficiency: Structural Search (Costly) vs. Parameter Tuning (Free)
LLM Strategies (Structure)
Parameter Tuning (Numerical)
25k
50k
75k
100k
125k
150k
175k
200k
5.90
5.95
6.00
6.05
6.10
Zoom-in: Early Phase Efficiency
Figure 4: Optimization Efficiency Analysis on TSP-50.
Evolutionary Topology and Structural Diversity Analysis.
Figure 5 visualizes the structural dynamics of the
population through three evolutionary phases.
First, during the Early Exploration phase (Panel a), similarity scores remain uniformly low (avg ≈0.62). Since these
values fall below the structural threshold (0.7), the system predominantly triggers Insight Transfer. This mechanism
allows islands to share high-level design rationales derived from best-worst comparisons without overwriting the
27


--- Page 28 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
I0
I1
I2
I3
I4
I5
Target Island
I0
I1
I2
I3
I4
I5
Source Island
0.62
0.59
0.62
0.57
0.66
0.62
0.70
0.74
0.66
0.63
0.59
0.70
0.67
0.64
0.61
0.62
0.74
0.67
0.64
0.62
0.57
0.66
0.64
0.64
0.58
0.66
0.63
0.61
0.62
0.58
(a) Early Exploration
I0
I1
I2
I3
I4
I5
Target Island
0.62
0.73
0.71
0.67
0.70
0.62
0.66
0.75
0.72
0.74
0.73
0.66
0.81
0.77
0.80
0.71
0.75
0.81
0.82
0.86
0.67
0.72
0.77
0.82
0.85
0.70
0.74
0.80
0.86
0.85
(b) Mid-Stage Diffusion
I0
I1
I2
I3
I4
I5
Target Island
0.72
0.73
0.71
0.70
0.72
0.72
0.79
0.88
0.85
0.88
0.73
0.79
0.80
0.75
0.78
0.71
0.88
0.80
0.83
0.87
0.70
0.85
0.75
0.83
0.83
0.72
0.88
0.78
0.87
0.83
(c) Late Convergence
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
Avg. TSED Similarity (N = 3)
Figure 5: Evolution of Structural Similarity (TSED) across islands.
actual code structure. Consequently, the heatmap shows that islands improve while maintaining distinct algorithmic
trajectories (light colors), effectively preserving population diversity.
Second, the Mid-Stage Diffusion phase (Panel b) captures the transition point. As certain islands (I3, I4, I5) converge on
similar optimal structures, their similarity crosses the threshold, triggering Code Transfer. This switches the mode to
direct exploitation, leading to the rapid formation of a high-similarity cluster (deep blue blocks). In contrast, Island 0
remains structurally distinct (similarity < 0.7); therefore, it continues to receive only high-level insights. This prevents
forced homogenization and allows Island 0 to persist as a unique structural niche.
Third, the Late Convergence phase (Panel c) demonstrates the long-term stability of this dual mechanism. While the
dominant cluster solidifies via Code Transfer, the persistence of Island 0 (similarity ≈0.71) confirms that Insight
Transfer successfully prevents total mode collapse. This proves that the adaptive switching mechanism effectively
balances the rapid exploitation of successful structures with the preservation of alternative algorithmic logic.
Analysis of Adaptive Strategy Selection
We analyzed the temporal distribution of operator selection probabilities.
Figure 6 reveals that the system spontaneously evolves a multi-stage optimization strategy. In the Crossover phase
(Panel a), the algorithm maintains a balanced usage of Structural Crossover (e1) and Backbone Extraction (e2) in the
initial iterations. However, as the population matures, e2 gradually becomes the dominant strategy. This trend indicates
that the system learns to shift from aggressively mixing diverse structures (Exploration) to consolidating and exploiting
successful algorithmic backbones (Exploitation) to stabilize convergence.
More notably, the Mutation phase (Panel b) exhibits a distinct clean-then-refine curriculum. The Simplification operator
(m3, yellow bars) dominates the early-to-mid stages. As evidenced by the substantial complexity of the evolved TSP
heuristic presented in Appendix F.1, LLM-generated solutions for hard combinatorial problems often initiate with
verbose control flows and redundant logic. The UCB selector effectively identifies this structural bloat as the primary
bottleneck, prioritizing simplification to distill the algorithmic backbone. Subsequently, in the late stages (e.g., Iterations
10-12), we observe a significant resurgence of Structural (m1) and Weight Mutation (m2). This transition confirms that
once the code logic is streamlined, the system adaptively switches focus to fine-grained parameter tuning and structural
perturbation to escape local optima.
D.6
Consumption of Time and Token
We further analyze the computational cost of different LLM-based heuristic design methods in terms of time and token
consumption. Table 12 reports the time cost as well as the total numbers of input and output tokens consumed across
different problem settings and algorithmic frameworks. All statistics are collected under the same experimental setup to
ensure a fair comparison. All methods are configured according to the settings described in the original papers.
As shown in the Table 12, different methods exhibit distinct cost profiles depending on their search strategies and
interaction patterns with large language models.
It is worth noting that ReEvo, HSEvo, and TIDE adopt a warm-start strategy, where seed functions are used to initialize
the population. These methods aim to expand and refine existing heuristics to escape local optima. Their effectiveness
crucially depends on how the initial seed functions are exploited and extended during the search process.
28


--- Page 29 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
2
3
4
5
6
7
8
9
10
11
12
Evolution Iteration
0.00
0.25
0.50
0.75
1.00
Selection Probability
(a) Adaptive Crossover Strategy Selection
e1 (Structural Crossover)
e2 (Backbone Extraction)
2
3
4
5
6
7
8
9
10
11
12
Evolution Iteration
0.00
0.25
0.50
0.75
1.00
Selection Probability
(b) Adaptive Mutation Strategy Selection
m1 (Structure Mut)
m2 (Weight Mut)
m3 (Simplify/Opt)
Figure 6: Dynamics of Adaptive Strategy Selection via UCB
Table 12: Comparison of time cost and token consumption for different LLM-based AHD methods.
Methods
Consumption
TSP-Constructive
KP-Constructive
TSP-ACO
MKP-ACO
EoH
Time
3h
5h
4h
4h
Input Token
0.5M
0.6M
0.4M
0.5M
Output Token
0.2M
0.2M
0.2M
0.2M
ReEvo
Time
1.5h
2h
0.6h
3h
Input Token
0.6M
0.6M
0.2M
0.8M
Output Token
0.2M
0.2M
0.1M
0.2M
HSEvo
Time
5h
2.5h
2h
3h
Input Token
1.7M
0.6M
0.8M
0.6M
Output Token
0.6M
0.2M
0.2M
0.2M
MCTS-AHD
Time
8.5h
5h
8h
7h
Input Token
1.5M
1.1M
1.2M
1.5M
Output Token
0.6M
0.4M
0.5M
0.5M
TIDE
Time
8h
2.5h
3h
4h
Input Token
0.8M
0.7M
1M
0.6M
Output Token
0.4M
0.4M
0.5M
0.4M
In the original papers of ReEvo and HSEvo, they adopt fixed evaluation budgets (e.g., 100 or 450 evaluations), under
which the methods should have converged.
Due to the relatively small evaluation budgets adopted in their default settings, ReEvo and HSEvo exhibit lower
computational costs. To more fairly assess the potential of warm-start methods and examine whether increased
computational budgets lead to more substantial performance improvements, we further evaluate these methods under a
unified budget of 800 evaluations, and compare the resulting solution quality as well as the corresponding computational
cost in Table 13.
To better characterize the effectiveness and efficiency of seed-based heuristic improvement, we introduce one additional
evaluation metric. The metric normalizes this improvement by the total number of tokens consumed, measuring the
relative performance gain per token with respect to the seed heuristic. It captures cost-aware efficiency, quantifying
how effectively computational resources are converted into heuristic improvements. In the table, Improve denotes the
relative improvement of the designed heuristics over the seed functions per token (in millions, M), which is computed
as the relative gap (in %) between the designed heuristics and the seed functions divided by the total number of tokens
consumed during evolution.
29


--- Page 30 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Table 13: Comparison of cost and improvement efficiency for ReEvo, HSEvo and TIDE on KP constructive and TSP
ACO. Gap represents the result of heuristics after 800 evaluations relative to the optimal solution (optimal solutions for
KP are obtained using OR-Tools, and for TSP using LKH3 [Lin and Kernighan, 1973], and lower is better). Improve
denotes the relative improvement (%) over seed functions per M tokens (larger is better).
KP-Constructive
Methods
Time
Tokens
LLM requests
100
200
Input
Output
Total
Gap
Improve
Gap
Improve
ReEvo
2.5h
1.2M
0.3M
1.5M
1382
0.08%
0.023%
0.07%
0.013%
HSEvo
5h
0.7M
0.2M
0.9M
589
0.11%
0.012%
0.08%
0.008%
TIDE
2.5h
0.7M
0.4M
1.1M
788
0.02%
0.089%
0.02%
0.066%
TSP-ACO
Methods
Time
Tokens
LLM requests
50
100
Input
Output
Total
Gap
Improve
Gap
Improve
ReEvo
3.5h
1.5M
0.3M
1.8M
1384
2.15%
2.69%
6.16%
6.27%
HSEvo
4h
1.2M
0.3M
1.5M
589
1.72%
3.50%
5.25%
8.03%
TIDE
3h
1M
0.5M
1.5M
788
1.60%
3.57%
4.37%
8.52%
E
Prompt Engineering Details.
In this section, we provide the comprehensive prompt templates utilized in our evolutionary framework. To ensure
reproducibility and maintain methodological consistency with state-of-the-art approaches, our prompt engineering
strategy builds upon the foundational operators proposed in the EOH [Liu et al., 2024a]. Specifically, the recombination
strategies (E1, E2) and mutation strategies (M1, M3) are adopted from EoH. The M2 operator has been fine-tuned to
emphasize scoring logic refinement, directing the LLM to analyze decision boundaries rather than performing generic
parameter perturbation.
Distinctively, to support the specific dynamics of our distributed island model, we introduce two novel prompt
mechanisms: Meta-Cognitive Insight Extraction and Selective Reset. These additions are critical for facilitating
high-level knowledge transfer between sub-populations and enabling knowledge-guided recovery from deep local
optima.
E.1
Recombination Prompt Strategy (E1)
The E1 operator serves as an exploration-centric crossover mechanism. Unlike standard genetic crossover that splices
gene sequences, E1 leverages the LLM to synthesize a structurally novel algorithm by analyzing k parent solutions
selected via tournament. Crucially, this prompt integrates meta-cognitive insights, defined as the accumulated knowledge
from the island’s evolutionary history, to guide the generation process away from known failures and towards promising
heuristics. Furthermore, the prompt structure ensures valid Python execution and enforces a Chain-of-Thought (CoT)
process via the [Thought] and [KEY PARAMETERS] blocks.
Prompt Template for Operator E1 (Explorative Crossover)
[System Message]
You are an expert algorithm engineer. Design efficient heuristic functions. Output your algorithm description inside a brace,
then implement it in Python.
[User Message]
Problem Description: {{self.problem_desc}}
Function Signature: {{self.func_name}}
Goal: Design a novel heuristic algorithm.
I have k existing algorithms:
No.1 algorithm:
{{Parent 1 Thought}}
Code:
30


--- Page 31 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
{{Parent 1 Code}}
. . .
No.k algorithm:
. . .
Task: Implement as a function named {{self.func_name}}_v2 with the same signature. Create a new algorithm that has a
totally different form from the given ones.
You must strictly follow this output format:
• [Thought]: Summarize in exactly 2 sentences: the core idea.
• [KEY PARAMETERS]: List a moderate number of tunable parameters and their roles (avoid too many hardcoded values).
• [Code]: Complete executable Python code in a code block. Include all imports.
Contextual Insight:
{{Local Island Insight}}
{{Neighbor Island Insight (if applicable)}}
E.2
Recombination Prompt Strategy (E2)
The E2 operator functions as a Conceptual Crossover mechanism. While standard crossover exchanges code fragments,
E2 instructs the LLM to abstract the underlying logic shared by the parent solutions. By explicitly requesting the
identification of a common backbone, this operator synthesizes a new heuristic that retains the structural consensus of
high-performing parents while introducing novel variations in implementation. This approach effectively stabilizes
beneficial traits while preventing the population from collapsing into identical code clones.
Prompt Template for Operator E2 (Backbone Extraction)
[System Message]
You are an expert algorithm engineer. Design efficient heuristic functions. Output your algorithm description inside a brace,
then implement in Python.
[User Message]
Problem Description: {{self.problem_desc}}
Function Signature: {{self.func_name}}
Goal: Design a novel heuristic algorithm.
I have k existing algorithms:
No.1 algorithm:
{{Parent 1 Thought}}
Code:
{{Parent 1 Code}}
. . .
No.k algorithm:
. . .
Task: Implement as a function named {{self.func_name}}_v2 with the same signature. Identify the common backbone
idea in these algorithms, then create a new algorithm motivated from it but with a different form.
You must strictly follow this output format:
• [Thought]: Summarize in exactly 2 sentences: the core idea.
• [KEY PARAMETERS]: List a moderate number of tunable parameters and their roles (avoid too many hardcoded values).
• [Code]: Complete executable Python code in a code block. Include all imports.
Contextual Insight:
{{Local Island Insight}}
{{Neighbor Island Insight (if applicable)}}
E.3
Mutation Prompt Strategy (M1)
The M1 operator implements a Structural Mutation mechanism. Unlike simple parameter perturbation, M1 prompts the
LLM to generate a variant that possesses a different algorithmic form while remaining a modified version of the parent.
A distinguishing feature of this operator is the injection of cross-island knowledge: the prompt context optionally
31


--- Page 32 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
includes insights from a neighboring island. This allows the mutation process to be guided not only by local history but
also by external success strategies, effectively bridging the gap between local exploitation and global exploration.
Prompt Template for Operator M1 (Structural Mutation)
[System Message]
You are an expert algorithm engineer. Design efficient heuristic functions. Output your algorithm description inside a brace,
then implement in Python.
[User Message]
Problem Description: {{self.problem_desc}}
Function Signature: {{self.func_name}}
Goal: Design a novel heuristic algorithm.
Current algorithm:
{{Parent Thought}}
Code:
{{Parent Code}}
Task: Implement as a function named {{self.func_name}}_v2 with the same signature. Create a new algorithm that has a
different form but can be a modified version of the provided one.
You must strictly follow this output format:
• [Thought]: Summarize in exactly 2 sentences: the core idea.
• [KEY PARAMETERS]: List a moderate number of tunable parameters and their roles.
• [Code]: Complete executable Python code in a code block. Include all imports.
Contextual Insight:
{{Local Island Insight}}
{{Neighbor Island Insight (Injection from adjacent population)}}
E.4
Mutation Prompt Strategy (M2)
The M2 operator implements a Scoring Logic Refinement strategy. While M1 focuses on high-level architectural
changes, M2 directs the LLM to analyze the parent’s internal decision-making process, specifically targeting the scoring
function and its constituent weightings. This operator instructs the model to deconstruct the existing scoring mechanics
and synthesize a variant with modulated behaviors or alternative logic paths. By integrating contextual insights, M2
enables the evolutionary process to perform nuanced structural adjustments that refine how the heuristic prioritizes
conflicting objectives, thereby sharpening the solution’s precision without discarding its proven backbone.
Prompt Template for Operator M2 (Scoring Logic Refinement)
[System Message]
You are an expert algorithm engineer. Design efficient heuristic functions. Output your algorithm description inside a brace,
then implement in Python.
[User Message]
Problem Description: {{self.problem_desc}}
Function Signature: {{self.func_name}}
Goal: Design a novel heuristic algorithm.
Current algorithm:
{{Parent Thought}}
Code:
{{Parent Code}}
Task: Implement as a function named {{self.func_name}}_v2 with the same signature. Identify the main scoring
components and create a new algorithm with different configurations or score function.
You must strictly follow this output format:
• [Thought]: Summarize in exactly 2 sentences: the core idea.
• [KEY PARAMETERS]: List the key controllable factors and their roles (for downstream optimization).
• [Code]: Complete executable Python code in a code block. Include all imports.
32


--- Page 33 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
Contextual Insight:
{{Local Island Insight}}
E.5
Mutation Prompt Strategy (M3)
The M3 operator functions as a Regularization Mutation strategy designed to enhance heuristic robustness and combat
overfitting. It instructs the LLM to perform a critical analysis of the parent function, specifically targeting components
suspected of being over-specialized to in-distribution data. These potentially brittle or overly complex segments are then
strategically pruned or streamlined. The outcome is a more parsimonious and computationally lean implementation
theorized to exhibit superior generalization to out-of-distribution scenarios while preserving the original function
signature to ensure architectural compatibility.
Prompt Template for Operator M3 (Regularization Mutation)
[System Message]
You are an expert algorithm engineer. Design efficient heuristic functions. Output your algorithm description inside a brace,
then implement in Python.
[User Message]
Problem Description: {{self.problem_desc}}
Function Signature: {{self.func_name}}
Goal: Design a novel heuristic algorithm.
Current algorithm Code:
{{Parent Code}}
Task: Implement as a function named {{self.func_name}}_v2 with the same signature.
Identify the main components in the function below. Analyze whether any components can be overfit to specific instances.
Simplify or optimize the components to enhance generalization. Provide the different revised code, keeping the function
name, inputs, and outputs unchanged.
You must strictly follow this output format:
• [Thought]: Summarize in exactly 2 sentences: the core idea.
• [KEY PARAMETERS]: List a moderate number of tunable parameters and their roles.
• [Code]: Complete executable Python code in a code block. Include all imports.
Contextual Insight:
{{Local Island Insight}}
E.6
Meta-Cognitive Prompt Strategy (Insight Extraction)
To prevent the evolutionary search from becoming a blind trial-and-error process, the framework employs a dedicated
Insight Extraction mechanism. This operator functions as a meta-cognitive feedback loop that converts raw code
execution data into semantic design principles. Rather than analyzing a solution in isolation, the system employs
a contrastive prompting strategy: it presents the LLM with the island’s current Elite (best performer) alongside the
local Straggler (worst performer). By forcing the model to explicitly articulate the mechanistic reasons behind the
performance gap, the system distills abstract strategies from concrete implementations. These generated insights are
then stored in the island’s memory and re-injected into subsequent mutation and crossover prompts, thereby guiding
future generations with accumulated architectural knowledge.
Prompt Template for Insight Extraction (Contrastive Analysis)
[System Message]
You are an expert in the domain of optimization heuristics. Identify the mechanisms responsible for the performance gap
and explain why it is effective in two paragraphs.
[User Message]
High Performance (Score: {{Best Objective}}):
33


--- Page 34 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
{{Best Code}}
Low Performance (Score: {{Worst Objective}}):
{{Worst Code}}
Instruction:
You are an expert in the domain of optimization heuristics. Identify the mechanisms responsible for the performance gap
and explain why it is effective in two paragraphs.
E.7
Selective Reset Prompt Strategy (Knowledge-Guided Restart)
To mitigate the risk of irreversible convergence where an island population becomes trapped in a deep local optimum,
the framework employs a Selective Reset mechanism. Unlike traditional restart strategies that re-initialize populations
randomly, effectively discarding all accumulated optimization progress, this operator leverages the Global Elite heuristic
to perform a knowledge-guided restart. When deep stagnation is detected via trajectory analysis, the system retrieves
a high-performing solution from the global archive. The LLM is then prompted to extract the underlying domain
insights from this elite reference and synthesize a structurally novel algorithm derived from it. This process effectively
transplants the global state-of-the-art logic into the stagnant island but forces a divergent implementation path, thereby
reinvigorating the local search space with high-potential genetic material.
Prompt Template for Selective Reset (Knowledge-Guided Restart)
[System Message]
You are an expert algorithm engineer. Design efficient heuristic functions. Output your algorithm description inside a brace,
then implement in Python.
[User Message]
Reference Elite (Score: {{Global Elite Score}}):
{{Global Elite Code}}
Instruction:
You are an expert in the domain of optimization heuristics. Extract domain insights from the Elite solution provided above,
then create a structurally novel and advanced algorithm. Output the design rationale followed by the implementation of
{{self.func_name}}_v2.
You must strictly follow this output format:
• [Thought]: Summarize in exactly 2 sentences: the core idea.
• [KEY PARAMETERS]: List a moderate number of tunable parameters and their roles.
• [Code]: Complete executable Python code in a code block. Include all imports.
F
Examples of LLM-Generated Heuristics
F.1
Best-Performing Heuristic for Constructive TSP


import
heapq
import
math
def
select_next_node_v2 (current_node , destination_node , unvisited_nodes ,
distance_matrix ):
if not
unvisited_nodes :
return
None
if len( unvisited_nodes ) == 1:
return
next(iter( unvisited_nodes ))
# Tunable
parameters
alpha = 0.8
beta = 0.1
gamma = 0.6
theta = 0.05
34


--- Page 35 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
candidate_ratio = 2.5
max_candidates = 12
unvisited_list = list( unvisited_nodes )
n_total = len(unvisited_list )
k_candidates = min(int(n_total * candidate_ratio ), max_candidates , n_total)
if k_candidates == 0:
k_candidates = 1
candidate_set = heapq.nsmallest(k_candidates , unvisited_list , key=lambda x:
distance_matrix [current_node ][x])
def
calculate_mst_cost (nodes):
if len(nodes) <= 1:
return 0
nodes = list(nodes)
edges = []
for i in range(len(nodes)):
for j in range(i + 1, len(nodes)):
u, v = nodes[i], nodes[j]
edges.append (( distance_matrix [u][v], u, v))
edges.sort ()
parent = {node: node for node in nodes}
rank = {node: 0 for node in nodes}
def find(x):
while
parent[x] != x:
parent[x] = parent[parent[x]]
x = parent[x]
return x
def union(x, y):
rx , ry = find(x), find(y)
if rx == ry:
return
False
if rank[rx] < rank[ry]:
parent[rx] = ry
elif rank[rx] > rank[ry]:
parent[ry] = rx
else:
parent[ry] = rx
rank[rx] += 1
return
True
mst_cost = 0
count = 0
for cost , u, v in edges:
if union(u, v):
mst_cost += cost
count += 1
if count == len(nodes) - 1:
break
return
mst_cost
def
simulate_refined_path (start , remaining_nodes , dest):
# Construct
greedy
path: start
-> all
remaining
-> dest
path = [start]
current = start
remaining = set( remaining_nodes )
if start in remaining:
remaining.remove(start)
while
remaining:
next_n = min(remaining , key=lambda x: distance_matrix [current ][x])
path.append(next_n)
35


--- Page 36 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
current = next_n
remaining.remove(current)
path.append(dest)
# Apply
iterative 2-opt until no improvement
improved = True
while
improved:
improved = False
best_gain = 0
best_i , best_j = -1, -1
n = len(path)
for i in range(1, n - 2):
for j in range(i + 2, n):
old_dist = distance_matrix [path[i -1]][ path[i]] +
distance_matrix [path[j -1]][ path[j]]
new_dist = distance_matrix [path[i -1]][ path[j -1]] +
distance_matrix [path[i]][ path[j]]
gain = old_dist - new_dist
if gain > best_gain:
best_gain = gain
best_i , best_j = i, j
if best_gain > 1e-9:
path[best_i:best_j] = reversed(path[best_i:best_j ])
improved = True
total_cost = sum( distance_matrix [path[i]][ path[i+1]] for i in range(len(
path) -1))
return
total_cost
best_score = float(’inf’)
next_node = candidate_set [0]
for
candidate in candidate_set :
remaining = set( unvisited_nodes )
remaining.discard(candidate)
direct_cost = distance_matrix [current_node ][ candidate]
mst_cost = calculate_mst_cost (remaining) if remaining
else 0
sim_cost = simulate_refined_path (candidate , remaining , destination_node )
dest_bias = distance_matrix [candidate ][ destination_node ]
score = (
alpha * direct_cost +
beta * mst_cost +
gamma * sim_cost +
theta * dest_bias
)
if score < best_score:
best_score = score
next_node = candidate
return
next_node


F.2
Best-Performing Heuristic for Online BPP


import
numpy as np
# Global
state
variables
for
priority_v2
_min_item_estimate_v2 = None
_recent_bin_usage_v2 = None
def
priority_v2(item , bins_remain_cap ):
36


--- Page 37 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
global
_min_item_estimate_v2 , _recent_bin_usage_v2
epsilon = 1e-8
bins_remain_cap = np.asarray(bins_remain_cap , dtype=float)
feasible = bins_remain_cap
>= item
if not np.any(feasible):
return -np.inf * np.ones_like( bins_remain_cap )
leftover = bins_remain_cap - item
# Key
parameters (streamlined
and
rebalanced)
fit_weight = 1.2
reuse_bonus_weight = 0.8
min_item_decay = 0.1
history_bonus_weight = 0.15
utility_target_ratio = 0.4
utility_width = 0.25
# --- Adaptive
minimum
item
estimation
---
if _min_item_estimate_v2
is None:
_min_item_estimate_v2 = float(item)
else:
_min_item_estimate_v2 = (1 - min_item_decay ) * min(_min_item_estimate_v2 ,
item) + min_item_decay * item
# --- Best -fit score
---
fit_score = fit_weight / (leftover + epsilon)
# --- Waste
penalty: penalize
unusable
leftovers (< estimated
min item) ---
usable_threshold = _min_item_estimate_v2
waste_penalty = np.where(
leftover < usable_threshold ,
( usable_threshold - leftover) / ( usable_threshold + epsilon),
0.0
)
# --- Reuse
bonus: Gaussian
centered at utility
target
---
estimated_bin_capacity = np.max( bins_remain_cap ) if len( bins_remain_cap ) > 0
else max(item , 1.0)
estimated_bin_capacity = max(estimated_bin_capacity , item)
norm_leftover = leftover / ( estimated_bin_capacity + epsilon)
deviation = (norm_leftover - utility_target_ratio ) / ( utility_width + epsilon
)
reuse_bonus = reuse_bonus_weight * np.exp ( -0.5 * deviation ** 2)
# --- History
bonus: favor
recently
used bins
---
history_bonus = np.zeros_like( bins_remain_cap )
if _recent_bin_usage_v2
is not None:
for idx , count in
_recent_bin_usage_v2 .items ():
if idx < len(history_bonus ):
history_bonus[idx] +=
history_bonus_weight * count
# Compute
raw
priorities to select bin
raw_priorities = fit_score + reuse_bonus - waste_penalty
raw_priorities = np.where(feasible , raw_priorities , -np.inf)
chosen_bin_idx = int(np.argmax( raw_priorities ))
# Update
usage
history
with
decay
if _recent_bin_usage_v2
is None:
_recent_bin_usage_v2 = {}
_recent_bin_usage_v2 = {k: v * 0.9 for k, v in
_recent_bin_usage_v2 .items ()
if v * 0.9 > 0.01}
_recent_bin_usage_v2 [chosen_bin_idx ] = _recent_bin_usage_v2 .get(
chosen_bin_idx , 0) + 1.0
37


--- Page 38 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
# Final
priority
including
history
priorities = fit_score + reuse_bonus - waste_penalty + history_bonus
priorities = np.where(feasible , priorities , -np.inf)
return
priorities


F.3
Best-Performing Heuristic for Offline BPP-ACO


import
numpy as np
def
heuristics_v2(demand , capacity):
n = demand.shape [0]
if n == 0:
return np.zeros ((0, 0))
# Algorithm
parameters
num_trials = 20
utilization_weight = True
perturb_factor = 0.1
# 0.0 = strict FFD , 1.0 = fully
shuffled
co_occurrence = np.zeros ((n, n), dtype=np.float32)
item_indices = np.arange(n)
for _ in range(num_trials):
# Create a perturbed
order: mostly
sorted
descending , with
controlled
randomness
sorted_idx = np.argsort(-demand)
# descending by size
num_perturb = int( perturb_factor * n)
if num_perturb > 0:
# Randomly
swap some
adjacent
items in the sorted
list
perm = sorted_idx.copy ()
for _ in range(num_perturb):
i = np.random.randint (0, n - 1)
perm[i], perm[i + 1] = perm[i + 1], perm[i]
else:
perm = sorted_idx
shuffled_demand = demand[perm]
shuffled_indices = item_indices[perm]
bins = []
# each bin is list of original
indices
bin_loads = []
# track
current
load for
efficiency
for idx , size in zip(shuffled_indices , shuffled_demand ):
placed = False
for b_idx , b in enumerate(bins):
if bin_loads[b_idx] + size
<= capacity:
b.append(idx)
bin_loads[b_idx] += size
placed = True
break
if not placed:
bins.append ([idx])
bin_loads.append(size)
# Update co -occurrence
with
optional
utilization
weighting
for b, load in zip(bins , bin_loads):
b = np.array(b)
weight = load / capacity if
utilization_weight
else 1.0
co_occurrence[b[:, None], b] += weight
38


--- Page 39 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
# Normalize by total
possible
weight per trial (max weight per trial =
num_trials if unweighted , or sum of bin
utilizations
otherwise)
if utilization_weight :
# Compute
expected
max: each
trial
contributes at most sum(bin_loads)/
capacity = total_demand/capacity , but we normalize by trials for
stability
compatibility = co_occurrence / num_trials
else:
compatibility = co_occurrence / num_trials
# Ensure
diagonal is 1
np. fill_diagonal(compatibility , 1.0)
# Symmetrize
for
numerical
robustness
compatibility = (compatibility + compatibility .T) / 2.0
# Row -wise
normalization to [0 ,1]
row_max = compatibility.max(axis=1, keepdims=True)
epsilon = 1e-8
normalized = np.divide(
compatibility ,
row_max ,
out=np.zeros_like(compatibility ),
where=row_max > epsilon
)
return
normalized


F.4
Best-Performing Heuristic for Constructive KP


import
numpy as np
def
select_next_item_v2 (remaining_capacity , values , weights):
epsilon = 1e-9
feasible_mask = weights
<= remaining_capacity + epsilon
feasible_indices = np.where( feasible_mask )[0]
if len( feasible_indices ) == 0:
return 0
n_items = len(values)
ratios = values / (weights + epsilon)
# Lookahead
simulation (same as original
for
consistency)
lookahead_scores = np.zeros(len( feasible_indices ))
for i, idx in enumerate( feasible_indices ):
new_cap = remaining_capacity - weights[idx]
if new_cap < epsilon:
lookahead_scores [i] = values[idx]
else:
mask = np.ones(n_items , dtype=bool)
mask[idx] = False
avail_w = weights[mask]
avail_v = values[mask]
avail_r = ratios[mask]
res_feas = avail_w
<= new_cap + epsilon
if not np.any(res_feas):
lookahead_scores [i] = values[idx]
else:
w_sub = avail_w[res_feas]
v_sub = avail_v[res_feas]
r_sub = avail_r[res_feas]
39


--- Page 40 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
order = np.argsort(-r_sub)
total_val = values[idx]
cap_left = new_cap
for j in order:
if w_sub[j] <= cap_left + epsilon:
total_val += v_sub[j]
cap_left
-= w_sub[j]
if cap_left < epsilon:
break
lookahead_scores [i] = total_val
# Normalize
lookahead
scores
max_look = np.max( lookahead_scores )
norm_look = lookahead_scores / (max_look + epsilon) if max_look > epsilon
else np.ones_like( lookahead_scores )
# Residual
capacity
with
tunable
exponent (soft
penalization)
residual_caps = remaining_capacity
- weights[ feasible_indices ]
residual_frac = np.clip(residual_caps / ( remaining_capacity + epsilon), 0, 1)
residual_exp = 0.939665
ineff = residual_frac ** residual_exp
# Entropy -modulated
density
weighting
item_ratios = ratios[ feasible_indices ]
global_max_ratio = np.max(ratios) + epsilon
rel_density = item_ratios / global_max_ratio
# Compute
entropy of normalized
ratios
over
feasible
set
prob = rel_density / (np.sum(rel_density) + epsilon)
entropy = -np.sum(prob * np.log(prob + epsilon))
entropy_temp = 1.733771
entropy_mod = np.exp(-entropy / ( entropy_temp + epsilon))
# Adaptive
density
factor
with
entropy
modulation
avg_rel_density = np.mean(rel_density) + epsilon
base_density_factor = np.clip(rel_density / avg_rel_density , 0.6, 1.5)
density_factor = base_density_factor * (1.0 + 0.3 * (1.0 - entropy_mod))
#
boost
when low
entropy (more
certainty)
# DE -inspired
adaptive
gamma
gamma_base = 0.276467
choice_pressure = min (1.0, len( feasible_indices ) / n_items)
de_scale = 0.906848
gamma = gamma_base * (1.0 + de_scale * choice_pressure )
# Penalty
term
incorporating
all hybrid
elements
penalty = gamma * ineff * (1.0 - norm_look) / density_factor
# Final
score: lookahead
minus
adaptive
penalty
scores = norm_look - penalty
best_local = np.argmax(scores)
return int( feasible_indices [best_local ])


F.5
Best-Performing Heuristic for CVRP-ACO


import
numpy as np
import
random
def
heuristics_v2(distance_matrix , coordinates , demands , capacity):
n = len(demands)
if n <= 1:
40


--- Page 41 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
return np.zeros ((n, n))
# Hybridized
parameters
from
Island 5 + adaptive
tuning
num_ants_factor = 3
max_iter = 30
alpha = 1.439913
beta = 1.816340
rho = 0.379887
elite_ratio = 0.562122
decay_factor = 1.444863
angular_tolerance = np.pi / 4
# ~45
degrees
num_ants = max(20, min (100, n * num_ants_factor ))
tau = np.full ((n, n), 1.0 / (n * n))
np. fill_diagonal(tau , 0)
best_cost = float(’inf’)
best_routes = None
depot_coord = np.array(coordinates [0])
# Precompute
angles
from
depot
angles = np.zeros(n)
for i in range(1, n):
vec = np.array(coordinates[i]) - depot_coord
angles[i] = np.arctan2(vec[1], vec [0])
# Adaptive
neighborhood: use median
instead of fixed
percentile
for
robustness
nonzero_dists = distance_matrix [ distance_matrix > 0]
influence_radius = np.median( nonzero_dists) if len( nonzero_dists ) > 0 else
1.0
# Demand
density
using
adaptive
radius
demand_density = np.zeros(n)
for i in range(1, n):
neighbors = [j for j in range (1, n) if distance_matrix [i][j] <=
influence_radius ]
if neighbors:
demand_density [i] = np.mean ([ demands[j] for j in neighbors ])
else:
demand_density [i] = demands[i]
if demand_density .ptp() > 0:
demand_density = ( demand_density - demand_density .min()) / demand_density
.ptp()
else:
demand_density = np.ones(n)
# Enhanced
visibility
matrix
visibility = np.zeros ((n, n))
base_demand = np.mean(demands [1:]) + 1e-8
for i in range(n):
for j in range(n):
if i == j or demands[j] <= 0 or distance_matrix [i][j] == 0:
continue
inv_dist = 0.776078 / distance_matrix [i][j]
urgency = demands[j] / capacity
density_factor = 1.134148 + demand_density [j]
if i == 0:
angular_factor = 0.263295
else:
angle_diff = abs(angles[i] - angles[j])
angle_diff = min(angle_diff , 2*np.pi - angle_diff)
41


--- Page 42 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
# Use cosine
similarity
within
tolerance
window
if angle_diff
<= angular_tolerance :
angular_factor = 0.405164 + np.cos(angle_diff)
else:
angular_factor = np.exp(- (angle_diff / angular_tolerance )
**2)
visibility[i][j] = inv_dist * (1.0 + urgency) * angular_factor *
density_factor
all_route_costs_history = []
all_routes_history = []
for
iteration in range(max_iter):
all_routes = []
all_costs = []
for ant in range(num_ants):
unvisited = set(range (1, n))
routes = []
total_cost = 0.0
while
unvisited:
route = [0]
load = 0
current = 0
while
True:
feasible = [j for j in unvisited if load + demands[j] <=
capacity]
if not
feasible:
break
probs = []
for j in feasible:
pher = tau[current ][j] ** alpha
vis = visibility[current ][j] ** beta
probs.append(pher * vis)
if sum(probs) == 0:
next_node = random.choice(feasible)
else:
probs = np.array(probs) / sum(probs)
next_node = np.random.choice(feasible , p=probs)
route.append(next_node)
load += demands[next_node]
unvisited.remove(next_node)
current = next_node
route.append (0)
routes.append(route)
cost = sum( distance_matrix [route[i]][ route[i+1]] for i in range(
len(route) -1))
total_cost += cost
all_routes.append(routes)
all_costs.append(total_cost)
all_route_costs_history .append(total_cost)
all_routes_history .append(routes)
if total_cost < best_cost:
best_cost = total_cost
best_routes = routes
42


--- Page 43 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
# Pheromone
evaporation
tau *= (1 - rho)
# Elite
update
with
deduplication
elite_count = max(1, int(elite_ratio * len(all_costs)))
sorted_indices = np.argsort(all_costs)
elite_indices = sorted_indices [: elite_count]
seen_signatures = set()
unique_elites = []
for idx in elite_indices :
sig = tuple(sorted(tuple(sorted(r)) for r in all_routes[idx]))
if sig not in seen_signatures :
seen_signatures .add(sig)
unique_elites.append(idx)
# Temporal -decay
visitation
entropy
visit_count = np.zeros(n)
total_visits = 0
recent_solutions = all_routes_history [-min(20, len( all_routes_history )):]
for t, routes in enumerate(reversed( recent_solutions )):
weight = decay_factor ** t
for route in routes:
for node in route [1: -1]:
visit_count[node] += weight
total_visits += weight
if total_visits > 0:
visit_prob = visit_count / total_visits
visit_prob = np.clip(visit_prob , 1e-10, None)
visit_prob /= visit_prob.sum()
elite_entropy = -np.sum(visit_prob * np.log(visit_prob + 1e -10))
else:
visit_prob = np.ones(n) / n
elite_entropy = np.log(n)
# Pheromone
update
with
entropy
scaling
for rank , idx in enumerate( unique_elites):
routes = all_routes[idx]
cost = all_costs[idx]
if cost == 0:
continue
delta_tau = (1.0 + elite_entropy ) / (cost * (rank + 1))
for route in routes:
for i in range(len(route) - 1):
u, v = route[i], route[i+1]
tau[u][v] += delta_tau
tau[v][u] += delta_tau
tau = (tau + tau.T) / 2
edge_scores = np.zeros ((n, n))
if best_routes is None:
return
edge_scores
# Final
entropy
for
scoring
visit_count = np.zeros(n)
total_visits = 0
recent_solutions = all_routes_history [-min(20, len( all_routes_history )):]
for t, routes in enumerate(reversed( recent_solutions )):
weight = decay_factor ** t
for route in routes:
for node in route [1: -1]:
visit_count[node] += weight
total_visits += weight
43


--- Page 44 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
if total_visits > 0:
visit_prob = visit_count / total_visits
visit_prob = np.clip(visit_prob , 1e-10, None)
visit_prob /= visit_prob.sum()
entropy = -np.sum(visit_prob * np.log(visit_prob + 1e -10))
else:
visit_prob = np.ones(n) / n
entropy = np.log(n)
# Compute
centroids
and slack
route_centroids = []
route_slack = []
for r in best_routes:
nodes = [node for node in r if node != 0]
if nodes:
coords = np.array ([ coordinates[node] for node in nodes ])
centroid = np.mean(coords , axis =0)
load = sum(demands[node] for node in nodes)
else:
centroid = depot_coord
load = 0
route_centroids .append(centroid)
route_slack.append(capacity - load)
best_total_cost = sum(
sum( distance_matrix [r[i]][r[i+1]] for i in range(len(r) -1))
for r in best_routes
)
base_weight = (1.0 / ( best_total_cost + 1e-8)) * (1.0 + entropy)
# Score
edges
for idx , route in enumerate(best_routes):
centroid = route_centroids [idx]
slack = route_slack[idx]
route_nodes = [node for node in route if node != 0]
if route_nodes:
node_coords = np.array ([ coordinates[node] for node in route_nodes ])
std_dev = np.std(np.linalg.norm(node_coords - centroid , axis =1)) if
len(route_nodes) > 1 else 1.0
else:
std_dev = 1.175239
for i in range(len(route) - 1):
u, v = route[i], route[i+1]
dist = distance_matrix [u][v]
if dist == 0:
continue
# Radial
alignment
with
cosine
similarity
if u != 0 and v != 0:
vec_u = np.array(coordinates[u]) - depot_coord
vec_v = np.array(coordinates[v]) - depot_coord
norm_u = np.linalg.norm(vec_u)
norm_v = np.linalg.norm(vec_v)
if norm_u > 0 and norm_v > 0:
cos_sim = np.dot(vec_u , vec_v) / (norm_u * norm_v)
radial_score = 0.406854 + max (0.0 , cos_sim)
else:
radial_score = 0.984288
else:
radial_score = 0.300000
44


--- Page 45 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
# Compactness
based on centroid
proximity
mid_point = (np.array(coordinates[u]) + np.array(coordinates[v])) /
2.0
compactness = np.exp(-np.linalg.norm(mid_point - centroid) / (std_dev
+ 1e-6))
# Slack
utilization
factor
slack_factor = 1.352515 + (slack / capacity)
# Distance
penalty
dist_penalty = 2.423188 / (dist + 1e-8)
# Frequency
from
visit
probability
freq_factor = (visit_prob[u] + visit_prob[v]) / 2.0
weight = base_weight * radial_score * compactness * slack_factor *
dist_penalty * freq_factor
edge_scores[u][v] += weight
edge_scores[v][u] += weight
total_score = edge_scores.sum()
if total_score == 0:
return
edge_scores
return
edge_scores / total_score


F.6
Best-Performing Heuristic for Car Mountain


import
numpy as np
import
random
import
math
def
choose_action_v2 (pos , v, last_action):
# KEY
PARAMETERS
goal_threshold = 0.5
min_velocity_base = 0.013
climb_vel_boost = 0.025
left_zone_base =
-0.45
coast_zone = 0.05
left_push_sensitivity = 0.8
stop_tolerance_factor = 6
# Normalize
position to [0, 1] for
dynamic
adjustments
norm_pos = (pos + 1.2) / 1.8
dynamic_min_velocity = min_velocity_base + climb_vel_boost * norm_pos
# Near goal: stabilize or gently
accelerate
right
if pos
>= goal_threshold:
return 1 if abs(v) < dynamic_min_velocity
else 2
# If moving
right
fast enough , keep
accelerating
if v > dynamic_min_velocity :
return 2
# If moving
left
significantly , accelerate
left to deepen
swing
if v < -dynamic_min_velocity / stop_tolerance_factor :
return 0
# Adjust
left push
boundary
based on current
velocity (slower = push
earlier)
effective_left_boundary = left_zone_base + left_push_sensitivity * abs(v)
# Decision
based on adjusted
zones and
velocity
state
if pos < effective_left_boundary :
# Deep in left: start
right
swing
return 2
45


--- Page 46 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
elif pos < 0.0:
# On left
slope: push left to gain
potential
energy
return 0
elif pos < coast_zone
and abs(v) < dynamic_min_velocity :
# Near
bottom
with low speed: coast to preserve
momentum
return 1
else:
# On right
slope or with
decent
speed: accelerate
right
return 2


F.7
Best-Performing Heuristic for DPP-GA


import
numpy as np
def
crossover_v2(parents , n_pop , blend_prob =0.5 ,
crossover_points =2):
"""
Generate
offspring
via hybrid
crossover: arithmetic
blend or multi -point
segment
crossover.
Parameters:
parents (np.ndarray): 2D array of shape (n_parents , n_decap)
n_pop (int): number of offspring to produce
blend_prob (float): probability of using
arithmetic
blending
crossover_points (int): number of crossover
points for
segment
crossover
Returns:
np.ndarray: offspring
array of shape (n_pop , n_decap)
"""
n_parents , n_decap = parents.shape
offspring = np.empty ((n_pop , n_decap), dtype=parents.dtype)
# Pre -generate
all random
choices
for
efficiency
rand_vals = np.random.rand(n_pop)
parent_indices = np.random.choice(n_parents , size =(n_pop , 2), replace=True)
# Ensure
distinct
parents
per
offspring
equal_mask = parent_indices [:, 0] == parent_indices [:, 1]
while np.any(equal_mask):
parent_indices [equal_mask , 1] = np.random.randint (0, n_parents , size=np.
sum(equal_mask))
equal_mask = parent_indices [:, 0] == parent_indices [:, 1]
p1 = parents[parent_indices [:, 0]]
p2 = parents[parent_indices [:, 1]]
# Arithmetic
blending
mask
blend_mask = rand_vals < blend_prob
# Apply
arithmetic
blend
where
applicable
if np.any(blend_mask):
alphas = np.random.rand(np.sum(blend_mask), 1)
offspring[blend_mask] = alphas * p1[blend_mask] + (1 - alphas) * p2[
blend_mask]
# Segment
crossover
for the rest
segment_mask = ~blend_mask
if np.any(segment_mask):
num_segment = np.sum(segment_mask)
seg_p1 = p1[segment_mask]
seg_p2 = p2[segment_mask]
child_seg = np.empty (( num_segment , n_decap), dtype=parents.dtype)
for i in range(num_segment):
46


--- Page 47 ---
TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design
# Generate
unique
sorted
crossover
points
if n_decap > 1 and
crossover_points > 0:
pts = np.sort(np.random.choice(n_decap - 1, size=min(
crossover_points , n_decap - 1), replace=False)) + 1
else:
pts = np.array ([], dtype=int)
segments = np.split(np.arange(n_decap), pts)
use_p1 = True
for seg in segments:
if use_p1:
child_seg[i, seg] = seg_p1[i, seg]
else:
child_seg[i, seg] = seg_p2[i, seg]
use_p1 = not use_p1
offspring[segment_mask] = child_seg
return
offspring


47
