--- Page 1 ---
Modular Foundation Model Inference at the Edge:
Network-Aware Microservice Optimization
Juan Zhu, Zixin Wang, Shenghui Song, Jun Zhang, and Khaled B. Letaief
Dept. ECE., The Hong Kong University of Science and Technology, Hong Kong
Email: jzhucu@connect.ust.hk, {eewangzx, eeshsong, eejzhang, eekhaled}@ust.hk
Abstract—Foundation models (FMs) unlock unprecedented
multimodal and multitask intelligence, yet their cloud-centric
deployment precludes real-time responsiveness and compro-
mises user privacy. Meanwhile, monolithic execution at the
edge remains infeasible under stringent resource limits and
uncertain network dynamics. To bridge this gap, we propose
a microservice-based FM inference framework that exploits
the intrinsic functional asymmetry between heavyweight core
services and agile light services. Our two-tier deployment strat-
egy ensures robust Quality of Service (QoS) under resource
contention. Specifically, core services are placed statically
via a long-term network-aware integer program with sparsity
constraints to form a fault-tolerant backbone. On the other
hand, light services are orchestrated dynamically by a low-
complexity online controller that integrates effective capacity
theory with Lyapunov optimization, providing probabilistic
latency guarantees under real-time workload fluctuations. Sim-
ulations demonstrate that our framework achieves over 84%
average on-time task completion with moderate deployment
costs and maintains strong robustness as the system load
scales.
Index Terms—Microservice, foundation model, resource con-
tention, edge AI.
I. Introduction
The recent advancements of foundation models (FMs)
bring a significant shift from pattern-matching to general
intelligence. Trained over massive and diverse datasets,
FMs, particularly multi-modal FMs, are adept at various
complex downstream inference tasks, such as movie gener-
ation and real-time augmented reality, by integrating dis-
tinct data modalities (e.g., video, text, image, and acoustic
signals) [1]. Nevertheless, mainstream FM-based services
are typically deployed in centralized cloud infrastructure,
a setup that inherently suffers from significant bandwidth
consumption and potential privacy concerns. Migrating
the deployment of FMs to the network edge stands out as
a promising solution to enable low-latency and privacy-
preserving mobile services [2].
However, deploying FMs on individual edge devices
via conventional monolithic architectures is impractical,
as the massive internal neurons (or model parameters)
demand significant computation and storage resources.
To overcome this limitation and leverage the distributed
resources at the network edge, the microservice (MS)
architecture is utilized for collaborative edge deployment
by functionally decomposing the FM into a collection of
small, loosely coupled services [3], with computationally
lightweight and heavyweight MSs deployed onto edge de-
vices and edge servers, respectively. Consequently, the FM
inference task can be modeled as a Directed Acyclic Graph
(DAG), where dispersed MSs are interconnected via net-
work links. However, the resulting distributed architecture
introduces communication overheads and complex service
dependencies, motivating recent optimization and graph-
learning approaches for dependency-aware deployment
that balance QoS assurance and cost eﬀiciency [4]–[6].
While these studies address structural complexities,
the fluctuating demands from geographically-dispersed
users pose a significant threat to operational stability.
To circumvent this, a robust adversarial reinforcement
learning framework was employed in [7] to counteract
box uncertainty in task arrivals. Meanwhile, such external
uncertainty is often compounded by internal resource
contention. While potentially cost-effective, the pragmatic
choice to co-locate multiple services on finite, hetero-
geneous nodes can lead to severe queuing delays and
significant tail latency [8]. Existing approaches attempt
to model this contention either explicitly via predefined
average-based conflict coeﬀicients [6], which performs
poorly for latency-sensitive systems due to heavy tail
latency, or implicitly through learning-based resource
managers [9]. Despite these efforts, many works treat
microservices as homogeneous components with similar
requirements, overlooking their distinct workload and
operational characteristics, which, however, are essential
for fine-grained resource match, QoS delivery, and eﬀicient
scaling in FM-based inference tasks.
Specifically, FM inference pipelines comprise hetero-
geneous MSs with fundamentally different deployment
behaviors. Core MSs, which encapsulate the computation-
intensive models (e.g., transformers, vision backbones),
exhibit long startup times and limited fault tolerance,
making them rigid yet performance-critical anchors of
the inference workflow. Conversely, light MSs perform
auxiliary, typically stateless operations such as pre- or
post-processing, forming an elastic tier that can be rapidly
instantiated and parallelized across shared resources to
sustain data flow toward the cores. This operational
asymmetry creates a multi-timescale coordination chal-
lenge, where slow-to-deploy, fault-sensitive core compo-
nents must coexist with agile, contention-prone light com-
ponents, demanding a type-aware deployment framework
arXiv:2601.19563v1  [cs.DC]  27 Jan 2026


--- Page 2 ---
Fig. 1: Illustration of FM-based inference application with the MS architecture. Squares denote core MSs, circles
denote light MSs, and different line styles represent different task types of inter-service dependencies.
for reliable, responsive, and cost-eﬀicient FM inference.
This work presents a two-tier FM inference framework
that leverages the core–light MS dichotomy to manage
system uncertainty, with three key contributions:
• For the first time, we design a hybrid MS deploy-
ment strategy for FM edge inference: Slow-startup
core services are statically placed to form a reli-
able system backbone based on long-term workload
statistics, while lightweight services are dynamically
deployed to elastically adapt to real-time system
fluctuations.
• To achieve a forward-looking and fault-tolerant
static core MS placement, we formulate a sparsity-
constrained integer program that co-optimizes for
cost and a statistical QoS score while ensuring
deployment diversity.
• To enable QoS-aware online decision-making under
resource contention, we pioneer the use of effective
capacity theory to link service parallelism with
statistical latency bounds, which is then integrated
into a Lyapunov optimization framework to derive
a low-complexity online algorithm for light MS
deployment.
II. System Model and Problem Formulation
In this section, we develop an MS-based FM inference
framework at the edge network, where the FMs are
decomposed into a collection of functionally distinct core
and light MSs deployed across the network. We consider
a heterogeneous edge network with varying resource
capacities, which consists of edge devices (EDs) and edge
servers (ESs) represented by V and interconnected by
the set of communication links, E. Specifically, resource
capacity of node v is denoted by Rv = [Rv,k]k∈[K], where
K is the number of distinct resource types (e.g., CPU,
RAM, GPU), and [K] = {1, 2, ..., K} is the set of integers
up to any K ∈Z>0.
A. Microservice Specification for FM Inference
An FM inference application architecture is charac-
terized by a fundamental functional asymmetry, which
Fig. 2: MS-based FM inference on a heterogeneous edge
network.
segregates MSs into two categories with distinct opera-
tional profiles, as illustrated in Fig. 2. Specifically, core
MSs (Mcr) are heavyweight and stateful services operated
under strict resource isolation to yield deterministic per-
formance, which form the computational backbone. Light
MSs (Mlt), in contrast, are stateless components with
small footprints and can be quickly redeployed elsewhere;
their performance is stochastic, a direct result of resource
contention incurred by their eﬀicient parallel processing
of concurrent tasks on shared resources.
Formally, we characterize each MS m ∈M = Mlt ∪
Mcr by its resource requirement vector rm = [rm,k]k∈[K],
and the computational workload, am (bits), that must
be executed to produce an output of size bm (bits). Its
processing rate, fm (bits/ms), is a deterministic constant
for a core MS but a random variable for any light MS to
capture the effects of resource contention. These MSs are
orchestrated to execute inference tasks. A task of type n is
represented by a DAG Gn = (Mn, Ln), where Mn ⊆M
denotes the required MSs (|Mn| = In) and Ln the date
dependencies. Consistent with multimodal data fusion,
these graphs typically form inverse-tree structures, where
each node may have multiple incoming but at most one
outgoing edge. starts with an input payload of An and
must meet an end-to-end latency constraint Dn.


--- Page 3 ---
B. Latency Formulation under Network Uncertainty
The system dynamics are driven by several stochastic
events. Users u ∈U stochastically generate tasks of type
n, and we denote the number of such arrivals at time t by
the random variable zt,u,n. These tasks are transmitted to
an associated edge device over a wireless fading channel,
where the signal-to-noise ratio γu is also random. These
external uncertainties, along with the processing rates of
light MS fm, are assumed to be stationary and statistically
independent over time, and their distributions can be
accurately profiled.
An admitted task j, uniquely identified by its origin
(u, n, t), joins the set of active tasks J(t) and is executed
along a routing path Pj = [(vi, mi)]i∈[In] determined by
our strategy. Its end-to-end latency comprises the uplink
transmission delay from user u to its first node, the
inter-node transmission and propagation delay along the
routing path, and the processing delay of each invoked
service, respectively expressed as
τ ul
j =
An
bu log(1 + γu),
(1)
τ tr
j (vi1, vi2) =
bmi1
w(i1,i2)
,
τ pp
j (vi1, vi2) = W(i1,i2)
l
,
(2)
τ pc
j (vi) = ami
fmi
.
(3)
Here, bu denotes the bandwidth allocated to user u,
and bu log(1 + γu) is its achievable uplink rate; w(i1,i2)
and W(i1,i2) are the bandwidth and distance of the link
between nodes vi1 and vi2; l is the propagation speed.
The DAG dependencies necessitate a recursive calcula-
tion for the completion time Tj at any node v, as a service
must wait for all its predecessors:
Tj(v1) = τ ul
j + τ pc
j (v1), v1 ∈Pj,
Tj(vi2) =
max
vi1,vi2∈Pj
vi1∈Vpa
Pj (vi2)
{
Tj(vi1) + τ tr
j (vi1, vi2)
(4)
+τ pp
j (vi1, vi2) + τ pc
j (vi2)
}
,
where Vpa
Pj (v) denotes the set of parent nodes of v in path
Pj. The total end-to-end latency of task j is
T E2E
j
= Tj(vIn), vIn ∈Pj.
(5)
Crucially, T E2E
j
is a complex, stochastic function of the
path j, making it challenging to satisfy the deadline Dn
while simultaneously optimizing for resource costs.
C. Problem Formulation
The deployment of core MSs remains fixed throughout
a finite time horizon T , governed by Xcr ∈N|V|×|Mcr|,
where xcr
v,m denotes the instance number of core MS m
placed on node v. In contrast, light MSs are deployed
dynamically, controlled by two time-varying tensors: Xlt ∈
N|V|×|Mlt|×|T | specifies the instance count xlt
v,m,t, while
Y ∈N|V|×|Mlr|×|T | defines the parallelism level yv,m,t,
the number of concurrent tasks an instance can process,
to manage resource contention.
The objective is to minimize the total system cost over
T . The cost of core MSs includes initial deployment and
ongoing maintenance for each instance:
Ccr(Xcr) =
∑
v∈V
∑
m∈Mcr
(
ccr,dp
m
+
∑
t∈T
ccr,mt
m
)
xcr
v,m,
(6)
where ccr,dp
m
, ccr,mt
m
are the one-time deployment price
and per-slot maintenance price, respectively. For light
MSs, the cost accounts for instantiation, maintenance,
and parallelism:
Clt(Xlt) =
∑
v∈V
∑
m∈Mlt
∑
t∈T
t̸=0
clt,dp
m
max{0, xlt
t,v,m −xlt
t−1,v,m}
+
∑
v∈V
∑
m∈Mlt
∑
t∈T
(clt,mt
m
+ clt,pl
m )xlt
v,m,
(7)
where clt,dp
m
, clt,mt
m
, and clt,pl
m
are the instantiation, per-
slot maintenance, and parallelism cost of light MS m,
respectively.
This optimization is subject to several operational
constraints. Firstly, the total resource consumption at any
node cannot exceed its capacity:
∑
m∈Mcr
rm,kxcr
v,m +
∑
m∈Mlt
rm,kxlt
v,m,t ≤Rv,k,
∀k ∈[K], v ∈V, t ∈T .
(8)
To ensure timeliness, the end-to-end latency of every task
must meet its deadline:
T E2E
j
≤Dn, ∀j ∈J(t), t ∈T .
(9)
Furthermore, the provisioned capacity must be suﬀicient
to handle the incoming workload. Let zv,m,t denote the
number of tasks requiring MS m at node v at time t, which
is determined by the routing paths of all concurrent tasks.
The deployment must adhere to the following capacity
constraints:
xcr
v,m ≥zv,m,t, ∀m ∈Mcr, v ∈V, t ∈T ,
(10)
xlt
v,m,tyv,m,t ≥zv,m,t, ∀m ∈Mlt, v ∈V, t ∈T .
(11)
Finally, all decision variables must be non-negative inte-
gers:
xcr
v,m, xlt
v,m,t, yv,m,t ∈N, ∀v ∈V, m ∈M, t ∈T .
(12)
The overall MS deployment problem can be formulated
as
min
Xcr,Xlt,Y
Ccr + Clt,
s.t.
(8) ∼(12).
(13)
Problem (13) is intractable due to a triad of compounding
challenges. First, stochasticity in arrivals and processing
rates makes the hard QoS constraint (9) analytically
unmanageable, as feasibility itself becomes probabilistic.
Second, the workload zv,m,t creates a circular dependency:
the optimal deployment strategy depends on the antici-
pated task load at each node, yet this load itself is a


--- Page 4 ---
direct consequence of how tasks are routed through the
deployed services. Finally, the problem is high dimensional
integer nonlinear programming and renders exact solution
methods computationally prohibitive, challenging to meet
real-time environments.
III. Network-Aware Microservice Deployment for FM
Edge Inference
To tackle the challenging problem (13), we propose a
two-tier deployment strategy that exploits the functional
asymmetry of the FM inference pipelines.
A. Reliable Core MS Deployment
The static core MS placement must be cost-eﬀicient and
also account for future QoS demands without knowledge
of exact, real-time task arrivals. To achieve this, we
introduce a heuristic QoS score Qv,m to quantify the
expected value of placing an instance of type m on node
v. By incorporating this score into the objective, we can
transform the stochastic deployment into a deterministic
integer program:
min
Xcr
∑
v∈V
∑
m∈Mcr
xcr
v,m(ccr
m −ξQv,m),
s.t.
C1: rm,kxcr
v,m ≤Rv,k, ∀k ∈[K], v ∈V,
C2:
∑
v∈V
˜zv,m ≤
∑
v∈V
xcr
v,m, ∀m ∈Mcr,
C3: xcr
v,m ∈N, ∀v ∈V, m ∈Mcr,
(14)
where, ccr
m = ccr,dp
m
+ ccr,mt
m
and ξ ≥0 is a weight
balancing cost against the QoS measure. The term ˜zv,m
is the average estimate for load zv,m,t. For this static
formulation, the real-time per-node capacity constraint
(10) is relaxed into a global constraint, which ensures
the total long-term capacity meets the total estimated
demand across the network.
The heuristics ˜zv,m and Qv,m are derived from a
mean-value analysis of latency profiles. To obtain these
estimates, we consider a typical task j (identified by its
origin (u, n, t)) requiring MS m ∈Mcr∩Mn at node v ∈V
and partition its estimated end-to-end latency into three
parts: the preceding latency to reach node v, dpr
j (v, m) =
maxv′∈Vpa
Pj,v (v) ¯Tj(v
′); the processing time at the current
node, dcu
j (v, m) = am
fm ; and the succeeding latency for all
subsequent MSs, dsu
j (v, m) = ∑
m′∈Mde
n (m)
am′
¯
fm′ , respec-
tively. Specifically, ¯Tj is calculated via (4) using mean
values for all random variables, Pj,v is the shortest path
from the task’s source to node v where path length is
measured as the sum of network and average computation
latencies, and Mde
n (m) is the set of descendant MSs of m
in Gn.
For the estimated load ˜zv,m, we apportion the mean
task arrival rate to nodes based on an exponential decay
of the preceding latency:
˜zv,m =
∑
u∈U
∑
n∈Nm
e−δdpr
j (v,m)
∑
v′∈V e−δdpr
j (v′,m) E[zu,n,t],
(15)
where Nm is the set of task type requiring MS m. This
model allocates higher load to nodes strategically closer
to users. The QoS score captures placement urgency and
is the product of the estimated load and an “urgency
metric,” ˜dv,m, which quantifies timeliness feasibility with
the capped ratio of the remaining time budget to the
estimated future processing time:
˜dv,m =
∑
u∈U
∑
n∈N
max
{
Dn −dpr
j (v, m) −dj(v, m)
dsu
j (v, m)
, C1
}
,
Qv,m = ˜zv,m ˜dv,m,
where C1 is a constant. A high QoS score thus signifies a
strategically valuable placement expected to serve a high
volume of tasks that can comfortably meet their deadlines.
While (14) is a standard integer program solvable by off-
the-shelf tools, solvers tend to yield sparse solutions that
consolidate all instances of an MS onto a single node and
lead to a single-point vulnerability. To enhance deploy-
ment diversity, we introduce a binary auxiliary variable
ˆxv,m for each xcr
v,m and add the following constraints:
C4: xcr
v,m ≤C2ˆxv,m, ∀v ∈V, m ∈Mcr,
C5: xcr
v,m ≥C3ˆxv,m, ∀v ∈V, m ∈Mcr,
C6:
∑
v∈V
∑
m∈Mcr
ˆxv,m ≥κ,
(16)
where C2 and C3 are suﬀiciently large and small positive
constants, respectively. C4 and C5 jointly force ˆxv,m = 1
if and only if xcr
v,m > 0. C6 imposes a minimum of κ non-
zero deployments across the network, thereby preventing
over-centralization. The parameter κ tunes the trade-off
between the objective value of (14) and system reliability.
B. Dynamic Light MS Deployment
With the static placement of core MSs fixed, the
remaining node capacities are dedicated to the dynamic
deployment of light MSs, which constitutes an online
stochastic optimization problem formulated as:
min
Xlt
t ,Yt
Clt,
s.t.
C1:
∑
m∈Mlt
rm,kxlt
v,m,t ≤Rlt
v,k, ∀v ∈V, k ∈[K],
C2: T E2E
j
≤Dn, ∀j ∈J(t), t ∈T ,
C3: xlt
v,m,tyv,m,t ≤zv,m,t, ∀v ∈V, m ∈Mlt, t ∈T ,
C4: xlt
v,m,t, yv,m,t ∈N, ∀v ∈V, m ∈Mlt, t ∈T ,
(17)
where Rlt
v,k = Rv,k −∑
m∈Mcr rm,kxcr
v,m. The instance
counts Xlt
t and parallelism levels Yt must be decided at
each slot based solely on the current system state, without
knowledge of future arrivals, to optimize long-term cost.


--- Page 5 ---
To address (17), we employ the Lyapunov drift-plus-
penalty framework [10], a powerful tool for online stochas-
tic control. The core idea is to transform C2 into a queue
stability condition. Specifically, a virtual queue Hj(t) is
introduced for each task j to track its cumulative deadline
violations:
Hj(t+1) = max {Hj(t) + Tj(t) −Dn, ζ} , ∀j ∈J(t), (18)
where Tj(t) is latency experienced by task j under
the decisions made by time t. Different from [10], we
introduce a floor ζ > 0 to prevent the virtual queue from
collapsing to zero, thereby keeping the controller proac-
tively incentivized to maintain low latency rather than
reacting only once latency has already accumulated. The
framework then seeks to minimize the following drift-plus-
penalty expression at each slot, subject to instantaneous
operational constraints C1, C3, and C4:
L = ηClt +
∑
j∈J(t)
ϕjHj(t)[Tj(t) −Dn], vi ∈Pj,
(19)
where η ≥0 is a tunable parameter controlling cost-
latency trade-off, and ϕj is a task-specific priority weight.
A key challenge in minimizing (19) lies in the stochastic
nature of the latency term Tj(vi), which depends non-
linearly on the deployment decisions {Xlt
t , Yt}. While
network latency is deterministic once a route is chosen, the
processing times are uncertain due to resource contention.
We adopt the effective capacity model [11] to bridge this
gap. Rooted in large deviations theory, this framework an-
alytically characterizes the maximum constant workload
arrival rate that a stochastic server can sustain under
a prescribed latency violation probability. The effective
capacity of a light MS m is given by:
Ec
m(θ) = −lim
t→∞
ln E[e−θFm(0,t)]
θt
,
(20)
where F(0, t) = ∑t−1
τ=0 fm(t) is the cumulative service
process. The QoS exponent θ > 0 directly links the
effective capacity to the tail probability of the latency
distribution:
P{d > D} ≈
Ec
m(θ)
E[fm(t)]e−θEc
m(θ)D.
(21)
This relationship enables the pre-calculation of a de-
terministic mapping d = gm,ϵ(y) between the chosen
parallelism level y and the resulting processing time d that
satisfies the violation probability ϵ, thereby facilitating
eﬀicient online adaptation.
Even with this transformation, the per-slot optimiza-
tion problem remains a complex non-linear integer pro-
gram that cannot be solved optimally within the tight
time constraints of online operation. We therefore propose
a low-complexity greedy heuristic, detailed in Algorithm
1, which iteratively makes the single deployment decision
that yields the largest marginal decrease in (19).
At the beginning of each time slot, we first determine
the set of busy instances from the previous slot, xlt,bs
v,m,t−1,
which are still processing ongoing tasks. Then, in a greedy
loop, for all tasks in the waiting queue Jqu(t), we calculate
the potential change in the objective, ∆v,mL, for every
feasible incremental deployment (adding one instance of
m on node v). This involves routing each task to the
instance that minimizes its next-hop latency, which is the
sum of network delay and the QoS-aware processing delay
from our mapping function:
∆Tj(v, m) = τ tr
j (vj, v) + τ pp
j (vj, v) + gm,ϵ(yv,m,t + 1)(v),
where vj the node currently hosting the last completed
service for task j. The algorithm then implements the
deployment with the most negative ∆v,mL, updates the
system state, and repeats until no further cost-effective
deployments can be made. Assuming the greedy selection
loop runs M times, the per-slot complexity is O(M(1 +
|Jqu(t)||V||Mlt|)).
Algorithm 1: Greedy Online Light MS Deployment
1 for each t ∈T do
2
xlt
v,m,t ←xlt,bs
v,m,t−1 and yv,m,t ←0 for all (v, m);
3
Observe queue Hj(t) for all j ∈Jqu(t);
4
while True do
5
(v∗, m∗) ←
arg min
v∈V, m∈Mlt ∆v,mL;
6
if ∆v∗,m∗L < 0 then
7
xlt
v∗,m∗,t ←xlt
v∗,m∗,t + 1;
8
else if ∆v∗,m∗L = ∞then
9
break;
10
else
11
∆v∗,m∗L ←∞;
12
if Rv∗,k(t) > rm∗,k, ∀k ∈[K] then
13
∆v∗,m∗L ←ηclt
m∗;
14
for each j ∈Jqu(t) do
15
(vj, mj) ←
arg min
v∈V, m∈Mlt ∆Tj(v, m);
16
yvj,mj,t ←yvj,mj,t + 1;
17
∆v∗,m∗L ←
∆v∗,m∗L + ϕjHj(t)∆Tj(vj, mj);
18
else
19
∆v∗,m∗L ←∞;
IV. Performance Evaluation
We evaluate an FM-based edge inference application
comprising 4 task types, 6 core MSs, and 9 light MSs,
with dependencies shown in Fig. 1, deployed on an edge
network (Fig. 2). Key parameters are listed in TABLE I;
values for each run are sampled from predefined ranges.
Our proposal derives the processing latency of light MSs
from the effective capacity models configured with a
violation probability ϵ = 0.2.
We evaluate our proposal against three baseline meth-
ods:


--- Page 6 ---
TABLE I: Key Simulation Parameters
rm,k/Rv,k(CPU;RAM;GPU;VRAM)
am(MB)
bm(MB)
fm(MB/ms)
cdp
m , cmt
m , cpl
m
Core MS
[2,16];[1,4];[4,32];[4,32]
[2,16]
[0.1,1]
[8,32]
20.0; 4.0; 0.0
Light MS
[0.5,2];[0,0.5];[0.25,4];[0,1]
[0.5,2]
[0.25,1.5]
Gamma([1,2],[1,20])
4.0; 1.0; 0.5
ED
[1,64];[1,32];[0,64];[0,64]
zu,n,t(/ms)
Dn(ms)
γu(Gbs)
An(MB)
w(MB/ms)
ES
[128,256];[64,128];[1024,2048];[256,512]
Poisson([0.15,1.5])
[50,100]
Nakagami([1.5,3],[0.5,1])
[0.5,4]
[0.1,1.0]
Fig. 3: Violin-plot comparison of on-time task completion
rate and total system cost across four deployment strate-
gies.
-2
0
2
4
6
8
System cost (k)
1.0×
1.5×
2.0×
System load
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
Task completion rate
PropAvg: Completion
PropAvg: OnTime
Ours: Completion
Ours: OnTime
PropAvg: Cost
Ours: Cost
Fig. 4: Comparison of the proposed framework and the
PropAvg ablation under escalating system loads.
• LBRR: Services are allocated to the least-loaded
nodes. Incoming tasks are then scheduled across
available instances using a Round-Robin policy.
• GA: A metaheuristic optimizer that seeks a near-
optimal deployment by minimizing a fitness function
combining total system cost and QoS violation rates.
• PropAvg: A direct ablation of our framework. It
employs the same two-tier logic as our proposal but
replaces the Effective Capacity model with a simpler
mean-value estimation for processing delays.
Fig. 3 visualizes, via violin plots, the distributions of
the on-time task completion rate and the total system cost
across four deployment strategies. Narrow, concentrated
violins indicate stable performance, whereas wider ones
suggest inconsistency. An effective deployment should
exhibit a compact distribution with high completion rates
and moderate costs. Our proposal exhibits a sharply
peaked and compact distribution centered around a high
completion rate (above 84%), indicating consistently re-
liable QoS across trials. Its cost distribution is similarly
concentrated around a justifiable level, demonstrating sta-
ble and eﬀicient resource utilization. The LBRR method,
while simple, yields results in a low-cost, low-performance
regime, an expected outcome given its deadline-agnostic
nature. In contrast, GA results exhibit a widely dis-
tributed distribution for both metrics, reflecting high
variability in the results. This instability suggests that
the metaheuristic search struggles to converge in the
vast and stochastic optimization space, leading to under-
provisioning and inconsistent performance. The PropAvg
ablation variant produces slightly lower costs than our
proposal but a broader and skewed completion-rate distri-
bution with a long lower tail. This pattern reveals its core
limitation: mean-based estimation fails to capture tail-
latency effects, reducing cost marginally at the expense
of intermittent but severe QoS degradation.
Fig. 4 compares our proposal and the PropAvg ab-
lation under escalating system loads (1.0×, 1.5×, 2.0×
multipliers to the mean of the task arrival distribution).
Bars with error bars (left axis) show total and on-time
completion rates, and markers (right axis) indicate system
cost. Higher and closer completion rates imply stronger
QoS resilience, while lower, steadier cost curves indicate
better scalability. As system load increases, both methods
show declining completion performance and rising costs,
with growing variance and steeper trends. While PropAvg
maintains a growing cost advantage, its completion rates
rapidly deteriorate as the system becomes saturated.
Moreover, the gap between its total and on-time comple-
tion rates widens significantly, reflecting frequent deadline
violations even for finished tasks. In contrast, our proposal
maintains both high completion and on-time rates, with
only marginal gap expansion and controlled cost scaling.
These results demonstrate that average-based methods
like PropAvg are brittle under stress, while our proposal
achieves robust, time-consistent QoS and cost-eﬀicient
scalability even in heavy-load conditions.
V. Conclusions
This paper addressed QoS-aware and cost-eﬀicient MS
deployment for FM inference at the resource-contended
edge under uncertainties in task arrivals and wireless


--- Page 7 ---
channels. Our solution is a hybrid static-dynamic frame-
work founded on the intrinsic functional asymmetry
between core and light MSs. The static tier provides a
reliable computational backbone by co-optimizing long-
term operational cost, statistical QoS, and deployment
diversity of core services. The dynamic tier complements
this with an online control mechanism that governs light
services under stochastic conditions, regulating latency
violations through effective-capacity-based Lyapunov op-
timization. Numerical evaluations confirmed that the
proposed framework consistently achieves superior cost-
QoS balance and resilience under intensifying loads. This
marks a critical advance for robust edge AI, with fu-
ture work poised to explore learning-based solvers and
accuracy-aware optimization.
References
[1] J. Du et al., “Distributed foundation models for multi-modal
learning in 6G wireless networks,” IEEE Wireless Commun.,
vol. 31, no. 3, pp. 20–30, Jun. 2024.
[2] Z. Wang et al., “Edge large AI models: Revolutionizing 6G
networks,” IEEE Commun. Mag., vol. 63, no. 10, pp. 36–42,
Oct. 2025.
[3] I. Nadareishvili et al., Microservice architecture: aligning prin-
ciples, practices, and culture.
O’Reilly Media, Inc., Jul. 2016.
[4] C. Wang et al., “Dependency-aware microservice deployment
for edge computing: A deep reinforcement learning approach
with network representation,” IEEE Trans. Mobile Comput.,
vol. 23, no. 12, pp. 14 737–14 753, Dec. 2024.
[5] W. Lv et al., “Microservice deployment in edge computing based
on deep Q learning,” IEEE Trans. Parallel Distrib. Syst., vol. 33,
no. 11, pp. 2968–2978, Nov. 2022.
[6] W. Lv et al., “Graph-reinforcement-learning-based dependency-
aware microservice deployment in edge computing,” IEEE
Internet Things J., vol. 11, no. 1, pp. 1604–1615, Jan. 2024.
[7] Z. Yu et al., “Microservice deployment in space computing
power networks via robust reinforcement learning,” IEEE
Trans. Mobile Comput., 2025, early access.
[8] S. Luo et al., “An in-depth study of microservice call graph
and runtime performance,” IEEE Trans. Parallel Distrib. Syst.,
vol. 33, no. 12, pp. 3901–3914, Dec. 2022.
[9] K. Fu et al., “QoS-aware and resource eﬀicient microservice de-
ployment in cloud-edge continuum,” in Proc. IEEE IPDPS’21,
vitual, May 2021.
[10] Z. Wang et al., “Federated fine-tuning for pre-trained foun-
dation models over wireless networks,” IEEE Trans. Wireless
Commun., vol. 24, no. 4, pp. 3450–3464, Apr. 2025.
[11] M. Amjad, L. Musavian, and M. H. Rehmani, “Effective
capacity in wireless networks: A comprehensive survey,” IEEE
Commun. Surv. Tutor., vol. 21, no. 4, pp. 3007–3038, Jul. 2019.
