--- Page 1 ---
Can Reasoning Models Reason about Hardware?
An Agentic HLS Perspective
Luca Collini Graduate Student Member, IEEE, Andrew Hennessee, Ramesh Karri Fellow, IEEE,
Siddharth Garg Member, IEEE
Abstract—Recent Large Language Models (LLMs) such as
OpenAI o3-mini and DeepSeek-R1 use enhanced reasoning
through Chain-of-Thought (CoT). Their potential in hardware
design, which relies on expert-driven iterative optimization,
remains unexplored. This paper investigates whether reasoning
LLMs can address challenges in High-Level Synthesis (HLS) de-
sign space exploration and optimization. During HLS, engineers
manually define pragmas/directives to balance performance and
resource constraints. We propose an LLM-based optimization
agentic framework that automatically restructures code, inserts
pragmas, and identifies optimal design points via feedback
from HLS tools and access to integer-linear programming (ILP)
solvers. Experiments compare reasoning models against con-
ventional LLMs on benchmarks using success rate, efficiency,
and design quality (area/latency) metrics, and provide the first-
ever glimpse into the CoTs produced by a powerful open-source
reasoning model like DeepSeek-R1.
Index Terms—Agentic EDA, HLS, Design Space Exploration
I. INTRODUCTION
Large Language Models (LLMs) are rapidly improving in
their capabilities. Recently introduced models, notably Ope-
nAI o3-mini and DeepSeek-R1, leverage Chain-of-Thought
(CoT) mechanisms to “reason” about user prompts, achieving
state-of-the-art results in challenging problem domains like
mathematics, coding, and scientific benchmarks [1]. CoT is
a prompt engineering technique that encourages LLMs to
break down problems into intermediate steps. These models
undergo specialized training that emphasizes the development
of reasoning skills. For instance, DeepSeek’s R1 model has
been trained on extensive datasets that are rich in mathematical
and logical content. Some models incorporate system-level
prompts or architectural features that guide their behavior to
further enhance autonomous reasoning.
This brings us to the motivating question: Can reasoning
models (e.g., DeepSeek-R1, o3-mini) reason about hardware
design, and how do they compare to non-reasoning models
(e.g.., DeepSeek-V3)? Despite access to powerful electronic
design automation (EDA) tools, many design tasks rely on
engineer expertise. For instance, system architects reason
about the performance of a design consisting of multiple IP
blocks/modules, and allocate resources to optimize perfor-
mance based on intuition, experience, or using optimizers like
integer linear programming (ILP) solvers.
In this paper, we use high-level synthesis (HLS) to explore
the reasoning capabilities of state-of-art LLMs. HLS starts
All authors are with the NYU Tandon School of Engineering, Brooklyn,
NY 11201 (e-mail: {lc4976, ajh9498@nyu.edu, rkarri, sg175}@nyu.edu).
with a C/C++ specification and uses pragmas and directives to
express design optimizations. While translation from a C/C++
design with pragmas to hardware is automated, HLS does not
tackle the task of identifying pragmas and directives for the
best performance under a resource constraint. System archi-
tects do this task, exploring solutions until they are satisfied
with the result. Design space exploration tools can yield good
results but require orders of magnitude more samples than
those required by an expert.
This paper is the first to explore whether recent reasoning
models can aid or even replace system architects. To this end,
our novel contributions are:
• we open source an automated, agent-based HLS opti-
mization flow that automatically rewrites code, inserts
pragmas and performs full-system optimization to mini-
mize latency within a total area constraint.
• Our results on reasoning and baseline models demonstrate
that while the proposed agentic workflow improves on the
current state-of-the-art (sometimes even matching human
performance), reasoning models are comparable with
non-reasoning counterparts on hardware optimization.
• We provide the first glimpse into the chains-of-thought
(CoT) produced by a reasoning model (DeepSeek-R1)
in the hardware context, revealing both the promise and
key gaps in its reasoning capabilities for hardware tasks.
Hardware optimization remains a challenging benchmark
for even the most powerful LLMs.
We begin by covering some necessary background.
II. BACKGROUND
A. High-Level Synthesis (HLS)
HLS tools transform high-level code (e.g., C/C++) into
a register-transfer level (RTL) implementation. Pragmas and
directives allow designers to guide and explore microarchi-
tectural trade-offs (e.g., parallelism, resource sharing) directly
within the high-level code, balancing performance, area, and
power constraints. The design space of possible pragma
optimization grows exponentially as the design complexity
increases. Designers use their knowledge to navigate this
complex design space and find solutions that satisfy their
constraints. Researchers have proposed numerous solutions to
perform design space exploration [2], [3], [4] using both clas-
sic and ML-based DSE approaches. These solutions require a
high number of syntheses and are often domain-specific.
Another approach is to identify possible solutions for the
subkernels (e.g., by spanning the unrolling factors) and use
arXiv:2503.12721v2  [cs.AI]  14 Apr 2025


--- Page 2 ---
optimizers like integer linear programming (ILP) solvers to
identify the best subkernel composition [5], [6], [7]. In this
work, we explore the capabilities of reasoning models to
approach this problem, evaluating the problem formulation and
the process (number and quality of formulated ILP problems,
number of syntheses, PPA, cost, and time) to explore whether
generative models follow a process similar to an engineer.
B. Large Language Models (LLMs)
LLMs are trained on massive amounts of text data and excel
at tasks such as code generation and translation, particularly
in languages such as C, C++, and Python [8]. However,
their performance declines when working with Hardware
Description Languages (HDLs), such as Verilog or VHDL,
due to the limited amount of training data available in those
languages [9]. Recent LLMs, such as OpenAI o3-mini and
DeepSeek-R1, have reasoning capabilities [1]. Unlike tradi-
tional LLMs, which are trained using supervised learning on
massive datasets, reasoning-enhanced LLMs integrate Chain-
of-Thought prompting, supervised fine-tuning on reasoning
tasks, and reinforcement learning to improve logical reason-
ing. During inference, rather than providing a final answer,
these LLMs generate intermediate reasoning steps, mimicking
human-like problem-solving. This approach improves perfor-
mance, particularly in reasoning tasks such as coding, mathe-
matics, science, and logical reasoning [10].
LLM-based frameworks have been proposed to implement
Verilog designs [11], [12], [13]. The design complexity is lim-
ited by the LLMs’ capabilities in Verilog generation. For this
reason, LLM frameworks for HLS have been proposed [14],
[15], [16], [17], as LLMs perform better with high-level
languages. The potential of new-generation reasoning models
has not yet been explored in this domain. We propose an
agent-based HLS optimization framework and explore whether
LLMs can reason about design constraints, such as latency and
area while refining solutions to meet performance targets.
III. HLS OPTIMIZATION AGENT
We frame two tasks to evaluate LLM reasoning: ➊Kernel-
level pragma insertion and code transformations; ➋Full
system optimization. We propose an agentic flow (as shown
in Figure 1) to perform these tasks, giving the LLM access to
tools (code inspection, system synthesis, and an ILP solver)
to perform these tasks.
a) Kernel-Level Pragma Insertion and Code Transfor-
mations: The goal in Task ➊is to identify different imple-
mentations spanning a range of area-performance tradeoffs
for each kernel (C/C++ function) in the design via code
transformations and pragma insertion. This is accomplished
with an LLM in a feedback loop—in each iteration, the
LLM is given the area, latency, and throughput result of the
base implementation and the solutions identified so far and
prompted to provide a new solution. We compile and run a
functional test to ensure that the LLM does not inadvertently
change the function (which was proven effective in [11], [14])
and use error feedback to prompt the LLM for bug fixes. Our
Fig. 1: HLS agentic flow with two optimization tasks.
agentic approach operates hierarchically, starting with the leaf-
level functions and moving up to the top module. As shown
in [14], working on multiple functions at a time often leads
to hallucinations and errors. This bottom-up approach allows
the LLM to focus on the function at hand. When traversing
up the kernel hierarchy, we need to choose an implementation
for each child function (from a previous iteration). We use a
greedy approach to select the lowest latency option for each
child. This approach does not guarantee an optimal solution,
and the final solution might not meet the area target. For this
reason, we keep every solution from task ➊for each function,
which will be the input for task ➋.
b) Full System Optimization: Task ➋has the goal of
optimizing the overall application, picking a configuration for
each kernel in the design to minimize latency while keeping
the area within a target constraint. These tasks are typically
solved by system architects via trial-and-error, heuristic ap-
proaches or, as has been shown in prior work, using a formal
optimizer like an ILP solver [5], [6], [7]. We implement an
LLM agent that can take one of four actions. 1) Inspect Kernel,
i.e., view its code or that of submodules (needed to minimize
context length and token counts); 2) Solve an Integer Linear
Programming (ILP) Problem; 3) Synthesize Solution, i.e., call
the HLS tool with specified pragma values; 4) Select Solution,
i.e., use the solution returned by the ILP solver. For the ILP
problem formulation, we prompt the LLM to provide a Python
script using the Google OR-Tools library. At each iteration,
the LLM provides an action. We parse the response, call the


--- Page 3 ---
respective tool, and report the outcome to the LLM agent.
The task ends when the agent selects a solution. Crucially,
system optimizations require an understanding of hardware;
for example, the latency of parallel modules is the max of
the two latencies and not their sum as would be the case in
sequential software execution. This plays an important role in
being able to formulate accurate ILP formulations.
IV. EXPERIMENTAL EVALUATION
A. Experimental Setup
We implemented the agentic flow in Figure 1 using Python.
We used Catapult HLS and target nandgate45 library. The two
tasks are implemented as two separate conversations with the
LLM. This reduces token use and avoids exceeding the context
size in the second task. The overall flow is implemented in
fifteen hundred lines of Python and is fully automated. Code,
benchmarks, and raw results are provided here.
Below is a shortened version of our system prompt; our
system and user prompt templates are available in our source
code. To avoid biasing the models, we did not provide addi-
tional instructions or examples of how a designer might ap-
proach the task (e.g., inspect function dependencies, formulate
an ILP model, evaluate results).
HLS Agent System Prompt
You are an HLS Optimization Agent tasked with
optimizing a C application accelerated using
High-Level Synthesis.
Your goal is to find the best combination of
function options that minimize latency while
keeping the total constraint as close as possible
to a target value.
At every iteration, you have four options:
<options>
Only reply with one of the five options following
the format provided.
Our choices of LLMs are as follows: DeepSeek-V3 serves
as our baseline LLM without reasoning. DeepSeek-R1 and
OpenAI o3-mini are the reasoning models. o3-mini is the
latest reasoning model released by OpenAI, offering a faster
response time and a lower cost compared to the previous o1
reasoning model. DeepSeek-R1 is a newly released reasoning
model that provides CoT reasoning tokens, which o3-mini
does not. This feature allows for a deep dive into the reasoning
behind the implemented agent actions (Section IV-D).
Table I presents the benchmarks and their characteristics.
We created six synthetic benchmarks that implement different
control and data dependency graphs including compositions
of sequential and parallel modules to evaluate whether LLMs
can reason about latency and area from a hardware perspective.
These benchmarks also mitigate concerns about data contam-
ination as they could not have been part of any training data.
Additionally, we selected six real-world benchmarks.
We ran the flow ten times for each benchmark, setting the
number of optimized kernel options for task ➊to five. For
each benchmark, we select an area target 10% below the initial
solution, allowing the LLM Agent to find a lower-area solution
and then determine the minimum latency at that area.
TABLE I: Benchmarks description and characterization.
#Kernels/
# Lines
# Operations
Benchmark Description
#Calls Total Min. Max. Total Min. Max.
SYN1
2 sequential functions
3/3
30
8
11
3
0
2
SYN2
2 parallel functions
3/3
31
9
11
4
1
2
SYN3
2 parallel pairs of
sequential functions
4/5
50
11
15
6
1
3
SYN4
2 parallel functions
followed by 2
sequential functions
4/5
50
11
15
8
1
3
SYN5
SYN3 in a for loop
4/5
54
11
17
8
1
3
SYN6
SYN4 in a for loop
4/5
54
11
17
10
1
4
AES
AES Cipher core
6/11
101
5
26
77
2
29
Present
Present Cipher core
6/10
96
10
24
74
4
26
SHA256
SHA256 core
2/2
70
17
53
127
11
116
KMP
KMP algorithm for
pattern searching
3/3
62
8
28
17
0
10
FIR+IIR
Runs FIR and IIR
in parallel on given
input stream
5/9
48
7
11
12
1
4
NW
Needleman-Wunsch
algorithm for sequence
alignment
4/5
109
6
52
73
5
37
B. Implementation Challenges
We encountered several challenges while implementing our
proposed framework. Reasoning and non-reasoning models
respond very differently to our prompts. Our system prompt
explicitly requests that explanations not be provided. Only the
two reasoning models adhered to this instruction, replying
with only the chosen action. In contrast, the chat model
(DeepSeek-V3) included some explanation either before or
after the chosen action. This made it more challenging to
parse the agent’s choice but provided insights into the chat
model’s decision-making process. We believe that because
the reasoning models approach the problem in the Chain-of-
Thought and provide the final answer only at the end, they
comply with the required output structure while still following
their statistical reasoning process. On the other hand, chat
models do not have separate reasoning and output tokens. As
a result, the explanations are, in some sense, their problem-
solving process leading to the final answer.
The official DeepSeek API experienced significant down-
time during our experiments, so we additionally used a third-
party provider for some of the DeepSeek-R1 experiments.
Since we did not observe differences in the quality of results,
we agglomerated the runs from first- and third-party APIs.
C. Experimental Results
We now present our experimental results, highlighting key
observations from our empirical evaluations.
a) Reasoning Models have Higher Success Rates But
at Higher Cost: Figure 2 presents the success rate, average
cost, and average runtime for the twelve benchmarks across
the ten runs. Here, success means that the flow reaches the
end, providing a result, regardless of whether the result meets
the target area or not.
DeepSeek-V3 has the lowest success rate, performing its
worst on AES with a success rate of 40%. However, it still
achieved a 100% success rate on multiple benchmarks while


--- Page 4 ---
SYN 1 SYN 2 SYN 3 SYN 4 SYN 5 SYN 6
AES
SHA PRESENT KMP FIR+IIR
NW
Benchmarks
0
20
40
60
80
100
Success Rate (%)
DeepSeek-V3
DeepSeek-R1
o3-mini
SYN 1 SYN 2 SYN 3 SYN 4 SYN 5 SYN 6
AES
SHA PRESENT KMP FIR+IIR
NW
Benchmarks
0.0
0.1
0.2
0.3
0.4
0.5
Cost [$]
SYN 1 SYN 2 SYN 3 SYN 4 SYN 5 SYN 6
AES
SHA PRESENT KMP FIR+IIR
NW
Benchmarks
0
20
40
60
80
100
120
140
Time [min]
Fig. 2: Success rate, cost, and runtime to run comparison
between DeepSeek-V3, DeepSeek-R1, and o3-mini.
maintaining the lowest cost (by at least a 4× margin) and a
runtime often comparable to o3-mini. DeepSeek-V3 exhibits
two distinct failure modes. The first, during task ➊, was due
to modifying the kernel’s functionality and being unable to
repair it. The second, during task ➋, resulted from reaching
the model’s context size limit. o3-mini is the most expensive
and fastest model. Its success rate is fairly consistent, and
failures occur only sporadically due to unintended function-
ality modifications that it could not fix. DeepSeek-R1 is
the slowest model, but it achieves the highest success rate,
failing only once due to insertion of bugs during task ➊.
The cost was calculated by multiplying the number of input
and output tokens by the respective API costs. We used the
full price provided without considering cached tokens or off-
peak discounts (which would favor the lower-cost models). At
the time of writing, the cost per million input/output tokens
is $0.27/$1.10 for DeepSeek-V3, $0.55/$2.19 for DeepSeek-
R1, and $0.55/$4.40 for o3-mini. Additionally, the DeepSeek
models are open source, allowing for trade-offs between cost
and performance depending on the chosen deployment options.
b) Synthesized Area-Latency Results are Comparable
Across Models: Figure 3 presents synthesis results for all
models and benchmarks. The bars represent the average val-
ues, with minimum and maximum ranges annotated, and blue
dashed lines show the target area for each benchmark. No
model consistently outperforms all others—in fact, the non-
reasonig DeepSeek-V3 model surpasses the reasoning ones
in a few benchmarks. In three benchmarks (SYN2, SYN4,
and Present) no model achieved the target area, although a
human engineer can be reasonably expected to do so by simply
picking the lowest area implementation of each module.
Table II presents the number of times each model meets
the target area for each benchmark. It also reports the score
SYN 1 SYN 2 SYN 3 SYN 4 SYN 5 SYN 6
AES
SHA PRESENT KMP FIR+IIR
NW
Benchmarks
103
104
105
Area [um2]
SYN 1 SYN 2 SYN 3 SYN 4 SYN 5 SYN 6
AES
SHA PRESENT KMP FIR+IIR
NW
Benchmarks
100
101
102
103
104
Latency [cycles]
DeepSeek-V3
DeepSeek-R1
o3-mini
Fig. 3: Synthesis result comparison between DeepSeek-V3,
DeepSeek-R1, and o3-mini. The vertical ranges represent the
min/max ranges. Log scales. The blue dash lines indicate the
target area used for the benchmark.
TABLE II: Performance summary. A and L are the obtained
area and latency. tgt and min refer to target and minimum.
A ≤Atgt
A ≤Atgt &&
L == Lmin
A > Atgt &&
A == Amin
Benchmark
V3
R1
o3-m
V3
R1
o3-m
V3
R1
o3-m
SYN1
8
9
9
8
9
9
-
-
-
SYN2
0
0
0
-
-
-
0
0
5
SYN3
0
3
3
0
0
3
-
-
-
SYN4
0
0
0
-
-
-
0
0
1
SYN5
7
10
7
0
4
3
-
-
-
SYN6
5
5
1
5
0
0
-
-
-
AES
4
10
9
4
0
0
-
-
-
SHA256
4
0
5
3
0
1
-
-
-
Present
0
0
0
-
-
-
0
3
0
KMP
9
10
10
1
0
0
-
-
-
FIR+IIR
5
10
2
5
10
2
-
-
-
NW
2
0
0
2
0
0
-
-
-
Total
44
57
46
28
23
18
0
3
6
achieved for each benchmark, which is determined based on
two scenarios: 1) A point is awarded each time a model meets
the area target and achieves the lowest latency among the runs
that meet the area target; 2) When no model meets the target
area, a point is awarded each time a model achieves the lowest
area across the results. From Table II, no model outperforms
all others. DeepSeek-R1 has the highest number of runs that
meet the area target, DeepSeek-V3 earns the most points in
scenario 1, and o3-mini scores the most points in scenario 2.
c) Performance Breakdowns by Task: Our evaluations
thus far do not distinguish whether the achieved result quality
is the result of a well-performed task ➊or task ➋. If the
model selects a strong (weak) set of optimized kernels, task ➋
becomes easier (harder). Figure 4 shows the latency and area
for each AES subkernel optimized in task ➊by each model.
Interestingly, we find that the non-reasoning model produced
better designs for Cipher, the largest submodule in AES.
To better evaluate the models on task ➋, we reran it in
isolation for the AES benchmark, starting from the same
subkernel implementations for all models. In this setting,
DeepSeek-R1 performed the best, achieving an area of 3596


--- Page 5 ---
0
2000
4000
6000
8000
10000
Area
0
200
400
600
800
1000
Latency
AddRoundKey
SubBytes
Cipher
ShiftRows
xtime
MixColumns
o3-mini
deepseek-chat
deepseek-reasoner
Fig. 4: Solutions for AES sub-kernels for each model.
µm2 (the area target was 3800µm2) and a latency of 736
cycles. DeepSeek-V3’s best result reached an area of 3707µm2
and a latency of 763 cycles. o3-mini’s best result had an area
of 4339µm2 (failing to meet the target area) and a latency
of 638 cycles. These results significantly differ from the best
overall results, underscoring the impact of task ➊on achieving
strong performance in task ➋.
d) All Models Struggle to Formulate ILPs Correctly,
but Show Promise: Focusing on task ➋, we evaluate the
models’ ability to formulate integer linear programming (ILP)
problems for optimizing subkernel selection. An ILP problem
formulation for this task consists of three parts:
• Defining 1-hot binary variables for each subkernel option;
• Formulating the area and latency models;
• Formulating the optimization objective and constraints.
All models correctly define 1-hot binary variables, but differ in
how they treat objectives and constraints. All models struggle
to model latency.
DeepSeek-V3 constrains the area while minimizing latency.
Interestingly, DeepSeek-R1 and o3-mini opt for a multi-
objective formulation: min(α · latency + |area −target|),
using a Lagrange multiplier for area instead of explicitly
constraining it. Setting the area target as a constraint causes the
solver to fail if the target is unreachable. When this occurs, the
model increases the limit and retries until a solution is found.
This approach is inefficient and often leads to DeepSeek-V3
exhausting its context tokens and failing.
The latency model formulation task proved challenging.
Kernel latency depends on the structure of the data flow graph,
and parallelism can be inferred by looking at dependencies in
the code. Table III presents data flow graphs, correct latency
models, and the latency models generated by the LLMs.
The results show that the models struggle to infer par-
allelism in the kernels. The only correct ILP formulations
occur in cases where no parallelism exists between subkernels,
such as in AES, or by chance, as in Needleman-Wunsch. By
default, the models either consider only ftop (the top-level
function latency), or sum all latencies, sometimes factoring
in the number of function calls.
e) Non-reasoning Model Call more Actions: Figure 5
presents the average number of actions taken by each model
across all benchmarks.
From the plots, DeepSeek-V3 has the highest average
number of actions, while reasoning models follow a direct
approach, potentially because they rely more on “learned”
behavior. As an example, DeepSeek-R1 never opted to inspect
TABLE III: Latency formulations by models for benchmarks.
For AES– Add Key; Shift Row; Substitution Box; Mix Column.
For NeedWun– Fill Matrix; TraceBack; Reverse String.
Data Flow Graph
Latency Formulations
Models
fA
fB
fC
ftop
SYN5
fA
+
➀
ftop
R1
➁
ftop + fA + fB + fC
R1, o3-m
➂
ftop + 2fA + fB + fC
R1
➃
max(ftop, fA, fB, fC)
o3-m
Correct: ftop + max(fA + fC, fB + fA)
fA
fB
fC
fC
ftop
SYN6
➀
ftop
R1, o3-m
➁
ftop + fA + fB + fC
V3, R1
➂
ftop + fA + fB + 2fC
R1
➃
ftop + max(fA, fB, fC)
R1, o3-m
Correct: ftop + max(fA, fB) + 2fC
SB
AK
SR
ftop
AES
MC
AK
SB
SR
AK
x9
➀ftop + AK + SB + SR + MC
R1, o3-m
➁ftop + 3AK + SB + SR + MC R1, o3-m
Correct: ftop + AK + SB + SR + MC *
TB
FM
RS
ftop
NEEDWUN
RS
➀
ftop
o3-m
➁
FM + TB
R1, o3-m
➂
ftop + FM + TB + RS
V3, R1
Correct: ftop + FM + TB + RS
* This is a good approximation.
DeepSeek-V3
DeepSeek-R1
o3-mini
Models
0
1
2
3
4
5
6
Calls
Synthesis
Python
Inspect
Fig. 5: Average # actions across all benchmarks for each LLM.
code (note that it does have access to various latency-area
points for each submodule). However, not inspecting code
limits its ability to accurately model latency in ILP problems
since it cannot identify dependencies between modules.
D. A Deep Dive into Reasoning
In this section, we examine LLM reasoning, including
decision-making processes and the motivations behind actions.
Only DeepSeek-R1 provides access to the reasoning tokens;
o3-mini does not provide them, and DeepSeek-V3 is not a
reasoning model. We conducted an automated log analysis and
performed a deep dive into some interesting cases.
Every model starts with a synthesis action (100% for
DeepSeek-V3, ∼90% for DeepSeek-R1 and o3-mini). The
following excerpt illustrates this behavior:
Excerpt from Reasoning Tokens: DeepSeek-R1
Given the confusion, perhaps the best first step
is to synthesize a configuration to see what the
actual area and latency are. For example, select
the lowest latency options for each function and
see what the total area is. So the first request
would be to synthesize: fill_matrix option 2
(lowest latency) reverse_string option 1 (lowest
area among low latency) traceback option 0
(lowest latency) needwun option 0 (only option).


--- Page 6 ---
From DeepSeek-R1’s reasoning tokens, the model tends to
overanalyze and become confused, often prompting a synthesis
to evaluate the outcome.
Next we explore how DeepSeek-R1 formulates ILP prob-
lems for full-system optimization. As noted before, DeepSeek-
R1 never inspects the code (see also Figure 5), limiting its
ability to identify data dependencies, infer parallelism and
model latency.
Excerpt from Reasoning Tokens: DeepSeek-R1
Assuming that the total area is the sum of all
selected function options’ areas, and the total
latency is the sum of their latencies (though in
reality, it’s more complex due to parallelism and
pipelining).
Latency: C6 + OddFact + Fib + 2 ∗EvenSum
Correct: C6 + max(OddFact, Fib) + 2 ∗EvenSum
Indeed, as seen above, DeepSeek-R1 acknowledges that the
problem may be more complex but ultimately chooses not to
account for parallelism. However, it does recognize the two
calls to EvenSum.
Excerpt from Reasoning Tokens: o3-mini
We will compute a "design metric" for each
combination as follows:
overall_lat = c6_laty + delta_lat_child
overall_area = c6_area + delta_area_child
For delta, we consider the overhead due to
calls: compute6 calls: odd_factorial (once),
fibonacci (once), and even_sum twice. We assume
the effective latency overhead is the maximum
latency among these calls.
Latency: C6 + max(OddFact, Fib, EvenSum)
Correct: C6+ max(OddFact, Fib) + 2*EvenSum
In SYN6, o3-mini has the right idea but fails to accurately
model the total latency. Although incorrect, these formulations
guide the optimizer by minimizing the latency of the child
functions. The LLMs occasionally inferred correct informa-
tion. For example, DeepSeek-R1 correctly infers that functions
called multiple times must use the same configuration:
Excerpt from Reasoning Tokens: DeepSeek-R1
The call graph shows compute4 calls two even_sum
instances, but the options are per function, not
per instance. So even if there are two instances
of even_sum in compute4, they both use the same
function’s option. Selecting even_sum’s option
once applies to all instances.
Other times, the reasoning does not follow design sense:
Excerpt from Reasoning Tokens: DeepSeek-V3
Let’s revisit the configuration with ‘iir 1‘
and ‘fir 0‘, which achieved a latency of 8 and
an area of 4784.2. Although the area is below
the target, the latency is significantly better.
To balance this, let’s try increasing the area
slightly by using ‘iir 1‘ and ‘fir 1‘, which
previously achieved a latency of 11 and an area
of 6019.0, closer to the target.
The model favors worse latency and area because the
area is too low compared to the target. Any engineer would
understand that a higher area should only be favored if it is
below the target and paired with better latency.
We also observe a trend in DeepSeek-V3’s action calls.
As shown in Figure 5, it performs many more synthesis and
inspect calls compared to DeepSeek-R1 and o3-mini. Log
analysis reveals a pattern: DeepSeek-V3 synthesizes a design,
inspects alternative functions, and, if promising, resynthesizes
with the updated functions. This trial-and-error approach con-
tinues until all viable combinations are explored. DeepSeek-
V3 only chooses to formulate an ILP problem if the trial-and-
error approach fails to meet the target.
V. RELATED WORKS
C2HLSC [14], [15] is an automatic repair and optimization
framework for HLS based on an LLM feedback loop. We
employ a similar feedback loop for pragma insertion in task ➊,
but we iterate multiple times, providing synthesis feedback,
whereas C2HLSC iterates only once. The LLM then performs
task ➋, finding kernels that satisfy the constraints.
TABLE IV: Area (µm) and latency (cycles) comparison with
manual and previous work results on overlapping benchmarks.
C2HLSC [15]
This work
Manual
Sonnet3.5
GPT4-o
DeepSeek-V3 DeepSeek-R1
o3-mini
Impl.
Benchmark
Area Lat.
Area
Lat.
Area
Lat.
Area
Lat.
Area
Lat.
Area Lat.
AES
2965 853
2975
853
2622
239
2665
1029
2630 1096
3386 193
SHA256
41924
83 41794
83 37894
67 41743
84 37894
67 36090
48
Present
22245 897 19985 6193 14918
4347 12799
2226 13235 3697 12056
37
Table IV compares the results on benchmarks common
with C2HLSC [15] and provides reference results obtained
with a manual implementation. From the results, our agentic
approach achieves a better area on these benchmarks, with
latency reduced only in the present benchmark. The manual
implementations are derived from the same C code, without
added pragmas, used in this work. While there is some margin
for improvement, it is minimal. HLSPilot [18] optimizes HLS
code using LLMs by combining, profiling, and optimizing C
code for HLS. This framework is not open, and the paper does
not specify the automation level of the flow. The results are
presented as runtime achieved on an FPGA.
VI. CONCLUSION
We propose and evaluate an agentic HLS framework using
different LLMs, both with and without a built-in reasoning
mechanism, to investigate if and how new-generation reason-
ing models can aid design automation. While our framework
improves state-of-the-art LLM-based HLS optimization, rea-
soning models do not outperform non-reasoning models in this
task as they do in math and coding benchmarks [1]. This raises
a question for future work: Do these models perform better
in these benchmarks due to training contamination? Can we
improve the efficacy of reasoning models with ad-hoc prompt
engineering? We open-source our framework.


--- Page 7 ---
REFERENCES
[1] DeepSeek-AI, “Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning,” 2025. [Online]. Available: https:
//arxiv.org/abs/2501.12948
[2] H. Kuang, X. Cao, J. Li, and L. Wang, “Hgbo-dse: Hierarchical gnn
and bayesian optimization based hls design space exploration,” in 2023
International Conference on Field Programmable Technology (ICFPT),
2023, pp. 106–114.
[3] Y.-k. Choi and J. Cong, “Hls-based optimization and design space explo-
ration for applications with variable loop bounds,” in 2018 IEEE/ACM
International Conference on Computer-Aided Design (ICCAD), 2018,
pp. 1–8.
[4] A. Mahapatra and B. C. Schafer, “Optimizing rtl to c abstraction
methodologies to improve hls design space exploration,” in 2019 IEEE
International Symposium on Circuits and Systems (ISCAS), 2019, pp.
1–5.
[5] C. Ostler, K. S. Chatha, V. Ramamurthi, and K. Srinivasan, “Ilp
and heuristic techniques for system-level design on network processor
architectures,” ACM Transactions on Design Automation of Electronic
Systems (TODAES), vol. 12, no. 4, pp. 48–es, 2007.
[6] S. Pouget, L.-N. Pouchet, and J. Cong, “Automatic hardware pragma
insertion in high-level synthesis: A non-linear programming approach,”
ACM Transactions on Design Automation of Electronic Systems, vol. 30,
no. 2, pp. 1–44, 2025.
[7] V. G. Castellana, A. Tumeo, and F. Ferrandi, “High-level synthesis of
parallel specifications coupling static and dynamic controllers,” in 2021
IEEE International Parallel and Distributed Processing Symposium
(IPDPS).
IEEE, 2021, pp. 192–202.
[8] J.
Liu,
C.
S.
Xia,
Y.
Wang,
and
L.
ZHANG,
“Is
your
code generated by chatgpt really correct? rigorous evaluation of
large
language
models
for
code
generation,”
in
Advances
in
Neural
Information
Processing
Systems,
A.
Oh,
T.
Naumann,
A.
Globerson,
K.
Saenko,
M.
Hardt,
and
S.
Levine,
Eds.,
vol.
36.
Curran
Associates,
Inc.,
2023,
pp.
21 558–21 572.
[Online].
Available:
https://proceedings.neurips.cc/paper files/paper/
2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
[9] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, “Asleep at
the keyboard? assessing the security of github copilot’s code contribu-
tions,” in IEEE Symposium on Security and Privacy, 2022, pp. 754–768.
[10] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi,
Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning
in large language models,” in Proceedings of the 36th International
Conference on Neural Information Processing Systems, ser. NIPS ’22.
Red Hook, NY, USA: Curran Associates Inc., 2022.
[11] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri,
“Autochip: Automating hdl generation using llm feedback,” 2024.
[Online]. Available: https://arxiv.org/abs/2311.04887
[12] J. Blocklove, S. Garg, R. Karri, and H. Pearce, “Evaluating llms for
hardware design and test,” in 2024 IEEE LLM Aided Design Workshop
(LAD), 2024, pp. 1–6.
[13] A. Nakkab, S. Q. Zhang, R. Karri, and S. Garg, “Rome was not built
in a single step: Hierarchical prompting for llm-based chip design,”
in Proceedings of the 2024 ACM/IEEE International Symposium on
Machine Learning for CAD, ser. MLCAD ’24.
New York, NY,
USA: Association for Computing Machinery, 2024. [Online]. Available:
https://doi.org/10.1145/3670474.3685964
[14] L. Collini, S. Garg, and R. Karri, “C2hlsc: Can llms bridge the software-
to-hardware design gap?” in 2024 IEEE LLM Aided Design Workshop
(LAD), 2024, pp. 1–12.
[15] ——, “C2hlsc: Leveraging large language models to bridge the
software-to-hardware design gap,” 2024. [Online]. Available: https:
//arxiv.org/abs/2412.00214
[16] K. Xu, G. L. Zhang, X. Yin, C. Zhuo, U. Schlichtmann, and B. Li,
“Automated c/c++ program repair for high-level synthesis via large
language models,” in Proceedings of the 2024 ACM/IEEE International
Symposium on Machine Learning for CAD, ser. MLCAD ’24.
New
York, NY, USA: Association for Computing Machinery, 2024. [Online].
Available: https://doi.org/10.1145/3670474.3685953
[17] S. Swaroopa, R. Mukherjee, A. Debnath, and R. S. Chakraborty,
“Evaluating large language models for automatic register transfer
logic generation via high-level synthesis,” 2024. [Online]. Available:
https://arxiv.org/abs/2408.02793
[18] C. Xiong, C. Liu, H. Li, and X. Li, “Hlspilot: Llm-based high-level
synthesis,” 2024. [Online]. Available: https://arxiv.org/abs/2408.06810
