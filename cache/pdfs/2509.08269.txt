--- Page 1 ---
1
A Systematic Survey on Large Language Models
for Evolutionary Optimization: From Modeling to
Solving
Yisong Zhang, Ran Cheng, Guoxing Yi, and Kay Chen Tan
Abstract—Large Language Models (LLMs) possess substantial
reasoning capabilities and are increasingly applied to optimiza-
tion tasks, particularly in synergy with evolutionary compu-
tation1. However, while recent surveys have explored specific
aspects of this domain, they lack an integrative perspective
that connects problem modeling with solving workflows. To
address this gap, we present a systematic review of recent
developments and organize them within a structured framework.
First, we classify existing research into two primary stages:
LLMs for optimization modeling and LLMs for optimization
solving2. Second, we divide the latter into three paradigms
based on the role of the LLM: stand-alone optimizers, low-
level components embedded within algorithms, and high-level
managers for algorithm selection and generation. Third, for each
category, we analyze representative methods, distill technical
challenges, and examine their interplay with traditional ap-
proaches. Finally, we review interdisciplinary applications across
the natural sciences, engineering, and machine learning. Based
on this analysis, we highlight key limitations and point toward
future directions for developing self-evolving agentic ecosystems.
An up-to-date collection of related literature is maintained at
https://github.com/ishmael233/LLM4OPT.
Index Terms—Large Language Models, Evolutionary Algo-
rithms, Optimization Modeling, Optimization Solving.
I. INTRODUCTION
Optimization techniques have become indispensable for
addressing complex problems across diverse domains [1], [2],
including engineering design [3], economic planning [4], and
scientific discovery [5]. In practice, real-world challenges are
typically abstracted into mathematical optimization models,
which are then solved by optimization algorithms. Broadly,
these algorithms fall into two categories: exact methods and
approximate methods [6]. Exact methods are theoretically
capable of guaranteeing global optimality. However, they often
suffer from exponential growth in computational complexity,
which renders them impractical for large-scale problems. In
contrast, approximate methods, such as heuristics [6], can effi-
ciently tackle large and complex problems, although they lack
convergence guarantees. Moreover, the No Free Lunch (NFL)
theorem [7] dictates that no single optimization algorithm can
consistently outperform others across all problem classes. Con-
sequently, the practical deployment of optimization algorithms
Yisong Zhang and Ran Cheng contributed equally to this work.
1This survey primarily focuses on evolutionary optimization, i.e., opti-
mization based on evolutionary computation. For brevity, we use the term
optimization throughout to denote this scope.
2In this survey, the terms optimization modeling and optimization solving
are used as concise forms of optimization problem modeling and optimization
problem solving, respectively.
typically demands substantial expertise for configuration [8],
selection [9], and even customized design [10]. This heavy
reliance on domain knowledge constitutes a critical barrier to
transferring optimization technology from theory to practice.
To address this barrier, machine learning [11] has emerged
as a promising approach to alleviate the complexity of algo-
rithm configuration and reduce dependence on expert knowl-
edge. Among these methods, Reinforcement Learning (RL)
[12], [13] has been widely applied at multiple levels of
optimization algorithms due to its flexible decision-making
framework [14]. At the internal structural level, RL has been
employed for population initialization [15], fitness evaluation
[16], operator selection [17], and parameter control [18]. At
the higher strategic level, RL has enabled automated algorithm
selection [19] and even algorithm generation [20]. However,
a persistent limitation is that most RL-based approaches
are trained on narrowly defined problem sets. This focus
restricts their generalization capabilities. Consequently, when
confronted with new problems, retraining can be costly and
time-consuming.
These limitations motivate the exploration of Large Lan-
guage Models (LLMs) [21], [22], which bring a distinct
and complementary set of capabilities. LLMs are trained on
massive, heterogeneous corpora. As a result, they demonstrate
strong semantic understanding, compositional abstraction, and
in-context learning. These features enable zero-shot or few-
shot transfer across heterogeneous tasks [23]–[25]. Such
strengths make LLMs particularly useful when objectives, con-
straints, or design criteria are naturally expressed in language
or structured symbols. Furthermore, they are valuable when
rapid, readable candidate generation, explanation, or cross-
domain priors are required, such as in drug discovery [26]
or finance [27].Within the optimization domain, LLMs offer
distinct capabilities for both modeling and solving. On the
modeling side, LLMs assist in translating informal specifica-
tions into formal mathematical formulations [28]. On the solv-
ing side, they function either as stand-alone iterative solvers
for language-structured problems [29] or as components that
complement algorithmic search [30]. However, despite this
potential, the integration of LLMs into optimization remains
in a nascent stage. Existing studies indicate substantial limita-
tions in current approaches. For instance, LLMs operating as
stand-alone optimizers often fail to surpass the performance
of classical algorithms [31]. Furthermore, while fine-tuning
with domain-specific data can enhance task outcomes, the
construction of such datasets remains costly and expertise-
arXiv:2509.08269v4  [cs.NE]  7 Jan 2026


--- Page 2 ---
2
Section III
Background
Evolutionary algorithms
Large language models
Prompt engineering
Fine-tuning
Primary paradigms
 Standard procedure 
Classic paradigms
Challenges
Section I
Introduction
Section II
Related Surveys 
and Taxonomy 
Background
Motivation
Taxonomic framework and definitions
Comparison with existing research
Section IV
LLMs for Optimization Modeling
Prompt-based methods
Learning-based methods
Two-stage framework
Multi-agent framework
Interactive framework
Fine-tuning
Modeling from text descriptions
  Two principal pathways 
Data synthesis
Standard framework: ORLM
Section V
LLMs for Optimization Solving
Low-level LLMs for OAs
High-level LLMs for OAs
Initialization & Evaluation
Evolutionary operators
Algorithm configuration
Single round & Iterative 
Algorithm generation
Algorithm selection
OAs
LLMs as Optimizers
Direct
LLMs
LLMs as optimizers
OPRO-based
Learning-based
Section VI
Applications
&
Section VII
Vision for 
the Field
Transitioning from static to dynamic methods
Bridging the gap between modeling and solving
Toward an 
agentic ecosystem 
for optimization
Computer science
Natural sciences
Engineering and industry
Biology
Management
Design
Machine learning
Security
Chemistry
EAs
Fig. 1. Overall organization of this survey.
intensive [28].
To address these challenges and provide a systematic per-
spective, this survey presents a comprehensive analysis of the
field. The main contributions are summarized as follows:
• We present a comprehensive literature review of research
on LLMs for optimization, in which we systematically
summarize recent advances, representative methods, and
key applications.
• We propose a structured taxonomy that categorizes ex-
isting work into optimization modeling and optimization
solving. We further divide the latter into three paradigms:
LLMs as optimizers, low-level LLMs embedded in algo-
rithms, and high-level LLMs for algorithm selection and
generation. This taxonomy provides an organizing lens to
facilitate method comparison and landscape understand-
ing.
• We contrast LLM-driven approaches with traditional
methods to identify fundamental challenges and research
gaps. Additionally, we outline promising future directions
toward building an intelligent optimization ecosystem.
The overall structure of this survey is illustrated in Fig. 1.
Section II reviews related surveys and introduces the proposed
taxonomy. Section III provides the necessary background on
key technologies. Subsequently, Sections IV and V constitute
the core of this work; they focus on LLMs for optimization
modeling and solving, respectively. Section VI examines in-
terdisciplinary applications of these approaches. Finally, Sec-
tion VII outlines future research directions, and Section VIII
concludes with a summary of key findings.
II. RELATED SURVEYS AND TAXONOMY
Several surveys and reviews have examined the use of LLMs
in optimization. Table I summarizes their scope and contrasts
them with our study. Wu et al. [32] and Huang et al. [33]
reviewed the field from a bidirectional perspective, which
considers both LLM-enhanced optimization algorithms and
optimization algorithm-enhanced LLMs. However, regarding
the former, their discussions are limited to scenarios where
LLMs serve as solvers or for algorithm generation. Con-
sequently, they do not analyze the broader relationship to
the optimization process. Yu et al. [34] adopted a similar
classification and proposed a general framework for LLM-
assisted optimization. Chao et al. [35] focused specifically on


--- Page 3 ---
3
TABLE I
COMPARISON OF RELATED SURVEYS ON LLMS FOR OPTIMIZATION. THIS
WORK PROVIDES A SYSTEMATIC COVERAGE EMPHASIZING ALL FOUR
CATEGORIES: LLMS FOR MODELING (LM), LLMS AS OPTIMIZERS (LO),
LOW-LEVEL LLMS FOR OPTIMIZATION ALGORITHMS (LL), AND
HIGH-LEVEL LLMS FOR OPTIMIZATION ALGORITHMS (HL).
Ref.
Venue
LM
LO
LL
HL
Wu et al. [32]
TEVC, 2024
✓
✓
Huang et al. [33]
SWEVO, 2024
✓
✓
Chao et al. [35]
Research, 2024
✓
Yu et al. [34]
arXiv, 2024
✓
✓
✓
Liu et al. [38]
arXiv, 2024
✓
✓
✓
Ma et al. [39]
TEVC, 2024
✓
✓
Our work
arXiv, 2025
✓
✓
✓
✓
Evolutionary Algorithms (EAs) [36], [37]. They highlighted
component-level analogies between EAs and LLMs to suggest
bidirectional knowledge transfer. Nevertheless, their coverage
is restricted to cases where LLMs act as operators. Other
related works adopted different perspectives. For instance,
Liu et al. [38] provided a broad overview of LLMs in
algorithm design, in which they categorized the roles of LLMs
as optimizers, predictors, extractors, and designers. Ma et
al. [39] surveyed meta-black-box optimization. Although this
survey mentioned LLMs, it primarily emphasized conventional
reinforcement learning approaches. Overall, existing reviews
concentrate mainly on the solving stage of optimization. As
a result, they pay limited attention to the modeling stage.
Even in [38], modeling is only briefly introduced as a minor
application.Building on existing surveys, this paper focuses on
the modeling-to-solving workflow. It provides an integrative
view that links modeling and solving perspectives. To this end,
we propose a new taxonomy that articulates the relationship
between LLMs and the optimization process. Specifically, we
divide the field into two main categories: LLMs for optimiza-
tion modeling and LLMs for optimization solving. The latter is
further subdivided into three paradigms: LLMs as optimizers,
low-level LLM-assisted optimization algorithms, and high-
level LLM-assisted optimization algorithms. The definitions
of these categories are as follows:
• LLMs for Optimization Modeling aim to automatically
transform unstructured natural language descriptions of
problems into mathematical optimization models that can
be interpreted and solved by machines. This area repre-
sents the foundational step toward a fully LLM-driven
optimization workflow. Crucially, the central challenge is
the transition from ambiguous natural language to pre-
cise mathematical formulations. Existing approaches fall
mainly into prompt-based and learning-based methods.
Prompt-based approaches typically rely on carefully de-
signed prompting strategies or multi-agent collaboration,
which offer fast deployment without additional training.
In contrast, learning-based approaches often require data
synthesis and fine-tuning. While these steps incur training
costs, they substantially improve reliability in complex
scenarios.
• LLMs as Optimizers treat LLMs themselves as general-
purpose optimizers that solve optimization problems
through iterative natural language interaction, inde-
pendent of traditional algorithmic frameworks. This
paradigm leverages the in-context learning and reasoning
abilities of LLMs to explore optimization trajectories and
generate promising candidate solutions. Although this
approach faces challenges such as model dependency
and scalability, it represents the earliest and most direct
attempt to connect large models with optimization tasks.
Consequently, it holds substantial pioneering value.
• Low-level LLM-assisted Optimization Algorithms em-
bed LLMs as intelligent components within traditional
optimization algorithms to enhance specific operations.
Unlike the stand-alone optimizer paradigm, this approach
integrates LLMs tightly with established frameworks.
Specifically, it exploits their analytical and domain knowl-
edge to improve performance. Typical applications in-
clude population initialization, operator design, parameter
control, configuration, and fitness evaluation. By empow-
ering traditional components to adapt more intelligently
to diverse problem characteristics and search states, this
paradigm improves both efficiency and solution quality
at the algorithmic core.
• High-level LLM-assisted Optimization Algorithms fo-
cus on top-level orchestration rather than internal com-
ponents. These algorithms generally fall into two sub-
categories: algorithm selection and algorithm generation.
Algorithm selection leverages LLMs to choose the most
suitable algorithm from a pool for different problem
instances. In contrast, algorithm generation enables LLMs
to autonomously design new algorithms tailored to spe-
cific tasks. By granting LLMs a global perspective over
the entire optimization workflow, this paradigm elevates
their role from internal assistant to top-level designer.
III. BACKGROUND
Current research on LLMs for internal component con-
trol, high-level orchestration, and algorithm generation centers
predominantly on EAs. Consequently, we first provide the
necessary background to contextualize this work. Section III-A
introduces the framework and core paradigms of EAs. Sec-
tion III-B subsequently outlines the underlying architectures
and key technologies of LLMs.
A. Evolutionary Algorithms
Evolutionary
Algorithms
(EAs)
represent
a
class
of
population-based, gradient-free optimization methods inspired
by the principles of biological evolution. As illustrated in
Fig. 2, the general framework of an EA begins with the
initialization of a population of candidate solutions. The rep-
resentation of these individuals varies widely, ranging from
numerical vectors to neural networks. Within each iteration, a
fitness function evaluates the individuals. Subsequently, the al-
gorithm selects parent individuals to produce offspring through
genetic operators, such as crossover and mutation. Selection
mechanisms then form a new population from the pool of
parents and offspring. This evolutionary cycle continues until


--- Page 4 ---
4
Particle Swarm
Genetic Algorithm
Genetic Programming
Evolution Strategy
Prompt Engineering
Parents Selection
Fitness Evaluation
New Population
Survivor Selection
Variation
P
O
Optimization
Paradigm
LLM For optimization relies on two techniques.
Fine-tuning
Zero-Shot / Few-Shot
Chain-of-Thought
ToT/GoT
Instruction Tuning
Alignment Tuning
(x, y) × N 
Para
PPL
BLEU
R & Q
HS
AA
RLHF/ SFT/ DPO
↨ 
+
Technique
encoder-only
encoder-decoder
decoder-only
BERT
RoBERTa
ERNIE
DeBERTa
T5
BART
GLM
TK
ChatGPT LLaMA
DeepSeek Qwen
Architecture
Initial Population
Fig. 2. Technological dependencies of LLMs for optimization, including EA paradigm and workflow, LLM architecture, and related enabling technologies.
the process satisfies a termination criterion, at which point it
returns the best-performing individual as the final solution.
Over decades of development, researchers have proposed a
diverse set of classical paradigms [40], [41]. Prominent exam-
ples include Genetic Algorithms (GAs) [42], Genetic Program-
ming (GP) [43], Differential Evolution (DE) [44], and Particle
Swarm Optimization (PSO) [45]. While these algorithms share
a common population-based iterative framework, they employ
distinct search mechanisms and target specific application
domains. For instance, GAs simulate natural selection and
genetic operators, which makes them well-suited for discrete
and combinatorial problems. In contrast, DE and PSO excel
in continuous optimization tasks through distinctive mutation
and information-sharing strategies. Furthermore, GP utilizes
a tree-structured encoding, which enables its application to
automatic programming and symbolic regression. Collectively,
these paradigms establish the theoretical foundation of evolu-
tionary optimization and provide diverse strategies for solving
increasingly complex problems.
Compared to traditional exact optimization methods, EAs
offer substantial advantages in handling non-convex, multi-
modal, and noisy problems. Consequently, their applications
span a wide range of domains, such as aerospace design [46]
and hyperparameter tuning in deep learning [47]. However,
the No Free Lunch (NFL) theorem implies that no single EA
achieves universal superiority across all problems at a low
computational cost. As a result, practitioners must often devote
substantial effort to the selection, configuration, and design of
suitable EA variants. To address this challenge, recent research
increasingly integrates EAs with machine learning methods
[48]. Notably, the combination of reinforcement learning with
evolutionary frameworks has produced approaches such as
MOEA/D-DQN [49] and SYMBOL [20]. Furthermore, the
emergence of Large Language Models (LLMs) has prompted
investigations into how LLMs can empower EAs [32]. These
studies extend from problem formulation [28] to automated
algorithm design [38].
B. Large Language Models
The foundation of Large Language Models (LLMs) lies
in the Transformer architecture [50], which captures global
input-output dependencies through self-attention. Building on
this architecture, researchers have developed three primary
paradigms: encoder-decoder, encoder-only, and decoder-only
models [51]. Encoder-decoder models, exemplified by BART
[52] and T5 [53], follow the original Transformer design and
excel at sequence-to-sequence tasks. In contrast, encoder-only
models, such as BERT [54], focus on contextual representation
learning for semantic understanding. Decoder-only models,
including ChatGPT [55] and DeepSeek [56], leverage au-
toregressive generation for open-domain tasks. Consequently,
decoder-only models currently dominate research on LLMs
for optimization due to their strong generative and reasoning
capabilities. To adapt these architectures for specific appli-
cations, researchers primarily employ two pathways: prompt
engineering [57] and fine-tuning [58].
Prompt engineering steers LLM outputs through carefully
designed instructions, a process that entails no modification of
model parameters. Early forms of this approach include zero-
shot and few-shot prompting [59], which exploit pre-trained
knowledge either directly or via limited in-context examples.
More advanced approaches incorporate structured reasoning,


--- Page 5 ---
5
such as Chain-of-Thought (CoT) [24], which enhances per-
formance on complex reasoning tasks by eliciting step-by-
step logical inference. Building on this foundation, CoT has
expanded into richer paradigms like Tree-of-Thought (ToT)
[60] and Graph-of-Thought (GoT) [61]. Furthermore, domain-
specific variants have emerged, such as Chain-of-Code (CoC)
[62] and Chain-of-Knowledge (CoK) [63].
Fine-tuning, in contrast, updates model parameters on task-
specific data. This approach enables deeper adaptation, al-
though it requires labeled data and computational resources.
Two strategies are particularly pivotal in this domain. First,
instruction fine-tuning [64] reformulates diverse tasks into
instruction–input–output triplets, thereby unifying them under
an instruction-following paradigm. This methodology sub-
stantially improves zero-shot generalization, as demonstrated
by Flan-T5 [65]. Second, alignment fine-tuning ensures that
model behavior aligns with human intentions and values.
Reinforcement Learning with Human Feedback (RLHF) [66]
achieves this alignment by optimizing a reward model de-
rived from human preferences. Alternatively, Direct Preference
Optimization (DPO) [67] simplifies the process by directly
optimizing for preferred outputs. Collectively, these techniques
are central to building safe, reliable, and useful LLMs.
IV. LLMS FOR OPTIMIZATION MODELING
Optimization modeling constitutes a pivotal phase in the
optimization workflow, yet it traditionally depends on exten-
sive expert knowledge. To address this dependency, Large
Language Models (LLMs) have emerged as a viable solution
for automated optimization modeling. This section introduces
two core paradigms: prompt-based methods and learning-
based methods. Prompt-based methods (Section IV-A) guide
LLMs through carefully designed instructions, which are often
implemented via two-stage workflows, multi-agent collabo-
ration, or interactive prompting. In contrast, learning-based
methods (Section IV-B) fine-tune LLMs to directly generate
mathematical formulations. This approach typically requires
large-scale synthetic data for training, thereby offering greater
reliability in complex tasks. Finally, we discuss the open
challenges in this field (Section IV-C). Representative studies
are summarized in Table I in the Supplementary Document,
while the main paradigms are illustrated in Fig. 3.
A. Prompt-based Methods
Early research in automated optimization modeling sought
to mitigate reliance on expert experience by analyzing feasible
solution samples or employing evolutionary algorithms. For
instance, Pawlak et al. demonstrated the preliminary feasibil-
ity of this approach by analyzing solution samples [68] or
generating constraints [69]. However, these methods face sig-
nificant constraints regarding sample quality and scope [70]. In
particular, they primarily focus on constraint generation rather
than the direct conversion of natural language descriptions into
complete mathematical models.
To address these limitations, recent methodologies have
prioritized the translation of unstructured natural language
into mathematical optimization models. Ramamonjison et al.
[71] proposed OptGen to bridge this gap, which subsequently
catalyzed the field through the NL4OPT competition [72].
This competition decomposed the modeling task into two
core stages: extracting entities (such as variables, parameters,
and constraints) via Named Entity Recognition (NER), and
generating a complete mathematical model from the annotated
description. While earlier works relied on lightweight models
like BERT [73], [74] and BART [75], [76], Almonacid et
al. [77] later introduced the use of Large Language Models
(LLMs) such as GPT-3.5. They employed a single-step mod-
eling approach that directly prompts the LLM to output the
mathematical model and verifies its correctness with a solver.
Nevertheless, while effective for simple tasks, this single-step
method lacks the sufficient accuracy and robustness required
for complex optimization modeling.
Consequently, researchers have progressively adopted a
two-stage framework that integrates LLMs with specialized
NER tasks. For instance, the Holy Grail 2.0 framework [78],
proposed by Tsouros et al., employs specialized models for the
NER stage and further decomposes the automated modeling
task into entity relationship identification, problem formal-
ization, and code generation. Building upon this hierarchical
structure, Li et al. [70] proposed a framework specifically for
Mixed-Integer Linear Programming (MILP). Their approach
utilizes a fine-tuned model to classify constraints, thereby
enabling the handling of logical constraints and binary vari-
ables that previous works often overlooked. Furthermore, this
two-stage methodology has proven effective in specialized
domains. For example, Jin et al. [79] applied it to energy man-
agement by requiring the LLM to extract critical parameters
prior to modeling, while [80] utilized it for travel planning
to extract user preferences and compute optimal plans.Beyond
the two-stage framework, multi-agent architectures extend the
modeling paradigm by replacing the fixed NER-to-modeling
pipeline with a diverse set of expert roles. These roles are
coordinated across multiple LLMs. This approach effectively
broadens the scope of coverage and strengthens cross-checking
capabilities. In contrast, interactive frameworks emphasize a
continuous dialogue with the user. This interaction refines the
formulation through iterative feedback, including constraint
modification, priority adjustments, and preference updates.
Consequently, the model evolves dynamically rather than
remaining statically defined.
In the domain of multi-agent frameworks, the Chain-of-
Experts [81] by Xiao et al. constitutes a foundational approach.
This method initializes 11 distinct expert types, including
terminology, modeling, programming, and code review ex-
perts, which operate under the coordination of a conductor
agent. The framework dynamically selects experts to construct
a forward chain-of-thought. It also incorporates a backward
reflection mechanism to address errors and inconsistencies
during modeling. Subsequent studies have further explored
these collaborative paradigms [82]–[85]. For instance, Ahma-
diTeshnizi et al. introduced OptiMUS [82] and OptiMUS-
0.3 [83]. These systems similarly utilize a conductor agent
to coordinate modeling, programming, and evaluation tasks,
wherein the manager reviews the content of each conversa-
tional step before task assignment. However, Wan et al. noted


--- Page 6 ---
6
Task: Transform the specific optimization problem described in natural language into a structured mathematical model.
Learning-based
Input: Natural Language Problem
Target: Model and Code
Data: Triple (Problem, Model, Code)
Energy Education
Retail
Software
Synthetic Data Generation
Expansion
Augmentation
Post-processing
Methods
Qwen
LLaMA
Models
Model Fine-tuning
Evaluation and Baselines
NL4OPT
MAMO
IndustryOR
AST
ER
SA
Prompt-based
How to tackle issues like data scarcity and privacy?
ChatGPT
Grok
Model selection
Gemini
Claude
Two-stage Framework
Multi-agent Framework
Interactive Framework
Addition & Reduction Priority Adjustment
Formulation
Entity Recognition
Formulation
Formulation
Collaboration System
A
N
Fig. 3. Illustration of LLMs for optimization modeling. Approaches are divided into two categories: (i) prompt-based methods, which are typically implemented
via two-stage prompting, multi-agent collaboration, or interactive frameworks; and (ii) learning-based methods, which generally follow a workflow involving
data synthesis, model fine-tuning, and evaluation.
that such conductor-orchestrated frameworks can result in un-
predictable workflows and insufficient mathematical accuracy.
To address these limitations, they proposed ORMind [84].
This cognitively-inspired framework mimics human cognitive
processes to enhance solution reliability and transparency.
It adopts a structured workflow alongside a counterfactual
reasoning mechanism. Furthermore, distinct research efforts
have targeted the evaluation phase. Mostajabdaveh et al. [86]
substituted the time-consuming process of traditional solver-
based verification with inter-agent peer verification. Similarly,
Talebi et al. [87] utilized multiple independent reviewer agents
to score modeling results for stochastic optimization problems.
This strategy facilitates a comprehensive assessment of mod-
eling quality.Existing two-stage and multi-agent frameworks
require users to provide a complete problem description before
the framework initiates. However, for many real-world opti-
mization problems, such as conference scheduling and travel
planning, user preferences change dynamically. This necessi-
tates a paradigm that supports continuous interaction between
the user and the model. To address this challenge, Zhang et al.
proposed the OptLLM [88] framework, which supports both
single-shot and interactive inputs. This framework employs
an interactive refinement module to help users progressively
refine the problem description, while a converter module
and a response module handle the subsequent modeling and
verification tasks. Lawless et al. [89] further advanced this
interactive framework by requiring the LLM to choose from
five tasks, such as adding constraints or adjusting priorities,
after each user input. Their MeetMeta system provides a
solution, explains why certain preferences cannot be met,
and allows users to make modifications. This process cre-
ates a continuous feedback loop. Subsequent research [90]
has focused on translating user priorities into optimization
constraints during a dialogue, which enables users to explore
trade-offs in tasks such as ridesharing coordination.
As research into prompt-based methods has progressed,
investigators have explored distinct avenues to address existing
challenges in LLM-based automated modeling [91]. Regarding
model generation, researchers have adopted specific strategies
to manage the vast hypothesis space. For instance, Astorga
et al. [92] combined LLMs with Monte Carlo Tree Search
(MCTS) to explore this hierarchical space efficiently. This
approach enhances search efficiency through symbolic pruning
and LLM-based evaluation. In contrast to this search-first strat-
egy, Li et al. [93] employed a generation-first approach. They
constrained the output of the LLM using predefined structures,
which avoids blind searches within the hypothesis space. A
post-processing verification and correction mechanism com-
plements this strategy to ensure that the generated models
are parsed correctly by solvers. Regarding model verification,
Huang et al. [94] proposed an automated evaluation framework
based on solver outputs to broaden the scope of validation.
They also constructed the MAMO benchmark, which extends
the NL4OPT benchmark by including systems of ordinary
differential equations and complex LP and MILP problems.
Conversely, Wang et al. [95] employed a modified Weisfeiler-
Lehman (WL) graph isomorphism algorithm to verify model
equivalence by transforming optimization models into a bi-
partite graph structure. They also created the OptiBench
benchmark, which contains numerous LP and MILP problems
designed to reflect the performance boundaries of LLMs.
Similarly, Zhai et al. [96] introduced the Quasi-Karp equiv-
alence theoretical framework and developed the EquivaMap
system. This system achieves a scalable method for auto-
mated equivalence detection through LLM-generated variable
mapping functions and lightweight verification.Prompt-based
frameworks in automated optimization modeling are con-
strained by the core capabilities of the underlying Large Lan-
guage Models (LLMs). Specifically, mainstream approaches
that rely on closed-source models, such as the GPT series,
face a dual challenge [28]. First, the scarcity of high-quality
training data in the optimization modeling domain restricts the


--- Page 7 ---
7
performance ceiling of these models. Second, the reliance on
API calls to commercial models introduces the risk of sensitive
information leakage, which poses security concerns in high-
stakes scenarios such as medical scheduling and defense
planning. These fundamental limitations, coupled with the
formal rigor required for optimization problems, necessitate a
transition from prompt engineering to learning-based methods.
In contrast to prompt-based strategies, learning-based methods
leverage fine-tuning on open-source models to achieve precise
mathematical expression generation, controllable data privacy
protection, and the efficient injection of domain knowledge.
B. Learning-based Methods
Learning-based methods fundamentally overcome the inher-
ent limitations of prompt engineering by fine-tuning model
parameters to internalize optimization modeling knowledge
within large language models. In early research, several studies
attempted to fine-tune models to enhance specific aspects of
automated modeling. For instance, the work [97] combined
traditional NLP methods with LLMs to obtain the Ner4OPT
model, which is specialized for the NER task. Additionally, Li
et al. [70] utilized a fine-tuned model to classify constraints to
uncover complex relationships between entities. While these
works involved model fine-tuning, their core idea was to serve
as auxiliary components within prompt-based two-stage frame-
works. Subsequently, the LM4OPT [98] framework proposed
by Ahmed et al. significantly improved the optimization mod-
eling capabilities of Llama-2-7b [99] through progressive fine-
tuning. However, due to its fine-tuning data being confined to
the NL4OPT dataset, its modeling performance still exhibited
a substantial gap compared to closed-source models like GPT-
4 [100].
Huang et al. proposed ORLM [28], a framework that
features a two-stage data synthesis strategy comprising expan-
sion and augmentation. This strategy systematically prepares
structured data for model fine-tuning by encompassing three
critical components: natural language problem descriptions,
formal mathematical models, and executable solver code.
Specifically, the expansion stage leverages a small set of seed
data and uses GPT-4 to iteratively generate new samples with
diverse scenarios and difficulty levels. The augmentation stage
subsequently enhances sample variety by employing strategies
such as modifying objective functions, altering constraints,
and rephrasing problems. During this iterative process, the
framework employs a match correction mechanism to address
minor grammatical errors in the code and removes duplicate
problems to ensure dataset integrity. Consequently, the authors
fine-tuned a series of open-source models, such as Mistral-7B
[101] and Deepseek-Math-7B-Base [102], using Instruction-
Tuning. Experiments demonstrate that these fine-tuned models
consistently outperform single-step GPT-4 modeling and yield
substantial performance improvements over classic prompt-
based methods, such as Chain-of-Experts [81] and OptiMUS
[82]. This workflow, combining data synthesis, model fine-
tuning, and evaluation, has been widely adopted by subsequent
research, which either proposes new data synthesis methods or
optimizes the fine-tuning stage.
A distinct subset of learning-based research prioritizes the
data synthesis component [103] to address the challenge of
data scarcity in optimization modeling. For instance, the ReSo-
cratic framework [104], developed by Yang et al., synthesizes
data via an inverse approach; it first generates optimization
examples with mathematical formulas and then translates them
into natural language problems, thereby creating a large-
scale dataset. Similarly, Lu et al. proposed OptMATH [105],
a scalable framework for bidirectional data synthesis that
automatically generates problems with controllable complexity
from seed data. This framework utilizes a forward modeling
and rejection sampling mechanism to verify the accuracy of
data pairs. To combine the diversity of forward generation
with the reliability of inverse generation, Zhou et al. de-
signed the DualReflect synthetic data pipeline [106], which
is tailored for dynamic programming problems. Beyond syn-
thesis methodologies, researchers have also investigated the
quality of synthesized content. Wang et al. observed that the
data paradigm of methods like ORLM creates an excessive
span between natural language and mathematical models.
To address this, they constructed the StructuredOR dataset
[107], which resolves the lack of detailed modeling process
annotations, such as variable definitions, in existing datasets.
This contribution provides comprehensive annotation infor-
mation for model learning. Furthermore, Wu et al. focused
on the generation of complex optimization problems. Their
Step-Opt framework [108] systematically increases problem
complexity through iterative generation and employs a step-
wise validation mechanism to ensure data rigor.Distinct from
data synthesis, a parallel line of research focuses on fine-
tuning methods [109]–[112]. Specifically, Jiang et al. [109]
employed multi-instruction fine-tuning to enhance model ac-
curacy in problem formalization and solver code generation.
To ensure robustness, they also incorporated model alignment
and self-correction mechanisms that mitigate hallucinations.
Similarly, Chen et al. [110] proposed the SIRL framework
by integrating reinforcement learning. This method utilizes an
external optimization solver as a validator to provide verifiable
reward signals. Consequently, the approach effectively ad-
dresses the hallucination problem, thereby improving the fac-
tual correctness and trustworthiness of the generated content.
Furthermore, Amarasinghe et al. [111] extended fine-tuning
to specific business challenges. Their method optimizes cost-
effective LLMs for tasks such as production scheduling, which
demonstrated superior performance in real-world applications.
C. Challenges
The increasing application of LLMs in optimization has
driven efforts to bridge natural language descriptions of
optimization problems with formal mathematical models.
Frameworks such as Chain-of-Experts [81] and ORLM [28]
demonstrate that LLMs can complete modeling tasks effi-
ciently. Moreover, these models uncover implicit constraints
that human experts may overlook, thereby introducing a new
paradigm for optimization modeling. Nevertheless, existing
research faces several key challenges:
• Prompt-based methods leverage zero-shot linguistic
translation to enable rapid deployment without large-


--- Page 8 ---
8
scale annotated datasets. However, this paradigm is fun-
damentally constrained by the mathematical reasoning
capabilities of pretrained models. While enterprise-grade
deployments effectively mitigate data leakage risks, a
core technical bottleneck remains. Specifically, current
closed-source models exhibit persistent gaps in formal
mathematical reasoning and numerical rigor when trans-
lating natural language into optimization formulations.
This limitation stems from the lack of mathematical struc-
ture in pretraining corpora and the absence of symbolic
reasoning mechanisms. Consequently, future progress
requires both advancing intrinsic reasoning capabilities
through specialized pretraining and developing hybrid
architectures that augment LLM outputs with external
validators.
• Learning-based methods pursue supervised knowledge
internalization through synthetic data generation and
model fine-tuning. Despite its effectiveness, this paradigm
faces fundamental bottlenecks regarding data scarcity
and lifecycle management, irrespective of the deploy-
ment model. High-quality, domain-specific optimization
datasets are expensive to synthesize and validate. Fur-
thermore, repeating the fine-tuning process across various
problem domains and model scales substantially increases
costs. Additionally, maintaining tuned models against per-
formance regression lacks established protocols. Finally,
rapid advances in reasoning-focused foundation models
may render carefully engineered multi-stage pipelines
obsolete. Thus, advancing this paradigm requires data-
efficient synthesis strategies and sustainable maintenance
frameworks.
V. LLMS FOR OPTIMIZATION SOLVING
Optimization solving represents the core execution phase
of the workflow, where the integration of Large Language
Models (LLMs) has propelled the field into a distinct new
phase. To systematize this landscape, we categorize existing
research into three paradigms: LLMs as optimizers, low-level
LLM-assisted optimization algorithms, and high-level LLM-
assisted optimization algorithms. Section V-A introduces the
first paradigm, i.e., LLMs as optimizers, which employs LLMs
to directly solve problems without reliance on traditional
frameworks (see Fig. 4). Subsequently, Sections V-B and V-C
discuss how LLMs enhance optimization algorithms at the
component level and the orchestration or design level, re-
spectively (illustrated in Figs. 5 and 6). Finally, Section V-D
outlines the key challenges in this domain, while representative
studies are summarized in Table II in the Supplementary
Document.
A. LLMs as Optimizers
LLMs
possess
strong
in-context
learning
capabilities,
thereby enabling them to solve optimization problems through
iterative prompting. Yang et al. [29] pioneered this direction
with the OPRO framework, which formulates optimization
tasks in natural language and iteratively refines solutions based
on historical trajectories. The principal strength of OPRO
Problem 
Description
Task:  Harness large language models as optimizers.
OPRO-based
Q: We aim to find 
the minimum of a 
function ...
Initial Solution Iterative Optimization 
Final Solution 
Optimization
Trajectory
Learning-based
Fine-tuning
Q: We aim to find 
the minimum of a 
function ...
A :  The minimum 
to the provided 
function is ...
Questions
A: The minimum 
to the provided 
function is ...
Direct Output
Fig. 4.
Illustration of LLMs as optimizers. This paradigm primarily relies
on interactive prompting for optimization, with a few studies incorporating
fine-tuning and pre-training for enhanced performance.
is prompt-driven optimization. Specifically, by exposing the
model to solution–feedback trajectories, the framework lever-
ages the ability of the LLM to infer patterns from context
and generate semantically coherent candidate updates. This
approach is particularly effective for discrete or language-
structured tasks, as well as for rapid prototyping in human-
in-the-loop workflows. Building on this paradigm, Guo et al.
[113] benchmarked LLMs on four classical optimization tasks,
i.e., gradient descent, hill climbing, grid search, and black-
box optimization. Subsequently, OPRO has been extended
to multiple application domains. Jiang et al. [114] applied
the framework to combinatorial optimization for structural
matrix ordering; Qiu et al. [115] developed LMCO for wireless
network design; Ghose et al. [116] employed it for parameter
tuning in open-source hardware workflows; and Jiang et al.
[117] adapted the paradigm to jailbreak attack analysis.
In contrast to the direct use of solution–feedback trajecto-
ries in OPRO, subsequent research has explored alternative
forms of iteration to guide LLMs. Lange et al. [118] sorted
discretized candidate solutions from best to worst and provided
the ordered sequence as context. This approach reframes the
task from pure generation to sequence recognition, thereby
leveraging the ability of the model to capture ordering pat-
terns. Cheng et al. [119] proposed the OPTO framework,
which feeds LLMs with structured execution traces, such as
intermediate variable values and function calls, to simulate
backpropagation-like optimization. Beyond textual iteration,
multimodal extensions have further broadened the paradigm by
incorporating visual information. Huang et al. [120] addressed
vehicle routing problems using both textual descriptions and
map images to mimic human spatial cognition. Elhenawy


--- Page 9 ---
9
Task:  Leverage large language models to enable low-level tasks: initialization, operators, configuration and evaluation.
Evolutionary Algorithm
Initialization
Evolutionary Operators
Algorithm Configuration
Evaluation
LLMs as decision-makers. 
LLMs as decision-makers. 
LLMs as predictors.
LLMs as predictors.
State
Domain Knowledge Selection
LLMs
Direct Output 
Prediction
Solution Features
Problem Features
Process Features
Configuration
Next iteration
Population
LLMs
Crossover
Mutation
Selection
Solution
Classification
 Regression
LLMs as SMs
LLM-assisted SMs
Termination Check
Fitness Evaluation
Evolutionary Operators
Population Initialization
Fig. 5.
Illustration of low-level LLMs for optimization algorithms. LLMs can be applied at various stages within EAs, including initialization, evolutionary
operators, algorithm configuration, and fitness evaluation.
et al. [121] solved the traveling salesman problem by com-
bining scatter plot inputs with a multi-agent collaboration
mechanism. Finally, Zhao et al. [122] extended this idea to
graph-structured problems by converting abstract graphs into
visual images, which enables multimodal LLMs to capture
topological features. Collectively, these studies demonstrate
that extending OPRO with sequence-based, trace-based, or
multimodal inputs substantially enriches the optimization ca-
pabilities of LLMs.Despite the progress demonstrated by iter-
ative prompting frameworks, systematic empirical evaluations
have revealed distinct performance boundaries. Zhang et al.
[123] highlighted the strong model dependency of OPRO,
observing that smaller models struggle to exploit long iterative
histories effectively. More broadly, Huang et al. [31] sys-
tematically evaluated LLMs on both discrete and continuous
black-box optimization tasks. This evaluation confirmed the
considerable strengths of LLMs in symbolic and language-
structured domains, yet it simultaneously exposed significant
weaknesses in purely numerical settings, such as continuous
optimization. These findings suggest that the applicability of
stand-alone LLM-based optimization is more constrained than
initially anticipated. Specifically, the performance gap widens
substantially when problems lack natural language structure,
when solutions demand precise numerical calibration, or when
evaluation budgets are limited, thereby necessitating high
sample efficiency.
To address these limitations, researchers have explored al-
ternative avenues for applying LLMs in optimization. One line
of work attributes the drawbacks to prompt engineering and
the inherent constraints of current models, and consequently
seeks to improve optimization capability through fine-tuning
or training. Abgaryan et al. [124] fine-tuned an LLM to
output solutions in a single step, which bypasses multi-round
iteration by training on instruction–solution pairs. Li et al.
[125] introduced POM, a pretrained optimizer model that
generalizes across black-box problems in a zero-shot manner.
Chen et al. [126] proposed LLOME, a two-level optimization
framework that incorporates preference learning to generate
biologically valid sequences under complex biophysical con-
straints. Conversely, an increasing body of research suggests
that LLMs may be more effective when used to assist,
rather than replace, traditional optimization algorithms. This
perspective motivates the next paradigm of low-level LLM-
assisted optimization, where LLMs are embedded into existing
frameworks to enhance their performance.
B. Low-level LLMs for Optimization Algorithms
The collective intelligence of EAs and the knowledge-driven
reasoning of LLMs exhibit a natural complementarity [35].
While EAs specialize in heuristic exploration via population-
based search, LLMs contribute semantic understanding and
generative capabilities. These attributes guide the search pro-
cess, thereby enhancing both efficiency and solution qual-
ity. This collaborative paradigm, which embeds LLMs as
components within EAs, defines the direction of low-level
assistance in optimization. Accordingly, this section system-
atically reviews research in this area across four key stages:
initialization, evolutionary operators, algorithm configuration,
and evaluation.
1) Initialization: In the optimization process, the quality
of the initial solution strongly influences both convergence
efficiency and final performance. By leveraging their rich
background knowledge, LLMs can provide high-quality initial
solutions that embed prior information, thereby narrowing the
search space and establishing a stronger foundation for sub-
sequent iterations. Representative applications have demon-
strated this potential across diverse domains. In Neural Archi-
tecture Search (NAS) [127], [128], Yu et al. [129] employed
LLMs to propose plausible architectural components, introduc-
ing valuable prior knowledge before the search began. Jawahar
et al. [130] used LLMs as predictors to guide the initialization
phase of NAS more efficiently. Beyond NAS, Teukam et al.
[131] applied LLM-based initialization in genetic algorithms
for bioengineering, generating high-quality mutant libraries to
accelerate enzyme design. In financial planning, De et al. [132]
demonstrated that LLMs can generate viable initial portfolios


--- Page 10 ---
10
for optimization tasks. Despite these advances, challenges
remain. Zhao et al. [133] highlighted the high computational
cost of using LLMs for initialization, as well as their difficulty
in consistently satisfying strict constraints and generating
feasible solutions, particularly in large-scale or highly complex
problem settings.
2) Evolutionary Operators: Evolutionary operators are the
core drivers of EAs. Before the advent of LLMs, researchers
explored the use of neural networks, such as attention mech-
anisms and feed-forward networks [134], to simulate the
behavior of individual operators or even entire algorithms.
With the rise of iterative prompting approaches such as OPRO
[29], attention has shifted toward employing LLMs directly to
simulate operator behavior. This paradigm enables more adap-
tive search during optimization, as LLMs can exploit problem
and solution characteristics without additional training, thereby
surpassing the performance of conventional operators in some
settings.
Several frameworks utilize LLMs as core evolutionary op-
erators to enhance search capabilities [135]. For instance,
the LMX framework [136] employs few-shot prompting to
position LLMs as intelligent operators for the crossover and
recombination of text-based genomes. This approach facili-
tates the generation of semantically coherent variants without
the need for additional training. Building on this concept, Liu
et al. [137] proposed the LMEA framework, in which LLMs
perform selection, crossover, and mutation under prompt guid-
ance. Additionally, LMEA incorporates a temperature self-
adaptive mechanism to balance exploration and exploitation.
Other studies have targeted specific operators or algorithms.
Specifically, PAIR [138] focuses on selection, LMPSO [139]
adapts LLMs to PSO, and LEO [140] explores LLM-driven
strategies for balancing exploration and exploitation.
This research direction has recently expanded into the
domain of multi-objective optimization. Liu et al. [141] pi-
oneered the integration of zero-shot prompting with MOEA/D
[142]. This integration enables LLMs to function as search
operators, which inspired the design of efficient white-box
operators. To mitigate interaction costs, Wang et al. [143]
proposed a hybrid scheme wherein LLMs generate only 10%
of the population, while traditional operators produce the
remainder. Similarly, Liu et al. [144] employed LLMs only
when population improvement proved insufficient, relying on
NSGA-II [145] in other instances. Consequently, these hybrid
approaches demonstrate the efficacy of selectively integrating
LLMs to balance performance gains with computational effi-
ciency.
3) Algorithm Configuration: The performance of EAs is
strongly influenced by hyperparameter and operator configu-
rations. Traditionally, configuration has been studied through
static tuning [146] and dynamic control [147]. In recent years,
dynamic configuration has become particularly prominent,
largely driven by RL methods [148]–[150], given the natu-
ral alignment between the Markov Decision Process (MDP)
framework [151] and adaptive configuration. However, RL-
based approaches usually require problem-specific training,
leading to high computational costs and limited general-
ization. Leveraging their reasoning abilities, LLMs provide
a promising alternative by analyzing optimization features
and recommending adaptive configurations without extensive
retraining.
Several representative works illustrate this direction. Kramer
et al. introduced an LLM-based feedback loop for both the
static tuning [152] and dynamic control [153] of evolutionary
strategies. Custode et al. [154] applied an OPRO-based mech-
anism, which feeds optimization trajectories back to the LLM
for step-size control. Additionally, they prompted the model to
justify its parameter updates. Zhang et al. [155] addressed the
redundancy of trajectory-based feedback by proposing a meta-
prompt constructed from features of the solution space, deci-
sion space, and optimization process. They further integrated
traditional adaptive control and prior optimization experience,
thereby enabling LLM-guided adaptive operator selection.
Certain algorithm generation frameworks also incorpo-
rate hyperparameter tuning. For example, EoH [30] employs
prompt templates for parameter tuning during heuristic mu-
tation, while LLaMEA [156] integrates hyperparameter op-
timization to reduce iteration costs. However, the primary
contribution of these works lies in algorithm generation rather
than EA configuration. Consequently, they fall outside the
strict scope of configuration research.
4) Evaluation: In computationally expensive optimization
problems, Surrogate Models (SMs) [157] are widely used to
approximate costly evaluations in real-world settings, thereby
alleviating computational bottlenecks. Recently, LLMs have
been investigated for surrogate modeling. These models lever-
age pattern recognition and reasoning capabilities, which im-
proves evaluation efficiency.
Research has focused on deploying LLMs both as direct
surrogates and as managers of the optimization process. Hao
et al. [158] introduced an LLM-based surrogate that reframes
model-assisted selection as classification and regression. This
approach enables the evaluation of new solutions from his-
torical data without additional training. Building on this idea,
Nguyen et al. [159] developed the LICO framework to address
data scarcity in domains such as molecular optimization.
In addition to acting as direct surrogates, LLMs have been
employed to manage surrogate-assisted algorithms. Rios et
al. [160] incorporated LLMs into the selection and training
of SMs for engineering optimization. Furthermore, Xie et al.
[161] proposed LLM-SAEA, which uses collaborative experts
to dynamically select SMs and infill criteria. This framework
reduces reliance on extensive domain expertise.
More recently, the application of LLMs has been extended
to multi-task optimization. Zhang et al. [162] treat LLMs as
meta-surrogates that facilitate knowledge transfer across tasks
through unified token sequence representations. Collectively,
these studies highlight the potential of LLMs as both direct
surrogates and adaptive coordinators. However, further work
is required to assess their robustness and scalability in large-
scale applications.
C. High-level LLMs for Optimization Algorithms
In contrast to low-level assistance, which targets micro-level
interventions within EAs, high-level assistance operates at the


--- Page 11 ---
11
Algorithm Selection
How to match problem instances and algorithms?
Instances:
Algorithms:
Features Selector
LLMs
A. FE
B. SC
Convert to Code
Code
LLMs
NN Features
Code & Features
Meta-Prompt LLMs Target
Task:  Leverage large language models to enable high-level tasks: algorithm selection and algorithm generation.
A. Feature Extraction
B. Selector Construction
Algorithm Generation
How to generate algorithms without experts ? 
LLMs
Two Paradigms
Two Directions
RL
Symbol set:
Component pool:
A. Single-step Generation
B. Iterative Generation
Experts
&
Prompt
:
GA
PSO
GP
Classic Paradigms
:
Code
Heuristic
Algorithm Individual
Meta-Prompt C/M+S Target
LLMs
Target
Repair
Fig. 6. Illustration of high-level LLM-assisted optimization algorithms. Algorithm selection involves two key stages: feature extraction and selector construction.
Algorithm generation has evolved from single-step to iterative generation, which reflects the shift toward more adaptive and context-aware design.
macro level. The primary objective of this paradigm is to
enhance solving performance by leveraging LLMs to select
among existing algorithms or to generate entirely new ones.
This strategy enables the system to capitalize on broader op-
timization capabilities, thereby exploiting performance syner-
gies across methods. Accordingly, we systematically examine
two major forms of high-level assistance: algorithm selection
and algorithm generation.
1) Algorithm Selection: Algorithm selection aims to iden-
tify the most suitable algorithm from a portfolio for a specific
problem instance. This challenge stems from substantial per-
formance disparities among algorithms, which often exhibit
distinct optimization behaviors and application domains [9].
Traditionally, researchers formulate algorithm selection as a
machine learning task, wherein a selector is trained on problem
instances and their features to predict the optimal algorithm
for unseen cases [163]. This process typically comprises two
stages: feature extraction and selector construction. In the
first stage, the quality of extracted features, such as statistical
descriptors [164] or optimization characteristics [165], deter-
mines the efficacy of the training samples. In the second stage,
selector construction utilizes classification [166], regression
[167], or hybrid models [168] to finalize the decision logic.
Building upon this established workflow, recent studies have
integrated LLMs into both feature extraction and selector
construction. Wu et al. introduced the AS-LLM framework
[169], which pioneered the application of LLMs to this do-
main. AS-LLM leverages the code understanding capabilities
of LLMs to automatically extract high-dimensional features
from code or descriptive text. Specifically, a representation
module combines these LLM-extracted features with a selec-
tion mechanism to isolate the most relevant subset. Subse-
quently, separate networks process the algorithm and problem
features to prevent premature interaction, while a similarity
calculation module determines the final selection. In contrast,
Zhang et al. proposed InstSpecHH [170], a framework that
emphasizes selector construction. This approach employs a
pre-selection step based on Euclidean distance to narrow the
candidate algorithms. Following this, the model processes
natural language descriptions of the problem and the pre-
selected algorithms, thereby exploiting the semantic reasoning
capabilities of the LLM for context-aware selection. Distinct
from these approaches, other works address solver selection
[82], [83] by integrating LLMs into broader optimization
modeling workflows. However, these methods differ from the
classical definition of algorithm selection and are therefore
categorized separately.
2) Algorithm Generation: In contrast to algorithm config-
uration and selection, algorithm generation aims to automati-
cally design optimization algorithms from problem character-
istics without relying on pre-defined component or algorithm
pools. Reinforcement learning-based approaches have made
progress in this direction; however, they still depend on man-
ually specified component or symbol sets [39]. For instance,
the GSF framework [171] can generate algorithms within a
generic EA template, yet each component pool must be pre-
specified. Similarly, Chen et al. proposed SYMBOL [20],
which removes the component pool constraint but continues
to rely on a manually defined symbol set. Large Language
Models (LLMs) offer a promising avenue to overcome these
limitations; by directly producing algorithmic structures and
logic, LLMs bypass the need for pre-specified pools, which
establishes algorithm generation as a pivotal branch of LLM-
assisted optimization. Consequently, research in this domain
has evolved from early single-step generation toward more
advanced iterative generation frameworks [169].
Single-step generation represents the initial exploration of
this field. Pluhacek et al. [172] utilized GPT-4 to decompose
and recombine six metaheuristic algorithms, thereby creating
new hybrid variants. Similarly, Zhong et al. [173] combined
GPT-3.5 with a prompt engineering framework to generate
ZSO, i.e., a novel metaheuristic, in a single step. While these
studies demonstrate the feasibility of LLM-driven algorithm
generation, they also reveal the limitations of single-step


--- Page 12 ---
12
methods, which depend heavily on the inherent capabilities of
the model. Consequently, current research has shifted toward
iterative generation approaches to achieve greater adaptability
and robustness.
Iterative
generation
has
since
become
the
dominant
paradigm, advancing from foundational studies to real-world
applications. A pioneering work in this direction is Funsearch
[174], which integrates LLMs with evolutionary algorithms
to iteratively generate program snippets in function space.
Building on this concept, Liu et al. proposed AEL [175]
and EoH [30], which shift the focus from purely code-based
search to the co-evolution of abstract heuristic concepts and
executable programs. Specifically, the core innovation of EoH
is its dual representation co-evolutionary mechanism; it repre-
sents heuristic ideas as natural language thoughts and leverages
the LLM as a translation engine to derive corresponding
executable code snippets. The critical strength of EoH stems
from the tight coupling between high-level concepts and low-
level implementations. This paradigm enables the evolutionary
search to explore the abstract algorithmic design space through
the evolution of thoughts, which is notably more efficient than
searching the pure code space, while simultaneously ensuring
implementation fidelity. Consequently, EoH demonstrates su-
perior performance on challenging combinatorial optimization
benchmarks and generates robust heuristics that outperform
widely used handcrafted baseline algorithms.To support this
line of work, Liu et al. developed the LLM4AD platform
[176] and analyzed the multimodal, rugged fitness landscape
of LLM-assisted algorithm search [177]. Subsequently, they
fine-tuned LLMs specifically for algorithm design by intro-
ducing diversity-aware ranking, sampling strategies, and direct
preference optimization. These fine-tuned models outperform
generic ones [178]. Furthermore, the EoH framework was
extended into MEoH [179] for multi-objective heuristic search
and EoH-S [180] for co-evolving complementary sets of algo-
rithms. These extensions have been applied to edge server task
scheduling [181], Bayesian optimization [182], and adversarial
attack design [183].
In parallel, Van Stein and B¨ack developed the LLaMEA
series [156], [184], which represents a distinct trajectory
by integrating LLMs directly as operators into evolutionary
algorithms for mutation and selection tasks. This integration
enables the generation of heuristics. Unlike EoH, which fo-
cuses on concept-code coevolution, LLaMEA uses the LLM
to refine code within a standard evolutionary loop, based on
performance metrics and feedback from runtime evaluations.
To characterize this generation process, the authors introduced
code evolution graphs [185] and behavior space analysis
[186]. Additionally, they built the BLADE framework [187]
to provide standardized evaluation for LLM-driven algorithm
generation. Subsequent extensions, such as LLaMEA-HPO
[188], incorporate hyperparameter optimization to improve
efficiency. These methods have been applied to Bayesian
optimization [189] and photonic structure design [190].
Beyond these two primary research lines, several studies
have investigated alternative search paradigms [191]–[196].
For instance, Ye et al. proposed ReEvo [197], which treats the
LLM as a hyper-heuristic and introduces a reflective evolution
mechanism. Similarly, Zheng et al. developed MCTS-AHD
[198], which employs Monte Carlo Tree Search to explore the
heuristic space and avoid local optima. Pham et al. introduced
HSEvo [199], which combines harmony search with genetic
algorithms and utilizes diversity metrics to balance exploration
and exploitation. Collectively, these efforts highlight a transi-
tion from single-step demonstrations toward robust iterative
frameworks that adaptively generate optimization algorithms.
D. Challenges
Methods for applying LLMs to optimization exhibit distinct
characteristics across different structural levels. The direct
use of LLMs as optimizers, exemplified by OPRO [29],
represents the initial integration of LLMs with optimization.
Low-level approaches, such as LMEA [137] and LMX [136],
have been deployed in domains including NAS and molecular
discovery. High-level methods for algorithm generation, such
as EoH [30] and LLaMEA [156], currently represent the most
active research frontier. However, despite these advancements,
several critical bottlenecks persist:
• LLMs as Optimizers: The primary challenge within
this paradigm is a fundamental categorical mismatch.
LLMs function as sequence predictors rather than nu-
merical optimizers. Consequently, trajectory-based search
approaches must reformulate optimization as a linguis-
tic comprehension task. This method utilizes in-context
learning to infer patterns from solution-feedback histo-
ries. However, this reformulation introduces substantial
technical bottlenecks. Specifically, non-structured trajec-
tories cause attention dispersion and middle-loss effects.
In these scenarios, the model fails to attend meaning-
fully to intermediate critical states, which severely de-
grades reasoning efficacy [31], [123]. This limitation is
substantially exacerbated in the context of population-
scale evolutionary computation. Thus, while the paradigm
succeeds for discrete, language-structured problems, it is
ineffective for numerical precision or high-dimensional
continuous optimization. Future work must either develop
structured trajectory compression schemes to preserve
optimization-critical information, thereby reducing con-
text overhead, or acknowledge that LLMs function more
effectively as problem-understanding assistants than as
end-to-end optimizers.
• Low-level LLMs for Optimization Algorithms: The
fundamental challenge in this domain is the explosion of
the combinatorial decision space. Although LLMs prove
effective as local operators for discrete variation tasks,
their application to global tasks, such as initialization and
configuration, necessitates the coordination of hundreds
of interdependent decisions. These decisions span vast
combinatorial or continuous spaces. As autoregressive
sequence generators, LLMs lack explicit mechanisms for
global constraint satisfaction. Furthermore, they exhibit
conservative behavior in parameter control, which results
in insufficient exploration [133], [154]. This deficiency
arises because LLMs optimize for token prediction likeli-
hood rather than decision optimality in high-dimensional


--- Page 13 ---
13
spaces. Consequently, while this paradigm functions
when the decision scope is narrow and feedback is
immediate, performance deteriorates as decision interde-
pendence and dimensionality increase. Future work must
develop structured hierarchical decomposition strategies
to partition high-level decision problems into manageable,
LLM-addressable subproblems.
• High-level LLMs for Optimization Algorithms: Algo-
rithm generation frames optimization as a search problem
over the algorithm space. Candidates represent combina-
tions of heuristic ideas and executable code, which form
an algorithmic semantic space. Current research employs
metaheuristic search methods, primarily EC and MCTS,
to navigate this space. However, two fundamental chal-
lenges emerge. First, the computational cost is substantial.
Treating algorithms as individuals requires frequent LLM
calls and expensive benchmark-based fitness evaluations,
thereby making the search prohibitively slow. Second,
the difficulty of precise, hierarchical design limits in-
novation. LLMs struggle to faithfully decompose prob-
lem statements and accurately map that understanding
to fine-grained design decisions that span algorithmic
frameworks, operator construction, and parameter set-
tings. Consequently, current methods mainly recombine
existing strategies rather than synthesize fundamentally
novel approaches. To advance this frontier, research must
develop low-cost surrogate evaluators to accelerate the
search. Additionally, coupling LLMs with formal reason-
ing modules is essential to enable hierarchical decompo-
sition and constraint-aware synthesis.
VI. APPLICATIONS
Beyond classical numerical and combinatorial optimiza-
tion, Large Language Models (LLMs) demonstrate substantial
utility in optimization tasks across diverse interdisciplinary
domains. This section highlights representative applications in
computer science, natural sciences, and engineering. Section
VI-A discusses applications in machine learning and computer
security. Section VI-B addresses use cases in chemistry, biol-
ogy, and physics. Finally, Section VI-C focuses on modern
engineering and industrial applications.
A. Computer Science
In computer science, LLM-assisted optimization has been
explored in multiple areas, with neural architecture search
[200] and computer security [183] standing out as repre-
sentative examples. Other emerging applications [201]–[204]
further illustrate the breadth of this direction.
In the domain of Neural Architecture Search (NAS), the
integration of Large Language Models (LLMs) has primarily
focused on enhancing low-level evolutionary operators. Al-
though limited studies have treated LLMs as direct optimizers
[205], the prevailing research emphasizes their utility in low-
level assistance. Specifically, Chen et al. [200] utilized LLMs
as crossover and mutation operators within EAs [206]. In
this framework, crossover generates new architectures through
few-shot prompting, while mutation introduces diversity via
mixed-temperature sampling. Building on this foundation,
Nasir [207] applied the approach to Quality-Diversity algo-
rithms, which achieves more efficient exploration through a
dual-archive mechanism. Furthermore, Morris et al. [208]
enhanced search diversity using role-based prompts, and Mo
et al. [209] extended this paradigm to graph neural networks.
Beyond operator assistance, LLMs have also proven effective
for initialization. For instance, Yu et al. [129] leveraged
pre-training, while Jawahar et al. [130] employed few-shot
learning to provide high-quality initial suggestions.
In the context of security-related optimization, LLMs func-
tion both as stand-alone optimizers and as auxiliary operators.
Jiang et al. [117] applied LLMs directly to generate jailbreak
attack suffixes through iterative self-reflection. In contrast,
complementary research has explored low-level assistance
[210], [211], where LLMs act as heuristic operators to guide
jailbreak prompt optimization. This strategy improves both
efficiency and transferability. Additionally, Guo et al. [183]
investigated the LLM-enabled optimization of the generation
function responsible for producing adversarial samples. Con-
sequently, this approach supports the continuous innovation
and evaluation of attack strategies.
Beyond NAS and security, LLM-based optimization has
been successfully adapted to diverse emerging tasks. Wang
et al. [201] formulated data augmentation as an optimiza-
tion problem by representing augmentation strategies as ed-
itable natural language instructions. Through the simulation
of crossover and mutation, LLMs explored strategies tailored
to long-tailed distributions. Similarly, Li et al. [202] focused
on GPU computing, where LLMs iteratively generated and op-
timized CUDA kernel code. This process achieved systematic
performance improvement without human intervention.
B. Natural Sciences
In the natural sciences, including chemistry [212], biology
[213], and physics [214], many fundamental problems can
be formulated as complex optimization tasks. These problems
are typically characterized by high-dimensional search spaces,
implicit domain knowledge, and computationally expensive
simulations. Consequently, the reasoning and generative capa-
bilities of LLMs provide a promising approach to addressing
these challenges.
In chemistry, LLM-assisted optimization has been widely
explored, particularly within the domain of molecular dis-
covery [212]. Most studies emphasize low-level integration
methods, which embed LLMs directly into EAs. For example,
Wang et al. [215] integrated LLMs enhanced with chemi-
cal knowledge to perform crossover and mutation, thereby
guiding molecular design effectively. Similarly, Guevorguian
et al. [216] replaced genetic operators with fine-tuned LLMs
and introduced a prophetic model to avoid stagnation. They
later extended this concept to multi-objective optimization for
molecular discovery [217]. Beyond operator-level assistance,
researchers have also explored surrogate modeling. Specifi-
cally, Nguyen et al. [159] applied LLMs to mitigate data
scarcity in molecular optimization.
Similar to chemistry, biological applications often adopt
low-level paradigms. The LLM-GA framework [213] uses


--- Page 14 ---
14
LLMs for population initialization, which generates high-
quality mutant libraries for enzyme design. Likewise, Tran et
al. [218] and Wang et al. [219] employed LLMs as intelligent
mutation or crossover operators for protein design. Beyond
operator-level integration, Chen et al. [126], [220] developed
a two-level optimizer that iteratively modifies biological se-
quences. Furthermore, Lv et al. [221] proposed a workflow
that spans both modeling and solving. This system translates
natural language descriptions into protein function predictions
and iteratively optimizes the sequences.
In contrast, applications in physics exhibit greater diversity
in their integration levels, spanning fluid dynamics [214],
[222], semiconductor physics [223], and statistical physics
[224], [225]. At the modeling level, Du et al. [224] used
GPT-3.5 to derive partial differential equations from data,
while Zhang et al. [214] applied LLMs to turbulence closure
modeling. Ma et al. [226] proposed a holistic workflow that
integrates both modeling and solving. At the solving level, Li
et al. [223] incorporated LLMs into optimization workflows
to generate and debug code for optimizing laser parameters.
Similarly, Zhang et al. [222] directly used LLMs as optimizers
to generate parameter combinations for fluid dynamics.
C. Engineering and Industry
In engineering and industry, LLMs are increasingly applied
to complex optimization problems. Representative domains
include wireless networks and communications [115], in-
dustrial design [190], and edge computing and scheduling
[181]. Among these, wireless networks and industrial design
represent the most active areas of research.
In the domain of wireless networks, LLMs are employed
across the full workflow, ranging from problem modeling
to direct solving. Regarding modeling, Peng et al. proposed
LLM-OptiRA [227]. This system automatically detects non-
convex components in wireless resource allocation problems
and reformulates them into solvable forms. Similarly, Wen et
al. [228] introduced Retrieval-Augmented Generation (RAG)
to incorporate external expert knowledge, thereby improving
modeling accuracy. Regarding the solving phase, Qiu et al.
[115] directly utilized LLMs as optimizers for combinatorial
tasks, such as access point placement. Subsequent studies have
extended this paradigm to resource allocation [229], power
control [230], and multi-UAV deployment [231]. Additionally,
Li et al. [232] explored low-level assistance by embedding
LLMs into a multi-objective optimization framework for ISAC
systems in UAV networks. More recently, Hou et al. proposed
the LHS framework [233], which combines heuristic recom-
mendation, enhanced initialization, and iterative refinement.
In industrial design, LLMs facilitate both algorithm con-
figuration and automated generation. For example, Ghose et
al. [116] designed an LLM-based agent for chip design. This
agent performs parameter tuning to optimize performance,
power, and area. Jiang et al. [114] applied LLMs as optimizers
for design structure matrices, thereby capturing interdependen-
cies within engineering systems. Furthermore, Yin et al. [190]
explored algorithm generation in optical design. Specifically,
they applied the LLaMEA framework [156] to discover novel
algorithms for photonic structure design. These examples
demonstrate that LLMs offer distinct approaches to established
industrial optimization problems.
Beyond networks and design, LLMs have also been applied
to edge computing and scheduling. For instance, Yatong et
al. [181] leveraged LLMs for task scheduling in edge server
environments. This application demonstrates the utility of
LLMs in distributed system optimization.
VII. VISION FOR THE FIELD
The rapid emergence of LLMs for optimization has pro-
duced diverse methods across modeling, solving, and appli-
cation domains. However, despite substantial progress, the
field remains in a nascent stage. To guide future research,
we highlight three pivotal paradigm shifts that will define the
next phase of development: bridging modeling and solving,
transitioning from static to dynamic methods, and establishing
a broader agentic optimization ecosystem.
A. Bridging Modeling and Solving
Although LLMs have been successfully applied to both op-
timization modeling and solving, these domains remain largely
disjoint. Frameworks such as OptiMUS [82], [83] and ORLM
[28] begin to connect the pipeline. Specifically, OptiMUS
employs an agent-based conductor for modeling and program-
ming, while ORLM fine-tunes LLMs to generate mathematical
models and solver code. However, the solving step in these
systems continues to depend on external, traditional solvers.
This dependency confines LLMs to the front-end, **which
precludes** the incorporation of advances in LLM-assisted
solving, such as adaptive configuration, selection, or algorithm
generation.
A critical future direction is to close this gap by construct-
ing end-to-end LLM-driven workflows. Within this paradigm,
problem understanding, model formulation, algorithm design,
execution, and evaluation form a self-sufficient loop. This
integration eliminates the reliance on external solvers. **Con-
sequently**, it harnesses the generative and reasoning capa-
bilities of LLMs to create unified, closed-loop optimization
frameworks that autonomously tackle complex problems.
B. Transitioning from Static to Dynamic Methods
Current research on Large Language Models (LLMs) for op-
timization is predominantly static. These approaches typically
rely on pre-specified configurations and offline designs, rather
than adapting algorithm behavior in response to real-time
optimization signals. In contrast, the reinforcement learning
community has established robust paradigms for dynamic
configuration [49], [234], selection [235], and generation [20].
Within these frameworks, algorithm behavior adapts contin-
uously to real-time optimization features. For instance, dy-
namic selection methods leverage the complementary strengths
of multiple algorithms via adaptive scheduling. Similarly,
dynamic generation synthesizes novel operators tailored to
evolving search states.
Despite the potential of dynamic methods, dynamic al-
gorithm selection and generation remain largely unexplored


--- Page 15 ---
15
in LLM-based optimization. Current efforts are limited to
isolated tasks, such as interactive modeling [89] or adaptive
parameter control [154], [155]. To address this limitation, we
envision a decisive shift toward methods that enable the self-
evolution of algorithms. In these frameworks, LLMs contin-
ually refine, recombine, and reinvent optimization strategies
based on feedback from the search process. Consequently,
such self-evolving systems transform LLMs from passive assis-
tants into adaptive entities capable of lifelong learning, thereby
bridging optimization with open-ended algorithmic innovation.
C. Toward an Agentic Ecosystem for Optimization
The role of LLMs in optimization must be conceptualized
not as isolated engines, but as autonomous agents within a
broader agentic ecosystem. In such systems, LLMs function
as specialized agents that collaborate with human experts,
domain-specific solvers, and simulators through communica-
tion and coordination. Recent studies demonstrate this po-
tential, particularly regarding the capture of tacit expertise.
For instance, LLMs can encode driver knowledge in routing
[236], domain constraints in scheduling [237], and user prefer-
ences in traveling [238]. Consequently, this capability renders
abstract knowledge actionable within optimization workflows
[81], [89], [204], [239]. Furthermore, agentic interfaces en-
hance accessibility and trust. Specifically, LLM-based agents
can diagnose infeasibility in natural language [240], explain
decision-making details [241], [242], or interpret optimization
outcomes for end users [243]–[245].
Optimization workflows can be reimagined as multi-agent
processes. Within this paradigm, LLMs assume complemen-
tary roles, including problem formulation, solver orchestration,
explanation, and evaluation. These specialized agents work
together in dynamic teams. Such ecosystems integrate soft
knowledge that is difficult to formalize mathematically, while
simultaneously maintaining transparency and interpretability
for human collaborators. By evolving toward agentic ecosys-
tems, the field can transcend the limitations of single-model
pipelines. As a result, these systems achieve adaptive, collabo-
rative, and human-aligned optimization in complex real-world
environments.
VIII. CONCLUSION
This survey presented a structured review of the emerging
field of LLMs for optimization. As an interdisciplinary syn-
thesis of LLMs and optimization methods, this field reshapes
the foundations of optimization research. Consequently, this
integration brings new perspectives to long-standing chal-
lenges and inspires a series of novel paradigms. By structuring
the discussion from modeling to solving, we clarified how
LLMs can participate in, and ultimately transform, the entire
optimization workflow.
On the modeling side, we examined how LLMs trans-
late unstructured natural language into formal mathematical
formulations. In particular, we outlined the main workflows
and highlighted the shift from prompt-based to learning-based
methods. On the solving side, we proposed a three-level
taxonomy grounded in the interaction between LLMs and
evolutionary algorithms: (i) LLMs as stand-alone optimizers,
(ii) low-level LLMs embedded into algorithmic components,
and (iii) high-level LLMs serving as orchestrators for algo-
rithm selection and generation. Beyond these methodological
perspectives, we discussed how LLMs for optimization are
applied across computer science, the natural sciences, en-
gineering, and industry. Representative applications, such as
neural architecture search, protein and molecular design, and
wireless communication optimization, demonstrate both the
versatility of the field and its potential for substantial real-
world impact.
Looking forward, we envision LLMs for optimization evolv-
ing from isolated tools into dynamic, self-evolving, and agentic
systems. These systems will close the loop between modeling
and solving, adapt continuously to real-time feedback, and
collaborate with humans and other agents in complex environ-
ments. By outlining these trajectories, we intend for this survey
to serve not only as a systematic map of current progress but
also as a catalyst for deeper exploration and innovation in this
expanding domain.
REFERENCES
[1] Z.-G. Chen, Z.-H. Zhan, S. Kwong, and J. Zhang, “Evolutionary
computation for intelligent transportation in smart cities: A survey,”
IEEE Computational Intelligence Magazine, vol. 17, no. 2, pp. 83–
102, 2022.
[2] E. Cambria, B. White, T. Durrani, and N. Howard, “Computational
intelligence for natural language processing,” IEEE Computational
Intelligence Magazine, vol. 9, no. 1, pp. 19–63, 2014.
[3] T. Chai, Y. Jin, and S. Bernhard, “Evolutionary complex engineering
optimization: opportunities and challenges,” IEEE Computational In-
telligence Magazine, vol. 8, no. 3, pp. 12–15, 2013.
[4] A. Mahor, V. Prasad, and S. Rangnekar, “Economic dispatch using
particle swarm optimization: A review,” Renewable and Sustainable
Energy Reviews, vol. 13, no. 8, pp. 2134–2141, 2009.
[5] K. Terayama, M. Sumita, R. Tamura, and K. Tsuda, “Black-box
optimization for automated discovery,” Accounts of Chemical Research,
vol. 54, no. 6, pp. 1334–1346, 2021.
[6] P. Festa, “A brief introduction to exact, approximation, and heuristic
algorithms for solving hard combinatorial optimization problems,” in
Proceedings of the International Conference on Transparent Optical
Networks, 2014, pp. 1–20.
[7] D. H. Wolpert and W. G. Macready, “No free lunch theorems for
optimization,” IEEE Transactions on Evolutionary Computation, vol. 1,
no. 1, pp. 67–82, 2002.
[8] F. Hutter, H. H. Hoos, K. Leyton-Brown, and T. St¨utzle, “Paramils:
An automatic algorithm configuration framework,” Journal of Artificial
Intelligence Research, vol. 36, pp. 267–306, 2009.
[9] P. Kerschke, H. H. Hoos, F. Neumann, and H. Trautmann, “Automated
algorithm selection: Survey and perspectives,” Evolutionary Computa-
tion, vol. 27, no. 1, pp. 3–45, 2019.
[10] H. M¨uller-Merbach, “Heuristics and their design: A survey,” European
Journal of Operational Research, vol. 8, no. 1, pp. 1–23, 1981.
[11] J. Zhang, Z. Zhan, Y. Lin, N. Chen, Y. Gong, J. Zhong, H. S. Chung,
Y. Li, and Y. Shi, “Evolutionary computation meets machine learning:
A survey,” IEEE Computational Intelligence Magazine, vol. 6, no. 4,
pp. 68–75, 2011.
[12] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement
learning: A survey,” Journal of Artificial Intelligence Research, vol. 4,
pp. 237–285, 1996.
[13] Y. Li, “Deep reinforcement learning: An overview,” arXiv preprint
arXiv:1701.07274, 2017.
[14] N. Mazyavkina, S. Sviridov, S. Ivanov, and E. Burnaev, “Reinforcement
learning for combinatorial optimization: A survey,” Computers &
Operations Research, vol. 134, p. 105400, 2021.
[15] T. N. Mundhenk, M. Landajuela, R. Glatt, C. P. Santiago, D. M. Faissol,
and B. K. Petersen, “Symbolic regression via neural-guided genetic
programming population seeding,” arXiv preprint arXiv:2111.00053,
2021.


--- Page 16 ---
16
[16] Y. Wang, T. Zhang, Y. Chang, X. Wang, B. Liang, and B. Yuan, “A
surrogate-assisted controller for expensive evolutionary reinforcement
learning,” Information Sciences, vol. 616, pp. 539–557, 2022.
[17] J. E. Pettinger and R. M. Everson, “Controlling genetic algorithms
with reinforcement learning,” in Proceedings of the Genetic and
Evolutionary Computation Conference, 2002, pp. 692–692.
[18] A. Eiben, M. Horvath, W. Kowalczyk, and M. C. Schut, “Rein-
forcement learning for online control of evolutionary algorithms,”
in Proceedings of the International Workshop on Engineering Self-
Organising Applications, 2006, pp. 151–160.
[19] M. G. Lagoudakis and M. L. Littman, “Algorithm selection using
reinforcement learning,” in Proceedings of the International Conference
on Machine Learning, 2000, pp. 511–518.
[20] J. Chen, Z. Ma, H. Guo, Y. Ma, J. Zhang, and y. Gong, “Symbol:
Generating flexible black-box optimizers through symbolic equation
learning,” arXiv preprint arXiv:2402.02355, 2024.
[21] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen,
J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J. Nie, and J. Wen, “A
survey of large language models,” arXiv preprint arXiv:2303.18223,
2023.
[22] X. Yan, Y. Xiao, and Y. Jin, “Generative large language models
explained [ai-explained],” IEEE Computational Intelligence Magazine,
vol. 19, no. 4, pp. 45–46, 2024.
[23] R. Patil and V. Gudivada, “A review of current trends, techniques, and
challenges in large language models (llms),” Applied Sciences, vol. 14,
no. 5, p. 2074, 2024.
[24] J. Wei, X. Wang, D. Schuurmans, M. Bosma, I. Brian, F. Xia, E. Chi,
Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning
in large language models,” Advances in Neural Information Processing
Systems, vol. 35, pp. 24 824–24 837, 2022.
[25] S. Feng, W. Shi, Y. Wang, W. Ding, V. Balachandran, and Y. Tsvetkov,
“Don’t hallucinate, abstain: Identifying llm knowledge gaps via multi-
llm collaboration,” arXiv preprint arXiv:2402.00367, 2024.
[26] F. Mi, Y. Li, Y. Zeng, J. Zhou, Y. Wang, C. Xu, L. Shang,
X. Jiang, S. Zhao, and Q. Liu, “Pangu-bot: efficient generative dia-
logue pre-training from pre-trained language model,” arXiv preprint
arXiv:2203.17090, 2022.
[27] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,
P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large
language model for finance,” arXiv preprint arXiv:2303.17564, 2023.
[28] C. Huang, Z. Tang, S. Hu, R. Jiang, X. Zheng, D. Ge, B. Wang, and
Z. Wang, “Orlm: A customizable framework in training large models
for automated optimization modeling,” Operations Research, 2025.
[29] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen,
“Large language models as optimizers,” in Proceedings of the Interna-
tional Conference on Learning Representations, 2023, pp. 1–10.
[30] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu,
and Q. Zhang, “Evolution of heuristics: Towards efficient auto-
matic algorithm design using large language model,” arXiv preprint
arXiv:2401.02051, 2024.
[31] B. Huang, X. Wu, Y. Zhou, J. Wu, L. Feng, R. Cheng, and K. C. Tan,
“Exploring the true potential: Evaluating the black-box optimization
capability of large language models,” arXiv preprint arXiv:2404.06290,
2024.
[32] X. Wu, S. Wu, J. Wu, L. Feng, and K. C. Tan, “Evolutionary
computation in the era of large language model: survey and roadmap,”
IEEE Transactions on Evolutionary Computation, vol. 29, no. 2, pp.
534–554, 2025.
[33] S. Huang, K. Yang, S. Qi, and R. Wang, “When large language model
meets optimization,” Swarm and Evolutionary Computation, vol. 90,
p. 101663, 2024.
[34] H. Yu and J. Liu, “Deep insights into automated optimization with
large language models and evolutionary algorithms,” arXiv preprint
arXiv:2410.20848, 2024.
[35] W. Chao, J. Zhao, L. Jiao, L. Li, F. Liu, and S. Yang, “When
large language models meet evolutionary algorithms,” arXiv preprint
arXiv:2401.10510, 2024.
[36] T. Bartz-Beielstein, J. Branke, J. Mehnen, and O. Mersmann, “Evolu-
tionary algorithms,” Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, vol. 4, no. 3, pp. 178–195, 2014.
[37] C. M. Fonseca and P. J. Fleming, “An overview of evolutionary
algorithms in multiobjective optimization,” Evolutionary Computation,
vol. 3, no. 1, pp. 1–16, 1995.
[38] F. Liu, Y. Yao, P. Guo, Z. Yang, Z. Zhao, X. Lin, X. Tong, M. Yuan,
Z. Lu, Z. Wang, and Q. Zhang, “A systematic survey on large language
models for algorithm design,” arXiv preprint arXiv:2410.14716, 2024.
[39] Z. Ma, H. Guo, Y. Gong, J. Zhang, and K. C. Tan, “Toward automated
algorithm design: A survey and practical guide to meta-black-box-
optimization,” IEEE Transactions on Evolutionary Computation, 2025.
[40] R. Cheng and Y. Jin, “A competitive swarm optimizer for large scale
optimization,” IEEE Transactions on Cybernetics, vol. 45, no. 2, pp.
191–204, 2014.
[41] Y. Tian, R. Cheng, X. Zhang, and Y. Jin, “Platemo: A matlab platform
for evolutionary multi-objective optimization [educational forum],”
IEEE Computational Intelligence Magazine, vol. 12, no. 4, pp. 73–
87, 2017.
[42] J. H. Holland, “Genetic algorithms,” Scientific American, vol. 267,
no. 1, pp. 66–73, 1992.
[43] J. R. Koza, “Genetic programming as a means for programming
computers by natural selection,” Statistics and Computing, vol. 4, no. 2,
pp. 87–112, 1994.
[44] S. Das and P. N. Suganthan, “Differential evolution: A survey of
the state-of-the-art,” IEEE Transactions on Evolutionary Computation,
vol. 15, no. 1, pp. 4–31, 2010.
[45] J. Kennedy and R. Eberhart, “Particle swarm optimization,” in Pro-
ceedings of the International Conference on Neural Networks, vol. 4,
1995, pp. 1942–1948.
[46] A. Arias-Montano, C. A. C. Coello, and E. Mezura-Montes, “Multiob-
jective evolutionary algorithms in aeronautical and aerospace engineer-
ing,” IEEE Transactions on Evolutionary Computation, vol. 16, no. 5,
pp. 662–694, 2012.
[47] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
survey,” Journal of Machine Learning Research, vol. 20, no. 55, pp.
1–21, 2019.
[48] E.-G. Talbi, “Machine learning into metaheuristics: A survey and
taxonomy,” ACM Computing Surveys, vol. 54, no. 6, pp. 1–32, 2021.
[49] Y. Tian, X. Li, H. Ma, X. Zhang, K. C. Tan, and Y. Jin, “Deep rein-
forcement learning based adaptive operator selection for evolutionary
multi-objective optimization,” IEEE Transactions on Emerging Topics
in Computational Intelligence, vol. 7, no. 4, pp. 1051–1064, 2022.
[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in Neural Information Processing Systems, vol. 30, 2017.
[51] P. Cai, Y. Fan, and F. Leu, “Compare encoder-decoder, encoder-only,
and decoder-only architectures for text generation on low-resource
datasets,” in Proceedings of the International Conference on Broadband
and Wireless Computing, Communication and Applications, 2021, pp.
216–225.
[52] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,
V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-
sequence pre-training for natural language generation, translation, and
comprehension,” arXiv preprint arXiv:1910.13461, 2019.
[53] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning
with a unified text-to-text transformer,” Journal of Machine Learning
Research, vol. 21, no. 1, pp. 5485–5551, 2020.
[54] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “Bert: pre-training
of deep bidirectional transformers for language understanding,” in
Proceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Tech-
nologies, 2019, pp. 4171–4186.
[55] L. Floridi and M. Chiriatti, “Gpt-3: its nature, scope, limits, and
consequences,” Minds and Machines, vol. 30, no. 4, pp. 681–694, 2020.
[56] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,
Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:
when the large language model meets programming–the rise of code
intelligence,” arXiv preprint arXiv:2401.14196, 2024.
[57] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. El-
nashar, J. Spencer-Smith, and D. C. Schmidt, “A prompt pattern
catalog to enhance prompt engineering with chatgpt,” arXiv preprint
arXiv:2302.11382, 2023.
[58] Z. Han, C. Gao, J. Liu, J. Zhang, and S. Q. Zhang, “Parameter-efficient
fine-tuning for large models: A comprehensive survey,” arXiv preprint
arXiv:2403.14608, 2024.
[59] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large lan-
guage models are zero-shot reasoners,” Advances in Neural Information
Processing Systems, vol. 35, pp. 22 199–22 213, 2022.
[60] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and
K. Narasimhan, “Tree of thoughts: Deliberate problem solving with
large language models,” Advances in Neural Information Processing
Systems, vol. 36, pp. 11 809–11 822, 2023.
[61] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk,


--- Page 17 ---
17
and T. Hoefler, “Graph of thoughts: solving elaborate problems with
large language models,” in Proceedings of the AAAI Conference on
Artificial Intelligence, 2024, pp. 17 682–17 690.
[62] C. Li, J. Liang, A. Zeng, X. Chen, K. Hausman, D. Sadigh,
S. Levine, L. Fei-Fei, F. Xia, and B. Ichter, “Chain of code: Reasoning
with a language model-augmented code emulator,” arXiv preprint
arXiv:2312.04474, 2023.
[63] X. Li, R. Zhao, Y. K. Chia, B. Ding, S. Joty, S. Poria, and
L. Bing, “Chain-of-knowledge: Grounding large language models
via dynamic knowledge adapting over heterogeneous sources,” arXiv
preprint arXiv:2305.13269, 2023.
[64] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,”
Advances in Neural Information Processing Systems, vol. 36, pp.
34 892–34 916, 2023.
[65] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,
Y. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu,
Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat,
K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao,
Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin,
A. Roberts, D. Zhou, Q. V. Le, and J. Wei, “Scaling instruction-
finetuned language models,” Journal of Machine Learning Research,
vol. 25, no. 70, pp. 1–53, 2024.
[66] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton,
F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano,
J. Leike, and R. Lowe, “Training language models to follow instructions
with human feedback,” Advances in Neural Information Processing
Systems, vol. 35, pp. 27 730–27 744, 2022.
[67] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and
C. Finn, “Direct preference optimization: Your language model is
secretly a reward model,” Advances in Neural Information Processing
Systems, vol. 36, pp. 53 728–53 741, 2023.
[68] T. P. Pawlak and K. Krawiec, “Automatic synthesis of constraints from
examples using mixed integer linear programming,” European Journal
of Operational Research, vol. 261, no. 3, pp. 1141–1157, 2017.
[69] T. P. Pawlak and M. O’Neill, “Grammatical evolution for constraint
synthesis for mixed-integer linear programming,” Swarm and Evolu-
tionary Computation, vol. 64, p. 100896, 2021.
[70] Q. Li, L. Zhang, and V. Mak-Hau, “Synthesizing mixed-integer lin-
ear programming models from natural language descriptions,” arXiv
preprint arXiv:2311.15271, 2023.
[71] R. Ramamonjison, H. Li, T. T. Yu, S. He, V. Rengan, A. Banitalebi-
Dehkordi, Z. Zhou, and Y. Zhang, “Augmenting operations research
with auto-formulation of optimization models from problem descrip-
tions,” arXiv preprint arXiv:2209.15565, 2022.
[72] R. Ramamonjison, T. Yu, R. Li, H. Li, G. Carenini, B. Ghaddar, S. He,
M. Mostajabdaveh, A. Banitalebi-Dehkordi, Z. Zhou, and Y. Zhang,
“Nl4opt competition: Formulating optimization problems based on
their natural language descriptions,” in Proceedings of the Neural
Information Processing Systems Competition Track, 2023, pp. 189–
203.
[73] K. Wang, Z. Chen, and J. Zheng, “Opd@ nl4opt: An ensemble
approach for the ner task of the optimization problem,” arXiv preprint
arXiv:2301.02459, 2023.
[74] X. Doan, “Vtcc-nlp at nl4opt competition subtask 1: An ensemble pre-
trained language models for named entity recognition,” arXiv preprint
arXiv:2212.07219, 2022.
[75] N. Gangwar and N. Kani, “Highlighting named entities in input for
auto-formulation of optimization problems,” in Proceedings of the
International Conference on Intelligent Computer Mathematics, 2023,
pp. 130–141.
[76] Y. Ning, J. Liu, L. Qin, T. Xiao, S. Xue, Z. Huang, Q. Liu, E. Chen,
and J. Wu, “A novel approach for auto-formulation of optimization
problems,” arXiv preprint arXiv:2302.04643, 2023.
[77] B. Almonacid, “Towards an automatic optimisation model genera-
tor assisted with generative pre-trained transformer,” arXiv preprint
arXiv:2305.05811, 2023.
[78] D. Tsouros, H. Verhaeghe, S. Kadıo˘glu, and T. Guns, “Holy grail
2.0: From natural language to constraint models,” arXiv preprint
arXiv:2308.01589, 2023.
[79] M. Jin, B. Sel, F. Hardeep, and W. Yin, “Democratizing energy man-
agement with llm-assisted optimization autoformalism,” in Proceedings
of the International Conference on Communications, Control, and
Computing Technologies for Smart Grids, 2024, pp. 258–263.
[80] T. de la Rosa, S. Gopalakrishnan, A. Pozanco, Z. Zeng, and
D. Borrajo, “Trip-pal: Travel planning with guarantees by combin-
ing large language models and automated planners,” arXiv preprint
arXiv:2406.10196, 2024.
[81] Z. Xiao, D. Zhang, Y. Wu, L. Xu, Y. J. Wang, X. Han, X. Fu, T. Zhong,
J. Zeng, and M. Song, “Chain-of-experts: When llms meet complex
operations research problems,” in Proceedings of the International
Conference on Learning Representations, 2023.
[82] A. AhmadiTeshnizi, W. Gao, and M. Udell, “Optimus: Scalable opti-
mization modeling with (mi) lp solvers and large language models,”
arXiv preprint arXiv:2402.10172, 2024.
[83] A. AhmadiTeshnizi, W. Gao, H. Brunborg, S. Talaei, C. Law-
less, and M. Udell, “Optimus-0.3: Using large language models to
model and solve optimization problems at scale,” arXiv preprint
arXiv:2407.19633, 2024.
[84] Z. Wang, B. Chen, Y. Huang, Q. Cao, M. He, J. Fan, and X. Liang,
“Ormind: A cognitive-inspired end-to-end reasoning framework for
operations research,” arXiv preprint arXiv:2506.01326, 2025.
[85] K. Liang, Y. Lu, J. Mao, S. Sun, C. Yang, C. Zeng, X. Jin, H. Qin,
R. Zhu, and C.-P. Teo, “Llm for large-scale optimization model auto-
formulation: A lightweight few-shot learning approach,” 2025.
[86] M. Mostajabdaveh, T. T. Yu, R. Ramamonjison, G. Carenini, Z. Zhou,
and Y. Zhang, “Optimization modeling and verification from problem
specifications using a multi-agent multi-stage llm framework,” Infor:
Information Systems and Operational Research, vol. 62, no. 4, pp.
599–617, 2024.
[87] A. Talebi, “Large language model-based automatic formulation for
stochastic optimization models,” arXiv preprint arXiv:2508.17200,
2025.
[88] J. Zhang, W. Wang, S. Guo, L. Wang, F. Lin, C. Yang, and W. Yin,
“Solving general natural-language-description optimization problems
with large language models,” arXiv preprint arXiv:2407.07924, 2024.
[89] C. Lawless, J. Schoeffer, L. Le, K. Rowan, S. Sen, C. St. Hill, J. Suh,
and B. Sarrafzadeh, “I want it that way: Enabling interactive decision
support using large language models and constraint programming,”
ACM Transactions on Interactive Intelligent Systems, vol. 14, no. 3,
pp. 1–33, 2024.
[90] L. Gomez Tobon and E. Law, “Values in the loop: Designing interactive
optimization with conversational feedback,” in Proceedings of the ACM
Conference on Conversational User Interfaces, 2025, pp. 1–5.
[91] H. Deng, B. Zheng, Y. Jiang, and T. H. Tran, “Cafa: Coding as auto-
formulation can boost large language models in solving linear pro-
gramming problem,” in Proceedings of the Workshop on Mathematical
Reasoning and Artificial Intelligence at the Conference on Neural
Information Processing Systems, 2024.
[92] N. Astorga, T. Liu, Y. Xiao, and M. van der Schaar, “Autoformula-
tion of mathematical optimization models using llms,” arXiv preprint
arXiv:2411.01679, 2024.
[93] J. Li, R. Wickman, S. Bhatnagar, R. K. Maity, and A. Mukherjee,
“Abstract operations research modeling using natural language inputs,”
Information, vol. 16, no. 2, p. 128, 2025.
[94] X. Huang, Q. Shen, Y. Hu, A. Gao, and B. Wang, “Llms for math-
ematical modeling: Towards bridging the gap between natural and
mathematical languages,” arXiv preprint arXiv:2405.13144, 2024.
[95] Z. Wang, Z. Zhu, Y. Han, Y. Lin, Z. Lin, R. Sun, and T. Ding,
“Optibench: Benchmarking large language models in optimization
modeling with equivalence-detection evaluation,” 2024.
[96] H. Zhai, C. Lawless, E. Vitercik, and L. Leqi, “Equivamap: Leveraging
llms for automatic equivalence checking of optimization formulations,”
arXiv preprint arXiv:2502.14760, 2025.
[97] P. P. Dakle, S. Kadıo˘glu, K. Uppuluri, R. Politi, P. Raghavan, S. Ralla-
bandi, and R. Srinivasamurthy, “Ner4opt: Named entity recognition for
optimization modelling from natural language,” in Proceedings of the
International Conference on Integration of Constraint Programming,
Artificial Intelligence, and Operations Research, 2023, pp. 299–319.
[98] T. Ahmed and S. Choudhury, “Lm4opt: Unveiling the potential of large
language models in formulating mathematical optimization problems,”
INFOR: Information Systems and Operational Research, vol. 62, no. 4,
pp. 559–572, 2024.
[99] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient
foundation language models,” arXiv preprint arXiv:2302.13971, 2023.
[100] OpenAI, “Gpt-4 technical report,” arXiv preprint ArXiv:2303.08774,
2023.
[101] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier,
L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang,


--- Page 18 ---
18
T. Lacroix, and W. E. Sayed, “Mistral 7b,” CoRR, vol. abs/2310.06825,
2023.
[102] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang,
Y. Li, Y. Wu, and D. Guo, “Deepseekmath: Pushing the limits of
mathematical reasoning in open language models,” arXiv preprint
arXiv:2402.03300, 2024.
[103] V. Lima, D. T. Phan, J. Kalagnanam, D. Patel, and N. Zhou, “Toward
a trustworthy optimization modeling agent via verifiable synthetic data
generation,” arXiv preprint arXiv:2508.03117, 2025.
[104] Z. Yang, Y. Wang, Y. Huang, Z. Guo, W. Shi, X. Han, L. Feng,
L. Song, X. Liang, and J. Tang, “Optibench meets resocratic: Mea-
sure and improve llms for optimization modeling,” arXiv preprint
arXiv:2407.09887, 2024.
[105] H. Lu, Z. Xie, Y. Wu, C. Ren, Y. Chen, and Z. Wen, “Optmath:
A scalable bidirectional data synthesis framework for optimization
modeling,” arXiv preprint arXiv:2502.11102, 2025.
[106] C. Zhou, J. Yang, L. Xin, Y. Chen, Z. He, and D. Ge, “Auto-formulating
dynamic programming problems with large language models,” arXiv
preprint arXiv:2507.11737, 2025.
[107] T. Wang, W. Yu, Z. He, Z. Liu, H. Gong, H. Wu, X. Han, W. Shi,
R. She, F. Zhu, and T. Zhong, “Bpp-search: Enhancing tree of thought
reasoning for mathematical modeling problem solving,” arXiv preprint
arXiv:2411.17404, 2024.
[108] Y. Wu, Y. Zhang, Y. Wu, Y. Wang, J. Zhang, and J. Cheng, “Step-opt:
Boosting optimization modeling in llms through iterative data synthesis
and structured validation,” arXiv preprint arXiv:2506.17637, 2025.
[109] C. Jiang, X. Shu, H. Qian, X. Lu, J. Zhou, A. Zhou, and Y. Yu,
“Llmopt: Learning to define and solve general optimization problems
from scratch,” arXiv preprint arXiv:2410.13213, 2024.
[110] Y. Chen, J. Xia, S. Shao, D. Ge, and Y. Ye, “Solver-informed rl:
Grounding large language models for authentic optimization model-
ing,” arXiv preprint arXiv:2505.11792, 2025.
[111] P. T. Amarasinghe, S. Nguyen, Y. Sun, and D. Alahakoon, “Ai-copilot
for business optimisation: A framework and a case study in production
scheduling,” arXiv preprint arXiv:2309.13218, 2023.
[112] C. Zhou, T. Xu, J. Lin, and D. Ge, “Steporlm: A self-evolving
framework with generative process supervision for operations research
language models,” arXiv preprint arXiv:2509.22558, 2025.
[113] P. Guo, Y. Chen, Y. Tsai, and S. Lin, “Towards optimizing with large
language models,” arXiv preprint ArXiv:2310.05204, 2023.
[114] S. Jiang, M. Xie, and J. Luo, “Large language models for com-
binatorial optimization of design structure matrix,” arXiv preprint
arXiv:2411.12571, 2024.
[115] K. Qiu, S. Bakirtzis, I. Wassell, H. Song, J. Zhang, and K. Wang,
“Large language model-based wireless network design,” IEEE Wireless
Communications Letters, vol. 13, no. 12, pp. 3340–3344, 2024.
[116] A. Ghose, A. B. Kahng, S. Kundu, and Z. Wang, “Orfs-agent:
Tool-using agents for chip design optimization,” arXiv preprint
arXiv:2506.08332, 2025.
[117] W. Jiang, Z. Wang, J. Zhai, S. Ma, Z. Zhao, and C. Shen, “An
optimizable suffix is worth a thousand templates: Efficient black-box
jailbreaking without affirmative phrases via llm as optimizer,” arXiv
preprint arXiv:2408.11313, 2024.
[118] R. T. Lange, Y. Tian, and Y. Tang, “Large language models as evolution
strategies,” arXiv preprint arXiv:2402.18381, 2024.
[119] C. Cheng, A. Nie, and A. Swaminathan, “Trace is the next autodiff:
Generative optimization with rich feedback, execution traces, and llms,”
Advances in Neural Information Processing Systems, vol. 37, pp.
71 596–71 642, 2024.
[120] Y. Huang, W. Zhang, L. Feng, X. Wu, and K. C. Tan, “How multimodal
integration boost the performance of llm for optimization: Case study
on capacitated vehicle routing problems,” in Proceedings of the IEEE
Symposium for Multidisciplinary Computational Intelligence Incuba-
tors, 2025, pp. 1–7.
[121] M. Elhenawy, A. Abutahoun, T. I. Alhadidi, A. Jaber, H. I. Ashqar,
S. Jaradat, A. Abdelhay, S. Glaser, and A. Rakotonirainy, “Visual
reasoning and multi-agent approach in multimodal large language
models (mllms): Solving tsp and mtsp combinatorial challenges,” arXiv
preprint arXiv:2407.00092, 2024.
[122] J. Zhao, K. H. Cheong, and W. Pedrycz, “Bridging visualization and
optimization: Multimodal large language models on graph-structured
combinatorial optimization,” arXiv preprint arXiv:2501.11968, 2025.
[123] T. Zhang, J. Yuan, and S. Avestimehr, “Revisiting opro: The limitations
of small-scale llms as optimizers,” arXiv preprint arXiv:2405.10276,
2024.
[124] H. Abgaryan, A. Harutyunyan, and T. Cazenave, “Llms can schedule,”
arXiv preprint arXiv:2408.06993, 2024.
[125] X. Li, K. Wu, X. Zhang, H. Wang, J. Liu, and Y. B. Li, “Pretrained
optimization model for zero-shot black box optimization,” Advances
in Neural Information Processing Systems, vol. 37, pp. 14 283–14 324,
2024.
[126] A. Chen, S. D. Stanton, F. Ding, R. G. Alberstein, A. M. Watkins,
R. Bonneau, V. Gligorijevi´c, K. Cho, and N. C. Frey, “Generalists vs.
specialists: Evaluating llms on highly-constrained biophysical sequence
optimization tasks,” arXiv preprint arXiv:2410.22296, 2024.
[127] Y. Xue, Y. Wang, J. Liang, and A. Slowik, “A self-adaptive mutation
neural architecture search algorithm based on blocks,” IEEE Compu-
tational Intelligence Magazine, vol. 16, no. 3, pp. 67–78, 2021.
[128] Y. Huang, K. Sun, Y. Ma, and R. Cheng, “Exploring neural architecture
search spaces via visual analytics [application notes],” IEEE Compu-
tational Intelligence Magazine, vol. 20, no. 4, pp. 99–112, 2025.
[129] C. Yu, X. Liu, Y. Wang, Y. Liu, W. Feng, X. Deng, C. Tang, and J. Lv,
“Gpt-nas: Evolutionary neural architecture search with the generative
pre-trained model,” arXiv preprint arXiv:2305.05351, 2023.
[130] G. Jawahar, M. Abdul-Mageed, L. V. Lakshmanan, and D. Ding, “Llm
performance predictors are good initializers for architecture search,”
arXiv preprint arXiv:2310.16712, 2023.
[131] Y. G. N. Teukam, F. Zipoli, T. Laino, E. Criscuolo, F. Grisoni, and
M. Manica, “Integrating genetic algorithms and language models for
enhanced enzyme design,” arXiv preprint, 2024.
[132] I. De Zarz`a, J. De Curt`o, G. Roig, and C. T. Calafate, “Optimized
financial planning: Integrating individual and cooperative budgeting
models with llm recommendations,” AI, vol. 5, no. 1, pp. 91–114, 2023.
[133] J. Zhao, T. Wen, and K. H. Cheong, “Can large language models be
trusted as evolutionary optimizers for network-structured combinatorial
problems?” IEEE Transactions on Network Science and Engineering,
pp. 1–17, 2025.
[134] R. Lange, Y. Tian, and Y. Tang, “Evolution transformer: In-context
evolutionary optimization,” in Proceedings of the Genetic and Evolu-
tionary Computation Conference Companion, 2024, pp. 575–578.
[135] Y. Huang, X. Lv, S. Wu, J. Wu, L. Feng, and K. C. Tan, “Advancing
automated knowledge transfer in evolutionary multitasking via large
language models,” arXiv preprint arXiv:2409.04270, 2024.
[136] E. Meyerson, M. J. Nelson, H. Bradley, A. Gaier, A. Moradi, A. K.
Hoover, and J. Lehman, “Language model crossover: Variation through
few-shot prompting,” ACM Transactions on Evolutionary Learning,
vol. 4, no. 4, pp. 1–40, 2024.
[137] S. Liu, C. Chen, X. Qu, K. Tang, and Y. S. Ong, “Large language mod-
els as evolutionary optimizers,” in Proceedings of the IEEE Congress
on Evolutionary Computation, 2024, pp. 1–8.
[138] S. Ali, M. Ashraf, S. Hegazy, F. Salem, H. Mokhtar, M. M. Gaber, and
M. T. Alrefaie, “Pair: A novel large language model-guided selection
strategy for evolutionary algorithms,” arXiv preprint arXiv:2503.03239,
2025.
[139] Y. Shinohara, J. Xu, T. Li, and H. Iba, “Large language models as
particle swarm optimizers,” arXiv preprint arXiv:2504.09247, 2025.
[140] S. Brahmachary, S. M. Joshi, A. Panda, K. Koneripalli, A. K. Sagotra,
H. Patel, A. Sharma, A. D. Jagtap, and K. Kalyanaraman, “Large
language model-based evolutionary optimizer: Reasoning with elitism,”
Neurocomputing, vol. 622, p. 129272, 2025.
[141] F. Liu, X. Lin, S. Yao, Z. Wang, X. Tong, M. Yuan, and Q. Zhang,
“Large language model for multiobjective evolutionary optimization,”
in Proceedings of the International Conference on Evolutionary Multi-
Criterion Optimization, 2025, pp. 178–191.
[142] Q. Zhang and H. Li, “Moea/d: A multiobjective evolutionary algorithm
based on decomposition,” IEEE Transactions on Evolutionary Compu-
tation, vol. 11, no. 6, pp. 712–731, 2007.
[143] Z. Wang, S. Liu, J. Chen, and K. C. Tan, “Large language model-aided
evolutionary search for constrained multiobjective optimization,” in
Proceedings of the International Conference on Intelligent Computing,
2024, pp. 218–230.
[144] W. Liu, L. Chen, and Z. Tang, “Large language model aided multi-
objective evolutionary algorithm: A low-cost adaptive approach,” arXiv
preprint arXiv:2410.02301, 2024.
[145] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and
elitist multiobjective genetic algorithm: Nsga-ii,” IEEE Transactions
on Evolutionary Computation, vol. 6, no. 2, pp. 182–197, 2002.
[146] C. Huang, Y. Li, and X. Yao, “A survey of automatic parameter
tuning methods for metaheuristics,” IEEE Transactions on Evolutionary
Computation, vol. 24, no. 2, pp. 201–216, 2019.
[147] G. Karafotias, M. Hoogendoorn, and ´A. E. Eiben, “Parameter control
in evolutionary algorithms: Trends and challenges,” IEEE Transactions
on Evolutionary Computation, vol. 19, no. 2, pp. 167–187, 2014.


--- Page 19 ---
19
[148] R. Chen, B. Yang, S. Li, and S. Wang, “A self-learning genetic algo-
rithm based on reinforcement learning for flexible job-shop scheduling
problem,” Computers & Industrial Engineering, vol. 149, p. 106778,
2020.
[149] J. Pei, J. Liu, and Y. Mei, “Learning from offline and online experi-
ences: A hybrid adaptive operator selection framework,” in Proceedings
of the Genetic and Evolutionary Computation Conference, 2024, pp.
1017–1025.
[150] Y. Zhang, G. Yi, H. Wang, Y. Cheng, Y. Chen, and Z. Wei, “An
improved abc algorithm based on deep reinforcement learning for
multi-uav target assignment,” in Proceedings of the China Automation
Congress, 2024, pp. 4921–4926.
[151] M. L. Puterman, “Markov decision processes,” Handbooks in Opera-
tions Research and Management Science, vol. 2, pp. 331–434, 1990.
[152] O. Kramer, “Large language models for tuning evolution strategies,”
arXiv preprint arXiv:2405.10999, 2024.
[153] ——, “Llama tunes cma-es,” in Proceedings of the European Sympo-
sium on Artificial Neural Networks, 2024.
[154] L. L. Custode, F. Caraffini, A. Yaman, and G. Iacca, “An investigation
on the use of large language models for hyperparameter tuning in evo-
lutionary algorithms,” in Proceedings of the Genetic and Evolutionary
Computation Conference Companion, 2024, pp. 1838–1845.
[155] Y. Zhang and G. Yi, “Laos: Large language model-driven adaptive
operator selection for evolutionary algorithms,” in Proceedings of the
Genetic and Evolutionary Computation Conference, 2025, pp. 517–
526.
[156] N. van Stein and T. B¨ack, “Llamea: A large language model evolu-
tionary algorithm for automatically generating metaheuristics,” IEEE
Transactions on Evolutionary Computation, vol. 29, no. 2, pp. 331–
345, 2024.
[157] Q. Liu, F. Lanfermann, T. Rodemann, M. Olhofer, and Y. Jin,
“Surrogate-assisted many-objective optimization of building energy
management,” IEEE Computational Intelligence Magazine, vol. 18,
no. 4, pp. 14–28, 2023.
[158] H. Hao, X. Zhang, and A. Zhou, “Large language models as surrogate
models in evolutionary algorithms: A preliminary study,” Swarm and
Evolutionary Computation, vol. 91, p. 101741, 2024.
[159] T. Nguyen and A. Grover, “Lico: Large language models for in-context
molecular optimization,” arXiv preprint arXiv:2406.18851, 2024.
[160] T. Rios, F. Lanfermann, and S. Menzel, “Large language model-assisted
surrogate modelling for engineering optimization,” in Proceedings of
the IEEE Conference on Artificial Intelligence, 2024, pp. 796–803.
[161] L. Xie, G. Li, Z. Wang, E. Chung, and M. Gong, “Large language
model-driven surrogate-assisted evolutionary algorithm for expensive
optimization,” arXiv preprint arXiv:2507.02892, 2025.
[162] X. Zhang, Y. Gong, and J. Zhang, “Large language model as meta-
surrogate for data-driven many-task optimization: A proof-of-principle
study,” arXiv preprint arXiv:2503.08301, 2025.
[163] X. Wu, J. Wu, Y. Zhou, L. Feng, and K. C. Tan, “Towards robustness
and explainability of automatic algorithm selection,” in Proceedings of
the International Conference on Machine Learning, 2025.
[164] F. Hutter, L. Xu, H. H. Hoos, and K. Leyton-Brown, “Algorithm
runtime prediction: Methods & evaluation,” Artificial Intelligence, vol.
206, pp. 79–111, 2014.
[165] E. Nudelman, K. Leyton-Brown, H. H. Hoos, A. Devkar, and
Y. Shoham, “Understanding random sat: Beyond the clauses-to-
variables ratio,” in Proceedings of the Principles and Practice of
Constraint Programming, 2004, pp. 438–452.
[166] S. M. Abdulrahman, P. Brazdil, J. N. Van Rijn, and J. Vanschoren,
“Speeding up algorithm selection using average ranking and active
testing by introducing runtime,” Machine Learning, vol. 107, no. 1,
pp. 79–108, 2018.
[167] L. Xu, F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Satzilla:
Portfolio-based algorithm selection for sat,” Journal of Artificial In-
telligence Research, vol. 32, pp. 565–606, 2008.
[168] L. Fehring, J. Hanselle, and A. Tornede, “Harris: Hybrid rank-
ing and regression forests for algorithm selection,” arXiv preprint
arXiv:2210.17341, 2022.
[169] X. Wu, Y. Zhong, J. Wu, B. Jiang, and K. C. Tan, “Large lan-
guage model-enhanced algorithm selection: Towards comprehensive
algorithm representation,” arXiv preprint arXiv:2311.13184, 2023.
[170] S. Zhang, S. Liu, N. Lu, J. Wu, J. Liu, Y. S. Ong, and K. Tang,
“Llm-driven instance-specific heuristic generation and selection,” arXiv
preprint arXiv:2506.00490, 2025.
[171] W. Yi, R. Qu, L. Jiao, and B. Niu, “Automated design of metaheuristics
using reinforcement learning within a novel general search framework,”
IEEE Transactions on Evolutionary Computation, vol. 27, no. 4, pp.
1072–1084, 2022.
[172] M. Pluhacek, A. Kazikova, T. Kadavy, A. Viktorin, and R. Senkerik,
“Leveraging large language models for the generation of novel meta-
heuristic optimization algorithms,” in Proceedings of the Companion
Conference on Genetic and Evolutionary Computation, 2023, pp.
1812–1820.
[173] R. Zhong, Y. Xu, C. Zhang, and J. Yu, “Leveraging large language
model to generate a novel metaheuristic algorithm with crispe frame-
work,” Cluster Computing, vol. 27, no. 10, pp. 13 835–13 869, 2024.
[174] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P.
Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi,
P. Kohli, and A. Fawzi, “Mathematical discoveries from program search
with large language models,” Nature, vol. 625, no. 7995, pp. 468–475,
2024.
[175] F. Liu, X. Tong, M. Yuan, and Q. Zhang, “Algorithm evolution using
large language model,” arXiv preprint arXiv:2311.15249, 2023.
[176] F. Liu, R. Zhang, Z. Xie, R. Sun, K. Li, X. Lin, Z. Wang, Z. Lu,
and Q. Zhang, “Llm4ad: A platform for algorithm design with large
language model,” arXiv preprint arXiv:2412.17287, 2024.
[177] F. Liu, Q. Zhang, X. Tong, K. Mao, and M. Yuan, “Fitness landscape
of large language model-assisted automated algorithm search,” arXiv
preprint arXiv:2504.19636, 2025.
[178] F. Liu, R. Zhang, X. Lin, Z. Lu, and Q. Zhang, “Fine-tuning large
language model for automated algorithm design,” arXiv preprint
arXiv:2507.10614, 2025.
[179] S. Yao, F. Liu, X. Lin, Z. Lu, Z. Wang, and Q. Zhang, “Multi-objective
evolution of heuristic using large language model,” in Proceedings
of the AAAI Conference on Artificial Intelligence, 2025, pp. 27 144–
27 152.
[180] F. Liu, Y. Liu, Q. Zhang, X. Tong, and M. Yuan, “Eoh-s: Evolution of
heuristic set using llms for automated heuristic design,” arXiv preprint
arXiv:2508.03082, 2025.
[181] W. Yatong, P. Yuchen, and Z. Yuqi, “Ts-eoh: An edge server task
scheduling algorithm based on evolution of heuristic,” arXiv preprint
arXiv:2409.09063, 2024.
[182] Y. Yao, F. Liu, J. Cheng, and Q. Zhang, “Evolve cost-aware acquisition
functions using large language models,” in Proceedings of the Interna-
tional Conference on Parallel Problem Solving from Nature, 2024, pp.
374–390.
[183] P. Guo, F. Liu, X. Lin, Q. Zhao, and Q. Zhang, “L-autoda: Large
language models for automatically evolving decision-based adversarial
attacks,” in Proceedings of the Genetic and Evolutionary Computation
Conference Companion, 2024, pp. 1846–1854.
[184] N. van Stein and T. B¨ack, “Llamea: Automatically generating meta-
heuristics with large language models,” in Proceedings of the Genetic
and Evolutionary Computation Conference Companion, 2025, pp. 81–
82.
[185] N. van Stein, A. V. Kononova, L. Kotthoff, and T. B¨ack, “Code
evolution graphs: Understanding large language model driven design
of algorithms,” in Proceedings of the Genetic and Evolutionary Com-
putation Conference, 2025, pp. 943–951.
[186] N. van Stein, H. Yin, A. V. Kononova, T. B¨ack, and G. Ochoa,
“Behaviour space analysis of llm-driven meta-heuristic discovery,”
arXiv preprint arXiv:2507.03605, 2025.
[187] N. van Stein, A. V. Kononova, H. Yin, and T. B¨ack, “Blade: Benchmark
suite for llm-driven automated design and evolution of iterative opti-
misation heuristics,” in Proceedings of the Genetic and Evolutionary
Computation Conference Companion, 2025, pp. 2336–2344.
[188] N. van Stein, D. Vermetten, and T. B¨ack, “In-the-loop hyper-parameter
optimization for llm-based automated design of heuristics,” ACM
Transactions on Evolutionary Learning, 2024.
[189] W. Li, N. van Stein, T. B¨ack, and E. Raponi, “Llamea-bo: A large
language model evolutionary algorithm for automatically generating
bayesian optimization algorithms,” arXiv preprint arXiv:2505.21034,
2025.
[190] H. Yin, A. V. Kononova, T. B¨ack, and N. van Stein, “Optimizing
photonic structures with large language model driven algorithm dis-
covery,” in Proceedings of the Genetic and Evolutionary Computation
Conference Companion, 2025, pp. 2354–2362.
[191] Y. Sun, F. Ye, X. Zhang, S. Huang, B. Zhang, K. Wei, and S. Cai, “Au-
tosat: Automatically optimize sat solvers via large language models,”
arXiv preprint arXiv:2402.10705, 2024.
[192] Y. Huang, S. Wu, W. Zhang, J. Wu, L. Feng, and K. C. Tan, “Au-
tonomous multi-objective optimization using large language model,”
IEEE Transactions on Evolutionary Computation, 2025.


--- Page 20 ---
20
[193] R. Zhang, F. Liu, X. Lin, Z. Wang, Z. Lu, and Q. Zhang, “Under-
standing the importance of evolutionary search in automated heuristic
design with large language models,” in Proceedings of the International
Conference on Parallel Problem Solving from Nature, 2024, pp. 185–
202.
[194] H. Yin, A. V. Kononova, T. B¨ack, and N. van Stein, “Controlling
the mutation in large language models for the efficient evolution of
algorithms,” in Proceedings of the International Conference on the
Applications of Evolutionary Computation, 2025, pp. 403–417.
[195] R. Li, L. Wang, H. Sang, L. Yao, and L. Pan, “Llm-assisted automatic
memetic algorithm for lot-streaming hybrid job shop scheduling with
variable sublots,” IEEE Transactions on Evolutionary Computation,
2025.
[196] H. Ling, S. Parashar, S. Khurana, B. Olson, A. Basu, G. Sinha, Z. Tu,
J. Caverlee, and S. Ji, “Complex llm planning via automated heuristics
discovery,” arXiv preprint arXiv:2502.19295, 2025.
[197] H. Ye, J. Wang, Z. Cao, F. Berto, C. Hua, H. Kim, J. Park, and G. Song,
“Reevo: Large language models as hyper-heuristics with reflective evo-
lution,” Advances in Neural Information Processing Systems, vol. 37,
pp. 43 571–43 608, 2024.
[198] Z. Zheng, Z. Xie, Z. Wang, and B. Hooi, “Monte carlo tree search for
comprehensive exploration in llm-based automatic heuristic design,”
arXiv preprint arXiv:2501.08603, 2025.
[199] P. V. T. Dat, L. Doan, and H. T. T. Binh, “Hsevo: Elevating automatic
heuristic design with diversity-driven harmony search and genetic
algorithm using llms,” in Proceedings of the AAAI Conference on
Artificial Intelligence, 2025, pp. 26 931–26 938.
[200] A. Chen, D. Dohan, and D. So, “Evoprompting: Language models for
code-level neural architecture search,” Advances in Neural Information
Processing Systems, vol. 36, pp. 7787–7817, 2023.
[201] P. Wang, Z. Zhao, H. Wen, F. Wang, B. Wang, Q. Zhang, and
Y. Wang, “Llm-autoda: Large language model-driven automatic data
augmentation for long-tailed problems,” Advances in Neural Informa-
tion Processing Systems, vol. 37, pp. 64 915–64 941, 2024.
[202] X. Li, X. Sun, A. Wang, J. Li, and C. Shum, “Cuda-l1: Improving cuda
optimization via contrastive reinforcement learning,” arXiv preprint
arXiv:2507.14111, 2025.
[203] Z. Ma, H. Guo, J. Chen, G. Peng, Z. Cao, Y. Ma, and Y. Gong,
“Llamoco: Instruction tuning of large language models for optimization
code generation,” arXiv preprint arXiv:2403.01131, 2024.
[204] F. Wang, H. Liu, Z. Dai, J. Zeng, Z. Zhang, Z. Wu, C. Luo, Z. Li,
X. Tang, Q. He, and S. Wang, “Agenttts: Large language model agent
for test-time compute-optimal scaling strategy in complex tasks,” arXiv
preprint arXiv:2508.00890, 2025.
[205] R. Zhong, Y. Cao, J. Yu, and M. Munetomo, “Large language model
assisted adversarial robustness neural architecture search,” in Proceed-
ings of the International Conference on Data-driven Optimization of
Complex Systems, 2024, pp. 433–437.
[206] Y. Yu and J. Zutty, “Llm-guided evolution: An autonomous model
optimization for object detection,” in Proceedings of the Genetic and
Evolutionary Computation Conference Companion, 2025, pp. 2363–
2370.
[207] M. U. Nasir, S. Earle, J. Togelius, S. James, and C. Cleghorn, “Llmatic:
Neural architecture search via large language models and quality
diversity optimization,” in Proceedings of the Genetic and Evolutionary
Computation Conference, 2024, pp. 1110–1118.
[208] C. Morris, M. Jurado, and J. Zutty, “Llm guided evolution-the automa-
tion of models advancing models,” in Proceedings of the Genetic and
Evolutionary Computation Conference, 2024, pp. 377–384.
[209] S. Mo, K. Wu, Q. Gao, X. Teng, and J. Liu, “Autosgnn: Automatic
propagation mechanism discovery for spectral graph neural networks,”
in Proceedings of the AAAI Conference on Artificial Intelligence, 2025,
pp. 19 493–19 502.
[210] X. Li, C. Zhang, J. Wang, F. Wu, Y. Li, and X. Jin, “Efficient and
stealthy jailbreak attacks via adversarial prompt distillation from llms
to slms,” arXiv preprint arXiv:2506.17231, 2025.
[211] M. Yu, J. Fang, Y. Zhou, X. Fan, K. Wang, S. Pan, and Q. Wen, “Llm-
virus: Evolutionary jailbreak attack on large language models,” arXiv
preprint arXiv:2501.00055, 2024.
[212] Z. Wang, K. Zhang, Z. Zhao, Y. Wen, A. Pandey, H. Liu, and
K. Ding, “A survey of large language models for text-guided molecular
discovery: From molecule generation to optimization,” arXiv preprint
arXiv:2505.16094, 2025.
[213] Y. G. Nana Teukam, F. Zipoli, T. Laino, E. Criscuolo, F. Grisoni, and
M. Manica, “Integrating genetic algorithms and language models for
enhanced enzyme design,” Briefings in Bioinformatics, vol. 26, no. 1,
p. bbae675, 2025.
[214] Y. Zhang, K. Zheng, F. Liu, Q. Zhang, and Z. Wang, “Autoturb:
Using large language models for automatic algebraic turbulence model
discovery,” Physics of Fluids, vol. 37, no. 1, 2025.
[215] H. Wang, M. Skreta, C.-T. Ser, W. Gao, L. Kong, F. Strieth-Kalthoff,
C. Duan, Y. Zhuang, Y. Yu, Y. Zhu, Y. Du, A. Aspuru-Guzik, K. Nek-
lyudov, and C. Zhang, “Efficient evolutionary search over chemical
space with large language models,” arXiv preprint arXiv:2406.16976,
2024.
[216] P. Guevorguian, M. Bedrosian, T. Fahradyan, G. Chilingaryan,
H. Khachatrian, and A. Aghajanyan, “Small molecule optimization
with large language models,” arXiv preprint arXiv:2407.18897, 2024.
[217] N. Ran, Y. Wang, and R. Allmendinger, “Mollm: Multi-objective large
language model for molecular design–optimizing with experts,” arXiv
preprint arXiv:2502.12845, 2025.
[218] T. V. Tran and T. S. Hy, “Protein design by directed evolution
guided by large language models,” IEEE Transactions on Evolutionary
Computation, vol. 29, no. 2, pp. 418–428, 2024.
[219] Y. Wang, J. He, Y. Du, X. Chen, J. C. Li, L. Liu, X. Xu, and S. Hassoun,
“Large language model is secretly a protein sequence optimizer,” arXiv
preprint arXiv:2501.09274, 2025.
[220] A. Chen, S. D. Stanton, R. G. Alberstein, A. M. Watkins, R. Bonneau,
V. Gligorijevic, K. Cho, and N. C. Frey, “Llms are highly-constrained
biophysical sequence optimizers,” in Proceedings of the Workshop on
AI for New Drug Modalities at the Conference on Neural Information
Processing Systems, 2024.
[221] L. Lv, Z. Lin, H. Li, Y. Liu, J. Cui, C. Y.-C. Chen, L. Yuan, and Y. Tian,
“Prollama: A protein large language model for multi-task protein
language processing,” IEEE Transactions on Artificial Intelligence,
2025.
[222] X. Zhang, Z. Xu, G. Zhu, C. M. J. Tay, Y. Cui, B. C. Khoo, and L. Zhu,
“Using large language models for parametric shape optimization,”
Physics of Fluids, vol. 37, no. 8, 2025.
[223] R. Li, C. Zhang, S. Mao, H. Huang, M. Zhong, Y. Cui, X. Zhou, F. Yin,
S. Theodoridis, and Z. Zhang, “From english to pcsel: Llm helps design
and optimize photonic crystal surface emitting lasers,” 2023.
[224] M. Du, Y. Chen, Z. Wang, L. Nie, and D. Zhang, “Large language
models for automatic equation discovery of nonlinear dynamics,”
Physics of Fluids, vol. 36, no. 9, 2024.
[225] R. Sun, C. Chen, F. Guo, and K. Liu, “Integrating llms and evolutionary
algorithms for spin glass optimization,” in Proceedings of the Interna-
tional Conference on Artificial Intelligence and Industrial Technology
Applications, 2025, pp. 1271–1274.
[226] P. Ma, T.-H. Wang, M. Guo, Z. Sun, J. B. Tenenbaum, D. Rus,
C. Gan, and W. Matusik, “Llm and simulation as bilevel optimizers: A
new paradigm to advance physical scientific discovery,” arXiv preprint
arXiv:2405.09783, 2024.
[227] X. Peng, Y. Liu, Y. Cang, C. Cao, and M. Chen, “Llm-optira: Llm-
driven optimization of resource allocation for non-convex problems in
wireless communications,” arXiv preprint arXiv:2505.02091, 2025.
[228] J. Wen, C. Su, J. Kang, J. Nie, Y. Zhang, J. Tang, D. Niyato, and
C. Yuen, “Hybridrag-based llm agents for low-carbon optimization
in low-altitude economy networks,” arXiv preprint arXiv:2506.15947,
2025.
[229] W. Lee and J. Park, “Llm-empowered resource allocation in wireless
communications systems,” arXiv preprint arXiv:2408.02944, 2024.
[230] H. Zhou, C. Hu, D. Yuan, Y. Yuan, D. Wu, X. Liu, and C. Zhang,
“Large language model (llm)-enabled in-context learning for wireless
network optimization: A case study of power control,” arXiv preprint
arXiv:2408.00214, 2024.
[231] Y. Wang, J. Farooq, H. Ghazzai, and G. Setti, “Multi-uav placement
for integrated access and backhauling using llm-driven optimization,”
in Proceedings of the IEEE Wireless Communications and Networking
Conference, 2025, pp. 1–6.
[232] H. Li, M. Xiao, K. Wang, D. I. Kim, and M. Debbah, “Large language
model based multi-objective optimization for integrated sensing and
communications in uav networks,” IEEE Wireless Communications
Letters, vol. 14, no. 4, pp. 979–983, 2025.
[233] J. Hou, K. Qiu, Z. Zhang, Y. Yu, K. Wang, S. Capolongo, J. Zhang,
Z. Li, and J. Zhang, “Wireless-friendly window position optimization
for ris-aided outdoor-to-indoor networks based on multi-modal large
language model,” arXiv preprint arXiv:2410.20691, 2024.
[234] M. Sharma, A. Komninos, M. L´opez-Ib´a˜nez, and D. Kazakov, “Deep
reinforcement learning based parameter control in differential evolu-
tion,” in Proceedings of the Genetic and Evolutionary Computation
Conference, 2019, pp. 709–717.
[235] H. Guo, Y. Ma, Z. Ma, J. Chen, X. Zhang, Z. Cao, J. Zhang,
and Y. Gong, “Deep reinforcement learning for dynamic algorithm


--- Page 21 ---
21
selection: A proof-of-principle study on differential evolution,” IEEE
Transactions on Systems, Man, and Cybernetics: Systems, vol. 54,
no. 7, pp. 4247–4259, 2024.
[236] Y. Liu, F. Wu, Z. Liu, K. Wang, F. Wang, and X. Qu, “Can language
models be used for real-world urban-delivery route optimization?” The
Innovation, vol. 4, no. 6, 2023.
[237] D. Jobson and Y. Li, “Investigating the potential of using large language
models for scheduling,” in Proceedings of the ACM International
Conference on AI-Powered Software, 2024, pp. 170–171.
[238] B. Li, K. Zhang, Y. Sun, and J. Zou, “Research on travel route planning
optimization based on large language model,” in Proceedings of the
International Conference on Data-driven Optimization of Complex
Systems, 2024, pp. 352–357.
[239] C. Li, R. Yang, T. Li, M. Bafarassat, K. Sharifi, D. Bergemann, and
Z. Yang, “Stride: A tool-assisted llm agent framework for strategic and
interactive decision-making,” arXiv preprint arXiv:2405.16376, 2024.
[240] H. Chen, G. E. Constante-Flores, and C. Li, “Diagnosing infeasible
optimization problems using large language models,” INFOR: Infor-
mation Systems and Operational Research, vol. 62, no. 4, pp. 573–587,
2024.
[241] D. Kikuta, H. Ikeuchi, K. Tajiri, and Y. Nakano, “Routeexplainer: An
explanation framework for vehicle routing problem,” in Proceedings of
the Pacific-Asia Conference on Knowledge Discovery and Data Mining,
2024, pp. 30–42.
[242] S. Wasserkrug, L. Boussioux, D. d. Hertog, F. Mirzazadeh, I. Birbil,
J. Kurtz, and D. Maragno, “From large language models and opti-
mization to decision optimization copilot: A research manifesto,” arXiv
preprint arXiv:2402.16269, 2024.
[243] G. Singh and K. K. Bali, “Enhancing decision-making in optimization
through llm-assisted inference: A neural networks perspective,” in
Proceedings of the International Joint Conference on Neural Networks,
2024, pp. 1–7.
[244] C. Chac´on Sartori, C. Blum, and G. Ochoa, “Large language models
for the automated analysis of optimization algorithms,” in Proceedings
of the Genetic and Evolutionary Computation Conference, 2024, pp.
160–168.
[245] P. Maddigan, A. Lensen, and B. Xue, “Explaining genetic programming
trees using large language models,” arXiv preprint arXiv:2403.03397,
2024.


--- Page 22 ---
22
Supplementary Document for “A Systematic Survey
on Large Language Models for Evolutionary
Optimization: From Modeling to Solving”
IX. REPRESENTATIVE WORKS
This section consolidates the detailed taxonomies of the literature reviewed in the main body of the survey, thereby ensuring
thoroughness and ease of reference. Specifically, Table II provides a comprehensive breakdown of works focused on LLMs
for optimization modeling. It details the specific objectives and methodological classification of each approach. Furthermore,
Table III systematically catalogs the research concerning LLMs for optimization solving. This table classifies works according
to the specific role of the LLM within the overall optimization workflow, such as a low-level operator or a high-level generator.
TABLE II
REPRESENTATIVE WORKS ON LLMS FOR OPTIMIZATION MODELING, CATEGORIZED INTO PROMPT-BASED AND LEARNING-BASED METHODS.
Method
Venue
Type
Technical Summary
Prompt-based Methods
Ner4OPT [97]
CPAIOR, 2023
Two-stage
Fine-tune models for named entity recognition by incorporating traditional NLP methods.
AOMG [77]
GECCO, 2023
Direct
Directly leverage LLMs to generate mathematical models.
HG 2.0 [78]
arXiv, 2023
Two-stage
Embed LLMs within the two-stage framework.
AMGPT [70]
arXiv, 2023
Two-stage
Employ fine-tuned models for constraint classification.
CoE [81]
ICLR, 2023
Multi-agent
Construct dynamic reasoning chains using 11 expert agents.
OptiMUS [82]
ICML, 2023
Multi-agent
Develop a conductor agent to coordinate modeling, programming, and evaluation processes.
MAMS [86]
INFOR, 2024
Multi-agent
Develop inter-agent cross-validation to replace solver-dependent verification.
EC [79]
SGC, 2024
Two-stage
Apply the two-stage framework to energy management systems.
CAFA [91]
NeurIPS, 2024
Two-stage
Enhance modeling performance through code-based formalization.
MAMO [94]
NAACL, 2024
Two-stage
Develop the MAMO benchmark with ordinary differential equation extensions.
NL2OR [93]
arXiv, 2024
Two-stage
Predefine abstract structural constraints to regulate the LLM outputs.
TRIP-PAL [80]
arXiv, 2024
Two-stage
Apply the two-stage framework to travel planning.
OptLLM [88]
arXiv, 2024
Interactive
Support both single-input and interactive-input modes.
OptiMUS-0.3 [83]
arXiv, 2024
Multi-agent
Introduce self-corrective prompts and structure-aware modeling based on OptiMUS.
MeetMate [89]
TiiS, 2024
Interactive
Develop an interactive system for user input processing with five selectable task options.
VL [90]
CUI, 2025
Interactive
Convert user priorities into optimization constraints during dialog interactions.
LLM-MCTS [92]
arXiv, 2025
Two-stage
Conduct hierarchical Monte Carlo tree search over the hypothesis space.
EquivaMap [96]
arXiv, 2025
Two-stage
Generate variable mapping functions via LLMs with lightweight verification.
MAP [87]
arXiv, 2025
Multi-agent
Deploy multiple independent reviewers to evaluate modeling results.
ORMind [84]
arXiv, 2025
Multi-agent
Replace the conductor agent with structured, predictable workflows.
LEAN-LLM-OPT [85]
ssrn, 2025
Multi-agent
Integrate RAG to enhance problem classification, and construct instances with query operations.
Learning-based Methods
LM4OPT [98]
INFOR, 2024
Fine-tuning
Progressively fine-tune models on the NL4OPT dataset.
ORLM [28]
OR, 2024
Data synthesis
Synthesize data through expansion and augmentation and fine-tune open-source models.
ReSocratic [104]
ICLR, 2024
Data synthesis
Propose inverse data synthesis methodology and construct the OPTIBENCH benchmark.
LLMOPT [109]
ICLR, 2024
Fine-tuning
Introduce model alignment and self-correction mechanisms to mitigate hallucination phenomena.
BPP-Search [107]
arXiv, 2024
Data synthesis
Solve the problem of missing detailed in data synthesis.
OptMATH [105]
arXiv, 2025
Data synthesis
Develop a scalable bidirectional data synthesis approach.
LLMBO [111]
arXiv, 2025
Data synthesis
Propose a method for fine-tuning cost-effective LLMs to tackle specific business challenges.
SIRL [110]
arXiv, 2025
Fine-tuning
Integrate external optimization solvers as verifiable reward validators for reinforcement learning.
Step-Opt [108]
arXiv, 2025
Data synthesis
Increase problem complexity through an iterative problem generation approach.
DPLM [106]
arXiv, 2025
Data synthesis
Combine the diversity of forward generation and the reliability of inverse generation.
OptiTrust [103]
arXiv, 2025
Data synthesis
Develop a verifiable synthetic data generation pipeline.
StepORLM [112]
arXiv, 2025
Fine-tuning
Introduce generative process supervision and a co-evolutionary loop to refine reasoning trajectories.


--- Page 23 ---
23
TABLE III
REPRESENTATIVE WORKS ON LLMS FOR OPTIMIZATION SOLVING, CATEGORIZED INTO LLMS AS OPTIMIZERS, LOW-LEVEL LLMS FOR OPTIMIZATION
ALGORITHMS AND HIGH-LEVEL LLMS FOR OPTIMIZATION ALGORITHMS.
Method
Venue
Type
Technical Summary
LLMs as Optimizers
OPRO [29]
ICLR, 2023
Prompt-based
Iteratively refine through optimization trajectories and problem descriptions.
toLLM [113]
KDD, 2023
Prompt-based
Design four canonical tasks to evaluate the performance boundaries of LLMs.
EvoLLM [118]
GECCO, 2024
Prompt-based
Replace traditional optimization trajectories with candidate solution quality ranking.
MLLMO [120]
MCII, 2024
Prompt-based
Utilize MLLMs to jointly process problem descriptions and map visualizations for CVRPs.
POM [125]
NeurIPS, 2024
Learning-based
Pre-train a general-purpose, zero-shot black-box optimization foundation model.
OPTO [119]
NeurIPS, 2024
Prompt-based
Replace traditional optimization trajectories with rich, structured execution traces.
VRMA [121]
arXiv, 2024
Prompt-based
Utilize MLLMs to process 2D planar point distribution maps as input.
ROPRO [123]
ACL, 2024
Prompt-based
Discuss the model dependency of OPRO and identify its limitations on small-scale models.
BBOLLM [31]
arXiv, 2024
Prompt-based
Conduct evaluation of LLMs on both discrete and continuous black-box optimization problems.
LLMS [124]
arXiv, 2024
Learning-based
Fine-tune LLMs with instruction-solution pairs for scheduling problems.
ECLIPSE [117]
NAACL, 2024
Prompt-based
Apply iterative optimization to jailbreaking attack strategies.
LLOME [126]
arXiv, 2024
Learning-based
Integrate preference learning to train LLMs for satisfying complex biophysical constraints.
LLMDSM [114]
arXiv, 2024
Prompt-based
Apply iterative optimization to design structure matrix sequencing.
LMCO [115]
WCL, 2024
Prompt-based
Apply iterative optimization to wireless network design.
MGSCO [122]
arXiv, 2025
Prompt-based
Process visual representations of abstract graph structures using MLLMs.
ORFS [116]
arXiv, 2025
Prompt-based
Apply iterative optimization to automated parameter tuning in chip design.
Low-level LLMs for Optmization Algorithms
LMX [136]
TELO, 2023
Operators
Leverage LLMs as intelligent operators for textual genome crossover and recombination.
GPT-NAS [129]
arXiv, 2023
Initialization
Utilize LLMs for NAS initialization with prior knowledge.
LMEA [137]
CEC, 2023
Operators
Employ LLMs as crossover, mutation, and selection operators to guide EAs.
LLM-PP [130]
arXiv, 2023
Initialization
Utilize LLMs as performance predictors to assist initialization processes.
LMOEA [141]
EMO, 2023
Operators
Empower MOEA/D with LLMs through zero-shot prompting as search operators in multi-objective optimization.
OFPLLM [132]
AI, 2023
Initialization
Assist non-expert users in initializing financial plans.
LEO [140]
Neucom, 2024
Operators
Utilize LLMs to guide individuals from dual pools for exploration and exploitation.
LLM-GA [131]
BiB, 2024
Initialization
Initialize genetic algorithms with LLMs to generate high-quality mutant pools for enzyme design.
GE [208]
GECCO, 2024
Operators
Enhance the creativity and diversity of the LLM by introducing different role-based prompts for NAS.
LLMAES [?]
ICIC, 2024
Operators
Generate minimal solutions within populations using LLMs to reduce interaction costs.
LLMTES [152]
arXiv, 2024
Configuration
Apply LLM-based feedback loops to sequentially optimize evolution strategies through static tuning.
LAEA [158]
SWEVO, 2024
Evaluation
Transform model-assisted selection tasks into classification and regression problems.
LICO [159]
arXiv, 2024
Evaluation
Apply LLMs as surrogate models in molecular science applications.
LLMSM [160]
CAI, 2024
Evaluation
Develop a collaborative framework using LLMs for model selection and training in engineering optimization.
LLMCES [154]
GECCO, 2024
Configuration
Implement step-size control for evolution strategies using OPRO-like mechanisms.
AKFLLM [135]
arXiv, 2024
Operators
Utilize LLMs to assist mutation and other generative stages in evolutionary multi-task optimization.
LLMAMO [144]
arXiv, 2024
Operators
Invoke LLMs for elite solution generation only upon insufficient population improvement.
LTC [153]
ESANN, 2024
Configuration
Apply LLM-based feedback loops to sequentially optimize CMA-ES through dynamic control.
LLMEVO [133]
arXiv, 2025
Operators
Validate LLM effectiveness in selection, crossover, and mutation phases while noting limitations in initialization.
LLM-GE [206]
GECCO, 2025
Operators
Utilize LLMs as crossover and mutation operators to optimize YOLO architectures for object detection.
LAOS [155]
GECCO, 2025
Configuration
Utilize state features to replace optimization trajectories for guiding LLM-based operator selection.
PAIR [138]
arXiv, 2025
Operators
Focus on utilizing LLMs as selection operators to enhance LMEA.
LLMMS [162]
arXiv, 2025
Evaluation
Utilize LLMs as meta-Surrogates to effectively facilitate cross-task knowledge sharing through token sequence representations.
LMPSO [139]
arXiv, 2025
Operators
Simulate PSO evolutionary processes using LLMs and refine LMEA for specific algorithms.
LLM-SAEA [161]
arXiv, 2025
Evaluation
Dynamically select appropriate proxy models and infill sampling criteria using LLMs.
High-level LLMs for Optmization Algorithms
LLMGM [172]
GECCO, 2023
Single-step
Decompose and recombine six existing metaheuristic algorithms to generate novel hybrid algorithms in single rounds.
FunSearch [174]
Nature, 2023
Iterative
Generate novel code snippets via LLMs and guide EAs to search through function space.
AEL [175]
arXiv, 2023
Iterative
Enhance FunSearch by integrating heuristic principles, demonstrating significant improvements on TSP instances.
AS-LLM [169]
IJCAI, 2023
Selection
Automatically extract high-dimensional algorithm features from code and descriptive text to facilitate algorithm selection.
EoH [30]
ICML, 2024
Iterative
Implement heuristic-code coevolution via preset prompt pairs for algorithm crossover and mutation.
ReEvo [197]
NeurIPS, 2024
Iterative
Utilize LLMs as hyper-heuristic algorithms with reflective evolution mechanisms to guide search processes.
AutoSAT [191]
arXiv, 2024
Iterative
Employ multiple heuristic strategies to guide LLMs in algorithm generation.
LLaMEA [156]
TEVC, 2024
Iterative
Utilize LLMs to guide EAs in mutation, selection, and other operations for generating advanced heuristics.
L-AutoDA [183]
GECCO, 2024
Iterative
Apply AEL to generate adversarial attack algorithms in cybersecurity.
MOELLM [192]
TEVC, 2024
Iterative
Introduce robust testing modules and dynamic selection strategies to generate multi-objective optimization algorithms.
LLMEPS [193]
PPSN, 2024
Iterative
Develop a baseline algorithm for automated algorithm design using EoH and ReEvo methodologies.
EvolCAF [182]
PPSN, 2024
Iterative
Apply EoH to evolve and generate cost-aware acquisition functions for Bayesian optimization.
TS-EoH [181]
arXiv, 2024
Iterative
Apply EoH to generate algorithms for edge server task scheduling.
MEoH [179]
AAAI, 2024
Iterative
Extend EoH to multi-objective versions, incorporating other metrics for comprehensive algorithm evaluation.
LLaMEA-HPO [188]
TELO, 2024
Iterative
Integrate hyperparameter optimization into LLaMEA’s iterative cycle to enhance overall efficiency.
CMLLM [194]
EvoApps, 2024
Iterative
Develop dynamic prompt mechanisms to control LLM mutation for enhancing LLaMEA performance.
HSEvo [199]
AAAI, 2024
Iterative
Combine harmony search with GAs and optimize algorithm portfolio quality/diversity through diversity metrics.
LLM4AD [176]
arXiv, 2024
Iterative
Develop an EoH-based platform to accelerate heuristic algorithm design.
MCTS-AHD [198]
arXiv, 2025
Iterative
Employ MCTS to explore heuristic spaces, organizing generated heuristics into tree structures.
CEG [185]
GECCO, 2025
Iterative
Develop methodologies to analyze LLM-generated code and its evolutionary patterns during optimization processes.
OPSLAD [190]
GECCO, 2025
Iterative
Apply LLaMEA to photonic structure optimization in industrial design.
BLADE [187]
GECCO, 2025
Iterative
Establish a standardized and reproducible evaluation framework for LLM-driven algorithm generation methods.
AutoHD [196]
arXiv, 2025
Iterative
Propose an LLM-guided heuristic discovery strategy for complex planning tasks.
LAMA [195]
TEVC, 2025
Iterative
Propose an automatic memetic algorithm enhanced by a heuristic designed with the assistance of LLMs.
LAS [177]
arXiv, 2025
Iterative
Analyze fitness landscapes of LLM-assisted algorithm search processes.
LLaMEA-BO [189]
arXiv, 2025
Iterative
Apply LLaMEA to automatically generate Bayesian optimization algorithms.
InstSpecHH [170]
arXiv, 2025
Selection
Leverage LLMs’ semantic comprehension and reasoning capabilities for context-aware algorithm selection.
BSALMD [186]
arXiv, 2025
Iterative
Conduct behavioral space analysis to examine algorithmic evolution trajectories and behaviors.
FLAAD [178]
arXiv, 2025
Iterative
Fine-tune LLMs with diversity-aware ranked sampling for algorithm generation.
EoH-S [180]
arXiv, 2025
Iterative
Extend EoH to evolve and generate complementary algorithm portfolios.
