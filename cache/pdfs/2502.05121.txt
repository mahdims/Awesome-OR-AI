--- Page 1 ---
arXiv:2502.05121v1  [hep-th]  7 Feb 2025
.
Reﬁning Integration-by-Parts Reduction of
Feynman Integrals with Machine Learning
Matt von Hippel,a Matthias Wilhelma,b
aNiels Bohr International Academy, Niels Bohr Institute, University of Copenhagen,
Blegdamsvej 17, 2100 Copenhagen Ø, Denmark
bCenter for Quantum Mathematics, Department of Mathematics and Computer Science,
University of Southern Denmark, Campusvej 55, 5230 Odense M, Denmark
Abstract: Integration-by-parts reductions of Feynman integrals pose a frequent
bottle-neck in state-of-the-art calculations in theoretical particle and gravitational-
wave physics, and rely on heuristic approaches for selecting integration-by-parts iden-
tities, whose quality heavily inﬂuences the performance. In this paper, we investi-
gate the use of machine-learning techniques to ﬁnd improved heuristics.
We use
funsearch, a genetic programming variant based on code generation by a Large
Language Model, in order to explore possible approaches, then use strongly typed
genetic programming to zero in on useful solutions. Both approaches manage to re-
discover the state-of-the-art heuristics recently incorporated into integration-by-parts
solvers, and in one example ﬁnd a small advance on this state of the art.


--- Page 2 ---
Contents
1
Introduction
1
2
Background
3
2.1
Integration-by-Parts identities for Feynman Integrals
3
2.2
Genetic Algorithms and Machine Learning
8
3
Initial Attempt: Genetic Algorithm
11
4
Exploration with Funsearch
11
5
Improved Heuristics via Strongly Typed Genetic Programming
19
6
Conclusions and Discussion
22
A Integration-by-Parts identities for the Benchmark Integral
23
1
Introduction
Perturbative Quantum Field Theory has proven to be a vastly successful theoreti-
cal framework for calculating precision predictions, with applications ranging from
collider physics to gravitational-wave physics. A crucial step in the calculation of
precision predictions is the reduction of the occurring Feynman integrals to a much
smaller set of so-called master integrals, using integration-by-parts (IBP) identi-
ties [1–3]. This IBP reduction is a major bottleneck in precision calculations, re-
quiring hundred thousands of CPU hours in current applications [4] and obstructing
other applications altogether.
IBP identities relate Feynman integrals with diﬀerent integer exponents of the
propagators as well as irreducible scalar products (ISP) in the numerator. They can
easily be derived for general values of the exponents, see e.g. ref. [5] for a textbook
treatment. In contrast, it is in most cases not possible to solve the resulting systems
of IBP identities in closed form, i.e. for general values of the exponents. Instead, IBP
reduction codes such as AIR [6], FIRE [7, 8], Reduze [9], LiteRed [10], Kira [11, 12],
FiniteFlow [13] and Blade [14] specialize the identities to a suﬃciently large set of
diﬀerent values for the integer exponents – the so-called seeds – to solve for the de-
sired integrals in terms of the master integrals. The choice of the seeds is determined
by a so-called seeding strategy, a heuristic whose quality heavily inﬂuences the per-
formance of IBP reduction. Recently, a new heuristic was proposed [4, 14–16], which
– 1 –


--- Page 3 ---
reduces the size of the resulting system of linear equations and thus the reduction
time by orders of magnitude. However, an optimal choice of seeds is in general not
known.1
Motivated by the discovery of the improved heuristic in refs. [4, 14–16], in this
paper we employ automated, machine-learning based methods to search for further
improved heuristics and optimal seeding strategies. Concretely, we use three diﬀerent
versions of genetic algorithms, two of which involve genetic programming.
Genetic algorithms imitate evolution by natural selection; see e.g. ref. [21] for
a textbook treatment. They involve a population of candidate solutions to a prob-
lem.
This population is subject to mutation, in which individuals are randomly
altered, and cross-over, in which pairs of individuals give rise to new members of the
population with traits drawn from each parent, imitating sexual reproduction. The
population is then selected based on its performance by some evaluation metric, with
the best individuals kept for the next generation.
In a subset of genetic algorithms, called genetic programming, the individuals
in the population are programs, parametrized e.g. with trees; see e.g. ref. [22]. A
recent example of genetic programming, called funsearch [23], instead parametrizes
programs in terms of text, more speciﬁcally Python code. Mutation and cross-over of
texts are then carried out using a Large Language Model (LLM). The LLM is given
the Python code representing two individuals in the population, then asked how to
improve on it. funsearch has shown some success in ﬁnding novel solutions to prob-
lems in pure mathematics [23]. It has the advantage that one typically needs to know
very little about the problem one is applying it to, as solutions are written in Python
code, not in any specialized framework, making it well-suited to exploratory work.
Moreover, the output of funsearch being Python code makes it easily interpretable,
and thus generalizable.
Throughout this paper, we use the two-loop triangle-box integral depicted in
ﬁg. 1 as a benchmark to measure to performance of candidate seeding strategies.
This integral was already used as a benchmark in ref. [15].
We begin by trying
a traditional genetic algorithm, treating the list of seed integrals used as a binary
vector. This approach is very slow to converge. We then explore the problem using
funsearch. With some judicious prompting, funsearch is able to ﬁnd solutions that
not only reach the state of the art, but in an example ﬁnd slight improvements on it.
We then use the insights we gained via funsearch to proposed a more specialized
list of operations appropriate to our problem, running these via strongly type genetic
programming as implemented in the genetic algorithm library DEAP [24]. Strongly
1There exist attempts to improve IBP reduction by choosing a particular suitable basis in the
space of IBP identities via syzygy methods, see refs. [17, 18] and references therein, which are
currently being implemented in IBP codes. Other approaches aim to replace IBP reduction via
intersection theory [19, 20] but have not produced competitive reduction codes yet.
– 2 –


--- Page 4 ---
typed genetic programming converges much faster than funsearch, ﬁnding the same
best solution in only thirty generations.
The remainder of this paper is structured as follows. In the next section, we
will provide background on integration-by-parts methods for Feynman integrals in
subsection 2.1 and background on genetic algorithms, genetic programming, and
machine learning in subsection 2.2. We then describe our initial attempt with a
genetic algorithm operating on binary vectors in section 3, before describing our use
of funsearch in section 4 and the strongly typed genetic programming approach in
section 5. Finally we conclude and discuss ways in which these approaches might be
used in the future in section 6. We provide the identities used for the reduction of
our benchmark Feynman integral in appendix A.
2
Background
In this section, we provide a brief introduction to integration-by-part identities for
Feynman integrals as well as to genetic algorithms.
2.1
Integration-by-Parts identities for Feynman Integrals
In perturbative Quantum Field Theory beyond the leading order, one typically faces
the task of evaluating a large number of Feynman integrals. We give a brief intro-
ductions to these integrals here, illustrated with one example; see e.g. ref. [5] for a
detailed text-book treatment.
For each Feynman integral, we have an associated graph with E external edges
as well as further internal edges.
Associated to each external edge is an exter-
nal momentum vector pµ
j with j = 1, . . . , E and µ = 0, . . . , D −1, a vector in
D-dimensional Minkowski space. These vectors satisfy so-called momentum conser-
vation, PE
j=1 pµ
j = 0. The integration in a Feynman integral is with respect to L loop
momentum vectors kµ
l (l = 1, . . . , L) in D-dimensional Minkowski space, correspond-
ing to the L cycles in the associated graph. We organizing Feynman integrals into
families indexed by sets of integers a1, . . . , an, where n = L(L + 1)/2 + L(E −1) is
the number of independent loop-momentum-dependent products that can be formed
from the momenta. The general form of an integral family is
I(a1, . . . , an) =
Z
QL
l=1 dDkl
Qn
i=1[Di(kµ
1, . . . , kµ
L)]ai ,
(2.1)
where the polynomials Di ≡Di(kµ
1, . . . , kµ
L) are always non-trivial functions of at
least one of the loop momenta kµ
l , and can also be functions of the external momenta
and masses. For the vast majority of applications, the Di are quadratic in these
variables and Lorentz invariant. The dimension of space-time D is usually taken to
be non-integer in a regularization technique called dimensional regularization [25].
– 3 –


--- Page 5 ---
p1
k1
k1 + p1
k1 + k2
k2
k2 −p1
k2 + p3
p3
p2
Figure 1. The two-loop triangle-box integral we used to benchmark diﬀerent heuristics.
The quantities at the arrows specify the momenta ﬂowing through the edges of the graph.
Note that the ai, used to index a family with a common set of Di, can be positive, in
which case the corresponding Di is a propagator associated to an edge of the graph.
Indices ai that are larger than one correspond to higher powers of the propagator.
Moreover, ai can be negative, corresponding to irreducible scalar products (ISPs) in
the numerator, or zero, in which case Di is absent in that member of the family.
To illustrate, consider the Feynman integral depicted in ﬁg. 1. This integral was
one of the examples used to demonstrate the advantages of the improved seeding
strategy in ref. [15], and we will use it to benchmark our machine-learning approaches
throughout this paper. It has two loops, L = 2, and three external momenta, E = 3,
and thus n = 7. We can parametrize the seven Di as follows:
D1 = k2
1 ,
D2 = k2
2 ,
D3 = (k1 + k2)2 ,
D4 = (k1 + p1)2 ,
D5 = (k2 + p3)2 ,
D6 = (k2 −p1)2 ,
D7 = (k1 + p3)2 .
(2.2)
The ﬁrst six of these polynomials, D1, . . . , D6, correspond to internal edges in the
graph depicted in ﬁg. 1, and are squares of the momenta ﬂowing through those edges.
For a Feynman integral depicted by the graph in ﬁg. 1, these quantities will appear
in the denominator of the corresponding integrand, so a1, . . . , a6 will be positive. For
the methods described in this section to work, the full set of Di must form a basis
for quadratic polynomials in the kµ
l and pµ
j , up to terms that can be expressed only
in terms of Lorentz-invariant combinations of external momenta. As D1, . . . , D6 do
not constitute such a basis on their own, we must add D7, which is thus referred
to as an ISP, and is an example of the ISPs mentioned above. These will not occur
as denominators in this family of integrals but may appear as numerators, so the
corresponding a7 will be zero or negative.
– 4 –


--- Page 6 ---
The Feynman integral depicted in ﬁg. 1 depends on the invariant quantities
p2
1 = m2
1, p2
2 = m2
2 and p2
3 = m2
3 that can be formed from the three external momenta
pµ
1, pµ
2 and pµ
3 with pµ
1 + pµ
2 + pµ
3 = 0. Here, the square p2
i denotes the (pseudo-) norm
with respect to the Lorentz product. The overall dependence on these dimensionful
quantities is determined by dimensional analysis, such that the integral has a non-
trivial dependence only on the two dimensionless ratios m2/m1 and m3/m1. For
simplicity, we will thus set m1 ≡1.
Within dimensional regularization, any integral that is independent of both all
non-zero Lorentz invariants composed from the external momenta and all of the
masses is set to zero, and this condition can be enforced independently for the inte-
gration over each loop momentum; see ref. [26] for a pedagogical review that should
clarify some topics discussed here for mathematical readers.
These integrals are
called “trivial”.
Not all integrals built from a given set of Di are linearly independent. They are
related by integration-by-parts (IBP) identities [1, 2] generated by expressions of the
form2
0 =
Z
L
Y
i=1
dDki
d
dkµ
l
qµ
Qn
i=1 Dai
i
,
(2.3)
where the so-called IBP vector qµ is built from external momenta and loop mo-
menta. While eq. (2.3) holds for any choice of IBP vector, it is suﬃcient to consider
qµ ∈{pµ
j , kµ
l } to obtain a generating set of all IBP identities, and we will do so in
this paper. Note, however, that more tailored choices of IBP vectors can be advan-
tageous, as used by syzygy methods [17, 18]. Moreover, there are Lorentz invariance
(LI) identities generated by acting on Feynman integrals with generators of Lorentz
transformations. These identities are not independent from the IBP identities [27],
but are often useful to include, and we will include them in the examples in this
paper.
Provided one chooses the set Di so as to be a basis for ISPs of the loop momenta
with each other and the external momenta, applying the product and chain rules
in eq. (2.3) generates linear relations between Feynman integrals I(a1, . . . , an) with
shifted exponents ai, with coeﬃcients that are rational functions in the masses, the
dimension, and ISPs of the external momenta (together called kinematic parameters).
One wants to solve these relations, so as to represent all of the integrals that appear
in a given calculation in terms of a minimal basis of so-called master integrals.
In our example, there are eight IBP identities – two loop momenta kl times four
total independent momenta {k1, k2, p1, p2} that can serve as IBP vectors – and one
2Here, we use Einstein’s summation convention, i.e. the sum over the repeated index µ is implied.
– 5 –


--- Page 7 ---
identity generated by Lorentz transformations. An example of an IBP identity is
(D−a2−a6−a3−2a0)I(a1, a2, a3, a4, a5, a6, a7) −a2I(a1 −1, a2, a3 + 1, a4, a5, a6, a7)
+ a2I(a1, a2 −1, a3 + 1, a4, a5, a6, a7) + a6m2
3I(a1, a2, a3, a4, a5, a6, a7 + 1)
−a3I(a1 −1, a2, a3, a4 + 1, a5, a6, a7) −a6I(a1 −1, a2, a3, a4, a5, a6, a7 + 1)
+ a3I(a1, a2, a3, a4 + 1, a5, a6, a7) = 0 .
(2.4)
We give the full set of identities for this integral in appendix A. When these identities
are solved, all integrals in this family can be expressed in terms of a basis of 16 master
integrals. An example choice of such basis is
I(0, 1, 1, 1, 0, 0, 0),
I(1, 0, 1, 0, 1, 0, 0),
I(1, 1, 0, 1, 1, 0, 0),
I(0, 0, 1, 1, 1, 0, 0),
I(1, 0, 1, 1, 1, 0, 0),
I(1, −1, 1, 1, 1, 0, 0),
I(0, 1, 1, 1, 1, 0, 0),
I(−1, 1, 1, 1, 1, 0, 0),
I(1, 1, 1, 1, 1, 0, 0),
I(1, 0, 1, 0, 0, 1, 0),
I(1, 1, 0, 1, 0, 1, 0),
I(1, 0, 1, 0, 1, 1, 0),
I(1, −1, 1, 0, 1, 1, 0),
I(1, 0, 0, 1, 1, 1, 0),
I(1, 1, 0, 1, 1, 1, 0),
I(1, 0, 1, 1, 1, 1, 0) .
(2.5)
Sometimes, it is possible to solve the systems of IBP identities in full generality
for all values of ai; see ref. [10] for heuristic code to ﬁnd these solutions.3 However,
there is no known algorithm that can ﬁnd these solutions in all cases. As such, in
practice one often instead solves a system generated by a ﬁnite list of seeds a1, . . . , an
in eq. (2.3). If this ﬁnite list is large enough, it will still allow the reduction of the
integrals of interest in terms of a minimal basis. In practice, seeds corresponding to
trivial sectors are often excluded, and we also do this here.
For later convenience, we deﬁne the following quantities for a given member of
an integral family, I(a1, . . . , an):
t ≡
X
ai>0
1 ,
r ≡
X
ai>0
ai ,
d ≡r −t =
X
ai>0
(ai −1) ,
s ≡−
X
ai<0
ai .
(2.6)
The quantity t counts the total number of propagators, r the sum of propagator pow-
ers, d the total number of propagator repetitions, and s the total numerator power.
Since repetitions of propagators are typically depicted by dots on the respective edge
in the graph, d is also referred to as the total number of dots. In any given problem
one typically has one so-called top sector in each integral family, giving a maximal
list of which ai are allowed to be positive; the other ai will always be zero or negative.
Thus there is a maximum value for t, tmax, determined by the top sector.
A number of heuristic seeding strategies have been proposed by diﬀerent authors
and implemented in diﬀerent IBP codes, starting with Laporta’s golden rule [3]. We
will refer to these strategies as follows:4
3See also ref. [28].
4While Laporta’s golden rule was proposed ﬁrst [3], more recent implementations of IBP codes
have been using rectangular seeding [29] before the improved seeding strategy was discovered [4, 14–
16].
– 6 –


--- Page 8 ---
• Rectangular Seeding: Use all seeds a1, . . . , an such that r ≤rmax and s ≤smax
for choices of rmax and smax that include the integrals of interest.
• Golden Rule: Use seeds constrained as above, and also demand that d ≤dmax
for a choice of dmax that includes the integrals of interest, so that the integrals
corresponding to the seeds keep the same number of propagator repetitions in
lower sectors.
• Improved Seeding: Use seeds constrained as above, and also require s ≤t −
l + 1 for a choice of parameter l that includes the integrals of interest, so that
integrals in lower sectors also have fewer powers of ISPs in the numerator.
For future reference, we will mention that in the case that dmax = 0, the additional
condition imposed by improved seeding can be simply written as P
i ai ≥l −1.
In general one wants to deﬁne rmax, smax, and dmax to be as small as possible
while still including the integrals of interest.
However, Laporta observed certain
minimal values for these parameters below which one does not achieve a complete
reduction
[3]. In particular, he noted that one sometimes needs to take at least
dmax = 1, even if d = 0 for all integrals of interest.
In the example we use to benchmark diﬀerent approaches throughout this paper,
as in ref. [15], we would like to reduce the integral I(1, 1, 1, 1, 1, 1, −3) to master
integrals, so we need smax = 3. We consider cases with either rmax = 6 or rmax = 7,
following Laporta’s observation that one sometimes needs dmax = 1 even if one is
only interested in integrals with d = 0. To gain an intuition about the size of the
corresponding systems of IBP equations, we list some examples:
• Rectangular seeding with smax = 3 and rmax = 7 yields 14,588 seeds.
• The golden rule with smax = 3, rmax = 7, and dmax = 1 yields 2,148 seeds.
• Improved seeding with smax = 3, rmax = 6, dmax = 0, and l = 4 yields 92 seeds.
As mentioned above, IBP systems for Feynman integrals are systems of linear
equations with rational function coeﬃcients. Historically, such systems were solved
using symbolic algebra. However, this proved excessively cumbersome for larger sys-
tems, and in recent years the community has instead turned to methods using ﬁnite
ﬁelds [13, 30–32]. By substituting in integers for the kinematic parameters, one can
solve the IBP system over a ﬁnite (large prime) ﬁeld with much less computational
cost than solving the full symbolic system. If one does this for a suﬃcient number of
diﬀerent points, one can use ﬁnite ﬁeld reconstruction techniques to determine the
rational functions present in the solution. Meanwhile, solving at just a single point
is enough to identify a list of master integrals for the system, provided the point is
suﬃciently generic.
– 7 –


--- Page 9 ---
Throughout this paper, once we have generated a set of seeds and the resulting
system of IBP equations, we use Kira [11, 12] to check that the system that we
have generated is suﬃciently large to provide a full reduction of the target integral
I(1, 1, 1, 1, 1, 1, −3) in terms of the master integrals. To do this, we have Kira perform
only its initialization step, which uses pyRed to solve the system over a ﬁnite ﬁeld at
a single kinematic point, and check whether the number of master integrals is equal
to the number 16 found by solving the rectangular system.
2.2
Genetic Algorithms and Machine Learning
Genetic algorithms are heuristic methods used to search for solutions that score well
on an evaluation metric by emulating evolution via natural selection. They begin with
a population of individuals, represented by a DNA-like string of symbols (typically,
integers), their “genotype”. These genotypes are mutated, and crossed-over i.e. the
genotypes of two individuals are mixed, to create a new population, from which the
ﬁttest elements (i.e. the ones with the best evaluation metric) are selected. This
process is repeated over a number of generations, in the hope of creating populations
with better evaluation metric, while still retaining the genetic diversity that allows
for further improvement. See e.g. ref. [21] for a textbook introduction to genetic
algorithms.
Genetic algorithms have several features which have motivated a variety of meth-
ods. We mention several, highlighting the methods used in parts of this work:
• There are many ways to select for the best individual, and selection can hap-
pen both on the parent (who will mate) and on the children (who will survive).
Instead of strictly selecting the best individuals, one can select individuals ran-
domly with probability weighted by their ﬁtness in various ways. Examples
include roulette selection, where probabilities are strictly weighted by their ﬁt-
ness, tournament selection, where individuals are compared in random smaller
groups and only the winners from those groups are preserved, and Boltzmann
selection, in which individuals are selected via a thermal partition function [33].
• It is observed that population diversity tends to decrease from one generation
to the next. As a cure, instead of just maintaining a single population, one can
maintain a number of islands, sub-populations which are independently subject
to crossover and selection. These islands interact with each other more rarely,
for example by occasionally removing the worst-performing islands and re-
populating them from the best-performing ones. Islands can help to preserve a
greater diversity of individuals in order to explore a wider range of possibilities.
• The method heavily depends on the representation chosen for the genotype,
and the operators chosen for cross-over.
For example, one could randomly
choose elements of the child to come from one or the other parent, or one could
– 8 –


--- Page 10 ---
and
>
<
arg1
0
arg2
+
arg1
3
Figure 2. A tree diagram for a simple program given in the main text.
form the child by splicing part of one parent with the complementary part of
another, randomly choosing the position of the splice.
Genetic programming is a particular use of genetic algorithms, in which the
individuals in the population each represent a program; see e.g. ref. [22] for an in-
troduction. Typically, these programs are represented as trees of operations. For
example, the Python function
def func(arg1,arg2):
return arg1>0 and arg2<arg1+3
can be represented by the tree shown in ﬁg. 2.
A program’s ﬁtness is determined by running it on a given set of inputs and eval-
uating the output. Mutation can involve randomly substituting individual elements
of a tree or replacing whole sub-trees with new randomly generated sub-trees; the
latter importantly allows the tree to grow if a small sub-tree is replaced by a larger
one. Crossover can involve replacing a sub-tree of one tree with a sub-tree from the
other tree.
funsearch [23] can be thought of as a genetic programming algorithm with a
few atypical features:
• Instead of representing programs as trees, it represents programs as text, specif-
ically as Python code.
• Instead of mutation and crossover based on elements of trees, it uses a Large
Language Model (LLM). Speciﬁcally, it uses a pretrained language model, ﬁne-
tuned for code generation (e.g. Copilot [34], Codey [35], CodeLlama [36], Code-
Stral [37]). The LLM is prompted with text from two programs from the popu-
lation ordered by ﬁtness and labeled v0 and v1 as well as an incomplete function
labeled v2. It then completes the function labeled v2, which is entered into the
– 9 –


--- Page 11 ---
population subject to selection. This both introduces random variation, as the
LLM will typically not reproduce the functions v0 or v1 verbatim, and a kind
of crossover, as the function v2 generated will be inﬂuenced by which functions
v0 and v1 the LLM is presented with.
• As the code generated can contain arbitrary Python functions, it is imperative
that evaluation of programs in funsearch take place in an appropriate sandbox
environment, as these functions will not always compile or run, and may have
adverse consequences for the system if they do.
Of the features described earlier, funsearch also uses Boltzmann selection and is-
lands, with the number of islands being ten in the public implementation we make
use of.
While we in practice want to minimize the time spent on the IBP reduction
of a set of Feynman integrals to master integrals, the number of seeds and thus
IBP equations used to solve the system provides an easier-to-measure proxy for this
quantity. Within the context of genetic algorithms, such a minimization problem
is typically formulated as a maximization problem for the ﬁtness. Throughout this
paper, we determine the ﬁtness f of an attempt to generate a seeding strategy as
follows. If NS is the number of seeds selected by a seeding strategy and NR is the
number of seeds in the rectangular IBP system we allow the seeding strategy to select
from, then the ﬁtness is
• f = −NS if the seeds solve the system,
• f = −NS −NR if the seeds do not solve the system for all algorithms except
for our initial attempt in section 3, where we take f = −NS −NR −1.
If the seeds successfully solve the IBP system in terms of a minimal basis of master
integrals, the strategy is assigned a ﬁtness score equal to minus the number of seeds
used, thus incentivizing strategies that select a small number of seeds. However, if
the seeds do not solve the system, the score receives an extra penalty of NR. This is
because in principle if one has failed to solve the system one needs to try again with
a list of seeds that we already know can solve the system, in this case the rectangular
system. (For one of the algorithms we try, we moreover punish failure to solve a
bit more strongly than successfully solving the system with the rectangular system,
hence the −1.)
In some of the runs of strongly typed genetic programming, we also experimented
with penalizing lists of zero seeds more strongly: such systems are easy to generate
and they score better than all other strategies that fail to solve the system. Thus
for some runs we doubled the penalty for lists of zero size in order to avoid this
local maximum. In practice, this did not appear to make a signiﬁcant diﬀerence in
performance.
– 10 –


--- Page 12 ---
3
Initial Attempt: Genetic Algorithm
We began with a genetic algorithm that used very little information about the
speciﬁcs of the problem, to provide a baseline and check whether the application
of more sophisticated methods is required.
Suppose that we follow rectangular seeding and end up with a list of NR seed
integrals.
We can specify a subset of seeds with a binary vector of length NR,
including only the relations generated by seeds corresponding to a 1 in the vector.
We performed a genetic algorithm on vectors of this form, generating an initial
population where each entry has a 50% chance to be 0 or 1.
We made sure that the vector with highest ﬁtness was always kept and otherwise
used roulette selection with probability proportional to NR + f, with a 5% chance
to mutate a random entry of each vector and a crossover operation that splices
complementary pieces of two vectors together at a random point. Pairs of vectors
were subject to crossover 90% of the time and remained unchanged the remaining
10%.
For the genetic algorithm in this section, we began with rectangular systems with
smax = 3 and either rmax = 6 or rmax = 7, which have 6,764 and 14,588 seeds, respec-
tively. The genetic algorithm described in this section performs relatively poorly at
this task, but still manages to make progress. Typically, an initial population with
between 100 and 500 random vectors will ﬁnd one that successfully solves the system
with half the number of seeds as in the rectangular system. Subsequent generations
generally lead to much less progress, though, with 100 generations only able to cut
500 additional seeds from the total and appearing to slow down with additional gen-
erations. Running 100 generations on a single CPU5 took about 24 hours. We also
tried beginning with a smaller system of seeds already included in the population,
namely one that followed improved seeding with smax = 3, rmax = 6, dmax = 1, and
l = 4, but in this case the algorithm never found any better solutions.
Given the lack of success of this initial attempt with a classic genetic algorithm,
it is warranted to employ more advanced versions of genetic algorithms, which we
will do in the subsequent sections.
4
Exploration with Funsearch
The authors of funsearch recommend using it in situations where it is unclear how
to design a genetic algorithm to take advantage of the structure of a problem, as a
tool for exploration [23]. This is precisely how we will use it here.
Speciﬁcally, we built oﬀof the fork of funsearch in ref. [38] which implements
two features left oﬀof the initial authors’ public implementation, namely sandbox-
5We used either an AMD EPYC 7F72 or an Intel Xeon CPU E5-2698 v4 @ 2.20GHz, depending
on the run.
– 11 –


--- Page 13 ---
ing (via containerization, which can be done via Podman [39] or Docker [40]) and
calls to the LLM (implemented via the llm package [41]). We modiﬁed this code
slightly, both to use an alternate containerization software available on our local
cluster (Apptainer [42, 43]) and to call Kira outside of the container to keep the con-
tainer environments lightweight. We use Code Llama 7B [36], a light-weight model
trained for code completion, as our LLM.
To use funsearch, one must specify an evaluation function and an initial function
titled priority which will be included in the initial prompts to the LLM labeled
priority v0. We use essentially the same evaluation function as described in the
previous section, with the exception of the extra −1 penalty for failing to solve the
system which we did not ﬁnd to be necessary here.
As above, we begin with a rectangular system, this time speciﬁcally with rmax =
7. We then use the priority functions generated by the LLM to choose which seeds
to use based on the list of ai for that seed.
We tried several initial priority functions. In practice, we found the code in
ﬁg. 3 to be the most successful. Note that here comments matter: the LLM is given
this function as a prompt including comments. As such, it will be biased towards
generating code that would typically have comments of this sort, for example code
that deﬁnes similar variables. This initial priority function corresponds to a golden
rule system with dmax = 1 with 2,148 seeds.
While the initial genetic algorithm found a better solution fairly quickly and
then advanced steadily but slowly, funsearch advanced in large jumps. We used
funsearch’s default settings, which resulted in a much smaller population but, be-
cause this meant fewer evaluations of new solutions, much faster generations.
It
fairly quickly found a solution with 444 seeds, and after 1000 generations had been
able to ﬁnd a solution with 214 seeds, shown in ﬁg. 4. Using the same CPU resources
of the previous section along with an Nvidia A100 or V100 GPU, this took about
16 hours. Running 1400 more generations found a solution with 92 seeds, shown in
ﬁg. 5, taking another 22 hours. This solution turned out to be equivalent to the im-
proved seeding strategy with dmax = 0 and l = 4. Finally, running for an additional
24 hours with 1400 more generations found a solution with 88 seeds, shown in ﬁg. 6.
It achieved this solution by imposing the same conditions as improved seeding, with
the extra condition that the number of propagators be four or more, t ≥4, thus
excluding seeds with three propagators which would otherwise be included in the
system. While we do not know if this strategy is only valid due to a quirk of the
problem we are considering, it is the case that it results in a smaller number of seeds
than any seeding strategy currently on the market would provide.
We note here that the solutions found by funsearch diﬀer from what a human
programmer would propose in several ways.
There are several lines of code that
simply have no eﬀect in our test case: for example, the code in ﬁg. 5 demands that
the number of propagators is two or greater, but this is already ensured by excluding
– 12 –


--- Page 14 ---
def priority(a_list: list[int]) -> bool:
"""Decides whether to include the seed a_list in the ibp system.
Returns True or False."""
len_alist=len(a_list)
#Number of propagators, which are entries in a_list greater than zero
num_props=sum(map(lambda x: 1 if x>0 else 0,a_list))
#Numbers of numerators, which are entries in a_list less than zero
numerators=sum(map(lambda x: 1 if x<0 else 0,a_list))
#Dots, the sum of all entries in a_list greater than one
dots=sum(map(lambda x: x-1 if x>1 else 0,a_list))
#The simplest choice: if there is more than one dot, exclude the seed
#else include it
if dots>1:
return False
else:
return True
Figure 3. Priority function used for the initial prompt to funsearch, corresponding to a
golden rule system with dmax = 1.
trivial sectors, while the code in ﬁg. 6 has several lines imposing conditions on ai less
than a fractional number, which as all ai are integer are equivalent to a condition
on ai less than one. Others impose unusual conditions that happen to restrict the
list of seeds but are probably not generally useful outside of this context, in a way
that would be clear to a human familiar with the problem: for example, the demand
in ﬁg. 4 that there be no more than four even numbers in the ai. Others still are
described misleadingly: the code in ﬁg. 5 and in its “descendant” in ﬁg. 6 deﬁne a
variable nz that they describe as the number of non-zero elements in a list, but
actually deﬁne the number as the sum of ai, which as mentioned in section 2.1 can
be constrained to replicate the improved seeding strategy in cases without dots such
as this one.
We were curious to what extent the results we obtained were inﬂuenced by the
level of detail present in our prompt, so we investigated what happens when we use
a prompt with an essentially empty priority function, shown in ﬁg. 7. This initial
function results in a rectangular system with 14,588 seeds. After running for a total
– 13 –


--- Page 15 ---
def priority(a_list: list[int]) -> bool:
"""Decides whether to include the seed a_list in the ibp system.
Returns True or False."""
#The number of negative entries in a_list
num_negs=sum(map(lambda x: 1 if x<0 else 0,a_list))
if num_negs>1:
return False
#The number of dots in a_list
dots=sum(map(lambda x: x-1 if x>1 else 0,a_list))
if dots>0:
return False
even_numbers=sum(map(lambda x: 1 if x%2==0 else 0,a_list))
if even_numbers>4:
return False
#The number of positive entries in a_list
num_props=sum(map(lambda x: 1 if x>0 else 0,a_list))
if num_props+num_negs<4:
return False
return True
Figure 4. A function generated by funsearch which gives 214 seeds for our test case.
– 14 –


--- Page 16 ---
def priority(a_list: list[int]) -> bool:
"""Decides whether to include the seed a_list in the ibp system.
Returns True or False."""\n
#The number of dots in a_list
dots=sum(map(lambda x: x-1 if x>1 else 0,a_list))
if dots>0:
return False
#The number of positive entries in a_list
num_props=sum(map(lambda x: 1 if x>0 else 0,a_list))
if num_props<2:
return False
#Number of nonzero elements in a_list
nz=sum(a_list)
if nz<3:
return False
if nz>8:
return False
#Number of elements less than 1 in a_list
n1=sum(map(lambda x: 1 if x<1 else 0,a_list))
if n1>4:
return False
return True
Figure 5. A function generated by funsearch which gives 92 seeds for our test case,
equivalent to improved seeding with dmax = 0 and l = 4.
– 15 –


--- Page 17 ---
def priority(a_list: list[int]) -> bool:
"""Decides whether to include the seed a_list in the ibp system.
Returns True or False."""
if len(a_list) < 4:
return False
dots=sum(map(lambda x: x-1 if x>1 else 0,a_list))
if dots>0:
return False
#The number of positive entries in a_list
num_props=sum(map(lambda x: 1 if x>0 else 0,a_list))
if num_props<2:
return False
#Number of nonzero elements in a_list
nz=sum(a_list)
if nz<3:
return False
if nz>8:
return False
#Number of elements less than 1 in a_list
n1=sum(map(lambda x: 1 if x<1 else 0,a_list))
if n1>3:
return False
#The number of entries in a_list that are less than 1/2
n12=sum(map(lambda x: 1 if x<1/2 else 0,a_list))
if n12>3:
return False
#Number of entries that are less than 1/4
n14=sum(map(lambda x: 1 if x<1/4 else 0,a_list))
if n14>3:
return False
#Number of elements less than 1/8
n18=sum(map(lambda x: 1 if x<1/8 else 0,a_list))
if n18>3:
return False
#Number of elements less than 1/16
n116=sum(map(lambda x: 1 if x<1/16 else 0,a_list))
if n116>3:
return False
return True
Figure 6.
A function generated by funsearch which gives 88 seeds, thus performing
better than the improved seeding strategy for our test case.
– 16 –


--- Page 18 ---
def priority(a_list: list[int]) -> bool:
"""Decides whether to include the seed a_list in the ibp system.
Returns True or False."""
return True
Figure 7. An essentially empty priority function used to prompt funsearch in test cases,
which returns the original rectangular system.
of 34 hours funsearch was able to ﬁnd a solution with 476 seeds, shown in ﬁg. 8.
Further runs from this point did not ﬁnd substantial improvements, suggesting that
without more extensive input funsearch plateaus here. The resulting code is quite
strange, with many redundant or irrelevant conditions on counts of ones and zeroes
described in repetitive and for the most part useless ways and some odd conditions
on adjacent entries.
In general, funsearch’s best solutions suggest that in addition to the constraints
usually imposed during seeding that we discussed in subsection 2.1 (such as restric-
tions on t, r, d, and s) we should also consider restrictions on the full sum of ai and
on the count of entries equal to 1 or 0. We will use these insights in the next section
to more eﬃciently ﬁnd our best seeding strategy using a more traditional approach
to genetic programming.
– 17 –


--- Page 19 ---
def priority(a_list: list[int]) -> bool:
"""Decides whether to include the seed a_list in the ibp system.
Returns True or False."""
# If there are too many 1’s, we can’t do anything.
if a_list.count(1) > 10:
return False
# If there are too many 0’s, we can’t do anything.
if a_list.count(0) > 10:
return False
# If there are too many 1’s and 0’s, we can’t do anything.
if a_list.count(1) + a_list.count(0) > 12:
return False
# Check if there are enough 1’s and 0’s.
if a_list.count(1) + a_list.count(0) >= 6:
return True
# If there are not enough 1’s and 0’s, we can’t do anything.
if a_list.count(1) + a_list.count(0) <= 4:
return False
# Check if we are in a trivial solution.
for i in range(len(a_list) - 1):
if a_list[i] == 0 and a_list[i + 1] == 0:
return False
# Check if we are in a trivial solution.
for i in range(len(a_list) - 1):
if a_list[i] == 1 and a_list[i + 1] == 1:
return False
# Find the number of 1’s and 0’s.
ones = a_list.count(1)
zeros = a_list.count(0)
# If there are not enough 1’s, we can’t do anything.
if ones < 3:
return False
# Find the number of 1’s in groups of 3 or more.
groups = 0
# If there are too many groups, we can’t do anything.
if groups > 4:
return False
Figure 8.
The best priority function found by funsearch from the essentially empty
prompt in ﬁg 7, resulting in 476 seeds.
– 18 –


--- Page 20 ---
5
Improved Heuristics via Strongly Typed Genetic Program-
ming
Based on the exploration in the previous section, we now use genetic programming to
evolve seeding strategies, which places more restrictions on what kind of conditions
can arise compared to funsearch. In genetic programming, one must specify
• the arguments of the program to evolve,
• a list of primitive elements, which are functions that may be included in the
program,
• optionally, a list of terminal elements, elements which are not functions or
arguments, and
• as in genetic algorithms in general, one also needs an evaluation function.
We will speciﬁcally use strongly typed genetic programming, in which our primi-
tive elements have speciﬁed data types for input and output which constrain which
elements can follow each other. We employ the implementation of strongly typed
genetic programming in the DEAP package [24]. Speciﬁcally, we use that package’s
eaSimple algorithm, with tournament selection using three-member tournaments.
As above, the program we want to evolve will be a function to decide whether or
not to include a seed with a given list of ai in the system, returning True if the seed
is to be included and False if not. In principle, one has a large number of choices
for how to specify the arguments of the program and the list of primitive elements
consistent with this goal. One intuitive choice would be to let the arguments be
the individual ai, and then have primitive elements that include simple operations
on integers (greater than >, less than <, equal to =, sum + and diﬀerence −),
boolean operations (and, or, not), and terminal elements including simple integers
(say, between −10 and +10). However, we ﬁnd that this choice performs poorly,
often not even ﬁnding a valid solution.
We can gain more insight by looking at the solutions which were successful in
funsearch. Typically, these solutions did not involve imposing conditions on the
individual ai. Instead, they used a relatively small list of variables constructed out
of the ai, including sums of the full list, total propagator and numerator powers,
number of dots, number of propagators, and number of zeros.
Inspired by this,
we chose the following lists of arguments, primitives, and terminal elements for our
genetic programming.
As arguments, we chose
• sum gt 0: the sum of all ai greater than zero,
• sum gt 1: the sum of all ai greater than one
– 19 –


--- Page 21 ---
• minus sum lt 0: minus the sum of all ai less than zero,
• sum all: the sum of all ai,
• count gt 0: the number of ai greater than zero,
• count gt 1: the number of ai greater than one,
• count lt 0: the number of ai less than zero,
• count eq 0: the number of ai equal to zero,
• count eq 1: the number of ai equal to one, and
• count all: the length of the list of ai.
As primitives, we chose
• and : and, which takes two booleans returning a boolean,
• gt: greater than, which takes two numbers returning a boolean,
• lt: less than, which takes two numbers and returns a boolean,
• eq: equal to, which takes two numbers and returns a boolean,
• add: addition, which takes two numbers returning a number, and
• sub: subtraction, which takes two numbers returning a number.
Finally, as the terminal elements we chose
• True,
• 0,
• r max,
• s max,
• and the integers between −10 and +10.
When we construct the initial population, we use DEAP’s function genHalfAndHalf
to generate a random valid tree with depth between 3 and 5. genHalfAndHalf has
a 50% chance of generating a tree where each leaf has the same depth, and a 50%
chance of generating a tree where each leaf can have diﬀerent depth. When building
trees, DEAP ﬁrst determines whether a node will be terminal, then chooses uniformly
between the appropriate bullet points above, choosing an argument or terminal ele-
ment for terminal nodes and one of the other primitives for non-terminal nodes. We
– 20 –


--- Page 22 ---
>
-
-
sum gt 0
0
+
+
sum gt 1
sum gt 1
+
+
0
sum gt 1
+
+
+
+
sum gt 1
sum gt 1
minus sum lt 0
sum gt 1
+
+
0
sum gt 1
+
+
0
sum gt 1
0
-
+
+
sum gt 1
-
+
+
sum gt 1
sum gt 1
-
-
sum gt 1
sum all
-
count lt 0
sum gt 1
-6
-
-
sum gt 1
sum all
-
sum all
sum gt 1
-6
Figure 9. A tree diagram for a program generated via strongly typed genetic programming
in DEAP which achieves our best-case seeding of 88 seeds.
thus include True as a primitive element because DEAP requires there to be a terminal
element of each type, and we list 0 separately in addition to being included in the
integers as constraining something (for example dots) to zero should be a frequent
move, so DEAP would have an equal chance of completing a branch of a tree with 0
speciﬁcally and completing it with a random uniformly chosen integer between −10
and +10 inclusive.
For each generation, two individuals have a 50% chance to crossover, replacing a
randomly chosen sub-tree of one with a randomly chosen sub-tree of another, and a
10% chance to mutate, replacing a randomly chosen sub-tree with a freshly generated
sub-tree of depth 0 to 2. Both operations are restricted to never generate individuals
with depth greater than 17.
Running with a population of 300, we found that we could ﬁnd the best solution
found by funsearch, with 88 seeds, in fairly few generations, with one run ﬁnding
this seeding strategy in only 18 generations while others found it in 30–40.6
A
particularly interesting function that achieved this best-case seeding is depicted in
ﬁg. 9.
The function in ﬁg. 9 may appear quite complicated, however, it can be readily
seen that many of the conditions it imposes are redundant.
Cleaning redundant
conditions and simplifying by removing statements which are always true in the
context of our example, we are left with
4sum all + count lt 0 > 15sum gt 1 + 12 .
(5.1)
This result achieves our best-case seeding in a fairly interesting way. As the multiplier
on sum gt 1 is quite high, it appearing on the right-hand size of the inequality forces
6The longest of these runs ran for eight hours on a single CPU, with some running for less than
three.
– 21 –


--- Page 23 ---
this term to vanish, as it is impossible for the left-hand size to be high enough to
be greater than it. This enforces d = 0. If there is at least one numerator then we
have P ai ≥3, which is equivalent to the condition imposed by improved seeding.
If we have no numerators then we instead have P ai > 3, which in this situation
demands that there be at least four propagators, the additional condition that takes
the number of seeds down to 88.
The results above were obtained with minimal tuning of the hyperparameters,
such as the probabilities for diﬀerent actions during the generation of the trees, the
probability for crossover and mutation, and the maximal depth. It is likely that a
scan over hyperparameters or the re-inclusion of individual ais with low probability
leads to even smaller systems of seeds. We leave corresponding investigations for
future work.
6
Conclusions and Discussion
Integration-by-parts reduction is a frequent bottle neck in state-of-the-art calcula-
tions in perturbative Quantum Field Theory, making it a crucial target for improve-
ments. In this paper, we have applied machine-learning techniques to this problem,
adding to a short but growing list of applications of machine learning to analytic
calculations in theoretical high-energy physics [44–47].
Surprisingly simple changes to the heuristics used for seeding integration-by-
parts systems can have a dramatic eﬀect. In such an environment, the ability to
try a large number of heuristics, recombining the best parts to form new ones, has
great potential, reproducing in a way the experimentation that can occur within a
scientiﬁc community. We have found that, using methods from genetic algorithms,
we can rediscover the latest strategy from known seeding algorithms [4, 14–16] and
even modestly improve on it. Knowing very little about the kinds of methods we
needed, we could ﬁnd these improvements via the methodology of funsearch [23],
generating code with a Large Language Model. With a bit of inspiration from these
results, we could use a more classic type of strongly typed genetic programming
instead, leading to much faster convergence.
In this paper, we have provided a proof of principle that genetic programming
can be used to improve seeding strategies, using a simple two-loop Feynman integral
as a benchmark. In the future, it would be interesting to consider also larger sets of
more complicated Feynman integrals. In contrast to other machine-learning meth-
ods, the strategies produced by funsearch and strongly typed genetic programming
are fully interpretable. Considering a range of diﬀerent integrals thus promises to
reveal fully general strategies that can be included in future IBP software. Comple-
mentarily, we could also imagine the machine-learning techniques being incorporated
into IBP software to ﬁnd optimal seeding strategies tailored to a given problem and
corresponding set of integrals. As current methods rely on solving the same IBP sys-
– 22 –


--- Page 24 ---
tem many times on diﬀerent kinematic points for rational reconstruction, it should
be possible to use genetic programming over the course of a reconstruction, opti-
mizing the seed list while evaluating at diﬀerent kinematic points so as to make the
subsequent evaluations faster.
In the genetic algorithms used in this work, we began with a large system of
seeds and applied a ﬁlter. One could imagine progressing in the opposite way, be-
ginning with a small list of seeds containing the integrals of interest and learning
how to expand eﬃciently to solve the full system. We will explore this idea in future
work [48].
Acknowledgements
We thank Justin Berman, Fran¸cois Charton, Jordan Ellenberg, Garrett Merz, Maja
Rudolph and Johann Usovitsch for fruitful discussions, Baptiste Rozi`ere and Alexan-
der Smirnov for communication, Fran¸cois Charton and Johann Usovitsch for com-
ments on the manuscript as well as Cynthia Rodr´ıguez for initial collaboration. Parts
of the computations done for this project were performed on the UCloud interactive
HPC system, which is managed by the eScience Center at the University of Southern
Denmark. Other parts were performed on SCIENCE AI Centre’s GPU cluster at
the University of Copenhagen. The work of MvH and MW was supported by the
research grant 00025445 from Villum Fonden. MW was further supported by the
Sapere Aude: DFF-Starting Grant 4251-00029B. MW moreover acknowledges the
warm hospitality of the Data Science Institute, University of Wisconsin.
A
Integration-by-Parts identities for the Benchmark Inte-
gral
In this appendix, we provide the explicit form of the IBP identities for the integral
depicted in ﬁg 1, which is the example we use as a benchmark for all machine-learning
approaches in this paper.
In total, there are 8 IBP identities for general indices ai:
(D −a3 −a7 −a4 −2a1)I(a1, a2, a3, a4, a5, a6, a7)
−a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7) + a3I(a1, a2 −1, a3 + 1, a4, a5, a6, a7)
+ a7m4I(a1, a2, a3, a4, a5, a6, a7 + 1) −a4I(a1 −1, a2, a3, a4 + 1, a5, a6, a7)
−a7I(a1 −1, a2, a3, a4, a5, a6, a7 + 1) + a4I(a1, a2, a3, a4 + 1, a5, a6, a7) = 0 , (A.1)
– 23 –


--- Page 25 ---
(D −2a2 −a6 −a3 −a5)I(a1, a2, a3, a4, a5, a6, a7)
+ a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7) −a3I(a1, a2 −1, a3 + 1, a4, a5, a6, a7)
+ a6I(a1, a2, a3, a4, a5, a6 + 1, a7) −a5I(a1, a2 −1, a3, a4, a5 + 1, a6, a7)
−a6I(a1, a2 −1, a3, a4, a5, a6 + 1, a7) + m4a5I(a1, a2, a3, a4, a5 + 1, a6, a7) = 0 ,
(A.2)
(−a4 + a1)I(a1, a2, a3, a4, a5, a6, a7) −a1I(a1 + 1, a2, a3, a4 −1, a5, a6, a7)
+ a1I(a1 + 1, a2, a3, a4, a5, a6, a7) + a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7)
−a3I(a1, a2 −1, a3 + 1, a4, a5, a6, a7) −a3I(a1, a2, a3 + 1, a4 −1, a5, a6, a7)
+ a3I(a1, a2, a3 + 1, a4, a5, a6 −1, a7) + a4I(a1 −1, a2, a3, a4 + 1, a5, a6, a7)
+ (−a7m2
3 + 2a7 + a7m4)I(a1, a2, a3, a4, a5, a6, a7 + 1)
−a4I(a1, a2, a3, a4 + 1, a5, a6, a7) + a7I(a1 −1, a2, a3, a4, a5, a6, a7 + 1)
−a7I(a1, a2, a3, a4 −1, a5, a6, a7 + 1) = 0 ,
(A.3)
(−a2 + a6)I(a1, a2, a3, a4, a5, a6, a7) + a2I(a1, a2 + 1, a3, a4, a5, a6 −1, a7)
−a2I(a1, a2 + 1, a3, a4, a5, a6, a7) + a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7)
−a3I(a1, a2 −1, a3 + 1, a4, a5, a6, a7) −a3I(a1, a2, a3 + 1, a4 −1, a5, a6, a7)
+ a3I(a1, a2, a3 + 1, a4, a5, a6 −1, a7) −a5I(a1, a2 −1, a3, a4, a5 + 1, a6, a7)
+ a5I(a1, a2, a3, a4, a5 + 1, a6 −1, a7) + (m4a5 −a5m2
3)I(a1, a2, a3, a4, a5 + 1, a6, a7)
−a6I(a1, a2 −1, a3, a4, a5, a6 + 1, a7) + a6I(a1, a2, a3, a4, a5, a6 + 1, a7) = 0 , (A.4)
a2I(a1 −1, a2 + 1, a3, a4, a5, a6, a7) + (a2 −a3)I(a1, a2, a3, a4, a5, a6, a7)
−a2I(a1, a2 + 1, a3 −1, a4, a5, a6, a7) −a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7)
+ a3I(a1, a2 −1, a3 + 1, a4, a5, a6, a7) −a6I(a1, a2, a3, a4, a5, a6 + 1, a7)
+ 2a5I(a1 −1, a2, a3, a4, a5 + 1, a6, a7) + a5I(a1, a2 −1, a3, a4, a5 + 1, a6, a7)
−a5I(a1, a2, a3 −1, a4, a5 + 1, a6, a7) −a5I(a1, a2, a3, a4, a5 + 1, a6, a7 −1)
+ (m4a5)I(a1, a2, a3, a4, a5 + 1, a6, a7) + a6I(a1, a2 −1, a3, a4, a5, a6 + 1, a7)
−a6I(a1, a2, a3 −1, a4, a5, a6 + 1, a7) + a6I(a1, a2, a3, a4 −1, a5, a6 + 1, a7) = 0 ,
(A.5)
(−a3 + a1)I(a1, a2, a3, a4, a5, a6, a7) + a1I(a1 + 1, a2 −1, a3, a4, a5, a6, a7)
−a1I(a1 + 1, a2, a3 −1, a4, a5, a6, a7) + a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7)
−a3I(a1, a2 −1, a3 + 1, a4, a5, a6, a7) + (a7m4)I(a1, a2, a3, a4, a5, a6, a7 + 1)
+ a4I(a1 −1, a2, a3, a4 + 1, a5, a6, a7) −a4I(a1, a2, a3 −1, a4 + 1, a5, a6, a7)
+ a4I(a1, a2, a3, a4 + 1, a5, a6 −1, a7) −a4I(a1, a2, a3, a4 + 1, a5, a6, a7)
+ a7I(a1 −1, a2, a3, a4, a5, a6, a7 + 1) + 2a7I(a1, a2 −1, a3, a4, a5, a6, a7 + 1)
−a7I(a1, a2, a3 −1, a4, a5, a6, a7 + 1) −a7I(a1, a2, a3, a4, a5 −1, a6, a7 + 1) = 0 ,
(A.6)
– 24 –


--- Page 26 ---
a2I(a1, a2 + 1, a3, a4, a5 −1, a6, a7) −a2I(a1, a2 + 1, a3, a4, a5, a6 −1, a7)
+ (a2 −a2m4)I(a1, a2 + 1, a3, a4, a5, a6, a7) −2a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7)
+ a3I(a1, a2, a3 + 1, a4 −1, a5, a6, a7) + a3I(a1, a2, a3 + 1, a4, a5 −1, a6, a7)
−a3I(a1, a2, a3 + 1, a4, a5, a6 −1, a7) + a3I(a1, a2, a3 + 1, a4, a5, a6, a7 −1)
−2a3m4I(a1, a2, a3 + 1, a4, a5, a6, a7) + (a5 −a6)I(a1, a2, a3, a4, a5, a6, a7)
−a5I(a1, a2, a3, a4, a5 + 1, a6 −1, a7) + a5m2
3I(a1, a2, a3, a4, a5 + 1, a6, a7)
+ a6I(a1, a2, a3, a4, a5 −1, a6 + 1, a7) −a6m2
3I(a1, a2, a3, a4, a5, a6 + 1, a7) = 0 ,
(A.7)
(a7 + a4 −2a1)I(a1, a2, a3, a4, a5, a6, a7) + a1I(a1 + 1, a2, a3, a4 −1, a5, a6, a7)
+ a1I(a1 + 1, a2, a3, a4, a5, a6, a7 −1) −(a1 + m4a1)I(a1 + 1, a2, a3, a4, a5, a6, a7)
−2a3I(a1 −1, a2, a3 + 1, a4, a5, a6, a7) + a3I(a1, a2, a3 + 1, a4 −1, a5, a6, a7)
+ a3I(a1, a2, a3 + 1, a4, a5 −1, a6, a7) −a3I(a1, a2, a3 + 1, a4, a5, a6 −1, a7)
+ a3I(a1, a2, a3 + 1, a4, a5, a6, a7 −1) −2a3m4I(a1, a2, a3 + 1, a4, a5, a6, a7)
−2a4I(a1 −1, a2, a3, a4 + 1, a5, a6, a7) + (a7m2
3 −2a7)I(a1, a2, a3, a4, a5, a6, a7 + 1)
+ a4I(a1, a2, a3, a4 + 1, a5, a6, a7 −1) + (a4m2
3 −2m4a4)I(a1, a2, a3, a4 + 1, a5, a6, a7)
−2a7I(a1 −1, a2, a3, a4, a5, a6, a7 + 1) + a7I(a1, a2, a3, a4 −1, a5, a6, a7 + 1) = 0 .
(A.8)
Moreover, there is a single LI relation, namely
a3(3 −m2
2 + m2
3)I(a1 −1, a2, a3, a4 + 1, a5, a6, a7)
+ (a3(m2
2 −m2
3 −1) + a4(1 + m2
3 −m2
2) + a5(m2
2 −m2
3 −1) −a6(1 + m2
2 + m2
3))×
I(a1, a2, a3, a4, a5, a6, a7)
+ a3(1 −m2
2 + 3m2
3)I(a1, a2, a3, a4 + 1, a5, a6, a7)
+ a6(m2
3m2
2 −3m2
3 −m4
3)I(a1, a2, a3, a4, a5, a6, a7 + 1)
+ 2a6m2
3I(a1, a2, a3, a4 −1, a5, a6, a7 + 1) −2a3I(a1, a2, a3, a4 + 1, a5, a6, a7 −1)
−2m2
3a4I(a1, a2, a3, a4, a5 + 1, a6 −1, a7)
+ a4(m2
3 + m2
2 −1)I(a1, a2 −1, a3, a4, a5 + 1, a6, a7)
+ a6(m2
2 −3m2
3 −1)I(a1 −1, a2, a3, a4, a5, a6, a7 + 1)
+ a4m2
3(1 + m2
2 −m2
3)I(a1, a2, a3, a4, a5 + 1, a6, a7)
+ a5(1 −m2
2 −m2
3)I(a1, a2, a3, a4, a5, a6 + 1, a7)
+ 2a5I(a1, a2, a3, a4, a5 −1, a6 + 1, a7)
+ a5(m2
3 −m2
2 −1)I(a1, a2 −1, a3, a4, a5, a6 + 1, a7) = 0 .
(A.9)
– 25 –


--- Page 27 ---
References
[1] F.V. Tkachov, A theorem on analytical calculability of 4-loop renormalization group
functions, Phys. Lett. B 100 (1981) 65.
[2] K.G. Chetyrkin and F.V. Tkachov, Integration by Parts: The Algorithm to Calculate
beta Functions in 4 Loops, Nucl. Phys. B 192 (1981) 159.
[3] S. Laporta, High precision calculation of multiloop Feynman integrals by diﬀerence
equations, Int. J. Mod. Phys. A 15 (2000) 5087 [hep-ph/0102033].
[4] M. Driesse, G.U. Jakobsen, G. Mogull, J. Plefka, B. Sauer and J. Usovitsch,
Conservative Black Hole Scattering at Fifth Post-Minkowskian and First Self-Force
Order, Phys. Rev. Lett. 132 (2024) 241402 [2403.07781].
[5] S. Weinzierl, Feynman Integrals. A Comprehensive Treatment for Students and
Researchers, UNITEXT for Physics, Springer (2022), 10.1007/978-3-030-99558-4,
[2201.03593].
[6] C. Anastasiou and A. Lazopoulos, Automatic integral reduction for higher order
perturbative calculations, JHEP 07 (2004) 046 [hep-ph/0404258].
[7] A.V. Smirnov, Algorithm FIRE – Feynman Integral REduction,
JHEP 10 (2008) 107 [0807.3243].
[8] A.V. Smirnov and M. Zeng, FIRE 6.5: Feynman integral reduction with new
simpliﬁcation library, Comput. Phys. Commun. 302 (2024) 109261 [2311.02370].
[9] A. von Manteuﬀel and C. Studerus, Reduze 2 - Distributed Feynman Integral
Reduction, 1201.4330.
[10] R.N. Lee, Presenting LiteRed: a tool for the Loop InTEgrals REDuction, 1212.2685.
[11] P. Maierh¨ofer, J. Usovitsch and P. Uwer, Kira—A Feynman integral reduction
program, Comput. Phys. Commun. 230 (2018) 99 [1705.05610].
[12] J. Klappert, F. Lange, P. Maierh¨ofer and J. Usovitsch, Integral reduction with Kira
2.0 and ﬁnite ﬁeld methods, Comput. Phys. Commun. 266 (2021) 108024
[2008.06494].
[13] T. Peraro, FiniteFlow: multivariate functional reconstruction using ﬁnite ﬁelds and
dataﬂow graphs, JHEP 07 (2019) 031 [1905.08019].
[14] X. Guan, X. Liu, Y.-Q. Ma and W.-H. Wu, Blade: A package for block-triangular
form improved Feynman integrals decomposition, 2405.14621.
[15] J. Usovitsch, “Improved integral reduction with kira.” Talk at QCD meets Gravity
at CERN, December 13th 2023. Slides available at
https://indico.cern.ch/event/1317494/contributions/5697745/attachments/2770593/482730
[16] Z. Bern, E. Herrmann, R. Roiban, M.S. Ruf, A.V. Smirnov, V.A. Smirnov et al.,
Amplitudes, supersymmetric black hole scattering at O
 G5
, and loop integration,
JHEP 10 (2024) 023 [2406.01554].
– 26 –


--- Page 28 ---
[17] J. Gluza, K. Kajda and D.A. Kosower, Towards a Basis for Planar Two-Loop
Integrals, Phys. Rev. D 83 (2011) 045012 [1009.0472].
[18] Z. Wu, J. Boehm, R. Ma, H. Xu and Y. Zhang, NeatIBP 1.0, a package generating
small-size integration-by-parts relations for Feynman integrals,
Comput. Phys. Commun. 295 (2024) 108999 [2305.08783].
[19] P. Mastrolia and S. Mizera, Feynman Integrals and Intersection Theory,
JHEP 02 (2019) 139 [1810.03818].
[20] H. Frellesvig, F. Gasparotto, M.K. Mandal, P. Mastrolia, L. Mattiazzi and
S. Mizera, Vector Space of Feynman Integrals and Multivariate Intersection
Numbers, Phys. Rev. Lett. 123 (2019) 201602 [1907.02000].
[21] A. Eiben and J. Smith, Introduction to Evolutionary Computing, Springer Berlin,
Heidelberg (2015), 10.1007/978-3-662-44874-8.
[22] J.R. Koza, Genetic programming as a means for programming computers by natural
selection, Statistics and Computing 4 (1994) 87.
[23] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M.P. Kumar, E. Dupont
et al., Mathematical discoveries from program search with large language models,
Nature 625 (2024) 468.
[24] F.-A. Fortin, F.-M. De Rainville, M.-A. Gardner, M. Parizeau and C. Gagn´e,
DEAP: Evolutionary algorithms made easy, Journal of Machine Learning Research
13 (2012) 2171.
[25] G. ’t Hooft and M.J.G. Veltman, Regularization and Renormalization of Gauge
Fields, Nucl. Phys. B 44 (1972) 189.
[26] S. Abreu, R. Britto and C. Duhr, The SAGEX review on scattering amplitudes
Chapter 3: Mathematical structures in Feynman integrals,
J. Phys. A 55 (2022) 443004 [2203.13014].
[27] R.N. Lee, Group structure of the integration-by-part identities and its application to
the reduction of multiloop integrals, JHEP 07 (2008) 031 [0804.3008].
[28] D.A. Kosower, Direct Solution of Integration-by-Parts Systems,
Phys. Rev. D 98 (2018) 025008 [1804.00131].
[29] P. Maierh¨ofer and J. Usovitsch, Kira 1.2 Release Notes, 1812.01491.
[30] A. von Manteuﬀel and R.M. Schabinger, A novel approach to integration by parts
reduction, Phys. Lett. B 744 (2015) 101 [1406.4513].
[31] T. Peraro, Scattering amplitudes over ﬁnite ﬁelds and multivariate functional
reconstruction, JHEP 12 (2016) 030 [1608.01902].
[32] J. Klappert and F. Lange, Reconstructing rational functions with FireFly,
Comput. Phys. Commun. 247 (2020) 106951 [1904.00009].
[33] M.d.l. Maza and B. Tidor, An analysis of selection procedures with particular
attention paid to proportional and boltzmann selection, in Proceedings of the 5th
– 27 –


--- Page 29 ---
International Conference on Genetic Algorithms, (San Francisco, CA, USA),
p. 124–131, Morgan Kaufmann Publishers Inc., 1993.
[34] https://news.microsoft.com/september-2023-event/.
[35] https://lablab.ai/tech/google/codey.
[36] B. Rozi`ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X.E. Tan et al., Code llama:
Open foundation models for code, 2308.12950.
[37] https://mistral.ai/en/news/codestral.
[38] J. Aalto. Available at https://github.com/jonppe/funsearch.
[39] “Podman: A tool for managing oci containers and pods.” Available at
https://github.com/containers/podman.
[40] D. Merkel, Docker: lightweight linux containers for consistent development and
deployment, Linux J. 2014 (2014) .
[41] S. Willison, “llm: Access large language models from the command-line.” Available
at https://github.com/simonw/llm.
[42] “Apptainer: Application containers for linux.” Available at
https://github.com/apptainer/apptainer.
[43] G.M. Kurtzer, V. Sochat and M.W. Bauer, Singularity: Scientiﬁc containers for
mobility of compute, PLOS ONE 12 (2017) 1.
[44] A. Dersy, M.D. Schwartz and X. Zhang, Simplifying Polylogarithms with Machine
Learning, Int. J. Data Sci. Math. Sci. 1 (2024) 135 [2206.04115].
[45] T. Cai, G.W. Merz, F. Charton, N. Nolte, M. Wilhelm, K. Cranmer et al.,
Transforming the bootstrap: using transformers to compute scattering amplitudes in
planar N = 4 super Yang–Mills theory, Mach. Learn. Sci. Tech. 5 (2024) 035073
[2405.06107].
[46] C. Cheung, A. Dersy and M.D. Schwartz, Learning the Simplicity of Scattering
Amplitudes, 2408.04720.
[47] Y.S. Koay, R. Enberg, S. Moretti and E. Camargo-Molina, Generating particle
physics Lagrangians with transformers, 2501.09729.
[48] J. Berman, F. Charton, M. von Hippel and M. Wilhelm. In progress.
– 28 –
