--- Page 1 ---
1
Optimizing NetGPT via Routing-Based Synergy
and Reinforcement Learning
Yuxuan Chen, Rongpeng Li, Xianfu Chen, Celimuge Wu, Chenghui Peng, Zhifeng Zhao, and Honggang Zhang
Abstract—Large language model (LLM) agents at the network
edge offer low-latency execution for routine queries. In contrast,
complex requests often require the superior capability of cloud
models, incurring higher latency and cost. To navigate this
quality-cost trade-off under dynamic network conditions, we pro-
pose a cloud-edge synergy for NetGPT that integrates network-
aware routing with on-edge self-improvement. Specifically, our
framework routes structured tool-calling requests to cloud or
edge agents via a novel scoring policy. We prove that, under
mild regularity assumptions, the optimal routing rule admits
a unique fallback threshold with monotone dependence on
bandwidth and round-trip time (RTT). Concurrently, based on
the dataset collected from requests routed to the cloud and
corresponding responses, we instantiate a schema-preserving
reinforcement learning (RL) to improve the capability of the
edge agent. We analyze a supervised finetuning (SFT)-anchored
composite objective that combines a reverse-KL trust-region step
with a forward-KL realignment toward the SFT prior, explaining
stability and constraining policy drift. Both the network-aware
routing policy and the edge agent are updated coherently. Ex-
periments across controlled network states and pricing schedules
demonstrate smooth quality-cost frontiers, consistent gains of
dynamic fallback thresholds over fixed policies, and sustained
reductions in offloading while maintaining task success and
schema-correct outputs.
Index Terms—Cloud-edge collaboration, large language mod-
els, tool-calling, task offloading, adaptive routing.
I. INTRODUCTION
The heterogeneous deployment of Large Language Models
(LLMs) across the network is forming a NetGPT [1]. In this
architecture, lightweight LLMs at the edge offer low latency
but may struggle with complex queries, while more powerful
and general-purpose cloud models lead to significant infer-
ence inefficiency and economic expenditure. The integration
of external tools introduces even greater time-sensitivity [2]
than vanilla LLM interactions [3]. Consequently, there is a
compelling need for a cloud-edge synergy for NetGPT. Such
a collaboration promises low-latency interactions at the edge
with on-demand access to the high-quality reasoning of the
cloud [4], [5].
Y. Chen and R. Li are with Zhejiang University, Hangzhou 310027, China,
(email: {cyx00, lirongpeng}@zju.edu.cn).
X. Chen is with the Shenzhen CyberAray Network Technology Co., Ltd,
China (e-mail: xianfu.chen@ieee.org).
C. Wu is with The University of Electro-Communications, Japan (e-mail:
celimuge@uec.ac.jp).
C. Peng is with Huawei Technologies Co., Ltd., Shanghai 210026, China
(email: pengchenghui@huawei.com).
Z. Zhao is with Zhejiang Lab, Hangzhou 310012, China as well as Zhejiang
University, Hangzhou 310027, China (email: zhaozf@zhejianglab.org).
H. Zhang is with Macau University of Science and Technology, Macau
999078, China (email: hgzhang@must.edu.mo).
While devising a simple binary routing scheme (i.e., edge
or cloud) sounds straightforward, an effective cloud-edge
synergy is far more complicated. We face three key challenges.
First, routing decisions are inherently state-dependent: the
incremental quality from escalating a hard query to a stronger
cloud model must be weighed against network conditions and
inference latency [6]. Second, preference data collected from
interacting with the cloud LLM can naturally be used to super-
vise the edge LLM [7] for capability improvement. Therefore,
the developed routing policy should have the capability of
automatically adapting to dynamic network conditions and
concept drift [8]. Third, LLM agents must adhere to the tool-
calling format (JSON) [9]. However, continuous improvement
risks eroding schema adherence [10].
Existing paradigms cannot simultaneously solve these chal-
lenges. For example, cost-aware cascades [11]–[13] and
preference-trained routers [14], [15] reduce average cost by
sending easy queries to smaller models and escalating hard
ones, but they typically operate as front-door or stage-wise
mechanisms that are agnostic to link dynamics and seldom
support continuous online improvement under schema con-
straints. Representation-learning routers further move beyond
shallow classifiers: contrastive designs such as RouterDC [16]
learn query-conditioned embeddings to score per-model utility,
while graph-based approaches like GraphRouter [17] cast
selection as inductive edge prediction on query-task-model
heterogeneous graphs, improving generalization to unseen
models and tasks, yet still treat latency and schema validity
as exogenous variables. Distributed or multi-agent variants
[18] emphasize ensemble specialization and coordination yet
still decouple decisions from explicit network signals or rely
on fixed acceptance rules. On the other hand, reinforcement
learning (RL)-based online alignment pipelines — such as
online direct preference optimization (DPO) [19] and itera-
tive RL with human feedback (RLHF) workflows [20] —
demonstrate how to continually refresh reward models and
policies from streaming feedback under stability anchors, but
they are typically designed for monolithic LLMs rather than
tool-calling cloud-edge agents. Consequently, there still exists
a methodological gap in optimizing NetGPT.
We address this gap by proposing a cloud-edge LLM
pipeline that unifies tool-calling, network-aware routing, and
online adaptation. Concretely, we use a unified router score
trained from preference and quality signals. The score is pro-
duced by a lightweight reward model (RM) and is periodically
refreshed using cached data from interactions with the cloud.
Based on this score, a state-dependent fallback threshold is
derived from network states like measured round-trip time
arXiv:2511.22217v1  [cs.NI]  27 Nov 2025


--- Page 2 ---
2
TABLE I: THE SUMMARY OF DIFFERENCES WITH RELATED LITERATURE.
References
Multi-
stage
Tool
Schema-
aware
Pref.
Quality
Network
Aware
Online
improv.
Brief description
[14]–[17]
#
#
 
#
#
Single-shot, front-door routing: a preference-
trained scorer picks the model before generation.
[11], [12],
[21]
 
#
 
#
#
Multi-stage
cascade:
apply
a
fixed
quality
threshold after each LLM to early-exit or es-
calate.
[22]
#
#
 
#
 
Contextual-bandit router: adapts model selection
online under cost/quality uncertainty; agnostic to
link dynamics and schema.
[13]
 
#
 
#
 
No explicit router: each LLM decides to for-
ward/reject; trained for system utility.
[18]
#
 
 
#
#
Multi-agent routing with schema-aware coordi-
nation across specialized agents.
This work
 
 
 
 
 
Cloud-edge,
tool-schema-aware
multi-stage
routing with state-dependent fallback thresholds
enabling online adaptation.
Notation: # indicates not included;  indicates fully included.
(RTT) and bandwidth. Simultaneously, the edge LLM policy
is also updated via RL [23] with a cross-entropy supervised
finetuning (SFT) anchor that preserves schema-correct tool-
calling while ensuring stable updates. Furthermore, we analyze
the theoretical simplicities and empirical advantages of the
proposed pipeline. While highlighting the key differences with
existing works in Table I, the main contributions of this paper
are summarized as follows.
• We cast cloud-edge routing as a unified score-threshold
policy whose acceptance boundary is an explicit function
of measured network RTT and bandwidth. Under mild
regularity assumptions on the confidence score, we the-
oretically prove that the optimal offloading rule admits
a unique state-dependent fallback threshold and can be
characterized by a first-order balance between the cloud’s
marginal quality gain and its marginal cost. This yields
monotone comparative statics to derive a network-aware
fallback threshold.
• We unify tool-schema adherence with continual improve-
ment by (i) using cached queries as on-policy supervision
to refresh a lightweight RM, and (ii) optimizing the edge
LLM policy via RL with an SFT anchor to maintain
schema-correct tool-calling and stability during updates.
By keeping routing decisions aligned with the evolv-
ing quality-latency-cost landscape, our design effectively
tackles the underlying structural drift.
• We present an end-to-end evaluation under controlled
network and pricing regimes, showing that dynamic,
network-aware fallback thresholds consistently dominate
fixed policies on the quality-cost frontier. Meanwhile,
SFT-anchored on-device RL contributes to preserving
task success and schema-correct tool-calling, and periodic
RM refresh keeps the router calibrated and improves
routing accuracy.
The remainder of the paper has been organized as follows.
Section II reviews the background and related work. Section III
describes the system model and formulates the optimization
problem. Section IV introduces the proposed dynamic fallback
threshold mechanism and online self-improvement framework
from both operational and theoretical perspectives. Section V
presents the experimental results. Section VI concludes the
paper.
Beforehand, major notations used throughout the paper are
summarized in Table II.
II. RELATED WORK
A. LLM Routing and Selective Inference
Model selection across heterogeneous LLMs has rapidly
evolved from early cost-aware cascades and difficulty-aware
routers to theoretically grounded and benchmarked systems.
Prototype cascades (e.g., [11]) reduce cost by answering
easy queries with cheaper models and escalating hard cases
to stronger models, showing sizable cost-quality gains but
relying on offline heuristics and static thresholds. [15] for-
malizes two-model routing via a learned difficulty predictor
that trades quality for cost and reports up to 40% fewer
calls to the large model with negligible quality loss. On the
supervised front, [14] trains a Bradley-Terry-Luce-style router
on preference pairs mined from multi-model comparisons and
evaluates routing under a cost/willingness-to-pay knob, and
[24] standardizes multi-LLM routing evaluation and highlights
the centrality of calibrated quality estimates.
Representation-learning routers broaden beyond shallow
classifiers: [16] uses dual-contrastive training to predict per-
model utility. [17] casts selection as inductive edge prediction
on a query-model-task heterogeneous graph to improve gener-
alization to unseen models and tasks. Concurrently, cascading
receives theoretical treatment: [25] presents a unified analysis
proving optimal strategies for both routing and cascades and
introduces cascade routing, which outperforms either paradigm
given accurate quality (and cost) estimators. Empirically ori-
ented works refine the design space—e.g., [26] explores MIX-
based cascades for reasoning workloads, [27] proposes dual-
mode routing that blends pre-generation and cascade policies,


--- Page 3 ---
3
TABLE II: Notations used in the paper.
Notation
Description
xi
Task/request index i
Hi,k
Context of task i at step k (history, tool outputs, etc.)
ui,k, u⋆
i,k
Structured actions produced respectively by the edge
LLM and the cloud model (tool, args, thought)
gψ
Reward model (router score model) with parameters
ψ
si,k
Router/reward-model score at (i, k)
τ(·)
Network-aware
fallback
threshold
(runtime:
τ(bSi,k))
di,k ∈{E, C}
Routing decision at (i, k): edge accept (E) or cloud
offload (C)
Si,k, bSi, k
Latent network state at step k and its one-step-ahead
estimate used in τ(bSi, k)
RTTi,k
Baseline round-trip path latency (propagation/queu-
ing; size-independent) at step k
BWi,k
Effective bandwidth at step k (determines transmis-
sion time Ttx)
εi,k, Σ
Gaussian jitter and its scale in the Gauss–Markov
model for time-varying network state
LE, LC
Latency of a local step (edge) and a cloud-offloaded
step (uplink-compute-downlink)
Li,k, L(xi)
Realized latency at (i, k); end-to-end latency of task
i
Ni,k
Token usage of the cloud model at (i, k)
Ci,k, C(xi)
Step cost and total task cost (latency penalty + cloud
cost)
Qi,k, Q(xi)
Step quality in [0, 1] and average task quality
J(x)
Utility measuring the quality-cost trade-off
λ
Trade-off coefficient on cost
πθ
Edge LLM policy with parameters θ
πSFT
Supervised reference policy used for SFT anchoring
ri,k
Learning reward for online updates at (i, k)
κ(S)
Network scaling of cloud cost under state S in
comparative statics (∆CS = κ(S)∆C)
ρ(s)
Local benefit–cost ratio at score s
f(s)
Density of the confidence score s
Ctok
Unit price per cloud token used in the monetary-cost
term
αRTT, βBW, γhist Coefficients in the linear dynamic fallback threshold
(RTT/BW/history sensitivities)
fω(·), ω
Lightweight neural router (PolicyNet) and its param-
eters
γ
Discount factor in the on-device RL objective
ρi,k
PPO importance ratio
Ai,k
Advantage estimate used in PPO
ϵ
PPO clipping range in the surrogate objective
βKL
Reverse-KL penalty coefficient in PPO
η
Forward-KL anchor (SFT realignment) weight
πt
Current policy at iteration t in the two-stage update
Ti,k
Cached tuple (xi, Hi,k, ui,k, u⋆
i,k, si,k)
BRM, BRL
Caches storing on-policy samples for RM and PPO
updates
and [28] augments routing with adaptive additional compu-
tation for small models to yield test-time optimal compute.
Bandit and non-parametric perspectives further question router
complexity: [22] frames selection as a contextual bandit to
adapt to cost/quality uncertainty, while [29] shows strong
performance of kNN-based routing versus learned routers on
standardized benchmarks, underscoring the role of locality in
embedding space. [18] extends routing to multi-agent settings
by jointly choosing collaboration modes, allocating roles, and
routing queries across heterogeneous LLMs, but still abstracts
away time-varying network effects and offers limited online
adaptation under bandit feedback—constraints that reduce
suitability for cloud-edge deployments.
Despite progress, several structural limitations persist for
our cloud-edge setting. First, network unawareness is per-
vasive: most studies treat cost as API price or FLOPs and
either ignore latency/bandwidth or treat them as constants;
even works that model cost explicitly rarely integrate time-
varying link measurements into the decision boundary, and
recent surveys acknowledge that many routers ignore latency
in practice [21]. Second, online self-improvement is rarely
a primary design goal: routers are commonly trained offline
with full supervision (quality labels for all models) and lack
the capability of online adaptation. Third, schema-constrained
tool-calling—now standard in agentic pipelines—poses a sta-
bility constraint largely absent from routing benchmarks. Mo-
tivated by these gaps, we propose a network-aware, schema-
preserving routing pipeline with continual improvement. It
uses a unified score to govern edge acceptance and offload,
adapts fallback thresholds explicitly to measured RTT and
bandwidth, and reuses cached escalated queries to refresh the
router’s reward model and the edge LLM policy.
B. Online Learning and Adaptive Optimization in Networked
Agents
In the LLM era, online alignment seeks to improve models
during deployment under bandit feedback, distribution shift,
and tight latency, rather than relying solely on offline prefer-
ence corpora. [30] further formalizes online preference opti-
mization with equivalences to Nash mirror descent and demon-
strates how to update policies from streaming preferences with
Kullback-Leibler (KL) control. To curb catastrophic forgetting
in continual DPO, [19] introduces an online DPO method with
a fast-slow memory that decouples rapid adaptation from a
stable anchor to preserve earlier competencies. Beyond pure
DPO, self-play and self-rewarding methods close the loop by
generating rewards on the fly. [31] demonstrates that LLM-as-
a-Judge can bootstrap iterative DPO and jointly improve both
the model and its internal judge. To mitigate judge drift, [32]
adds consistency regularization across iterations, improving
reward reliability in self-rewarding pipelines. A game-theoretic
perspective is provided by [33], which frames alignment as a
constant-sum game and proves convergence of iterative self-
play preference optimization updates.
Several works study robustness and grouping in continual
alignment. [34] seeks worst-group improvements to avoid
regressing minority preferences when updating online. On
the RL side, [35] presents a group-relative PPO variant that
removes the critic and estimates baselines from group scores,
offering a lighter-weight alternative for iterative preference-
driven improvement in reasoning-heavy tasks. From a system-
atic perspective, [20] describes online iterative RLHF work-
flows that construct proxy preference models, generate on-
policy data, and repeat align-evaluate-deploy cycles, reporting
consistent gains over offline pipelines. A complementary the-
oretical analysis is provided by [36], which generalizes online
RLHF to preference oracles beyond Bradley-Terry assump-
tions and develops sample-efficient procedures for querying
and updating on the fly.


--- Page 4 ---
4
Tool 
execute
Storage
Training Stage
Training 
Data
Edge LLM
RM
Dynamic
Threshold
Edge SFT
RM Train
τ₀ Estimation
Cloud LLM
Router
RM
Fallback 
Threshold
Edge LLM
Cloud LLM
Prompt 
Inference Stage
Input   Edge output
Score
Pass
Fail
Edge output
Cloud output
Execution
Results
Training 
data
Update
Edge Execution Loop
Cloud Execution Loop
Training Data Save
Input
Network 
state
Storage
Data
Edge RL Train
Fig. 1: Overview of the proposed cloud-edge pipeline.
Despite these advances, a significant gap remains for tool-
oriented, cloud-edge agents. First, the coupling between rout-
ing and learning implies that the same scalar signal acts as
an accept-reject score at the edge and a reward for online
adaptation. Nevertheless, the aforementioned decoupled de-
sign could compromise closed-loop improvement at deploy-
ment. Second, schema-preserving constraints, now standard
via JSON-schema structured outputs, are typically absent from
online preference updates even though they are essential for
maintaining the tool-calling validity under latency budgets.
Our approach addresses both by using a router-as-reward score
to drive state-dependent acceptance and on-device updates, and
by anchoring updates with an SFT anchor to ensure schema-
correct tool-calling and stable improvement.
III. SYSTEM MODEL AND PROBLEM FORMULATION
A. System Model
Fig. 1 provides a high-level overview of the cloud-edge
synergy pipeline in NetGPT, where in response to an input
query xi, the model alternates between internal reasoning and
tool invocation at each step.
As shown in Fig. 2, at each step k, the current context Hi,k
(comprising the accumulated tool outputs and reasoning traces
up to step k) is combined with the query xi to form the edge
prompt. The edge LLM policy πθ then samples an action:
ui,k = (tool, args, thought) ∼πθ(· | xi, Hi,k).
(1)
Here ui,k represents both the model’s reasoning and its pro-
posed tool operation. Correspondingly, the reward model (RM)
gψ evaluates ui,k and produces a scalar score si,k:
si,k = gψ(xi, Hi,k, ui,k) ∈R.
(2)
Subsequently, the routing decision can be made by comparing
the score with a network-aware fallback threshold τ(Si,k)
with Si,k denoting the networking state. In other words, if
si,k ≥τ(Si,k), the prompt will be solely handled at the edge;
otherwise, the more powerful cloud LLM produces a refined
output:
u⋆
i,k ∼πcloud(· | xi, Hi,k).
(3)
Lately, u⋆
i,k serves as the preferred answer for both routing
supervision and reward calibration. In our work, we assume
the edge LLM policy πθ, the RM gψ and the fallback threshold
τ(Si,k) has been initialized in Appendix A, while the cached
tuple Ti,k ≡(xi, Hi,k, ui,k, u⋆
i,k, si,k) will be appended to
the RM cache BRM and the RL cache BRL for online self-
improvement, as detailed in Appendix B.
1) Network State Model
Following ITU-T Y.1541 de-
lay/QoS objectives for IP networks [37] and IETF RFC
6349’s bandwidth–delay-product rationale for TCP throughput
measurement [38], we model the network state at step k of
query i with two primary observables:
Si,k =
 RTTi,k, BWi,k

,
(4)
where RTTi,k denotes round-trip time and BWi,k is band-
width.
2) Inference Cost and Quality Model As mentioned earlier,
we formalize the routing decision for step (i, k) as:
di,k =
(
Edge,
si,k ≥τ(·),
Cloud,
si,k < τ(·).
(5)
In words, di,k selects EDGE when the router score si,k exceeds
the fallback threshold τ(·), and CLOUD otherwise. Given this
decision di,k, we model the step latency as:
Li,k =
(
LE(xi, Hi,k),
di,k = Edge,
LC(xi, Hi,k, Si,k),
di,k = Cloud.
(6)
Here, LE is the on-device inference latency; LC is the end-
to-end offloading latency under the network state Si,k. We
decompose the cloud case into propagation, transfer, and cloud
compute:
LC(xi, Hi,k, Si,k)
(7)
=RTTi,k + Ttx(xi, Hi,k, u∗
i,k, Si,k) + Lcloud(xi, Hi,k),
where Ttx(·) is the payload-dependent transmission time under
limited bandwidth and Lcloud(·) is the cloud LLM’s compu-
tation time.
We couple latency and monetary terms into a step-level cost:
Ci,k = αLi,k + Ctok · Ni,k.
(8)
Here α ≥0 converts latency to a penalty so that quality-cost
trade-offs can be scalarized; Ctok accounts for the price per
cloud token used; and Ni,k is the number of tokens on the
cloud path (and Ni,k = 0 when di,k = Edge).
For each step (i, k), we define a quality score Qi,k ∈[0, 1]
that reflects how well the model’s output completes the re-
quired tool invocation or reasoning. The score is computed
on-device using a frozen evaluation model trained on a disjoint
dataset similar to the SFT or RM data. Each sample is cross-
checked by human auditing before freezing the evaluator
parameters. We formalize it as:
Qi,k = I[schema(ui,k) = 1] ˜qϕ(xi, Hi,k, ui,k).
(9)
Here ˜qϕ(·) ∈[0, 1] is the normalized task-quality score from
the evaluator, and the indicator I[·] ensures that any structural
violation (schema = 0) directly forces Qi,k = 0.


--- Page 5 ---
5
User query
As a fintech 
researcher, develop 
a federated credit 
scoring model using 
transaction data 
from 10 mobile 
banking clients.
Prompt Construct
Edge Prompt
 Local Query
 History step tools
 Tools list(name + 
args schema)
Cloud Prompt
 Constraints
 Local Query
 Historical step tools
 Result summary
 Tools list (name + 
args schema + 
description)
Edge agent
Edge policy:      (SFT 
+ RL; schema-preserving)
Output: 


,
(
,
,
)
i k
u
tool args
thought
Router
,
,
, )
( ,
,
i k
i
i k
i k
s
g
x H
u


Tool Execution
Execution: invoke tool
Feedback loop: 
Update history step for
prompt construct
Cloud agent
Latency:
Cost: 
C
i,k
tx
cloud
,
RTT
( ,
)
i
i k
L
T
L
x H



cloud
,
tok
,
(
)
i k
i k
c
N
C
N


Storage
ix


,
, 
i
i k
x
H
,i k
u
,i k
s
(
)
tS

YES
,i k
u


,
, 
i
i k
x
H
*
,i k
u
Enriched prompt for cloud
Step execution result
Main flow
Update context
,
,
(
)
FuncDyn(
)
i k
i k
S
S


NO
Append 
history
Network state
tS
,
1
,
( ,out)
i k
i k
H
H
u


,
,
(
)?
i k
i k
s
S


Threshold module
Fallback Threshold:
Reward model
Score:
Fig. 2: Online-stage of the proposed pipeline.
B. Problem Formulation
For each task xi, the task-level quality and cost are defined
by aggregating per-step quantities separately:
Q(xi) = 1
Ki
Ki
X
k=1
Qi,k,
C(xi) =
Ki
X
k=1
Ci,k.
(10)
We then define a utility function that balances these two
metrics:
J(xi) = Q(xi) −λC(xi),
λ > 0.
(11)
The trade-off coefficient λ controls the tolerance for latency
and cost under given quality priorities.
The overall optimization objective is to maximize the ex-
pected utility across all tasks by jointly learning the network-
aware fallback threshold function τ(·) and the edge LLM
policy parameters θ:
max
τ(·), θ Ex∼D[J(x)] = max
τ(·), θ E[Q(x) −λC(x)] .
(12)
Here, x ∼D denotes sampling a task from the deployment-
time workload distribution; τ(·) governs routing under the
observed network state S; and θ parameterizes the on-device
policy that generates structured tool calls.
The key difficulty to solve (12) lies in coupling short-
term network adaptation with long-term model improvement.
Existing formulations typically isolate routing from model
adaptation [14], [16], and often assume fixed or exogenous
communication costs or static model behavior [11], [15].
Nevertheless, decoupling of network-aware fallback thresh-
olds and underlying model improvement could significantly
compromise the efficiency. Instead, the fallback threshold τ(·)
should react to instantaneous variations of the network state
S while the edge LLM πθ and RM gψ continuously improve
according to observed data during online inference. These two
processes operate on different time scales but jointly determine
the system’s stability and efficiency.
IV. METHOD
To maximize the expected utility J, we combine dynamic
fallback threshold adaptation with schema-preserving RL, both
guided by a unified router score. The former responds rapidly
to changing network states, while the latter gradually improves
the edge LLM’s competence under data drift without compro-
mising tool-schema correctness.
A. Dynamic Fallback Thresholds
To characterize the routing behavior under varying network
states, we first analyze the threshold-based decision rule and
its optimality conditions. Without loss of generality, for each
request with confidence score s, we assume that the edge
execution yields quality QE and cost CE, while the cloud
execution yields QC and CC. We define the marginal cloud-
edge differences:
∆Q(s) ≜E[ QC −QE | s ],
∆C(s) ≜E[ CC −CE | s ].
(13)
Intuitively, ∆Q(s) is the expected quality gain (if any) from
escalating a score-s request to the cloud rather than accepting
the edge inference, while ∆C(s) is the induced extra cost
(e.g., latency, transmission, cloud inference). Let f(s) be the
density of the confidence score s, and we make the following
assumption.
Assumption 1 (Regularity of the score). The confidence score
s admits an absolutely continuous distribution with density f
that is strictly positive on the relevant support.
Assumption 1 only requires conditional continuity with
respect to the score, but does not specify any particular
parametric model for (QE, QC, CE, CC). Therefore, it can be
easily met.
We now express the sensitivities of
 Q(τ), C(τ)

with
respect to the fallback threshold τ.
Lemma 1 (Frontier sensitivities). Under Assumption 1,
dQ
dτ (τ) = f(τ) ∆Q(τ),
dC
dτ (τ) = f(τ) ∆C(τ).
(14)
Proof. By the definition of the score-threshold policy,
Q(τ) =
Z ∞
τ
E[QE | s]f(s) ds +
Z τ
−∞
E[QC | s]f(s) ds.
(15)


--- Page 6 ---
6
Applying Leibniz’s rule and continuity yields
dQ
dτ (τ) = −E[QE | s = τ]f(τ) + E[QC | s = τ]f(τ)
= f(τ)∆Q(τ).
(16)
The identity for dC/dτ is analogous.
■
A direct corollary of Lemma 1 is the frontier slope identity:
dQ
dC (τ) = ∆Q(τ)
∆C(τ) = ρ(τ),
(17)
i.e., the slope of the achievable (Q, C) frontier at fallback
threshold τ equals the local benefit–cost ratio of requests with
score s = τ.
Assumption 2 (Informativeness of the score). The cloud-
edge marginal cost differential ∆C(s) is strictly positive on
its support; ∆Q(·) and ∆C(·) are continuous with the ratio
ρ(s) ≜∆Q(s)/∆C(s) strictly decreasing in s.
Assumption 2 is natural: as the confidence score increases,
the cloud’s incremental benefit relative to the edge diminishes
faster than its incremental cost, making a single switch opti-
mal.
We next solve the scalar optimization maxτ J(τ) and show
the uniqueness of the optimal fallback threshold τ ∗.
Theorem 1 (Unique optimal fallback threshold (first-order
balance)). Under Assumption 1 and Assumption 2, the score-
threshold policy is optimal among all measurable policies that
depend only on the score, and the maximizer τ ∗of J(τ) is
unique. It is characterized by the first-order balance
∆Q
 τ ∗
= λ ∆C
 τ ∗
,
sign
 J′(τ)

= sign
 ρ(τ) −λ

.
(18)
Proof. By Lemma 1,
J′(τ) = dQ
dτ (τ) −λdC
dτ (τ)
= f(τ)(∆Q(τ) −λ∆C(τ))
(19)
= f(τ)∆C(τ)(ρ(τ) −λ) .
Assumption 1 and Assumption 2 ensure f(τ) > 0 and
∆C(τ) > 0, respectively. Hence, the sign of J′(τ) is the
sign of ρ(τ) −λ. Because ρ(·) is strictly decreasing and
continuous, there exists a unique τ ∗satisfying ρ(τ ∗) = λ;
at that point J′(τ ∗) = 0, and strict monotonicity of ρ implies
J′ changes sign, yielding a unique maximizer τ ∗. Optimality
of the fallback threshold structure among score-based policies
follows from the single-crossing of ρ(·). Finally, by Lemma 1,
the frontier slope at τ equals dQ/dC = ρ(τ), so it equals λ
at τ ∗.
■
Remark 1. Under the mild regularity conditions of Theorem 1,
the Pareto frontier traced by τ 7→(Q(τ), C(τ)) is strictly
monotone and differentiable. Meanwhile, the optimal routing
policy possesses three interpretable properties: (i) the optimal
measurable score-based policy is threshold-shaped; (ii) the
fallback threshold τ ∗is unique and satisfies a first-order
balance between the marginal expected quality gain and the
marginal offload cost; and (iii) at the unique maximizer τ ∗,
its tangent slope equals λ.
This result provides a principled basis for incorporating
network state S (e.g., bandwidth, RTT) into the fallback
threshold τ, which we extend to the state-aware case below.
Our goal is to characterize how the optimizer τ ∗(S) of
J(τ; S) = Q(τ; S) −λC(τ; S) shifts with S.
Assumption 3 (Separable network impact). There exists a
strictly positive, continuous scaling κ(S) such that ∆CS(s) =
κ(S) ∆C(s) for all s; moreover, the marginal quality gain is
governed by task difficulty captured by the score and is not
distorted by the link, i.e., ∆QS(s) = ∆Q(s).
By Assumption 3 and Theorem 1, the optimal fallback
threshold τ ∗(S) is characterized by
∆Q
 τ ∗(S)

= λ ∆CS
 τ ∗(S)

= λ κ(S) ∆C
 τ ∗(S)

,
(20)
or equivalently ρ
 τ ∗(S)

= λ κ(S).
Theorem 2 (Network influence on τ ∗). Under Assumption 2
and Assumption 3, if κ(S1) > κ(S2) then τ ∗(S1) < τ ∗(S2).
Proof. Start from the first-order condition at state S:
ρ(τ ∗(S)) = λ κ(S).
(21)
Pick S1, S2 with κ(S1) > κ(S2). Then
λ κ(S1) > λ κ(S2).
(22)
By strict monotonic decrease of ρ(·) (Assumption 2), the
equation ρ(τ) = λ κ(S) has a unique solution and the solution
moves in the opposite direction of the right-hand side. Hence
ρ
 τ ∗(S1)

> ρ
 τ ∗(S2)

=⇒
τ ∗(S1) < τ ∗(S2),
(23)
which proves the theorem.
■
Remark 2. Theorem 2 implies that if κ is strictly decreasing
in available bandwidth BW and strictly increasing in RTT or
cloud-unit price Ctok, then τ ∗is strictly increasing in BW
and strictly decreasing in RTT and Ctok.
Corollary 1 (Local sensitivity of τ ∗(S)). Suppose ρ is con-
tinuously differentiable and ρ′(τ) ≜
dρ
dτ < 0. Then τ ∗is
continuously differentiable in κ, and
dτ ∗
dκ (S) =
λ
ρ′ τ ∗(S)
 < 0.
(24)
Proof. Define the residual:
F(τ, κ) ≜ρ(τ) −λ κ.
(25)
At
 τ ∗(S), κ(S)

,
the
optimality
condition
gives
F
 τ ∗(S), κ(S)

=
Fτ(τ ∗, κ) dτ ∗+ Fκ(τ ∗, κ) dκ
=
0,
which implies:
dτ ∗
dκ = −Fκ
Fτ
= −−λ
ρ′(τ ∗) =
λ
ρ′(τ ∗).
(26)
This establishes the corollary.
■
Finally, the frontier’s local geometry inherits the same


--- Page 7 ---
7
Inputs
,i k
s
,
RTTi k
,
BWi k
,i k
Q
0
r
,
tt
,
,
,
(
)
RTT
BW
i k
i k
k
bw
h
k
i
i
ist
S
Q












,
,
(
)?
i k
i k
s
S


    update
,
(
)
i k
S

Edge
Cloud
     Calibrated in initialization stage.
0

BW
RTT
,
(
)
i k
S

,
,
,
,
,
i k
i k
i k
Q
S
s
Lightweight MLP
Linear(input→64)
Linear(64→32)
Linear(32→16)
Linear(16→1)
ReLU+Dropout
ReLU+Dropout
ReLU
Output
Sigmoid
,i k
p
Edge
Cloud
label =
edge
edge
,
,
cloud
cloud
,
,
argmax(
,
)
i k
i k
i k
i k
Q
C
Q
C




YES
NO
>0.5
≤0.5
(a)
(b)
Input
Fig. 3: Runtime routing modules and adaptive fallback thresh-
olding. (a) Dynamic fallback threshold controller τi,k
=
fϕ(Si,k, bQi,k), initialized from the offline-calibrated τ0 and
adjusted by the current network state (RTTi,k, BWi,k) and
recent quality bQi,k. The heatmap shows how network state
variables shape τi,k, with bQi,k fixed at its validation mean.
(b) Lightweight router fω (PolicyNet) takes [Si,k, si,k, bQi,k]
and outputs routing probability pi,k; decisions follow pi,k ≤
0.5 ⇒Edge, pi,k > 0.5 ⇒Cloud.
parameterization: for each fixed S and fallback threshold τ,
dQ
dC (τ; S) = ∆Q(τ)
∆CS(τ) = ρ(τ)
κ(S).
(27)
Therefore, improving network states (smaller κ(S)) steepens
the attainable quality-cost slope at the same fallback threshold.
Together with the first-order balance ρ(τ ∗(S)) = λ κ(S),
this shows that the network state shifts the optimal fallback
threshold monotonically. In practice, this motivates either a
linearized functional mapping (FuncDyn) or a lightweight one-
hidden-layer router (PolicyNet).
• FuncDyn: It computes the fallback threshold τi,k as a
smooth function of the observed link condition and recent
execution history:
τi,k = τ0 −αRTTRTTi,k + βBWBWi,k −γhist bQi,k,
(28)
Here τ0 denotes the base fallback threshold level obtained
from the initialization procedure described in Appendix
A, and serves as the reference point adjusted by the
observed network state. bQi,k denotes a running estimate
of Qi,k.
As shown in Fig. 3(a), τi,k increases with bandwidth
and decreases with RTT, consistent with the comparative
statics established in Theorem 2.
• PolicyNet: It eliminates the explicit fallback threshold
and learns a small neural policy that directly outputs the
routing decision. Given the feature tuple (Si,k, si,k, bQi,k),
the PolicyNet predicts the probability of routing to the
cloud:
pi,k = σ
 fω(Si,k, si,k, bQi,k)

≥0.5,
(29)
where the neural network fω(·) is a compact MLP (4
layers, ≈2, 900 parameters), ensuring it can be deployed
efficiently on the edge device. The neural network fω(·)
can be trained following a standard logistic objective,
Lpolicy(ω) = E[−yi,k log pi,k −(1 −yi,k) log(1 −pi,k)] ,
(30)
where the training label yi,k
=
arg max
 Qedge
i,k
−
λCedge
i,k , Qcloud
i,k
−λCcloud
i,k

.
B. Online Self-Improvement
After initialization, the edge agent continues to refine itself
(i.e., πθ), contingent on the unified score signal si,k and shared
cached edge–cloud pairs collected during deployment1. At step
(i, k), the router score acts as the immediate reward by
ri,k = si,k = gψ(xi, Hi,k, ui,k).
(31)
The goal is to maximize the expected discounted return in
max
θ
Eπθ
"X
k
γkri,k
#
,
0 < γ ≤1.
(32)
The discount factor γ controls how strongly future rewards
affect current updates.
Commonly, we follow
(RL-KL)
eπt+1 = arg max
π
E(s,a)∼dπt

Aπt(s, a) log π(a|s)

|
{z
}
policy improvement
−ηt KL(π ∥πt) .
(33)
and apply a clipped PPO update [23] by optimizing:
LRL(θ) = E
h
min(ρi,kAi,k, clip(ρi,k, 1 −ϵ, 1 + ϵ)Ai,k)
i
−βKLKL
 πθ∥πold

,
(34)
where
ρi,k = πθ(ui,k | xi, Hi,k)
πold(ui,k | xi, Hi,k),
Ai,k is the advantage estimate from rewards, ϵ indicates
the clipping range, and βKL is the KL penalty coefficient
controlling the update size.
On the other hand, to maintain structured outputs, every M
PPO updates are followed by a short SFT anchoring step:
(SFT-CE)
πt+1 = arg min
π
Es∼DSFT
h
H
 πSFT(·|s), π(·|s)

|
{z
}
= KL(πSFT ∥π)+const
i
.
(35)
where πSFT is the frozen supervised model. This projection
keeps the tool schema correct while allowing the edge LLM
policy to improve. Over time, πθ learns to approach the cloud’s
performance with reduced cost.
The first stage (RL-KL) performs a KL-regularized trust-
region update that delivers stable policy improvement for a
regularized MDP [39]. Meanwhile, the second stage (SFT-
CE) applies an SFT-anchored cross-entropy projection that
1We leave the details of caching setup in Appendix B.


--- Page 8 ---
8
pulls the policy back toward the SFT manifold to preserve
formatting/tool-calling habits [40]. Jointly, the alternation can
be viewed as block-coordinate ascent on a composite regular-
ized objective:
Jη,µ(π; πt, πSFT) = E s∼dπt, a∼π(·|s)[A(s, a)]
−η E s∼dπt[KL(π(·|s) ∥πt(·|s))]
−µ E s∼dπt[KL(πSFT(·|s) ∥π(·|s))].
(36)
Here A(s, a) is the same advantage estimator as defined in
Eq. (34), and dπt denotes the discounted state distribution
under the current policy.
Similar to the initialization method in Appendix A, the
reward model is incrementally updated using the same pair-
wise ranking loss with schema penalty as Eq. (38). Fine-
tuning is performed in small batches with early stopping to
prevent reward drift. This incremental refresh keeps gψ aligned
with the evolving edge LLM policy πθ and the time-varying
network state S, ensuring that the router signal si,k remains
consistent and informative.
Finally, we summarize the whole procedure for the cloud-
edge synergy in Algorithm 1.
V. PERFORMANCE EVALUATION
A. Experimental Setup
We
consider
a
NetGPT
scenario
with
a
hetero-
geneous
tool-calling
stack
where
the
edge
runs
DeepSeek-R1-Distill-Qwen-7B
(FP32,
non-
quantized) and the cloud uses DeepSeek-V3.2-Exp.
Both models are required to emit structured tool calls with a
tool name, arguments, and a brief thought trace, but we use
different prompting schemes at the edge and in the cloud.
At the edge, we use a compact schema that includes the
current query, tool names from previous steps, and the list
of currently available tools with their argument slots. In the
cloud, we use a ReAct-style prompt [3] with richer task
instructions and full tool and argument descriptions to enforce
the desired reasoning pattern and output format.
At the edge, we optimize the local LLM with PPO [23],
using the unified router score as a scalar reward. The score si,k
is produced by a lightweight reward model gψ instantiated on
Qwen2.5-1.5B-Instruct, fine-tuned on comparison judgments
favor cloud-level tool-calling quality and to penalize schema or
format violations. During deployment, gψ serves as the scoring
module of the router, while the routing policy leverages si,k,
the current network state Si,k, and the running quality estimate
bQi,k to decide whether to continue execution on the edge or
offload the next step to the cloud.
Workloads are drawn from a private corpus of tool-calling
tasks that matches our target skill mix. From this corpus2,
we sample 8, 000 examples for SFT-based initialization of
the edge LLM for stabilizing schema-compliant tool-calling,
2, 000 examples to train the reward model, and 8, 000 tasks
2As described in Section IV and Appendix B, only cloud-offloaded steps
are logged and reused.
Algorithm
1
Network-Aware Routing and Online Self-
Improvement
Inputs: dataset D, schema S, trade-off λ, link logs,
edge LLM policy πSFT, reward model gψ, horizon
M.
Outputs: updated πθ, τ(·) or fω, refreshed gψ.
Offline Initialization
1: Obtain πSFT by Eq. (37)
2: Obtain gψ by Eq. (38)
3: Obtain τ0 by Eq. (40)
4: Set πθ ←πSFT
Online Routing and Self-Improvement
1: for each task i do
2:
k←1, context Hi,1
3:
while task i not finished do
4:
ui,k ∼πθ(·|xi, Hi,k)
5:
Obtain si,k = gψ(xi, Hi,k, ui,k) by Eq. (2)
6:
Obtain τi,k by Eq. (28) or Obtain pi,k by Eq. (29)
7:
if si,k ≥τi,k or pi,k ≥0.5 then
8:
Execute ui,k locally
9:
else
10:
Query cloud ⇒u⋆
i,k
11:
Obtain Ci,k by Eq. (8)
12:
Store (xi, Hi,k, ui,k, u⋆
i,k, si,k, Si,k) in BRM
13:
end if
14:
Compute Qi,k by Eq. (9)
15:
Compute Li,k by Eq. (6)
16:
Update history bQi,k ←(1 −β) bQi,k−1 + βQi,k
17:
Update Hi,k+1 by Eq. (42)
18:
Add (xi, Hi,k, ui,k, ri,k=si,k) to BRL
19:
k←k+1
20:
end while
21: end for
22: for each idle window do
23:
Update πθ by PPO using Eq. (34) on BRL
24:
if every M steps then
25:
Align πθ to πSFT by Eq. (35)
26:
end if
27:
Update gψ using incremental RM training (Eq. (2)) on
BRM
28:
Recalibrate τi,k or fω using latest (Si,k, bQi,k)
29: end for
30: return πθ, τ(·) or fω, gψ
for inference experiments and simulated continual improve-
ment. Task difficulty scales with input length and tool-calling
complexity: each instance exposes 10−20 candidate tools and
includes between 0 and 8 previously executed steps. During
evaluation, requests are processed step-wise and metrics are
aggregated at the task level. To couple routing decisions with
network conditions, each request samples a throughput BW
(Mbps) and a round-trip time RTT (ms) from one of three
regimes:
• GOOD: BW ∈[120, 200], RTT ∈[20, 40];


--- Page 9 ---
9
“As an edge-device engineer, implement federated learning across 
50 IoT sensors collecting temperature and humidity data, using a 
lightweight CNN model with differential privacy guarantees..”
{
   "query": "As an edge-device engineer, implement...",
   "completed_steps": ["initialize_nodes"],
    "available_tools": [
      "initialize_nodes(node_count, model_class, dataset_name)",
      "Simulate_topology(node_count, Bandwidth_limits, latencies)",
      "...",
      "aggregate_global_model_secure_agg(local_models, ...)",
      "finish()"
    ]
}
Prompt Construct
{
  "name": "simulate_topology",
  "args": {
    "node_count": 51,
    "bandwidth_limits": {"value": 10, "count": 50},
    "latencies": {"value": 5, "count": 50}
  },
  "thought": "A star topology suits IoT sensors with one 
coordinating server. Set bandwidth to 10 Mbps and latency to 5 
ms."
}
LLM
Tool execution
Fig. 4: Example of structured text flow across stages.
• MID: BW ∈[30, 80], RTT ∈[40, 80];
• BAD: BW ∈[5, 15], RTT ∈[80, 130].
This design aligns our three buckets with established QoS
envelopes: according to the ITU-T Y.1541 [37] end-to-end
objectives, 20 −40 ms RTT reflects metro/near-region paths,
while 40 −80 ms RTT corresponds to typical regional paths,
and 80 −130 ms RTT indicates long-haul yet standards-
consistent paths. For bandwidth, we sample a link rate in Mbps
as a controlled variable but interpret its effect through the
TCP bandwidth-delay-product (BDP) rationale (RFC 6349)
[38], i.e., GOOD/MID/BAD correspond to low-/moderate-
/high-BDP regimes that govern achievable throughput and
window sizing. We model time-varying network state via a
one-step Gauss-Markov update Si,k = Si,k−1 + εi,k, where
εi,k ∼N(0, Σ2). For time-correlated links, such a method
underpins widely used network/wireless mobility abstractions
[41].
We study two policies: FuncDyn constructs a rule-based
dynamic fallback threshold τi,k from the normalized round-
trip time RTTi,k, bandwidth BWi,k, and a historical quality
bQi,k, and offloads when si,k
> τ(RTTi,k, BWi,k, bQi,k).
PolicyNet is a compact four-layer MLP that takes as input
xi,k =
 Si,k, si,k, bQi,k

and outputs a binary cloud-edge
routing decision. For comparison, we evaluate two routing
baselines. RouteLLM [14] is a learned one-shot router that,
given an input query, makes a single binary choice—answer on
the edge or offload to a stronger model—and keeps this choice
fixed throughout multi-step tool calling. FrugalGPT [11] im-
plements a fixed fallback threshold cascade, where requests
are passed from smaller to larger models according to a static
acceptance rule. All training and batched inference run on 8×
NVIDIA A800 (80 GB) GPUs.
Metrics include utility J, composite quality Q, total cost C,
and offload rate (fraction of requests routed to the cloud). Un-
less otherwise stated, all metrics are reported as averages over
the 8, 000 evaluation tasks. We record latency components but
report normalized costs so that Q and C remain commensurate
under the chosen λ. In addition, for frontiers and sensitivity
analyses, we sweep τ over a dense grid; for long traces, we
interleave regimes to emulate time-varying links.
B. Adaptive Routing under Time-Varying Links
We compare our dynamic controllers (FuncDyn and Poli-
cyNet) with RouteLLM, FrugalGPT, and two additional base-
lines (All Edge and All Cloud) across GOOD/MID/BAD
regimes, and Fig. 5 presents the corresponding results. It
shows FuncDyn yields the highest J while keeping the of-
fload rate moderate, reflecting effective state-aware accep-
tance. PolicyNet attains the highest Q with higher offload
and cost. In contrast, FrugalGPT ranks between the dynamic
controllers and RouteLLM across regimes. Under regime
switches, RouteLLM yields the lowest utility J, while our
dynamic controllers remain top-performing. Together with the
baselines All Edge and All Cloud, the comparison indicates
that coupling the router with both execution agents yields a
larger utility J across GOOD/MID/BAD.
We evaluate routing under interleaved network conditions
by tracking the per-request utility (J = Q −λC) over a long
trace. Fig. 6 plots the step-level J together with its windowed
mean for a dynamic, network-aware fallback threshold and for
a fixed fallback threshold representative of a static acceptance
policy. Across regime switches, the dynamic fallback threshold
maintains a consistently higher average J: when the link
deteriorates, the controller lowers the acceptance boundary
to curb offloading cost; as the link recovers, the boundary
lifts to exploit cloud quality gains. The utility series shows
smooth piecewise transitions without overshoot or oscillations,
indicating stable adaptation under non-stationary links.
C. Quality-Cost Frontier and Network Sensitivity
Fig. 7 reports the attainable quality-cost frontiers by sweep-
ing the fallback threshold τ under three fixed network condi-
tions (GOOD, MID, BAD). The curves are smooth, monotonic
in cost, and show diminishing marginal quality gains as
cost increases, empirically supporting the decreasing-benefit
Assumption 2 used in our analysis. Consistent with Eq. (27),
at the same fallback threshold, the frontier slope scales as
1/κ(S), so degraded links (larger κ) flatten the curve and shift
it rightward/downward relative to better network states. Across
regimes, the frontier shifts rightward (and slightly downward)


--- Page 10 ---
10
Fig. 5: Comparison among All Edge, All Cloud, RouteLLM [14], FrugalGPT [11], and the dynamic controllers (FuncDyn,
PolicyNet) under GOOD/MID/BAD links.
Fig. 6: Dynamic vs. fixed fallback threshold under time-varying links.
from GOOD to BAD, indicating that worse links require more
cost to reach the same quality and deliver lower quality at the
same cost—consistent with our theory that degraded networks
raise the effective offloading cost and push decisions toward
edge execution.
Fig. 8 shows the sensitivity of the score-threshold policy
to network conditions. We sweep the fallback threshold τ
under three network regimes while fixing the trade-off weight
at λ = 10, and plot the resulting quality, cost, and utility
curves. As RTT increases and bandwidth decreases from
GOOD to MID to BAD, both curves shift, and the utility-
maximizing fallback threshold moves leftward. The empirical
optima τ ∗
GOOD ≈4.54, τ ∗
MID ≈4.28, and τ ∗
BAD ≈3.59
follow the monotone comparative-statics pattern predicted by
our analysis: deteriorating links impose higher communication
costs, which discourage cloud offloading and favor more
permissive local acceptance at the edge. These results confirm
that the proposed score-threshold policy adapts smoothly and
predictably as link quality worsens.
For completeness, Fig. 9 further decomposes the fallback
threshold scan by plotting Q(τ) and C(τ) together with
overlaid utility curves J(τ) for different values of λ. Across all
settings, the J(τ) profiles remain unimodal, and the maximiz-
ing fallback threshold shifts leftward as λ increases, reflecting
heightened sensitivity to cost.


--- Page 11 ---
11
Fig. 7: Quality-Cost frontiers across fixed network regimes.
D. Effectiveness of PPO-based Learning
We assess the effectiveness of on-device policy optimization
with the SFT anchor to preserve schema-correct tool-calling.
Fig. 10 contrasts training with and without the SFT anchor. It
can be observed that the involvement of SFT leads to mono-
tonically increased reward with lower variance and a steadily
declining offload rate, indicating reliable improvement without
structural drift. These results corroborate that interleaved SFT
updates stabilize RL by constraining updates around a schema-
preserving reference. Fig. 11 shows the quality-cost frontier
before and after PPO training. After training, the frontier
strictly dominates the pre-training curve across a range of λ
values, suggesting that the utility gains are robust and not tied
to a single trade-off weight. From a multi-agent perspective,
the router, the edge LLM policy, and the cloud model operate
in a network feedback loop. The improvements in Fig. 10 and
the shift of the quality-cost frontier in Fig. 11 are consistent
with coordinated updates driven by online feedback rather than
offline tuning alone.
To quantify the router’s scoring quality, we examine risk-
coverage curves [42] before and after RM training. Notably,
for a fallback threshold τ, the coverage is the fraction of
requests accepted at the edge, while selective risk is the frac-
tion of these accepted cases for which the cloud would have
produced a better result (i.e., should-have-offloaded errors).
As shown in Fig. 12, the post-training curve lies uniformly
below the pre-training curve, especially in the medium-to-high
coverage range, demonstrating sharper discrimination near the
decision boundary.
VI. CONCLUSION AND FUTURE WORK
We have introduced a network-aware LLM routing frame-
work that unifies schema-constrained tool planning at the
edge with a router-as-reward loop for continual improve-
ment. We have cast offloading as state-dependent thresholding
of a unified RM score, aligning routing, cost, and quality
through a single, interpretable fallback threshold adapted to
the network state. We have implemented two deployable vari-
ants—FuncDyn and a lightweight PolicyNet—both adapting
online to time-varying links with minimal overhead. We have
(b)
(a)
(c)
Fig. 8: Network sensitivity of the score-threshold policy.
(a) Composite quality Q(τ), (b) total cost C(τ), and (c) util-
ity J(τ) = Q −λC under the three network states.
established the existence and uniqueness of the optimal fall-
back threshold and derived monotone comparative statics with
respect to bandwidth and RTT. Furthermore, we have validated
the effectiveness and superiority across extensive simulations.
Subsequent research will investigate multi-edge/multi-cloud
orchestration with cooperative routers and a shared reward
model, robustness of reward signals under distribution shift,
and hardware-in-the-loop evaluations on real network traces.
APPENDIX A
INITIALIZATION OF EDGE POLICY, RM, AND FALLBACK
THRESHOLD
We initialize the edge LLM policy πθ, the reward/router
model gψ that outputs the score si,k, and a fixed fallback
threshold τ0 for runtime use.


--- Page 12 ---
12
(b)
(a)
Fig. 9: Fallback threshold scan: Q(τ), C(τ), and J(τ) for
λ ∈{8, 10, 12}; vertical lines indicate τ ∗for each λ.
Fig. 10: PPO with vs. without SFT anchoring. Router-reward
trends and offload rate over training steps.
Particularly, we fit πθ on traces (xi, Hi,k, ui,k) with super-
vised cross-entropy:
LSFT(θ) = E[−log πθ(ui,k | xi, Hi,k)] .
(37)
Here xi is the task input, Hi,k is the historical step, and ui,k =
(tool, args, thought) is the teacher action that satisfies
the schema.
The
reward
model
outputs
a
scalar
score
si,k
=
gψ(xi, Hi,k, ui,k) ∈R for routing and as the learning reward.
We initialize gψ with a pairwise ranking loss:
LRM(ψ) = E
h
log
 1 + exp(−(s+
i,k −s−
i,k))
i
.
(38)
Here s+
i,k and s−
i,k are the RM scores of a preferred and a
non-preferred answer under the same context (xi, Hi,k). The
preferred answers come from the cloud LLM outputs, while
the non-preferred ones are generated by the edge LLM after
SFT initialization. This construction lets the RM learn the
Fig. 11: Utility comparison before and after RL.
Fig. 12: RM Risk-Coverage curves before vs. after training.
Post-training curves reduce risk at fixed coverage, indicating
more accurate router scoring near the decision boundary.
quality gap between edge and cloud responses under identical
conditions.
We select an initial fallback threshold τ0 on an offline
dataset Dinit. Each record in Dinit provides the score si,k and
the empirical utilities we would obtain by choosing edge or
cloud at that step:
Jedge
i,k
≜Qedge
i,k
−λCedge
i,k ,
Jcloud
i,k
≜Qcloud
i,k
−λCcloud
i,k
.
(39)
Here Q·
i,k
∈[0, 1] and C·
i,k
≥0 denote the measured
quality and the step-level cost (e.g., latency or monetary cost),
respectively. λ > 0 is the trade-off weight. We then choose
τ0 by empirical utility maximization under the hard fallback
threshold rule:
τ0
(40)
∈arg max
τ∈R
X
(i,k)∈Dinit
h
Jedge
i,k
I[si,k ≥τ] + Jcloud
i,k
I[si,k < τ]
i
,
which picks the fixed fallback threshold that yields the highest
average utility on Dinit.
Our training datasets are batch-generated with GPT-4o


--- Page 13 ---
13
Listing 1: SFT sample with structured tool call.
1 {
2
"query": "Optimize a federated
recommendation system for news articles
across mobile devices.",
3
"completed_steps": [
4
"initialize_nodes"
5
],
6
"thought": "After initializing nodes, we
need to simulate the network topology.
Since this involves mobile devices, we
should account for variable bandwidth
and latency. This will help model
realistic communication conditions
during federated training.",
7
"output": {
8
"name": "simulate_topology",
9
"args": {
10
"node_count": 5,
11
"bandwidth_limits": "10-100 Mbps",
12
"latencies": "50-200ms"
13
}
14
},
15
"available_tools": [
16
"initialize_nodes(node_count, model_class,
dataset_name)",
17
"simulate_topology(node_count,
bandwidth_limits, latencies)",
18
"train_local_models(nodes, dataset, epochs
, batch_size, learning_rate)",
19
"train_local_models_fedprox(nodes, dataset
, global_model, mu, epochs, batch_size
, learning_rate)",
20
"train_local_models_secure_agg(nodes,
dataset, epochs, batch_size,
learning_rate)",
21
"aggregate_global_model(local_models,
weights, nodes, dataset, topology)",
22
"aggregate_global_model_krum(local_models,
num_selected, nodes, dataset,
topology)",
23
"aggregate_global_model_median(
local_models, nodes, dataset, topology
)",
24
"aggregate_global_model_secure_agg(
local_models, nodes, dataset, topology
)",
25
"finish()"
26
]
27 }
[43], yielding 8, 000 SFT instances for schema-faithful ini-
tialization and 2, 000 preference pairs for reward modeling. In
addition, we generate 8, 000 tasks for inference experiments
and simulated continual improvement. To ensure diversity,
each batch draws a prompt uniformly at random from a
maintained pool containing multiple prompt templates. Every
template rigidly specifies the output schema (tool name, ar-
guments, and brief thought trace) to match our tool-calling
interface. After generation, we run a two-stage filtering pass:
(i) schema validation (JSON/tool-slot conformance and ar-
gument type checks), and (ii) quality screening (automatic
scoring to eliminate near-duplicates and low-utility samples).
The resulting corpora include: (a) SFT data, used to initialize
Listing 2: Preference pair: chosen vs. rejected sample.
1 {
2
"input": {
3
"query": "Stanford medical researchers
require federated training of a 3D U-
Net for brain tumor segmentation
across 7 hospitals, with differential
privacy epsilon=0.5 guarantees.",
4
"completed_steps": []
5
},
6
"chosen": {
7
"action": {
8
"name": "initialize_nodes",
9
"args": {
10
"node_count": 8,
11
"model_class": "3D U-Net",
12
"dataset_name": "
brain_tumor_segmentation"
13
}
14
},
15
"thought": "First initialize the federated
learning nodes with the 3D U-Net
model and brain tumor segmentation
dataset. Next steps will involve
setting up the network topology and
beginning the federated training
process with differential privacy
guarantees. Need to ensure the model
class and dataset name are correctly
specified for the medical imaging task
. Initializing 8 nodes (7 hospitals +
central server) with 3D U-Net models
for brain tumor segmentation."
16
},
17
"rejected": {
18
"action": {
19
"name": "initialize_nodes",
20
"args": {
21
"node_count": 7,
22
"model_class": "MedicalImagingModel",
23
"dataset_name": "TumorScans"
24
}
25
},
26
"thought": "Starting fresh with this
sensitive medical task. We’ll
initialize 7 nodes (one per hospital)
with a specialized medical imaging
model. Using a small batch size (16)
to handle potentially limited patient
data at each site."
27
}
28 }
schema-faithful tool use for the edge agent; and (b) preference
pairs, where a chosen action (typically the cloud-produced
response) is contrasted with a rejected action (typically an
edge-produced alternative) under the same context, supporting
reward modeling and preference optimization.
APPENDIX B
CACHING SETUP AND ON-POLICY LOGGING
With DeepSeek-V3.2-Exp as the fallback cloud LLM,
the cached records serve multiple purposes in the learning
pipeline:


--- Page 14 ---
14
• Cloud-offloaded traces provide training data for improv-
ing the edge LLM through RL.
• Near-threshold cloud cases, together with the small set
of above-threshold samples intentionally uploaded, con-
stitute a focused dataset for refining the reward model
around the decision boundary.
• Execution outputs contribute to prompt construction:
edge-side prompts append only the executed tool name
to completed_steps, whereas cloud-side prompts
additionally include a concise natural-language summary.
Therefore, we denote the cached tuple by
Ti,k ≜(xi, Hi,k, ui,k, u⋆
i,k, si,k).
Only steps offloaded to the cloud ( di,k = CLOUD ) append
Ti,k to the local cache BRM and BRL for later updates; on-
device steps retain only Hi,k.
To enrich the reward model’s learning signal, we randomly
sample a small fraction of steps whose scores are slightly
above the fallback threshold, i.e., si,k ≳τ(Si,k), and also
upload them to the cloud. These near-threshold samples cap-
ture marginal decision cases that are most informative for
distinguishing good and bad routing choices. After executing
the selected tool, we record a structured output
oi,k = (ti,k, σi,k),
(41)
where ti,k is the tool identifier and σi,k is a length-bounded
summary of the tool output (sufficient for the next-step LLM
to interpret the result while keeping context compact).
The incorporation of oi,k into the local state depends
on which model the next prompt is constructed for: edge-
side prompts use only the tool identifier, whereas cloud-side
prompts include both the identifier and its summary. Formally,
Hi,k+1 =
(
Hi,k ⊕ti,k,
to edge LLM,
Hi,k ⊕(ti,k, σi,k),
to cloud LLM.
(42)
Here ⊕denotes concatenation into the reasoning trace.
REFERENCES
[1] Y. Chen, R. Li, et al., “NetGPT: An AI-native network architecture for
provisioning beyond personalized generative services,” IEEE Network,
vol. 38, no. 6, pp. 404–413, 2024.
[2] OpenAI,
“Function
calling
and
other
api
updates,”
Online
blog post, Jun 2023. [Online]. Available: https://openai.com/index/
function-calling-and-other-api-updates/
[3] S. Yao, J. Zhao, et al., “ReAct: Synergizing reasoning and acting in
language models,” in Proceedings of the 11th International Conference
on Learning Representations (ICLR 2023), Kigali, Rwanda, May 2023.
[4] W. Shi, J. Cao, et al., “Edge computing: Vision and challenges,” IEEE
Internet of Things Journal, vol. 3, no. 5, p. 637–646, 2016.
[5] E. Li, Z. Zhou, et al., “Edge intelligence: On-demand deep learn-
ing
model
co-inference
with
device-edge
synergy,”
in
Proceed-
ings
of
the
2018
Workshop
on
Mobile
Edge
Communications
(MECOMM@SIGCOMM), August 2018, pp. 31–36.
[6] P. Mach and Z. Becvar, “Mobile edge computing: A survey on archi-
tecture and computation offloading,” IEEE Communications Surveys &
Tutorials, vol. 19, no. 3, pp. 1628–1656, 2017.
[7] L. Ouyang, J. Wu, et al., “Training language models to follow instruc-
tions with human feedback,” in Proceedings of the 36th International
Conference on Neural Information Processing Systems (NeurIPS 2022),
Red Hook, NY, USA, December 2022, pp. 2011–2021.
[8] O. Besbes, Y. Gur, et al., “Stochastic multi-armed-bandit problem with
non-stationary rewards,” in Proceedings of the 28th Conference on
Neural Information Processing Systems (NIPS 2014), Cambridge, MA,
USA, 2014, p. 199–207.
[9] T. Schick, J. Dwivedi-Yu, et al., “Toolformer: Language models can
teach themselves to use tools,” arXiv preprint arXiv:2302.04761, 2023.
[10] B. Agarwal, I. Joshi, et al., “Think inside the json: Reinforce-
ment strategy for strict LLM schema adherence,” arXiv preprint
arXiv:2502.14905, 2025.
[11] L. Chen and M. Zaharia, “FrugalGPT: How to use large language models
while reducing cost and improving performance,” in Transactions on
Machine Learning Research (TMLR) Workshop Track, Online, December
2024.
[12] K. Lu, H. Yuan, et al., “Routing to the expert: Efficient reward-guided
ensemble of large language models,” in Proceedings of the Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-HLT 2024), Mex-
ico City, Mexico, June 2024, pp. 1964–1974.
[13] H. Zheng, H. Xu, et al., “Disrouter: Distributed self-routing for LLM
selections,” arXiv preprint arXiv:2510.19208, 2025.
[14] I. Ong, A. Almahairi, et al., “RouteLLM: Learning to route llms from
preference data,” in Proceedings of the 13th International Conference
on Learning Representations (ICLR 2025), Singapore EXPO, Singapore,
April 2025.
[15] D. Ding, A. Mallick, et al., “Hybrid-LLM: Cost-efficient and quality-
aware query routing,” in Proceedings of the 12th International Confer-
ence on Learning Representations (ICLR 2024), Vienna, Austria, May
2024.
[16] S. Chen, W. Jiang, et al., “Routerdc: Query-based router by dual
contrastive learning for assembling large language models,” in Proceed-
ings of the 37th Annual Conference on Neural Information Processing
Systems (NeurIPS 2024), vol. 37, Vancouver, Canada, 2024, pp. 66 305–
66 328.
[17] T. Feng, Y. Shen, et al., “Graphrouter: A graph-based router for LLM
selections,” in Proceedings of the 13th International Conference on
Learning Representations(ICLR 2025), Singapore EXPO, Singapore,
April 2025.
[18] Y. Yue, G. Zhang, et al., “Masrouter: Learning to route llms for multi-
agent systems,” in Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers),
Vienna, Austria, July 2025, pp. 15 549–15 572.
[19] B. Qi, P. Li, et al., “Online direct preference optimization with fast–slow
memory,” arXiv preprint arXiv:2406.05534, 2024.
[20] H. Dong, W. Xiong, et al., “RLHF workflow: From reward modeling to
online RLHF,” arXiv preprint arXiv:2405.07863, 2024.
[21] X. Wang, Y. Liu, et al., “MixLLM: Dynamic routing in mixed large
language models,” in Proceedings of the 2025 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT 2025), Albuquerque, New
Mexico, 2025, pp. 10 912–10 922.
[22] Q. H. Nguyen, T. Dao, et al., “MetaLLM: A high-performant and
cost-efficient dynamic framework for wrapping LLMs,” arXiv preprint
arXiv:2407.10834, 2024.
[23] J. Schulman, F. Wolski, et al., “Proximal policy optimization algo-
rithms,” arXiv preprint arXiv:1707.06347, 2017.
[24] Q. J. Hu, J. Bieker, et al., “Routerbench: A benchmark for multi-LLM
routing system,” arXiv preprint arXiv:2403.12031, 2024.
[25] J. Dekoninck, M. Baader, et al., “A unified approach to routing and
cascading for LLMs,” in Proceedings of the First Workshop on Scalable
Optimization for Efficient and Adaptive Foundation Models (SCOPE),
co-located with ICLR 2025, Singapore Expo, Singapore, April 2025.
[26] M. Yue, J. Zhao, et al., “Large language model cascades with mixture of
thought representations for cost-efficient reasoning,” in Proceedings of
the 12th International Conference on Learning Representations (ICLR
2024), Vienna, Austria, May 2024.
[27] Y. Shen, Y. Liu, et al., “Sater: A self-aware and token-efficient approach
to routing and cascading,” in Proceedings of the 2025 Conference on
Empirical Methods in Natural Language Processing (EMNLP 2025),
Suzhou, China, November 2025, pp. 10 526–10 540.
[28] D. Ding, A. Mallick, et al., “Best-route: Adaptive LLM routing with
test-time optimal compute,” in Proceedings of the 42nd International
Conference on Machine Learning (ICML 2025) Poster, 2025.
[29] Y. Li, “Rethinking predictive modeling for LLM routing: When simple
knn beats complex learned routers,” arXiv preprint arXiv:2505.12601,
2025.
[30] D. Calandriello, Z. D. Guo, et al., “Human alignment of large language
models through online preference optimisation,” in Proceedings of the


--- Page 15 ---
15
41st International Conference on Machine Learning (ICML 2024),
Vienna, Austria, 2024.
[31] W. Yuan, R. Y. Pang, et al., “Self-rewarding language models,” in
Proceedings of the 41st International Conference on Machine Learning
(ICML 2024), Vienna, Austria, July 2024, pp. 57 905–57 923.
[32] Z. Wang, W. He, et al., “Cream: Consistency regularized self-rewarding
language models,” in Proceedings of the 13th International Conference
on Learning Representations (ICLR 2025), Singapore EXPO, Singapore,
April 2025.
[33] Y. Wu, Z. Sun, et al., “Self-play preference optimization for language
model alignment,” in Proceedings of the 13th International Conference
on Learning Representations (ICLR 2025), Singapore EXPO, Singapore,
April 2025.
[34] S. S. Ramesh, Y. Hu, et al., “Group robust preference optimization in
reward-free rlhf,” in Proceedings of the 38th Conference on Neural In-
formation Processing Systems (NeurIPS 2024), Vancouver, BC, Canada,
December 2024, pp. 37 100–37 137.
[35] Z. Shao, P. Wang, et al., “Deepseekmath: Pushing the limits of
mathematical reasoning in open language models,” arXiv preprint
arXiv:2402.03300, 2024.
[36] C. Ye, W. Xiong, et al., “Online iterative reinforcement learning from
human feedback with general preference model,” in Proceedings of the
38th Conference on Neural Information Processing Systems (NeurIPS
2024), Vancouver, BC, Canada, December 2024.
[37] Network performance objectives for IP-based services, International
Telecommunication Union, Telecommunication Standardization Sector
(ITU-T) Std. Y.1541, Dec. 2011, recommendation ITU-T Y.1541
(12/2011). [Online]. Available: https://www.itu.int/rec/dologin pub.asp?
id=T-REC-Y.1541-201112-I!!PDF-E&lang=e&type=items
[38] B. Constantine, G. Forget, et al., “Framework for TCP Throughput
Testing,”
RFC
6349,
August
2011.
[Online].
Available:
https:
//www.rfc-editor.org/info/rfc6349
[39] M. Geist, B. Scherrer, et al., “A theory of regularized Markov decision
processes,” in Proceedings of the 36th International Conference on
Machine Learning (ICML 2019), Long Beach, USA, June 2019, pp.
2160–2169.
[40] A. Banerjee, S. Merugu, et al., “Clustering with bregman divergences,”
Journal of Machine Learning Research, vol. 6, no. 58, pp. 1705–1749,
2005.
[41] S. Ghandour-Haidar, L. Ros, et al., “On the use of first-order autoregres-
sive modeling for rayleigh flat fading channel estimation with kalman
filter,” Signal Processing, vol. 92, no. 2, pp. 601–606, 2012.
[42] R. El-Yaniv and Y. Wiener, “On the foundations of noise-free selective
classification,” Journal of Machine Learning Research, vol. 11, pp.
1605–1641, 2010.
[43] OpenAI, “GPT-4o technical report,” https://openai.com/research/gpt-4o,
2024, accessed: 2025-11-20.
