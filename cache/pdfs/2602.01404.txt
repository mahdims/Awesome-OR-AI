--- Page 1 ---
BOA Constrictor: Squeezing Performance out of GPUs in the
Cloud via Budget-Optimal Allocation
ZHOUZI LI, Carnegie Mellon University, United States
CINDY ZHU, Carnegie Mellon University, United States
ARPAN MUKHOPADHYAY, University of Warwick, England
MOR HARCHOL-BALTER, Carnegie Mellon University, United States
BENJAMIN BERG, UNC Chapel Hill, United States
The past decade has seen a dramatic increase in demand for GPUs to train Machine Learning (ML) models.
Because it is prohibitively expensive for most organizations to build and maintain a large GPU cluster,
organizations instead choose to rent GPUs from cloud providers. The customer is responsible for devising a
policy for (i) deciding how many GPUs to rent at every moment in time to process a stream of ML training jobs
and (ii) allocating the rented GPUs among the currently active jobs in the system. Because ML training jobs
can be parallelized across different numbers of GPUs, the customer generally has many options for how many
GPUs to use for each job. Allocating more GPUs to a single training job will cause the job to complete more
quickly. However, the customer pays for each GPU-hour they use, and a training job receives a diminishing
marginal benefit from running on additional GPUs. Hence, allocating too many GPUs to a single training job
can dramatically increase the overall cost that the customer pays to the cloud provider. This gives rise to a
cost-performance tradeoff that customers must balance when running training jobs in the cloud.
To balance the cost-performance tradeoff, we develop BOA Constrictor, a new scheduler for ML training
jobs which uses a Budget-Optimal Allocation (BOA) policy to squeeze the highest level of performance out of
a cloud-deployed GPU cluster given a fixed budget constraint. While prior approaches have focused on either
fixed-sized clusters and heuristic approaches for balancing cost and performance, we explicitly formulate the
problem as a budget-constrained scheduling problem. In our stochastic model, the user has a time-average
monetary budget that they are willing to spend on training. This translates to a constraint on the time-average
number of GPUs that can be rented. We derive the BOA policy, which minimizes the average job completion
time (JCT) of a stream of arriving jobs subject to the userâ€™s budget. Our BOA policy can be computed efficiently
for any budget level and therefore provides users with the optimal tradeoff between cost and performance.
For a given budget level, we demonstrate that BOA Constrictor can reduce average JCT by 1.6Ã— in small-scale
implementation experiments and by 2Ã— in detailed, large-scale simulations compared to state-of-the-art
heuristic based schedulers. Furthermore, in both simulation and real-world experiments, BOA Constrictor
reduces the budget needed to achieve a given average JCT by up to 2Ã—.
1
Introduction
The explosion in Machine Learning (ML) over the past ten years has led to a dramatic increase
in demand for GPUs to train ML models [16]. Because it is prohibitively expensive for many
organizations to build and maintain a large GPU cluster, hyperscale cloud providers (Microsoft
Azure, Amazon AWS, Google Cloud) have seen explosive growth in demand for renting cloud-based
GPUs. For example, Amazonâ€™s profit from renting GPUs and related infrastructure is expected
to grow to 20 billion dollars by 2026, roughly 20% of the profit currently generated by AWS [2].
This trend makes the question of how cloud customers can efficiently train ML models using
cloud-based infrastructure one of the most important problems facing the computer systems
community [21, 22, 26, 29].
Authorsâ€™ Contact Information: Zhouzi Li, zhouzil@andrew.cmu.edu, Carnegie Mellon University, Computer Science Depart-
ment, United States; Cindy Zhu, cindyz@andrew.cmu.edu, Carnegie Mellon University, Computer Science Department,
United States; Arpan Mukhopadhyay, arpan.mukhopadhyay@warwick.ac.uk, University of Warwick, Computer Science De-
partment, England; Mor Harchol-Balter, harchol@cs.cmu.edu, Carnegie Mellon University, Computer Science Department,
United States; Benjamin Berg, ben@cs.unc.edu, UNC Chapel Hill, Computer Science Department, United States.
arXiv:2602.01404v1  [cs.DC]  1 Feb 2026


--- Page 2 ---
2
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
(a) Average JCT vs. operating budget.
(b) P95 JCT vs. operating budget.
Fig. 1. Our solution, BOA Constrictor balances the cost-performance tradeoff for training in the cloud. While
existing policies such as Pollux [26] do not explicitly balance the cost-performance tradeoff, BOA Constrictor
provides up to a 2Ã— improvement in average JCT and up to a 3Ã— improvement in P95 JCT for a given budget
by deriving a budget-optimal allocation policy.
In this cloud computing paradigm, the customer is responsible for devising a rental policy that
decides how many GPUs to rent at every moment in time [27]. The customer may be an individual
developer or researcher, or an organization such as a company or research lab that rents a pool
of GPUs to serve a stream of ML training jobs. The customer submits a stream of training jobs to
their cloud-based cluster, and is charged on a pay-for-what-you-use basis according to a set fee
per GPU-Hour of usage. The customer may increase or decrease their rental demands dynamically
over time to both suit the performance needs of their workloads and control costs. Specifically,
renting additional GPUs may decrease job completion times (JCT, also known as response time).
The cloud provider, in turn, aims to quickly allocate GPUs to customers. As a result, cloud-based
GPU instances can generally be rented or returned on a timescale of 1 - 2 minutes, making it feasible
for customers to scale the size of their cloud deployments frequently in response to changes in their
workloads. This raises the question of what rental policy a customer should employ to balance a
tradeoff between achieving good performance and limiting the costs paid to the cloud provider.
While the cost-performance tradeoff has been considered extensively in the context of ML
inference workloads, where the problem closely resembles the traditional autoscaling problem,
there has not been similar attention paid to devising rental policies for serving ML training
workloads. Crucially, training jobs are long-running (on the order of hours, or longer, depending
on the model) and can be configured to run in parallel across different numbers of GPUs. Hence, a
customer must decide not only how many GPUs to rent in total at each moment in time, but also
how to divide those GPUs across the jobs currently in the systems. These decisions must be made
online as training jobs arrive and complete, incorporating potential overheads of preemption, and
of rescaling the cluster size while jobs run. The problem is further complicated by the fact that the
performance characteristics of jobs change over the course of training.
Training jobs receive sublinear speedup. Modern ML training jobs can be parallelized across multiple
GPUs to complete training more quickly. Running on additional GPUs provides a job with additional
compute and memory that can be exploited by a variety of parallel schemes (e.g., model parallelism,
tensor parallelism, pipeline parallelism, see Section 2.1).
However, the benefits of parallelism are generally limited by a combination of sequential bottle-
necks, synchronization overheads, and statistical inefficiency (see Section 2.1). As a result, training
jobs generally receive a diminishing marginal benefit from running on additional GPUs. This effect
is commonly measured via a jobâ€™s speedup function, ğ‘ (ğ‘˜), which describes how much faster a job


--- Page 3 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
3
can train a model to a given level accuracy on ğ‘˜GPUs as opposed to some base configuration,
in our case one GPU. Due to the aforementioned limits to parallelism, a training jobâ€™s speedup
function is generally sublinear and concave.
(a) Speedup functions
(b) Training cost
Fig. 2. Performance of a Cifar10 training job on different numbers of GPUs. Changes to job hyperparameters
over the course of training cause a jobâ€™s speedup function to shift. Because the speedup functions are concave
and sublinear, training costs increase when the job is parallelized on a larger number of GPUs.
Sublinear speedup functions have a particular impact on the GPU rental problem where cost is
a first-order concern. If a job received a linear speedup from parallelism, it could run on ğ‘˜GPUs
and complete in 1
ğ‘˜th the time, meaning that the total GPU usage would remain constant. However,
because jobs receive a sublinear speedup, a job run on ğ‘˜GPUs will complete in
1
ğ‘ (ğ‘˜) as much
time, leading to a factor
ğ‘˜
ğ‘ (ğ‘˜) increase in the number of GPU-hours required to complete the job
(Figure 2b). Hence, customers who aim to run training jobs on a flexible pool of cloud resources
must balance a tradeoff between completing jobs more quickly and paying a higher cost to the
cloud providers.
Training jobs and training workloads change over time. Balancing the cost-performance tradeoff for
cloud-based ML training is particularly challenging because the problem is dynamic along several
dimensions. First, a jobâ€™s speedup function may change over the course of training (see Figure 2a) as
the optimal choice of job hyperparameters changes (see Section 2.1) [26, 29, 31]. It may be possible
to improve cost or performance by adapting allocation decisions as job speedup functions change.
Second, a customer does not deal with a single job, but rather a stream of training jobs that are
submitted over time. Each job may have a different speedup function that changes as the job runs.
As a result, the composition of the training workload may shift over time. Examination of publicly
available traces shows that job arrival times are highly-variable, showing significant bursts and
changes in the relative frequency of different model types and sizes (see Section 2.3). Notably, the
common modeling assumption of arrivals following a time-homogeneous Poisson process is a poor
reflection of reality.
In summary, a good GPU rental policy must respond to changes both in the speedup functions
of individual jobs, but also bursts and compositional changes in the overall workload.
Balancing cost and performance. Because GPUs are being rented from a flexible pool of cloud
resources, the customer has broad latitude to adjust the cluster size to adapt to these shifts. For
example, it might be beneficial to increase the number of rented GPUs when the jobs in the system
enter phases with higher speedup functions, or when there is a burst of highly-parallelizable
training jobs, since these jobs can efficiently utilize a high number of GPUs in parallel. Likewise,


--- Page 4 ---
4
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
given a small number of jobs with flatter speedup functions, it may be beneficial to decrease the
total number of rented GPUs in order to control costs.
To balance the cost-performance tradeoff, the customer must make two principal decisions:
(1) How many GPUs to rent from the cloud provider at every moment in time?
(2) How to divide the rented GPUs among the training jobs currently in the system?
The customer must answer these questions in real time as jobs arrive and depart from the system.
As we will detail in Section 2.4, existing schedulers address at most one of the above questions.
Furthermore, prior approaches have the added drawback that they generally make use of scheduling
heuristics to set job allocations rather than using mathematical analysis to provide performance
guarantees. As a result, there is no analysis of the average JCT in existing systems, and no mechanism
for weighing how changes in the cluster size will affect performance.
Our Solution: BOA Constrictor. This paper presents BOA Constrictor, a scheduling framework for
cost-efficient cloud-based training of ML models. To control costs, the customer provides BOA
Constrictor with an operating budget, the long-run average number of GPU-hours they wish to
expend per hour. Given a stream of training jobs, BOA Constrictor determines (i) the overall
cluster size and (ii) how to allocate the rented GPUs to each job in the system. Based on the
customerâ€™s budget and the overall workload composition, BOA Constrictor minimizes the average
job completion time (average JCT) across jobs. For example, the customer might aim to spend $10,000
per month on training, which is equivalent to maintaining an average cluster size of 40 GPUs. Over
the course of a day, BOA Constrictor will adjust the cluster size and job allocations. Note that both
of these rescaling actions come with overheads, so BOA Constrictor must limit the frequency of
these changes.
The core of BOA Constrictor is our Budget-Optimal Allocation policy (BOA), which minimizes
average JCT subject to an arbitrary budget constraint under a very general model of the system.
The mathematical analysis behind the BOA policy also allows the customer to use BOA Constrictor
to derive the entire cost/performance Pareto frontier (see Figure 1), providing a useful tool for the
customer to trade off cost and performance.
Contributions
The contributions of this paper are as follows: We begin by proving new theoretical results for
optimally solving the GPU rental problem that advance the current theoretical understanding of
scheduling for parallelizable jobs. Specifically,
â€¢ We consider the GPU rental problem from the perspective of a cloud service customer who has a
budget on the long-run average number of GPU-hours consumed per hour. The customer wants
the best possible performance for their money, and therefore aims to minimize the average JCT
across a stream of jobs while adhering to the budget. In our model (Section 3), job sizes and
interarrival times can follow very general distributions. Job speedup functions are permitted to
change from job to job and change over time for a single job. We solve the GPU rental problem
for this extremely general setting in Section 4. We refer to our optimal policy as the Budget
Optimal Allocation (BOA) policy. While this paper focuses on the homogeneous GPU setting, our
theoretical results can be easily generalized to the heterogeneous GPU setting, see Appendix E.
After proving these theoretical results, we implement BOA Constrictor, a cluster scheduler that
leverages the BOA policy. We evaluate our BOA Constrictor implementation using real-world
experiments on ML training jobs, as well as extensive, detailed simulations.


--- Page 5 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
5
â€¢ We implement BOA Constrictor on top of the AdaptDL framework [1] (Section 5). Using both
our implementation and a detailed ML training simulator, we evaluate BOA Constrictor using a
production trace of ML training jobs (Section 6).
â€¢ To aid in our comparison, we implement a variant of Pollux which uses goodput-based autoscaling
as originally proposed in [26]. No working implementation of this idea existed previously, and
implementing Pollux with autoscaling was a significant development effort (see Section 6). We
compare with Pollux [26] because it not only optimizes for JCT, but also dominates several other
proposed system that attempt to reduce job JCTs (see Section 6.1).
â€¢ BOA Constrictor implements the BOA policy in a way that decouples the optimization com-
putation from the policy execution (see Section 5.2). This allows us to run the optimization
computation primarily offline, with only very negligible overhead on the critical path of job
scheduling (see Section 5.4).
â€¢ For a given budget, BOA Constrictor improves average JCT by up to a factor of âˆ¼1.75Ã— in
implementation (Figure 1). Additionally, for a given average JCT, we can reduce the budget needed
to achieve this level of performance by up to a factor of âˆ¼2.2Ã—. Crucially, BOA Constrictor makes
very different scheduling decisions than state-of-the-art policies by relying on our theoretical
models instead of intuitive scheduling heuristics (see Section 6.4).
2
Background
This section describes the current state-of-the-art in training ML models in the cloud. We describe
how modern ML training jobs can be dynamically configured to leverage multiple GPUs (Section
2.1), why training workloads require dynamic scheduling and autoscaling policies (Sections 2.2 and
2.3), and how state-of-the-art approaches for scheduling training jobs fall short both in theory and
in practice (Section 2.4), revealing the need for a theoretically-grounded policy that optimizes the
cost-performance tradeoff as a primary concern.
2.1
Distributed Training in the cloud
To accelerate the ML training process, training jobs are typically distributed across multiple GPUs
in parallel. However, making efficient use of a set of GPUs to accelerate training requires careful
configuration of the training job to avoid excessive overheads.
Exploiting parallelism in distributed training. A single job can exploit parallelism in two central ways.
First, data parallelism involves replicating a full model across many GPUs and dividing batches
of training data across these replicas to process a higher number of samples in parallel per unit
time. Second, model parallelism involves spreading parts of a single model instance across multiple
GPUs, either by partitioning different layers of the model onto separate GPUs (pipeline parallelism)
or splitting a single tensor of model weights across multiple GPUs (tensor parallelism).
In this paper, we consider jobs that are configured to use data parallelism, although this assump-
tion is not core to our results. This choice is made to ease comparison with the relevant prior work.
As described in [26], data parallel training jobs experience two main overheads from parallelization.
First, these jobs periodically synchronize with a parameter server in order to compute gradients;
this prevents the jobâ€™s throughput (i.e., the number of training examples processed per second)
from scaling linearly with the number of GPUs occupied by the job. Second, computing gradients
on higher numbers of GPUs in parallel requires larger batches of data to be processed between
gradient updates, reducing the training efficiency, defined as the amount of accuracy the model
gains per training sample [26]. Combined, these effects give rise to the characteristic sublinear
speedup function measured for the data-parallel jobs in Figure 2a. Similar overheads produce
similarly sublinear speedup functions when different modes of parallelism are used [31].


--- Page 6 ---
6
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
There are also various proposals for sharing GPUs between multiple training jobs via software or
hardware based mechanisms [3]. The BOA policy we derive can be easily be adapted to handle phys-
ically partitioned GPUs (e.g., NVIDIA Multi-instance GPUs), but evaluating our BOA Constrictor
implementation under these conditions is outside the scope of this paper.
Hyperparameter selection. Training jobs expose a vast array of hyperparameters that can be tuned
to control the performance characteristics of the training process. These hyperparameters range
from specific training algorithm parameters such as learning rate and batch size [26] to higher-level
decisions such as the mode of parallelism [31] or the type of model being trained. The choice
of hyperparameters can significantly affect the speed of training [18], but finding the optimal
configuration is known to be difficult. Nonetheless, techniques such as Bayesian optimization and
multi-armed bandit theory have been used to explore the hyperparameter space and efficiently
find configurations that lead to shorter training times.
Recent works have noted that the choice of a degree of parallelism affects the optimal choices
of other hyperparameters (e.g., optimal batch size depends of the degree of parallelism [26]).
Several papers have suggested intertwining the hyperparameter search process with the scheduling
algorithm that chooses the degree of parallelism for each job [8, 21, 26, 29, 31]. It is not clear, in
general, how much of the improvement reported by these works comes from hyperparameter
optimization versus improved scheduling. We observe that the existing systems tend to leverage
highly-sophisticated hyperparameter search techniques in combination with overly simplistic
scheduling heuristics. This suggests that there is significant room for improvement solely by
optimizing the scheduling aspect of these systems.
We therefore focus on the scheduling aspects of the GPU rental problem rather than hyperpa-
rameter search. In our theoretical modeling and implementation results, we assume that for every
job, a hyperparameter search process has discovered a good configuration to use with each possible
number of GPUs. In other words, the speedup functions considered in our model represents the
speedup a job receives given a set of hyperparameters that have been optimized for that degree of
parallelism. We show in Section 6 that, even when we control for the effects of hyperparameter
optimization, we achieve an âˆ¼2Ã— improvement in cost or performance compared to existing sys-
tems. This improvement comes solely from improving the scheduling policy that sets the degree of
parallelism for each job and the overall cluster size. We also show via simulation in Section 6 that
BOA Constrictor maintains its advantage when hyperparameters such as batch size and learning
rate are adaptively tuned in the background by a separate hyperparameter selection process.
2.2
Metrics
Once a jobâ€™s hyperparameters have been selected and the job has been allocated a set of GPUs to
run on, we measure the jobâ€™s performance along three dimensions. The first is accuracy, which
we measure as the training loss with respect to a customer-defined loss function. We assume that
each job is submitted with some desired level of accuracy that the customer wishes to attain. We
refer to a jobâ€™s size as time required to attain a desired level of accuracy using a single GPU. We
measure the job completion time (JCT), which is the time from when the job arrives to the system
until it attains the desired level of accuracy and departs the system. Finally, we can measure a jobâ€™s
resource consumption, the number of GPU-hours expended during training.
2.3
Workloads
To understand why modern training workloads can benefit so greatly from improved scheduling
on flexibly-sized clusters, one must consider the composition of modern training workloads. We
observe that these workloads exhibit notable variability along several dimensions. This not only


--- Page 7 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
7
suggests that improved scheduling will greatly benefit average JCTs, but underscores the importance
of avoiding overly restrictive modeling assumptions when trying to derive optimal GPU allocation
policies. We note three main sources of variability based on the traces reported in [26, 29].
(1) Job sizes are highly-variable. Given some fixed number of GPUs, the time required
to complete a training job can vary by more than an order of magnitude depending on
the type of model being trained and the level of accuracy the customer requires. Notably,
assuming exponential job sizes is an unrealistic modeling assumption in this case. Improved
scheduling policies are known to reduce queueing times when job sizes are highly-variable
[14].
(2) Job arrivals are bursty. Jobs experience several bursts of arrivals over time. Hence, model-
ing arrivals via a stationary Poisson process is an unrealistic modeling assumption. Lever-
aging cluster autoscaling can reduce the effects of variability in the arrival process [10].
(3) Job speedup functions change over time. The speedup a job receives from parallelization
depends on both the jobâ€™s throughput and efficiency. As noted in [26], job efficiency tends
to be lower at the beginning of training. As a result, job speedup functions tend to shift
upwards over the course of training. Hence, assuming that jobs follow a single speedup
function for their entire lifetime is an unrealistic modeling assumption.
For a full description of the workloads used in our evaluation, see Section 6.1. These observations
about training workloads suggest that systems could benefit from more complex, theoretically
grounded allocation policies. However, we also find that training workloads violate the exponen-
tiality and independence assumptions commonly made by the performance modeling community.
2.4
Prior Work: Current Schedulers Fall Short
Prior theoretical work has separately considered the problem of scheduling parallelizable jobs
[4â€“7] and autoscaling [9, 10, 25], but we are not aware of any work which considers both problems
simultaneously to balance the cost-performance tradeoff. By developing a stochastic model of this
problem and solving for the BOA policy in Sections 3 and 4, we can show how real-world systems
can benefit by optimally balancing the cost-performance tradeoff.
These existing systems fall broadly into one of three categories.
Approach 1: Reservation-based systems. Reservation-based schedulers like Ray [22] and Tiresias
[13] and others [15, 30] ask the customer to specify the GPU requirements of each of their jobs. For
these systems, the goal is to provide the resources demanded for each job as quickly as possible.
To this end, Ray uses very simple heuristics related to data locality and load balancing, but does
not explicitly aim to minimize the average JCT across jobs. Tiresias recommends using the Gittins
index as a scheduling heuristic, ignoring the fact the Gittins index is suboptimal when scheduling
multiserver jobs unless the system load is extremely heavy [28]. Ray allows autoscaling, while
Tiresias does not, but all reservation-based systems inherently defer the cost-performance tradeoff
to the customer and provide no decision support in how to set GPU requirements for different
training jobs. Moreover, relying on customer-generated reservations also prevents the system from
changing job allocations to respond to changes in a job or the overall workload.
Approach 2: Policy-based allocation with a fixed cluster size. A wide variety of recently proposed
systems try to improve performance by a combination of hyperparameter tuning and dynamic GPU
allocation [12, 17, 19, 20, 23, 26, 29, 31, 32]. While approaches differ in exactly how job configurations
are tuned, which heuristics are used, and which performance metrics are optimized (e.g., makespan,
JCT, or fairness), these systems are similar in their high-level goals of choosing GPU allocations


--- Page 8 ---
8
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
to optimize overall performance across a stream of training jobs. All prior work only considers a
fixed-size cluster (except for [26], which will be discussed in Approach 3).
In this paper, we choose to compare with Pollux [26], because it has been shown to dominate the
JCT of most of the above policies. For example, Pollux dominates the reservation-based approach
of Tiresias [13] even when job allocations are chosen by an offline optimizer. Additionally, Pollux
dominates Optimus [24] even when Optimus is given an oracle to optimally select job hyperparam-
eters. More recently, Lucid [15] was shown to only outperform Pollux at very high loads. Hence,
by showing that BOA Constrictor outperforms Pollux at all budget levels, we also demonstrate that
BOA Constrictor compares favorably to a wide range of systems designed to schedule training jobs
in fixed-size clusters. More detail about Pollux is given in Section 6.
Approach 3: Policy-based with autoscaling. To the best of our knowledge, [26] is the only prior work
that mentions scaling the cluster size jointly with job allocations. While Pollux is designed to use
fixed-size clusters, the authors do propose a Pollux variant that scales the cluster size to maintain a
given level of cluster efficiency ([26] refers to this as goodput-based autoscaling). However, the
authors only evaluate their proposed idea in simulation, running only a single job, deferring further
discussion of this idea to future work. Hence, to compare against this approach, we are forced to
construct our own working implementation of Pollux with goodput-based autoscaling (see Section
6 for details on our implementation of Pollux with autoscaling).
3
Theoretical Model and Results
We model the GPU rental problem from the perspective of a customer who submits a stream of ML
training jobs to be run in the cloud. Crucially, our model makes extremely general distributional
assumptions about the various stochastic quantities in the system, meaning the results we derive
can applied to a wide range of real-world systems. We model the case where the customer rents a
single type of GPU, however our theoretical results generalize to the setting of heterogeneous GPUs
(see Appendix E). Due to space and cost constraints, we defer a full evaluation of BOA Constrictor
on a heterogeneous cluster to future work.
3.1
Theoretical Model
Training Jobs. Each training job has an associated job size that is defined as the amount of training
progress that must be made for the model to reach a desired level of accuracy. In practice, we
measure job size as a training jobâ€™s run time on a single GPU.
We assume the customer has ğ‘€different types (classes) of training jobs, each corresponding to a
different combination of model and training data source. Let ğ‘‹ğ‘–be a random variable denoting the
size of a type-ğ‘–job. We assume that type-ğ‘–jobs follow the same size distribution, but do not assume
job sizes are independent from one another.
A training jobâ€™s parallelizability is characterized by a speedup function, which indicates how the
speed of processing a job varies as a function of the number of GPUs allocated to the job. We note
that the speedup function of a training job can vary slowly with time as the training progresses. To
capture this slow variation with time, we divide each type-ğ‘–job into â„“ğ‘–statistical epochs [26]. Each
epoch ğ‘—âˆˆ{1, 2, ..., â„“ğ‘–} has an associated random size ğ‘‹ğ‘–ğ‘—and a speedup function ğ‘ ğ‘–ğ‘—which implies
that if, in epoch ğ‘—, the job is run on ğ‘˜GPUs, then the epoch will complete in time
ğ‘‹ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜) . Hence, a
jobâ€™s total size satisfies ğ‘‹ğ‘–= Ãâ„“ğ‘–
ğ‘—=1 ğ‘‹ğ‘–ğ‘—. To avoid the need of an integer programming formulation,
we assume that the number of GPUs allocated to a job can be fractional. As prior work such as [11]
has shown, for large systems, fractional allocations can be converted into integral allocations using
policies such as randomized rounding without any significant loss of efficiency.


--- Page 9 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
9
Arrival process and job size distribution. We assume type-ğ‘–jobs arrive to the system with some
long-run average rate ğœ†ğ‘–. We assume that the ğ‘—th epoch of each arriving type-ğ‘–has a long-run
average job size E[ğ‘‹ğ‘–ğ‘—]. We assume only that the long-run arrival rate and time-average job sizes
above converge to the finite mean values above with probability 1, and that these means are known
to the customer. We make no additional distributional or independence assumptions. In particular,
the customer does not need to know the distribution ğ‘‹ğ‘–ğ‘—, just its mean.
We define the system load contributed by type-ğ‘–jobs to be ğœŒğ‘–= ğœ†ğ‘–E[ğ‘‹ğ‘–]. Intuitively, this ğœŒğ‘–
represents the long-run average number of GPU-hours required by the stream of type-ğ‘–jobs when
each job runs on a single GPU. It is also helpful to describe the load contributed by individual job
epochs. We define ğœŒğ‘–ğ‘—= ğœ†ğ‘–E[ğ‘‹ğ‘–ğ‘—] to be the load contributed by the ğ‘—th epoch of the ğ‘–th job type.
Rescaling. Rescaling the number of GPUs allocated to a job will incur various overheads. This
includes the time required to checkpoint a jobâ€™s progress, set up the runtime environment, the
time needed to download job-specific container images and training data, and the time needed to
restore a training job from a checkpoint on its new GPUs (see Section 5 for details). For simplicity,
we begin Section 4 by considering the GPU rental problem without the above overheads, deriving
the BOA policy in this simplified case. Then, in Section 4.3, we will show how to generalize our
model to account for rescaling costs, and how to optimize our BOA policy to achieve a low average
JCT when rescaling costs are present. Much like the other stochastic quantities in our model, we
model rescaling time as an unknown stochastic quantity that follows an arbitrary distribution. We
assume only that a sequence of rescaling times will converge to some finite long-run average.
The cost-performance tradeoff. Our goal is to design allocation policies that optimize the tradeoff
between cost and performance by renting GPUs and allocating them to jobs over time. In terms of
performance, this paper focuses on reducing Job Completion Time (JCT), the time from when a job
arrives until it completes. We primarily consider minimizing the average JCT across a stream of
arriving jobs. Let ğ‘‡ğ‘–denote the JCT of a class-ğ‘–job, and let E[ğ‘‡ğ‘–] denote its average. Our theory
results also apply to any weighted average JCT, where each job type is assigned a weight. These
weights can be used to ensure fairness across classes or reflect the relative importance of different
job types. Although we do not explicitly optimize for tail performance, empirically BOA Constrictor
greatly improves the tradeoff between cost and P95 JCT as well (see Section 6).
In terms of cost, we assume that a GPU can be rented for a constant cost per hour. Hence, without
loss of generality, we measure the cost of a cluster over time in GPU-hours. To control costs, the
customer sets a time-average operating budget of ğ‘GPU-hours that the allocation policy must obey.
Formally, to measure the cost of deploying an allocation policy, ğœ‹, let ğ¾ğœ‹(ğ‘ ) denote the number of
GPUs rented at time ğ‘ . We measure the time-average cost of using the policy ğœ‹as
Time-average cost = ğ¾ğœ‹= lim
ğ‘¡â†’âˆ
âˆ«ğ‘¡
0 ğ¾ğœ‹(ğ‘ )ğ‘‘ğ‘ 
ğ‘¡
.
We restrict our analysis to rental policies such that ğ¾ğœ‹exists. The customerâ€™s goal is to find a policy,
ğœ‹, that minimizes average JCT (or average weighted JCT) subject to the constraint that ğ¾ğœ‹â‰¤ğ‘.
3.2
Modeling Assumptions
We now state all assumptions needed for our theoretical results.
Finite means: For the stochastic quantities in the system: interarrival times, job sizes, and rescaling
times, we assume that the time averages for sequences of each quantity converge to some finite mean
with probability 1. We make no other distributional assumptions. Notably, we allow correlations
between job sizes and interarrival times, allowing us to model bursty, non-stationary workloads.


--- Page 10 ---
10
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
Assumptions on speedup functions: We make mild assumptions on the form of the speedup
functions. For any job type ğ‘–, epoch ğ‘—, ğ‘ ğ‘–ğ‘—(ğ‘˜) should fulfill the following properties: (1) ğ‘ ğ‘–ğ‘—(ğ‘˜) is
defined on ğ‘˜âˆˆ[1, +âˆ) and is continuous; (2) Monotonicity: ğ‘ ğ‘–ğ‘—(ğ‘˜1) â‰¤ğ‘ ğ‘–ğ‘—(ğ‘˜2) for any 1 â‰¤ğ‘˜1 < ğ‘˜2;
(3) Concavity: ğ‘ ğ‘–ğ‘—(ğ‘˜1)/ğ‘˜1 â‰¥ğ‘ ğ‘–ğ‘—(ğ‘˜2)/ğ‘˜2 for any 1 â‰¤ğ‘˜1 < ğ‘˜2.
In practice, naÃ¯vely measured speedup functions like those in Figure 2a may violate the second
two assumptions. However, we can always remedy this by considering the monotonic concave hull
of the speedup function. Prior work [11] has shown that one can achieve performance equivalent
to this concave hull without incurring additional rescaling by alternating between allocations on
the concave hull over time. Furthermore, it is easy to see that a policy does not benefit from using
allocations not on the concave hull of the speedup function. We follow this approach to handle
non-concave speedup functions in our implementation (see Algorithm 1).
Feasibility: We assume that the budget limit, ğ‘, is high enough to keep up with the stream of
training jobs. Mathematically, we express this in terms of the system load contributed by each type
of job as follows: Ãğ‘€
ğ‘–=1 ğœŒğ‘–< ğ‘.
Because of the concavity of the speedup functions, ğœŒğ‘–denotes the minimum number of GPU-
hours required by type-ğ‘–jobs. Hence, the budget, ğ‘, must be at least the sum of these system load
terms. If this assumption is violated, either the budget constraint must be violated, or the number
of queued jobs in the system will explode over time, leading to an infinite average JCT.
4
Budget-optimal Allocation
In this section, we present our theoretical results and our BOA policy. Under the simplification
that there is no rescaling overheads, we can theoretically characterize the optimal policy for the
GPU rental problem, as shown below (Definition 4.1). The main theorem proving its optimality
is Theorem 4.7. Then in Section 4.3, we show how to generalize this policy to incorporate the
rescaling overheads.
Definition 4.1 (Optimal policy, BOA). Under the BOA policy, a type-ğ‘–job in epoch ğ‘—is always
assigned ğ‘˜ğ‘–ğ‘—GPUs, where ğ‘˜ğ‘–ğ‘—is the solution for the following optimization problem:
minimize
ğ‘˜ğ‘–ğ‘—
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
subject to
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) â‰¤ğ‘,
ğ‘˜ğ‘–ğ‘—â‰¥1.
(1)
Here ğœŒğ‘–ğ‘—:= ğœ†ğ‘–E[ğ‘‹ğ‘–ğ‘—] is the total load contributed by type-ğ‘–jobs in epoch ğ‘—, and ğ‘is the time-average
operating budget.
The optimization problem (1) can be rewritten as a convex optimization problem, making it easy
to solve numerically (see Theorem B.1 in Appendix B).
We make two key observations about the optimal policy for the GPU rental problem:
(1) No job queues under the optimal policy.
(2) Jobs with the same speedup function should be allocated the same number of GPUs.
Taken together, these observations clearly describe the behavior of an optimal policy. Namely,
whenever a job arrives, it is allocated some number of GPUs immediately, and the allocation of
a job does not change unless the job enters the next epoch (which means the speedup function
changes). Moreover, jobs in the same class and at the same epoch are always allocated the same
number of GPUs. We call this class of policies â€œfixed-widthâ€ policies (Definition 4.4). By optimizing


--- Page 11 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
11
average JCT within the class of fixed-width policies, we can derive the budget-optimal allocation
(BOA) policy for the GPU rental problem.
To make this high-level argument rigorous, we first consider an offline version of the GPU
rental problem as an auxiliary problem. This offline auxiliary problem is a purely technical tool
in our proof â€” we will ultimately derive an optimal online policy. The remainder of the section
is organized as follows: First, in Section 4.1, we define the offline problem, and characterize the
optimal policy for the offline problem. The two observations above are formalized as Lemma 4.2
and Lemma 4.3. Then in Section 4.2, we return to the online problem and prove Theorem 4.7.
4.1
Offline Optimal policy
In the offline version of the GPU rental problem, the arrival times and the sizes of each job at each
epoch are known to the customer a priori. We will examine the optimal offline policy using a sample
path argument to learn about the structure of an optimal online policy.
Formally, a sample path A is an infinite sequence of arrival times and job sizes, where ğ‘¥(â„“)
ğ‘–ğ‘—
denotes the size of the â„“th arrival of type ğ‘–job at epoch ğ‘—. Let ğ‘›ğ‘–(ğ‘¡) denote the number of type-ğ‘–
arrivals by time ğ‘¡, where ğ‘›(ğ‘¡) := Ãğ‘€
ğ‘–=1 ğ‘›ğ‘–(ğ‘¡) denotes the total number of arrivals by time ğ‘¡.
We say that the sample path A is well-behaved if its time average arrival rates and time average
job sizes both converge to their finite means. That is,
ğœ†ğ‘–= lim
ğ‘¡â†’âˆ
ğ‘›ğ‘–(ğ‘¡)
ğ‘¡
and
E[ğ‘‹ğ‘–ğ‘—] = lim
ğ‘¡â†’âˆ
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1 ğ‘¥(â„“)
ğ‘–ğ‘—
ğ‘›ğ‘–(ğ‘¡)
âˆ€ğ‘–.
Then, for any well-behaved sample path A, we have the following two key lemmas, corresponding
to the key properties of BOA above.
Lemma 4.2 (No qeueing). The optimal offline policy for scheduling any well-behaved sample
path, A, does not queue jobs.
Lemma 4.3 (Same allocation for same speedup). The optimal offline policy for scheduling any
well-behaved sample path, A, assigns the same number of GPUs to every type-ğ‘–job during epoch ğ‘—.
Furthermore, the optimal offline policy holds each jobâ€™s allocation constant during each epoch.
Proof Sketch. Both of these lemmas are proved by contradiction, showing that we can improve
the average JCT of any policy that violates the conditions. See Appendix A for details.
â–¡
We define the class of policies obeying Lemmas 4.2 and 4.3 to be the class of fixed-width policies.
Definition 4.4 (Fixed-width policy). A fixed-width policy chooses constants {ğ‘˜ğ‘–ğ‘—}. All type-ğ‘–jobs
are assigned ğ‘˜ğ‘–ğ‘—GPUs for the entire duration of epoch ğ‘—without queueing.
Our stochastic assumptions on arrivals and job sizes (Section 3.2) imply that a random sample
path is well-behaved with probability 1. Thus, it suffices to derive a policy that is optimal for the set
of well-behaved sample paths. The next lemma characterizes the operating budget of a fixed-width
policy under a well-behaved sample path.
Lemma 4.5 (Operating budget of a fixed width policy). Given a well-behaved sample path A,
the average JCT and operating budget of a fixed-width policy with parameters {ğ‘˜ğ‘–ğ‘—} are respectively
average JCT = 1
ğœ†
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
and
operating budget =
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
where ğœŒğ‘–ğ‘—= ğœ†ğ‘–Â· E[ğ‘‹ğ‘–ğ‘—].
Proof. See Appendix A for the rigorous proof.
â–¡
We now show that BOA is offline optimal for any well-behaved sample path A.


--- Page 12 ---
12
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
Lemma 4.6. For any well-behaved sample path A, BOA is the optimal offline policy.
Proof Sketch. Lemma 4.2 and Lemma 4.3 show that the optimal offline policy is a fixed-width
policy. Thus, it suffices to show that BOA is the optimal fixed-width policy.
For any fixed-width policy, Lemma 4.5 gives both the average JCT and the operating budget.
Thus, the optimal parameters ğ‘˜ğ‘–ğ‘—can be solved from the optimization problem (1).
â–¡
4.2
Returning to the online problem
We now return to the online problem and prove our main theorem, Theorem 4.7.
Theorem 4.7 (Optimality). The BOA policy is optimal in minimizing the average JCT.
Proof. Lemma 4.6 shows that for any well-behaved sample path, the BOA policy is offline
optimal. Note that the optimization problem (1) only depends on ğœŒğ‘–and not any specific job arrival
times or sizes. Because the ğœŒğ‘–values are known in the online setting, BOA is also an online policy.
Hence, because BOA is an optimal offline policy for any well-behaved sample path, and the set of
sample paths that are not well-behaved has measure 0, BOA is optimal in the online setting.
â–¡
4.3
Generalizing BOA to incorporate rescaling overheads
In the idealized model presented above, we assumed the system can rescale allocations instan-
taneously and without cost. In practice, as discussed in Section 3.1, changing a jobâ€™s allocation
incurs a rescaling overhead. During this rescaling period, the job consumes resources (budget) but
accumulates no progress in training, effectively increasing both the average JCT and the total cost.
We denote the rescaling time for a class-ğ‘–job by a random variable ğ‘…ğ‘–with mean E[ğ‘…ğ‘–] = ğ‘Ÿğ‘–.
Even with these overheads, Lemmas 4.2 and 4.3 still hold, which means the optimal policy remains
a fixed-width policy as defined in Definition 4.4. However, as we will show in Lemma 4.8, solving
for the optimal parameters {ğ‘˜ğ‘–ğ‘—} for the fixed-width policy requires solving a mixed-integer convex
programming (MICP) rather than a convex optimization problem in Lemma 4.5.
Lemma 4.8 (Fixed width policy with rescale). Given a fixed width-policy parameterized with
{ğ‘˜ğ‘–}, the average JCT and the operating budget are given by:
average JCT = 1
ğœ†
ğ‘€
âˆ‘ï¸
ğ‘–=1
ğœ†ğ‘–Â·
â„“ğ‘–
âˆ‘ï¸
ğ‘—=0
 E[ğ‘‹ğ‘–ğ‘—]
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) + ğ‘Ÿğ‘–Â· 1ğ‘–ğ‘—

,
(2)
operating budget = 1
ğœ†
ğ‘€
âˆ‘ï¸
ğ‘–=1
ğœ†ğ‘–Â·
â„“ğ‘–
âˆ‘ï¸
ğ‘—=0
ğ‘˜ğ‘–ğ‘—
 E[ğ‘‹ğ‘–ğ‘—]
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) + ğ‘Ÿğ‘–Â· 1ğ‘–ğ‘—

,
(3)
where 1ğ‘–ğ‘—is the indicator that is 1 if ğ‘˜ğ‘–ğ‘—â‰ ğ‘˜ğ‘–(ğ‘—âˆ’1) or ğ‘—= 0.
Directly minimizing (2) subject to keeping the operating budget within ğ‘results in a MICP, and
thus is computationally intractable. We therefore propose the BOA Width Calculator, a heuristic
that approximates the optimal solution via two mechanisms: Epoch Gluing and Budget Partitioning.
Epoch Gluing. To prevent excessive rescaling, we define a â€œglueâ€ configuration ğ‘”ğ‘–for job class ğ‘–,
which dictates that every set of ğ‘”ğ‘–consecutive epochs is treated as a single â€œsuper-epochâ€ with a
duration equal to the sum of the constituent epochs, and a speedup function equal to the weighted
average of the constituent speedups. Specifically, applying a glue configuration ğ‘”ğ‘–to job class ğ‘–is
equivalent to enforcing ğ‘˜ğ‘–0 = ğ‘˜ğ‘–1 = ... = ğ‘˜ğ‘–(ğ‘”ğ‘–âˆ’1); ğ‘˜ğ‘–ğ‘”ğ‘–= ğ‘˜ğ‘–(ğ‘”ğ‘–+1) = ... = ğ‘˜ğ‘–(2ğ‘”ğ‘–âˆ’1); etc.
Budget Partitioning. Even with glued epochs, we must satisfy the total budget constraint ğ‘.
The total cost in (3) consists of two parts: the running budget and the rescaling budget. Since the
rescaling cost depends on the allocation width ğ‘˜ğ‘–ğ‘—, which in turn depends on the running budget,


--- Page 13 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
13
we cannot know the split a priori. We address this by iteratively searching for a maximal effective
running budget, denoted as ğ‘ğ‘Ÿğ‘¢ğ‘›. We solve the convex optimization problem (1) assuming a budget
of ğ‘ğ‘Ÿğ‘¢ğ‘›to obtain candidate allocations {ğ‘˜ğ‘–ğ‘—} (and round the fractional solution ğ‘˜ğ‘–ğ‘—to the closest
integer on the non-decreasing concave hull of the speedup function ğ‘ ğ‘–ğ‘—). We can then compute the
total cost (via Lemma 4.8) using these {ğ‘˜ğ‘–ğ‘—}. We continually decrease ğ‘ğ‘Ÿğ‘¢ğ‘›until the total cost is â‰¤ğ‘.
The BOA Width Calculator searches for the best glue configuration and budget split to minimize
the average JCT. See the detailed pseudo code in Appendix C.
5
System Design
In Section 4, we derived the BOA policy, which minimizes the average JCT under the operating
budget constraint. In this section, we describe our implementation of BOA Constrictor â€” a real-
world scheduler for cloud-based ML training workloads that is based on the BOA policy.
Our new scheduler, BOA Constrictor, is implemented on top of the AdaptDL scheduling frame-
work [1]. We first provide a brief overview of the AdaptDL architecture (Section 5.1). We then
detail the specific mechanisms required to realize the BOA policy (Section 5.2). We then describe
the extensions that we made to the underlying AdaptDL framework (Section 5.3). Finally, we close
this section by discussing the system overheads imposed (Section 5.4).
5.1
How AdaptDL Enables Dynamic Resource Allocation
AdaptDL facilitates dynamic resource allocation based on the following core principles (Figure 3):
â€¢ Containerized Execution: Jobs are deployed as Kubernetes pods, comprising one or more
Docker containers. These pods can scale across multiple GPUs and cloud instances.
â€¢ Performance model for hyperparameter tuning and scheduler decision: As training jobs
execute, the system accumulates runtime statistics such as throughput and gradient accumulation
overheads. AdaptDL uses this data to train an underlying performance model that predicts a
jobâ€™s speedup (compared with a base hyperparameter on 1 GPU) given different hyperparameters
running on different number of GPUs. This performance model serves two purposes:
(1) Hyperparameter tuning. Once a job is allocated some number of GPUs, it dynamically adjusts
its internal hyperparameters to obtain the best speedup, as predicted by the performance
model.
(2) Generating a speedup function for scheduler decisions. For each number of GPUs that a job
might be allocated, the performance model can find the optimal hyperparameters to achieve
the highest speedup. The speedup function for the job is determined by using the best
possible speedup for each possible number of GPUs. The speedup function is then fed to
the AdaptDL scheduler to make further decisions, as stated in the next bullet.
â€¢ Resource Scheduling and Placement: The scheduler acts as a centralized coordinator. It
collects the current state (e.g., current epoch, speedup functions) of all active jobs and the
available nodes in the cluster. Based on the scheduling policy, the scheduler determines (i) the
GPU allocation for each job, and (ii) the desired total number of system nodes. The allocations
for jobs are then executed by the Kubernetes control plane, while the desired number of nodes is
sent to a Cluster Expander component which automatically provisions or releases cloud instances
to match the aggregate resource demand.
â€¢ Preemption and Resumption: To realize the dynamic allocations dictated by the scheduler, the
system supports transparent preemption and resizing through a checkpoint-restart mechanism.
Job states are periodically copied to distributed storage, allowing the scheduler to migrate jobs
or change their parallelism with minimal overhead.


--- Page 14 ---
14
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
5.2
Implementing the BOA Policy
The core of BOA Constrictor is the implementation of the BOA Policy derived in Section 4. Unlike
heuristic schedulers that reactively adjust allocations based on instantaneous cluster load, our
policy enforces a theoretically computed fixed-width allocation for each job.
Fixed-Width Execution. The scheduler maintains a set of parameters {ğ‘˜ğ‘–ğ‘—} for every class ğ‘–and
epoch ğ‘—. The execution logic is lightweight: at every scheduling interval, the scheduler simply looks
up the pre-computed ğ‘˜ğ‘–ğ‘—for each job depending on its class and current epoch.
(1) Job Allocation: The scheduler assigns ğ‘˜ğ‘–ğ‘—GPUs to each class-ğ‘–job in epoch ğ‘—. The actual
placement of the jobs is determined in a way to minimize rescaling (i.e., keep the placement
for jobs that are already running on the correct number of GPUs). If there are not enough
GPUs in the cluster, one of the remaining jobs runs on whatever GPUs are left, and other
remaining jobs queue and wait for the cluster expander to expand the cluster.
(2) Cluster Sizing: The desired number of GPUs in the cluster is determined by summing up
the number of GPUs required (determined by ğ‘˜ğ‘–ğ‘—) across all jobs. This total demand in GPUs
is then translated to the demand in nodes which is sent to the Cluster Expander component.
Asynchronous Parameter Computation. Our BOA Width Calculator described in Section 4.3 (Al-
gorithm 1) determines the optimal parameters {ğ‘˜ğ‘–ğ‘—}. In our implementation, this computation is
decoupled from the main scheduling loop. The width calculator runs as a background process,
ensuring that the critical path of job execution remains fast. While the scheduling loop executes
the fixed-width policy in real-time, using the current set of parameters (ğ‘˜ğ‘–ğ‘—), the width calculator
recomputes {ğ‘˜ğ‘–ğ‘—} periodically (e.g., every 15 minutes). This decoupling allows BOA Constrictor to
utilize increasingly accurate speedup predictions as jobs run. As the underlying profiling models
gather more data, the speedup functions ğ‘ ğ‘–ğ‘—become more precise. The width calculator then updates
{ğ‘˜ğ‘–ğ‘—} to reflect this improved understanding.
5.3
Extending the AdaptDL Framework
We extend the AdaptDL framework to help evaluate BOA Constrictor and improve user experience.
Global
Profiler
Cloud Service 
Provider
AdaptDL
Scheduler
Cluster 
Expander
BOA Policy
Jobs Info & 
Current Nodes
Desired Nodes
Allocations
Allocations
Jobs Info
Scheduler Pods
Job Pods
Job1
Job2 
Performance Model
External Arrivals
Pollux
Allocation
Policy
Offline
Width calculator
Speedup 
ğ‘˜!"
Fixed Width
Executor
Jobs
Allocations
Desired nodes
Fig. 3. An illustration of BOA Constrictor and the AdaptDL framework.
Profile Sharing. The original AdaptDL framework trains a performance model from scratch for
every new job. While not central to our implementation, we extended the system to allow jobs of
the same class to share performance models. This approach is realized by implementing a global
profiler on top of the AdaptDL schedulers. The main purpose of this modification is to isolate our
improvement in scheduling from the orthogonal hyperparameter tuning process (see Section 6.2).
Decision Support Tool. We leverage the predictability of the BOA policy to build an offline profiling
calculator. With the input of a workload trace, BOA Constrictor can provide the full Pareto frontier
by running Algorithm 1 on different budgets. This tool allows a customer to select an optimal
budget constraint ğ‘based on their desired average JCT before provisioning any real resources.


--- Page 15 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
15
Extensibility to Hyperparameter Tuning. While we deploy the batch size and learning rate tuning
used in [26] for each job in our evaluations, we point out that our BOA policy is compatible with any
hyperparameter tuning approach. The BOA policy and the hyperparameter optimization are solving
orthogonal problems: the BOA policy is optimizing system performance based on the speedup
functions, while the hyperparameter optimization is trying to improve the speedup functions. BOA
doesnâ€™t need to be concerned with the particular hyperparameter choices, so long as it is able to
obtain the speedup functions.
5.4
Scheduler Overheads
We compare the system overheads of BOA Constrictor with those of Pollux.
Real-time optimization. In each scheduling cycle, the policy must determine the allocation for
every active job. Ideally, this decision should be made in real-time to maintain system responsiveness.
BOA Constrictor incurs negligible overhead in this phase because it merely executes the pre-
computed fixed-width policy; this involves only retrieving the parameters ğ‘˜ğ‘–ğ‘—for all active jobs. In
our experiments, the mean real-time optimization latency for BOA Constrictor is merely 0.146 ms.
In contrast, Pollux must solve a combinatorial optimization problem online, requiring 4.39 seconds
on average, or up to 23.58 seconds on average when cluster autoscaling is triggered.
Offline width calculation. As detailed in Section 5.2, BOA Constrictor decouples the heavy
computation from the real-time loop. The BOA Width Calculator runs as a background process,
periodically updating ğ‘˜ğ‘–ğ‘—as job speedup profiles evolve. While this calculation is computationally in-
tensiveâ€”utilizing 1 CPU core for approximately 500 seconds per updateâ€”it operates asynchronously
and does not block the scheduler or the running jobs. Conversely, while Pollux avoids this specific
offline component, its online policy computation is itself heavy (also utilizing 1 CPU) and lies on
the critical path of scheduling decisions.
Rescaling overheads. Job rescaling overheadsâ€”including checkpointing, pod termination,
and restarting on new nodesâ€”are consistent with the underlying AdaptDL framework. For a
representative workload (e.g., CIFAR-10 training), we measured rescaling latencies of approximately
20 seconds on a â€œwarmâ€ machine (where container images are cached) and 120 seconds on a â€œcoldâ€
machine. Decomposing the 120-second cold start latency reveals that the majority of time is spent on
environment initialization: approximately 75 seconds for installing Python packages and initializing
CUDA contexts, followed by 25 seconds for loading the dataset from the Elastic File System (EFS)
to local storage. In the warm start scenario, where these initialization steps are cached, half of
the latency is due to the synchronization overhead required to establish connections between
distributed workers. While non-negligible, these rescaling overheads are acceptable (occasionally)
for a job with a typical lifetime of 30 minutes or more.
6
Evaluation
We evaluate BOA Constrictor through a combination of real-world experiments that train models
on AWS (Section 6.2), detailed simulation (Section 6.3), and sensitivity analysis (Section 6.4). We
begin by explaining our experimental setup and the allocation policies we compare against.
6.1
Experimental Setup
Competitor Policies. We compare BOA Constrictor directly to two competitor allocation policies
implemented using AdaptDL.
Pollux [26] is a cluster scheduler that adaptively tunes job allocations and hyperparameters to
maximize cluster efficiency. Cluster efficiency is a number between 0 and 1, defined as the sum
of speedups of all jobs in the system divided by the cluster size. Pollux was primarily designed to
reduce average JCT in a fixed-size cluster. Hence, to meet a customerâ€™s budget constraint, ğ‘, we


--- Page 16 ---
16
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
Task
Dataset
Model
% of workload
Image Classification
CIFAR-10
ResNet18
50.42%
Question Answering
SQuAD
BERT
21.67%
Speech Recognition
CMU-ARCTIC
DeepSpeech2
23.54%
Object Detection
PASCAL-VOC
YOLOv3
4.75%
Image Classification
ImageNet
ResNet50
0.62%
Table 1. Composition of the newTrace workload trace used for evaluating BOA Constrictor.
provision Pollux with a fixed-size cluster of ğ‘GPUs. We set all other tunable parameters to the
defaults suggested by the authors.
Pollux with Autoscaling is proposed as a conceptual idea in [26] but is not implemented or
evaluated. Specifically, the authors propose a variant of Pollux that scales the cluster size in order
to meet a target level of cluster efficiency. The concept in [26] is that it is inefficient to allocate a
high number of GPUs to an individual job, so cluster efficiency will tend to drop when there are few
jobs in the system. Hence, the cluster size can be scaled down when few jobs are present, to return
the cluster efficiency to its target level. Conversely, when there are many jobs in the system, the
cluster efficiency may exceed its target level, indicating that each job is receiving a small number
of GPUs. The cluster can then be scaled up to lower the cluster efficiency to its target level.
The proposal in [26] is evaluated only via a simulation running a single job, so to compare BOA
Constrictor with Pollux with Autoscaling we first created a fully functional implementation of
Pollux with Autoscaling that can process of stream of arriving training jobs; this required significant
engineering effort.
While [26] only suggests setting the target efficiency at 0.5, we explore the full range of the target
efficiency level, ğ‘; this is a performance knob that we can tune to evaluate the JCT on different
operating budgets. Specifically, we define Î” = min{.3(1 âˆ’ğ‘), .3ğ‘}. The cluster size is increased
whenever the cluster efficiency exceeds ğ‘+ Î”. The cluster size is decreased whenever the cluster
efficiency falls below ğ‘âˆ’Î”. The new cluster size is then set using combinatorial optimization
to select that cluster size and job allocations whose cluster efficiency is closest to ğ‘. Although
adjusting the target efficiency controls the GPU usage of the policy, there is no analysis predicting
the average number of GPU-hours that the policy will consume for a given value of ğ‘.
AdaptDL [1] also provides some limited support for exploring the autoscaling proposal in
[26]. Unfortunately, the approach provided in AdaptDL does not scale to handle multiple jobs
running simultaneously. The problem with the AdaptDL implementation is that it attempts to
jointly optimize cluster size and job allocations, which fails when running a large number of
jobs simultaneously. To fix this, we implemented the approach suggested in the simulations from
[26], which breaks this combinatorial optimization step into pieces, at the expense of longer run
times during autoscaling periods (see Section 5.4). By contrast, BOA Constrictor avoids costly
combinatorial optimization, using the BOA policy that requires only convex optimization.
Other systems have also been proposed to automate cluster scheduling for ML training work-
loads. We choose to only compare BOA Constrictor against Pollux (and its variant) because Pollux
has been shown to dominate many of these prior approaches (see Section 2.4).
Workload. To evaluate BOA Constrictor and the competitor policies, we use both the production
trace newTrace published as part of [29] and the workload-1 trace published as part of [26].
We summarize the job types in these workloads in Table 1, but full details (such as accuracy
requirements etc.) can be found in [26, 29]. Crucially, the job sizes in this trace are highly variable,
differing by a factor of 10Ã— between job types. The arrival process is also highly variable. The
inter-arrival time of newTrace trace has a squared coefficient of variation of ğ¶2 = 2.65.


--- Page 17 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
17
We subsample the original workload-1 trace to create a subtrace that is easier and more cost-
efficient to use in implementation experiments. To construct this trace, we narrow the set of job
types in Table 1 down to ResNet18, BERT, and DeepSpeech2. The resulting trace contains 85 jobs
and displays much of the same arrival rate and job size variability as the original trace.
6.2
Implementation Experiment
We conduct real-world experiments on AWS using BOA Constrictor and Pollux with autoscaling to
evaluate how each policy balances the cost-performance tradeoff.
Testbed. Our implementation experiments run on AWS using an autoscaled EKS cluster composed
of g4dn.12xlarge instances. Each instance has 4 NVIDIA T4 GPUs. This instance type was chosen
for two primary reasons. First, these same instances were used in the evaluation of Pollux, so this
enables a more direct comparison. Second, our main goal is to evaluate how BOA Constrictor
performs when managing a large number of nodes over the course of long training jobs. To perform
experiments at a larger scale (âˆ¼300 GPUs on âˆ¼80 nodes) without incurring prohibitively high costs,
we select a cost-efficient instance type.
Measurement. Job completion time and GPU usage are monitored by examining logging messages.
The GPU usage statistics we report include the time from when a node first enters a â€œstartingâ€ state
until it is marked for release by the allocation policy. In practice, there is some additional time from
when a node is marked to be released until it is fully reclaimed by the cloud provider. Because
this additional time is controlled by the cloud providerâ€™s autoscaling infrastructure, it will vary
from one cloud platform to the next, and we cannot directly optimize it. We therefore exclude this
reclamation time from our usage statistics. Appendix D shows that reclamation time affects all
policies we consider, but has a smaller impact on BOA Constrictor than the competitor policies.
Workload. To limit the overall cost of running experiments, our implementation experiments on
AWS use the subsampled trace of 85 jobs (as stated in Section 6.1). Our goal in these experiments is
both to show the benefits of BOA Constrictor using real cloud-based GPUs and to validate that
our simulator closely matches our implementation results. This enables us to evaluate larger scale
systems and traces via simulation.
Profiling information. Due to the cost constraints of running our implementation on AWS, we use
only a shorter trace for our implementation experiments; this is also the approach followed in prior
works (e.g. [26, 29]). The drawback of using a shorter trace is that the scheduler has less time to
learn job speedup functions and find good hyperparameter configurations for each job.
To counteract this effect, we make two modifications for both policies: (1) We seed the AdaptDL
profiling mechanism with job profiles and hyperparameter settings collected and identified during
an offline profiling step, and (2) we provide both policies with accurate speedup profiles. Hence, our
implementation experiments indicate how each policy will perform after running for long enough
that the relevant performance models and hyperparameter selection algorithms have converged.
These two modifications also help us isolate the performance gain that BOA Constrictor achieves
by improving scheduling. We note that these two modifications apply only to the implementation
experiments and not to the large-scale simulations. Moreover, BOA Constrictor benefits less from
the perfect speedup information than the competitor policies (see Figure 8).
Results. Figure 4 shows our implementation experiment results. We evaluate BOA Constrictor and
the competitor policies at a variety of GPU usage levels.


--- Page 18 ---
18
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
(a) Implementation
(b) Simulation
Fig. 4. Comparing BOA Constrictor and Pollux with autoscaling running 85 jobs from the workload-1 trace
[26]. Simulation results match implementation results, showing that BOA Constrictor significantly improves
the Pareto frontier of average JCT vs. operating budget.
The usage of BOA Constrictor is set by adjusting the target budget, ğ‘. The usage of Pollux with
autoscaling is implicitly set by iterating of the range of possible values of the target efficiency, ğ‘.
We perform a matching set of simulations using the simulator described in Section 6.3.
The implementation results in Figure 4a show an improvement in average JCT at all usage levels.
BOA Constrictor reduces average JCT by up to a factor of âˆ¼1.6Ã— when both policies use a budget
of âˆ¼40 GPU-hours per hour. Additionally, BOA Constrictor reduces the budget required to achieve
an average JCT of âˆ¼2100 seconds by a factor of âˆ¼2.2Ã—.
Discussion. To visualize the source of this performance gain, Figure 5 shows GPU usage over time
when both policies are configured to use a budget of âˆ¼55 GPU-hours per hour. BOA Constrictorâ€™s
advantage comes from the form of the BOA policy. The BOA policy computes the optimal allocation
for each job epoch offline and decides ahead of time which rescaling costs are worth paying for each
job. By contrast, Pollux with autoscaling makes allocation decisions online in a reactive manner,
and uses a scheduling quantum of 60 seconds to avoid rescaling too frequently. As a result, BOA
Constrictor reacts faster to bursts in arrivals.
Fig. 5. GPU usage of BOA Constrictor and Pollux with autoscaling implementations. Both policies use a
time-average operating budget of âˆ¼55 GPUs. BOA Constrictor lowers average JCT by reacting faster and
more aggressively to bursts in arrivals.
Pollux with autoscaling allows queueing and scales the cluster size based on target efficiency
rather than the optimal GPU demands of each job. Hence, once Pollux with autoscaling does react
to a burst, it tends to not scale the cluster size as aggressively as BOA Constrictor. Both of these
effects lead to higher JCTs for a given usage level.
The lowest-usage experiment for Pollux with autoscaling encountered stability issues where
Polluxâ€™s optimizer struggled to find allocations that met the policyâ€™s target efficiency. The resulting
policy rescaled jobs too frequently, leading to crashes when there were many jobs in the system.
For this experiment, we report the average JCT measured at the time of the crash. We see similar


--- Page 19 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
19
(a) filterTrace, Avg. JCT.
(b) filterTrace, P95 JCT.
(c) newTrace, Avg. JCT.
(d) newTrace, P95 JCT.
Fig. 6. Comparing BOA Constrictor, Pollux, and Pollux with autoscaling on the filterTrace (a,b) and newTrace
(c,d) production traces. In all cases, BOA Constrictor improves the Pareto frontier of both average JCT and
P95 JCT vs. operating budget.
behavior where Polluxâ€™s optimizer fails to maintain target efficiency during low-usage experiments
in simulation. This suggests that goodput-based autoscaling as an approach is fundamentally
limited in its ability to achieve very low resource utilization.
6.3
Simulation Experiments
Our simulations use the simulator from [29]. This simulator models each job using profiling
information from training jobs that ran on AWS using AdaptDL. Simulations include job rescaling
overheads and interference effects between co-located jobs based on real-world profiling data.
Crucially, the simulations depicted in Figure 4b match our implementation results in Figure 4a.
There are minor discrepancies between the two sets of experiments due to increased variability
in the real-world experiments which makes the speedup models of both policies less accurate.
However, this variability affects both policies similarly, so the percentage difference between the
policies in implementation is predicted closely by simulation. In light of this validation, we run a
series of larger-scale simulations to evaluate BOA Constrictor.
Longer experiments. We scale up our experiments by considering a sample of 918 jobs from the
newTrace production trace [29]. We obtain this trace by sampling the same three job types from
newTrace that were used in our implementation experiments, and we refer to this filtered trace
as filterTrace. Given the longer trace, the policies now must learn the speedup functions and
hyperparameter configurations while processing the jobs (in contrast to the setting in Section 6.2).
Figure 6a shows the average JCT for BOA Constrictor, Pollux, and Pollux with autoscaling at a
range of usage levels in simulation. These experiments confirm BOA Constrictorâ€™s improvements
to the cost-performance tradeoff as promised by our implementation experiments. Additional ex-
periments involving Pollux (without autoscaling) show that there can be a benefit to goodput-based
autoscaling as predicted in [26]. However, average JCT under BOA Constrictor still outperforms
Pollux with autoscaling by up to âˆ¼1.75Ã— by optimally balancing the cost-performance tradeoff.
One advantage of running a longer trace is that it enables us to consider tail performance in
addition to average JCT. Figure 6b shows the P95 JCT from each experiment in Figure 6a. Although
BOA Constrictor is designed to optimize for average JCT, the structure of the BOA policy is highly
beneficial for tail performance as well. Under both Pollux and Pollux-with-autoscaling, jobs in
the P95 of JCT are generally the unlucky jobs that experience significant queueing and frequent
preemption due to random chance. These effects are both mitigated under BOA Constrictor, where
the allocation to each job changes less. As a result, BOA Constrictor outperforms Pollux with
autoscaling by a factor of up to âˆ¼1.7Ã— with respect to P95 JCT.
More job types. We also simulate BOA Constrictorâ€™s performance on the newTrace trace from [26],
which is composed of 960 jobs drawn from all of the job types in Table 1. Figure 6 again shows a


--- Page 20 ---
20
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
significant advantage for BOA Constrictor, which achieves up to a âˆ¼1.8Ã— improvement in average
JCT and a âˆ¼1.6Ã— improvement in P95 JCT. This scaled-up simulation shows that BOA Constrictor
maintains its advantage as we consider additional job types and increased job size variability.
6.4
Sensitivity Analysis
We complete our evaluation by analyzing the robustness of the above results.
Fig. 7. The system efficiency over time. BOA Constrictor has an overall lower system efficiency compared
with Pollux with autoscaling, but provides better performance. This implies that system efficiency is a flawed
scheduling heuristic.
Is BOA Constrictor close to Pollux? One potential concern is whether the decisions made by BOA
Constrictor are close enough to the decisions made by Pollux that one could match BOA Constric-
torâ€™s performance by simply tuning the parameters of Pollux to avoid some additional overheads.
On the contrary, we find that BOA Constrictor and Pollux with autoscaling pursue fundamentally
different goals. Figure 7 shows the cluster efficiency over the course of the simulation experiments
depicted in Figure 6. Specifically, we consider the experiments where both policies use âˆ¼112 GPU-
hours on average. We see that Pollux runs at an average cluster efficiency of 0.73 while BOA
Constrictor runs with an average cluster efficiency of 0.64.
That is, BOA Constrictor uses the same number of GPU-hours on average, but achieves sig-
nificantly lower average JCT than Pollux with autoscaling by purposely using its resources less
efficiently than Pollux with autoscaling. This implies that the fundamental approach of Pollux
with autoscaling â€” pick a cluster size, maximize cluster efficiency given this cluster size â€” is
fundamentally flawed. The BOA policy makes allocation decisions that are not easily described in
terms of cluster efficiency.
How sensitive is BOA Constrictor to prediction error? Our experiments assume that the models used
to predict job speedups have converged. However, these converged models are still imperfect. This
raises the question of how sensitive BOA Constrictor is to errors in the speedup models. Figure
8 shows that BOA Constrictor is significantly more robust to modeling errors than Pollux with
autoscaling. Here, we compare each policyâ€™s performance when using an imperfect speedup model
to its performance when given perfect profiling information about each job. While the performance
of BOA Constrictor is essentially unchanged by prediction error, the average JCT under Pollux
with autoscaling increases by up to a factor of âˆ¼1.4Ã— in the face of prediction errors.
How sensitive are our results to workload variability? One final concern is whether our results hold
as workload variability becomes more extreme. We examine this question by generating a synthetic
trace based on newTrace. Our synthetic trace uses the same mixture of jobs, but we adjust the


--- Page 21 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
21
Fig. 8. Average JCT for policies with perfect / imperfect information on the speedup functions. It is shown
that BOA Constrictor is less sensitive to the accuracy of speedup functions than Pollux with autoscaling.
Fig. 9. Comparison of the sensitivity of BOA Constrictor and Pollux with autoscaling to arrival process
variability. Here, both policies use a budget âˆ¼120 GPUs to process a synthetic trace based on newTrace.
Pollux with autoscaling is much more sensitive to arrival process variability
job arrival times to control how bursty the workload is. Specifically, we construct a time-varying
Poisson process where jobs arrive at either a high rate or a low rate. To model more variable
workloads, we keep to the long-run average arrival rate fixed, but create increasingly short and
intense bursts of arrivals.
We describe the variability of arrivals via the squared coefficient of variation of the interarrival
time distribution, ğ¶2
ğ¼. Figure 9 shows that, as variability increases, BOA Constrictor outperforms
Pollux with autoscaling by an increasing margin. BOA Constrictor handles increasingly intense
bursts in traffic while the effects of Polluxâ€™s slow and overly conservative autoscaling process are
amplified. For reference, newTrace has ğ¶2
ğ¼= 2.65.
7
Conclusion
BOA Constrictor addresses the fundamental challenge of training machine learning models in the
cloud, where a customer must make two decisions at every moment of time: (1) how many total
GPUs to rent, and (2) how to distribute those GPUs across a stream of ML training jobs. To solve
this challenge, we model this problem as a budget-constrained scheduling problem, from which
the Budget Optimal Allocation (BOA) policy is derived. Importantly, our BOA policy provides the
user with the optimal tradeoff between cost and performance, leading to large improvements in
the average JCT one can achieve with a fixed budget compared to the state-of-the-art.


--- Page 22 ---
22
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
References
[1] Adaptdl Cluster Simulator: https://github.com/petuum/adaptdl/tree/osdi21-artifact/simulator. https://github.com/
petuum/adaptdl/tree/osdi21-artifact/simulator.
[2] Amazon, google could see combined $50b in revenue from renting out gpus.
[3] Maximizing gpu utilization with nvidiaâ€™s multi-instance gpu (mig) on amazon eks: Running more pods per gpu
for enhanced performance. https://aws.amazon.com/blogs/containers/maximizing-gpu-utilization-with-nvidias-multi-
instance-gpu-mig-on-amazon-eks-running-more-pods-per-gpu-for-enhanced-performance/.
[4] Berg, B., Dorsman, J., and Harchol-Balter, M. Towards optimality in parallel scheduling. ACM POMACS 1, 2 (2018).
[5] Berg, B., Harchol-Balter, M., Moseley, B., Wang, W., and Whitehouse, J. Optimal resource allocation for elastic
and inelastic jobs. In Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms and Architectures (2020),
pp. 75â€“87.
[6] Berg, B., Vesilo, R., and Harchol-Balter, M. heSRPT: Parallel scheduling to minimize mean slowdown. Performance
Evaluation 144 (2020), 102â€“147.
[7] Berg, B., Whitehouse, J., Moseley, B., Wang, W., and Harchol-Balter, M. The case for phase-aware scheduling of
parallelizable jobs. ACM SIGMETRICS Performance Evaluation Review 49, 3 (2022), 65â€“66.
[8] Dunlap, L., Kandasamy, K., Misra, U., Liaw, R., Jordan, M., Stoica, I., and Gonzalez, J. E. Elastic hyperparameter
tuning on the cloud. In Proceedings of the ACM Symposium on Cloud Computing (2021), pp. 33â€“46.
[9] Gandhi, A., Harchol-Balter, M., Raghunathan, R., and Kozuch, M. A. Distributed, robust auto-scaling policies
for power management in compute intensive server farms. In 2011 Sixth Open Cirrus Summit (2011), IEEE, pp. 1â€“5.
[10] Gandhi, A., Harchol-Balter, M., Raghunathan, R., and Kozuch, M. A. Autoscale: Dynamic, robust capacity
management for multi-tier data centers. ACM Transactions on Computer Systems (TOCS) 30, 4 (2012), 1â€“26.
[11] Ghanbarian, S., Mukhopadhyay, A., Mazumdar, R. R., and Guillemin, F. M. On optimal server allocation for
moldable jobs with concave speed-up. In Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic
Foundations, and Protocol Design for Mobile Networks and Mobile Computing (New York, NY, USA, 2024), MobiHoc â€™24,
Association for Computing Machinery, p. 191â€“200.
[12] Gu, D., Zhao, Y., Zhong, Y., Xiong, Y., Han, Z., Cheng, P., Yang, F., Huang, G., Jin, X., and Liu, X. Elasticflow:
An elastic serverless training platform for distributed deep learning. In Proceedings of the 28th ACM International
Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (2023), pp. 266â€“280.
[13] Gu, J., Chowdhury, M., Shin, K. G., Zhu, Y., Jeon, M., Qian, J., Liu, H., and Guo, C. Tiresias: A {GPU} cluster
manager for distributed deep learning. In 16th USENIX Symposium on Networked Systems Design and Implementation
(NSDI 19) (2019), pp. 485â€“500.
[14] Harchol-Balter, M. Performance Modeling and Design of Computer Systems: Queueing Theory in Action. Cambridge
University Press, 2013.
[15] Hu, Q., Zhang, M., Sun, P., Wen, Y., and Zhang, T. Lucid: A non-intrusive, scalable and interpretable scheduler
for deep learning training jobs. In Proceedings of the 28th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems, Volume 2 (2023), pp. 457â€“472.
[16] Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,
A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of ISCA (2017), pp. 1â€“12.
[17] Le, T. N., Sun, X., Chowdhury, M., and Liu, Z. Allox: compute allocation in hybrid clusters. In Proceedings of the
fifteenth european conference on computer Systems (2020), pp. 1â€“16.
[18] Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach
to hyperparameter optimization. Journal of Machine Learning Research 18, 185 (2018), 1â€“52.
[19] Li, M., Xiao, W., Yang, H., Sun, B., Zhao, H., Ren, S., Luan, Z., Jia, X., Liu, Y., Li, Y., et al. Easyscale: Elastic training
with consistent accuracy and improved utilization on gpus. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis (2023), pp. 1â€“14.
[20] Mahajan, K., Balasubramanian, A., Singhvi, A., Venkataraman, S., Akella, A., Phanishayee, A., and Chawla, S.
Themis: Fair and efficient {GPU} cluster scheduling. In 17th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 20) (2020), pp. 289â€“304.
[21] Misra, U., Liaw, R., Dunlap, L., Bhardwaj, R., Kandasamy, K., Gonzalez, J. E., Stoica, I., and Tumanov, A.
Rubberband: cloud-based hyperparameter tuning. In Proceedings of the Sixteenth European Conference on Computer
Systems (2021), pp. 327â€“342.
[22] Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I.,
et al. Ray: A distributed framework for emerging AI applications. In OSDI 18 (2018), pp. 561â€“577.
[23] Narayanan, D., Santhanam, K., Kazhamiaka, F., Phanishayee, A., and Zaharia, M. {Heterogeneity-Aware}
cluster scheduling policies for deep learning workloads. In 14th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 20) (2020), pp. 481â€“498.


--- Page 23 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
23
[24] Peng, Y., Bao, Y., Chen, Y., Wu, C., and Guo, C. Optimus: an efficient dynamic resource scheduler for deep learning
clusters. In Proceedings of the Thirteenth EuroSys Conference (2018), pp. 1â€“14.
[25] Psychas, K., and Ghaderi, J. A theory of auto-scaling for resource reservation in cloud services. Stochastic Systems
12, 3 (2022), 227â€“252.
[26] Qiao, A., Choe, S. K., Subramanya, S. J., Neiswanger, W., Ho, Q., Zhang, H., Ganger, G. R., and Xing, E. P. Pollux:
Co-adaptive cluster scheduling for goodput-optimized deep learning. In OSDI 21 (2021).
[27] Salvaris, M., Dean, D., Tok, W. H., Salvaris, M., Dean, D., and Tok, W. H. Microsoft ai platform. Deep Learning
with Azure: Building and Deploying Artificial Intelligence Solutions on the Microsoft AI Platform (2018), 79â€“98.
[28] Scully, Z., Grosof, I., and Harchol-Balter, M. The gittins policy is nearly optimal in the m/g/k under extremely
general conditions. Proceedings of the ACM on Measurement and Analysis of Computing Systems 4, 3 (2020), 1â€“29.
[29] Subramanya, S. J., Arfeen, D., Lin, S., Qiao, A., Jia, Z., and Ganger, G. R. Sia: Heterogeneity-aware, goodput-
optimized ml-cluster scheduling. In SOSP (2023).
[30] Xiao, W., Ren, S., Li, Y., Zhang, Y., Hou, P., Li, Z., Feng, Y., Lin, W., and Jia, Y. {AntMan}: Dynamic scaling on
{GPU} clusters for deep learning. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI
20) (2020), pp. 533â€“548.
[31] Zhang, X., Zhao, H., Xiao, W., Jia, X., Xu, F., Li, Y., Lin, W., and Liu, F. Rubick: Exploiting job reconfigurability for
deep learning cluster scheduling. Proceedings of Machine Learning and Systems 7 (2025).
[32] Zheng, P., Pan, R., Khan, T., Venkataraman, S., and Akella, A. Shockwave: Fair and efficient cluster scheduling for
dynamic adaptation in machine learning. In 20th USENIX Symposium on Networked Systems Design and Implementation
(NSDI 23) (2023), pp. 703â€“723.
A
Proof in offline setting
Lemma A.1 (No qeueing). For any well-behaved sample path A, no job queues under the optimal
policy.
Proof. Assume there is a job queueing in the optimal policy. Suppose the job is allocated ğ‘˜(ğ‘¡)
GPUs at time ğ‘¡. Then, we create a policy ğœ‹â€² that removes all the queueing from the optimal policy.
Mathematically, at any time ğ‘¡, ğœ‹â€² allocates the job ğ‘˜(ğ‘¡â€²) number of GPUs where ğ‘¡â€² is the smallest
time that satisfies ğ‘¡= ğ‘¡â€² âˆ’ğ‘(ğ‘¡â€²), where ğ‘(ğ‘¡â€²) is the jobsâ€™ total queueing time under the optimal
policy before time ğ‘¡â€².
Fig. 10. Illustration for the construction of policy ğœ‹â€².
Removing its waiting time makes the mean response time lower but leaves the total GPU-hours
the same, a contradiction. Note that for the online problem, we cannot create such a policy ğœ‹â€²
because we do not know what happens in the future.
â–¡
Lemma A.2 (Same allocation for same speedup). Under any well-behaved sample path A, any
type-ğ‘–job at epoch ğ‘—is always assigned the same number of GPUs, and this allocation does not change
until the epoch completes.


--- Page 24 ---
24
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
Proof. Suppose the optimal policy ğ‘ƒdoes not satisfy the statement in the lemma. Then either
ğ‘ƒassigns ğ‘˜1 â‰ ğ‘˜2 GPUs to different type-ğ‘–jobs at epoch ğ‘—, or ğ‘ƒassigns ğ‘˜1 â‰ ğ‘˜2 GPUs to a single
type-ğ‘–job at epoch ğ‘—at different times. Let ğ‘¥1,ğ‘¥2 be the work completed using ğ‘˜1 and ğ‘˜2 GPUs,
respectively.
We construct a policy ğ‘ƒâ€² which uses the same GPU-hours, but has lower mean response time
than ğ‘ƒ. ğ‘ƒâ€² is identical to ğ‘ƒexcept for the work ğ‘¥1 and ğ‘¥2: Instead of using two different assignments
ğ‘˜1 and ğ‘˜2, ğ‘ƒâ€² will choose a constant number of GPUs, ğ‘˜, to use in both instance. Let ğ‘¡1 =
ğ‘¥1
ğ‘ ğ‘–ğ‘—(ğ‘˜1) and
ğ‘¡2 =
ğ‘¥2
ğ‘ ğ‘–ğ‘—(ğ‘˜2) be the durations of each of the GPU assignments under ğ‘ƒ. We choose ğ‘˜to be the time
average of the two assignments by setting
ğ‘˜= ğ‘˜1 Â·
ğ‘¡1
ğ‘¡1 + ğ‘¡2
+ ğ‘˜2 Â·
ğ‘¡2
ğ‘¡1 + ğ‘¡2
.
(4)
The concavity of ğ‘ ğ‘–ğ‘—(ğ‘˜) implies that the total time to complete ğ‘¥1 and ğ‘¥2 is lower under ğ‘ƒâ€². To see
this, consider the average rate of work completion when processing ğ‘¥1 and ğ‘¥2 under both policies.
The average work rate under ğ‘ƒis
ğ‘¡1
ğ‘¡1 + ğ‘¡2
Â· ğ‘ ğ‘–ğ‘—(ğ‘˜1) +
ğ‘¡2
ğ‘¡1 + ğ‘¡2
Â· ğ‘ ğ‘–ğ‘—(ğ‘˜2).
The average work rate under ğ‘ƒâ€² is ğ‘ ğ‘–ğ‘—(ğ‘˜). By concavity, we have
ğ‘¡1
ğ‘¡1 + ğ‘¡2
Â· ğ‘ ğ‘–ğ‘—(ğ‘˜1) +
ğ‘¡2
ğ‘¡1 + ğ‘¡2
Â· ğ‘ ğ‘–ğ‘—(ğ‘˜2) â‰¤ğ‘ ğ‘–ğ‘—

ğ‘¡1
ğ‘¡1 + ğ‘¡2
Â· ğ‘˜1 +
ğ‘¡2
ğ‘¡1 + ğ‘¡2
Â· ğ‘˜2

= ğ‘ ğ‘–ğ‘—(ğ‘˜).
The total time to process ğ‘¥1 and ğ‘¥2 can be computed as the total work (ğ‘¥1 + ğ‘¥2) divided by the
average work rate. We thus have that ğ‘ƒâ€² completes the ğ‘¥1 + ğ‘¥2 work sooner than ğ‘ƒ. As a result, ğ‘ƒâ€²
has a lower mean response time than ğ‘ƒ.
It is easy to see that ğ‘ƒâ€² does not use more total GPU-hours than ğ‘ƒ. Specifically, note that ğ‘ƒuses
ğ‘˜1ğ‘¡1 + ğ‘˜2ğ‘¡2 GPU-hours to process ğ‘¥1 and ğ‘¥2. Let ğ‘¡â€²
1 and ğ‘¡â€²
2 be the times required to process ğ‘¥1 and ğ‘¥2
respectively under ğ‘ƒâ€². Then the GPU-hours used to process ğ‘¥1 and ğ‘¥2 under ğ‘ƒâ€² is
(ğ‘¡â€²
1 + ğ‘¡â€²
2)
 ğ‘˜1ğ‘¡1
ğ‘¡1 + ğ‘¡2
+ ğ‘˜2ğ‘¡2
ğ‘¡1 + ğ‘¡2

.
We have already shown that ğ‘¡â€²
1 + ğ‘¡â€²
2 â‰¤ğ‘¡1 + ğ‘¡2, giving
(ğ‘¡â€²
1 + ğ‘¡â€²
2)
 ğ‘˜1ğ‘¡1
ğ‘¡1 + ğ‘¡2
+ ğ‘˜2ğ‘¡2
ğ‘¡1 + ğ‘¡2

â‰¤(ğ‘¡1 + ğ‘¡2)
 ğ‘˜1ğ‘¡1
ğ‘¡1 + ğ‘¡2
+ ğ‘˜2ğ‘¡2
ğ‘¡1 + ğ‘¡2

= ğ‘˜1ğ‘¡1 + ğ‘˜2ğ‘¡2.
This leads to the contradiction, in that the policy ğ‘ƒâ€² is using less budget but achieving better
mean response time.
â–¡
Lemmas A.1 and A.2 show that the optimal policy in the offline setting is a fixed-width policy.
The following lemma develops an alternate formulation of the operating budget of a fixed width
policy.
Lemma A.3 (Operating budget of a fixed width policy). Given a well-behaved sample path
A, the operating budget of a fixed width policy with parameters {ğ‘˜ğ‘–ğ‘—} is
Â¯ğµ:= lim
ğ‘¡â†’âˆ
âˆ«ğ‘¡
0 ğ¾(ğ‘ )ğ‘‘ğ‘ 
ğ‘¡
(ğ‘)=
lim
ğ‘¡â†’âˆ
Ãğ‘›(ğ‘¡)
ğ‘–=1 ğµ(ğ‘–)
ğ‘¡
(ğ‘)=
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) .
Here ğµ(ğ‘–) is defined to be the GPU-hours used to complete the ğ‘–ğ‘¡â„arriving job.


--- Page 25 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
25
Proof. Part (a) of our claim says that tracking the total GPU usage at every time ğ‘¡is equivalent
to tracking the GPU-hours used to process each job, ğµ(ğ‘–).
To prove part (b), we show that we can take the limit of this equivalent formulation to prove our
claim. Note that the fixed width policy assigns ğ‘˜ğ‘–GPUs to any type-ğ‘–job. Hence, the GPU-hours
spent on the â„“ğ‘¡â„type-ğ‘–job is Ã
ğ‘—
ğ‘¥(â„“)
ğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) .
Thus we have that
lim
ğ‘¡â†’âˆ
Ãğ‘›(ğ‘¡)
ğ‘–=1 ğµ(ğ‘–)
ğ‘¡
= lim
ğ‘¡â†’âˆ
Ãğ‘€
ğ‘–=1
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1
Ã
ğ‘—
ğ‘¥(â„“)
ğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
ğ‘¡
=
ğ‘€
âˆ‘ï¸
ğ‘–=1
âˆ‘ï¸
ğ‘—
ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
Â©Â­
Â«
lim
ğ‘¡â†’âˆ
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1 ğ‘¥(â„“)
ğ‘–ğ‘—
ğ‘¡
ÂªÂ®
Â¬
,
where
lim
ğ‘¡â†’âˆ
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1 ğ‘¥(â„“)
ğ‘–ğ‘—
ğ‘¡
= lim
ğ‘¡â†’âˆ
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1 ğ‘¥(â„“)
ğ‘–ğ‘—
ğ‘›ğ‘–(ğ‘¡)
ğ‘›ğ‘–(ğ‘¡)
ğ‘¡
= ğœ†ğ‘–E[ğ‘‹ğ‘–ğ‘—] = ğœŒğ‘–ğ‘—.
â–¡
We now show that BOA is offline optimal for any well-behaved sample path A.
Lemma A.4. For any well-behaved sample path A, BOA is the optimal offline policy.
Proof. Lemmas A.1 and A.2 show that the optimal offline policy is a fixed-width policy. Thus,
it suffices to show that BOA is the optimal offline fixed-width policy.
For any job in A of typeğ‘–and size ğ‘¥ğ‘–ğ‘—at epoch ğ‘—, the JCT under a fixed width policy parameterized
with ğ‘˜ğ‘–is Ã
ğ‘—
ğ‘¥ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) . Thus, we have that
E[ğ‘‡] := lim
ğ‘¡â†’âˆ
Ãğ‘›(ğ‘¡)
ğ‘–=1 ğ‘‡ğ‘–
ğ‘›(ğ‘¡)
= lim
ğ‘¡â†’âˆ
ğ‘€
âˆ‘ï¸
ğ‘–=1
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1
Ã
ğ‘—ğ‘¥(â„“)
ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)ğ‘›(ğ‘¡)
=
ğ‘€
âˆ‘ï¸
ğ‘–=1
âˆ‘ï¸
ğ‘—
1
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
Â©Â­
Â«
lim
ğ‘¡â†’âˆ
Ãğ‘›ğ‘–(ğ‘¡)
â„“=1 ğ‘¥(â„“)
ğ‘–ğ‘—
ğ‘›(ğ‘¡)
ÂªÂ®
Â¬
= 1
ğœ†
ğ‘€
âˆ‘ï¸
ğ‘–=1
Ã
ğ‘—ğœŒğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) .
Moreover, by Lemma 4.5, the operating budget is Ã
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) . Thus solving the optimal set of ğ‘˜ğ‘–ğ‘—is
equivalent to solving the following optimization problem:
minimize
ğ‘˜ğ‘–ğ‘—
1
ğœ†
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
subject to
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) â‰¤ğ‘.
This is the same as the optimization problem (1) except for a constant 1/ğœ†. This shows that BOA
is the optimal fixed width policy.
â–¡


--- Page 26 ---
26
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
B
Convex optimization Translation
In this appendix, we rewrite the optimization problem (1) into a convex optimization problem. For
completeness, we state the optimization again as below:
minimize
ğ‘˜ğ‘–ğ‘—
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
subject to
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) â‰¤ğ‘,
ğ‘˜ğ‘–ğ‘—â‰¥1.
(5)
This is not immediately a convex optimization problem because the constraint terms
ğ‘˜ğ‘–ğ‘—
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—)
are not necessarily convex. However, by applying a change of variables, we can translate it into a
convex optimization problem.
Theorem B.1. The optimization problem (1) can be solved in the following two steps:
(1) Solve the convex optimization problem (6) to get the optimal solution ğ‘§ğ‘–ğ‘—;
(2) Get the optimal ğ‘˜ğ‘–ğ‘—= ğ‘ âˆ’1
ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—) where ğ‘ âˆ’1
ğ‘–ğ‘—is the inverse of the speedup function ğ‘ ğ‘–ğ‘—.
Proof. Define ğ‘§ğ‘–ğ‘—:=
1
ğ‘ ğ‘–ğ‘—(ğ‘˜ğ‘–ğ‘—) . Define ğ‘‘ğ‘–ğ‘—:= supğ‘˜â‰¥1 ğ‘ ğ‘–ğ‘—(ğ‘˜) (if it does not exist, let ğ‘‘ğ‘–ğ‘—= âˆ). If exists
ğ‘˜such that ğ‘ ğ‘–ğ‘—(ğ‘˜) = ğ‘‘ğ‘–ğ‘—, denote the smallest one by ğœ‰ğ‘–ğ‘—; Otherwise, let ğœ‰ğ‘–ğ‘—be âŠ¥.
Note that since ğ‘ ğ‘–ğ‘—is non-decreasing and concave, we have that it is strictly increasing in [1, ğœ‰ğ‘–ğ‘—]
([1, âˆ) if ğœ‰ğ‘–ğ‘—=âŠ¥). Thus we can define the function ğ›½ğ‘–ğ‘—= ğ‘ âˆ’1
ğ‘–ğ‘—to be the inverse of the speedup
function on [1,ğ‘‘ğ‘–ğ‘—] ([1,ğ‘‘ğ‘–ğ‘—) if ğœ‰ğ‘–ğ‘—=âŠ¥). Using the fact that ğ‘ ğ‘–is increasing, positive and concave, we
have that ğ›½ğ‘–ğ‘—is decreasing and convex.
By definition of ğ‘§ğ‘–ğ‘—, we have that ğ‘˜ğ‘–ğ‘—= ğ›½ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—). Substituting this into the optimization problem
(1), we have
minimize
ğ‘§ğ‘–ğ‘—
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘§ğ‘–ğ‘—
subject to
âˆ‘ï¸
ğ‘–,ğ‘—
ğœŒğ‘–ğ‘—ğ‘§ğ‘–ğ‘—ğ›½ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
) â‰¤ğ‘
1 â‰¥ğ‘§ğ‘–ğ‘—â‰¥1
ğ‘‘ğ‘–
.
(6)
Note that
ğ‘‘
ğ‘‘ğ‘§ğ‘–ğ‘—
(ğ‘§ğ‘–ğ‘—ğ›½ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
)) = ğ›½ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
) âˆ’1
ğ‘§ğ‘–ğ‘—
ğ›½â€²
ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
),
and
ğ‘‘2
ğ‘‘ğ‘§2
ğ‘–ğ‘—
(ğ‘§ğ‘–ğ‘—ğ›½ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
)) = âˆ’1
ğ‘§2
ğ‘–ğ‘—
ğ›½â€²
ğ‘–( 1
ğ‘§ğ‘–ğ‘—
) + 1
ğ‘§2
ğ‘–ğ‘—
ğ›½â€²
ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
) + 1
ğ‘§2
ğ‘–ğ‘—
ğ›½â€²â€²
ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
) = 1
ğ‘§2
ğ‘–ğ‘—
ğ›½â€²â€²
ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—
) > 0.
Thus the optimization problem (6) is a convex optimization, which we can numerically solve for
the optimal ğ‘§ğ‘–ğ‘—.
Finally, given the optimal ğ‘§ğ‘–ğ‘—, we can get the optimal ğ‘˜ğ‘–ğ‘—by letting ğ‘˜ğ‘–ğ‘—= ğ›½ğ‘–ğ‘—( 1
ğ‘§ğ‘–ğ‘—).
â–¡
C
Detailed BOA width calculator
We provide the detailed pseudocode for our BOA width calculator as follows.


--- Page 27 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
27
Algorithm 1: BOA Width Calculator
Input: Total budget ğ‘, Job classes 1, . . . , ğ‘€with epoch counts ğ‘™1, . . . ,ğ‘™ğ‘€.
Output: Optimal parameters {ğ‘˜âˆ—
ğ‘–ğ‘—} for the fixed-width policy.
// First Step: Generate a set of glue configurations.
1 G â†âˆ…
2 for ğ‘›â†1 to 50 do
3
foreach job class ğ‘–âˆˆ{1, . . . , ğ‘€} do
4
Define candidate set ğ‘†ğ‘–= {20, 21, . . . , 2âŒŠlog2 ğ‘™ğ‘–âŒ‹}
5
Sample ğ‘”ğ‘–uniformly from ğ‘†ğ‘–
6
end
7
Add configuration ğº= {ğ‘”1, . . . ,ğ‘”ğ‘€} to G
8 end
// Second Step: Solve a feasible solution for each glue configuration.
9 E[ğ‘‡]min â†âˆ
10 {ğ‘˜âˆ—
ğ‘–ğ‘—} â†âˆ…
11 foreach ğºâˆˆG do
12
Construct super-epochs by gluing adjacent epochs for each class ğ‘–according to ğ‘”ğ‘–âˆˆğº
13
ğ‘ğ‘Ÿğ‘¢ğ‘›â†ğ‘
14
ğ‘â€² â†âˆ
15
while ğ‘â€² > ğ‘do
16
Compute {ğ‘˜ğ‘–ğ‘—} by solving Optimization Problem (1) with budget ğ‘ğ‘Ÿğ‘¢ğ‘›
17
Round each ğ‘˜ğ‘–ğ‘—to the nearest integer on the non-decreasing concave hull of ğ‘ ğ‘–ğ‘—
18
Compute E[ğ‘‡] and total cost ğ‘â€² using {ğ‘˜ğ‘–ğ‘—} (Lemma 4.8)
19
if ğ‘â€² > ğ‘then
20
ğ‘ğ‘Ÿğ‘¢ğ‘›â†0.99 Â· ğ‘ğ‘Ÿğ‘¢ğ‘›
21
else
22
if E[ğ‘‡] < E[ğ‘‡]min then
23
E[ğ‘‡]min â†E[ğ‘‡]
24
{ğ‘˜âˆ—
ğ‘–ğ‘—} â†{ğ‘˜ğ‘–ğ‘—}
25
end
26
end
27
end
28 end
29 return {ğ‘˜âˆ—
ğ‘–ğ‘—}
D
Usage Statistics
As discussed in Section 6.1, the operating budget reported in Figure 4a reflects the effective GPU
hoursâ€”the duration during which GPUs are actively allocated to jobs. This metric excludes the
platform-dependent latency between when a node is marked for release by the allocator and when
it is actually reclaimed by the cloud platform.
Figure 11 extends Figure 4a by providing a comparison of the total usage for both policies. These
results demonstrate that these infrastructure-level reclamation overheads affect the operating
budget of both policies, but the negative effect on BOA Constrictor is smaller. Moreover, even with


--- Page 28 ---
28
Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, and Benjamin Berg
these infrastructure-level reclamation overheads, BOA Constrictor still significantly improves the
Pareto frontier of average JCT vs. operating budget.
Fig. 11. Actual Usage v.s. Effective Usage
E
Extension to heterogeneous servers
While the core of BOA Constrictor is derived for a homogeneous cluster, our theoretical results
(Section 4) generalize to settings with heterogeneous GPUs. We discuss the main idea in this
appendix without rigorous proofs.
In many cloud environments, customers can choose from various GPU types â„âˆˆğ», each with
distinct performance characteristics and rental costs. Here, similar to prior work [29], we focus on
the case that the ML training jobs can run on any number of GPUs of any type (which results in
different speed), but they cannot be run on different types of GPUs simultaneously.
E.1
Model with heterogeneous servers
For each job of class ğ‘–at epoch ğ‘—, its speedup function running on type â„GPUs is denoted by
ğ‘ (â„)
ğ‘–ğ‘—(ğ‘˜). In other words, if a class-ğ‘–job in epoch ğ‘—is run on ğ‘˜GPUs of type â„, the epoch completes
in time ğ‘‹ğ‘–ğ‘—/ğ‘ (â„)
ğ‘–ğ‘—(ğ‘˜). Notably, ğ‘ (â„)
ğ‘–ğ‘—(1) is not necessarily normalized to 1, representing the varying
processing speed across hardware.
Different GPUs have different rental price per hour. Let ğ‘(â„) denote the cost per hour for renting
a single GPU of type â„. The customer sets a time-average monetary operating budget ğ‘that the
policy must obey. We maintain the assumptions that ğ‘ (â„)
ğ‘–ğ‘—(ğ‘˜) is non-decreasing and concave for all
â„.
E.2
Structural Properties and Variables
In the heterogeneous setting without rescaling overheads, the following properties still hold:
â€¢ No Queueing: The optimal policy does not queue jobs; every job is assigned to a server
immediately upon arrival or epoch change.
â€¢ Type-Specific Fixed-Width: For a specific server type â„, all jobs of the same class and
epoch should be assigned the same number of type â„GPUs, ğ‘˜(â„)
ğ‘–ğ‘—.
Note that a job can only be assigned to one type at a time. Thus, we are able to introduce the
assignment fraction ğ‘(â„)
ğ‘–ğ‘—
âˆˆ[0, 1], representing the fraction of jobs of class ğ‘–at epoch ğ‘—assigned
to type â„GPUs (Ã
â„âˆˆğ»ğ‘(â„)
ğ‘–ğ‘—
= 1 for all ğ‘–, ğ‘—).


--- Page 29 ---
BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation
29
E.3
Heterogeneous BOA Optimization Problem
The goal is to minimize the average JCT subject to the monetary budget constraint ğ‘. The optimiza-
tion problem for the set of parameters {ğ‘˜(â„)
ğ‘–ğ‘—} and {ğ‘(â„)
ğ‘–ğ‘—} is:
minimize
ğ‘˜(â„)
ğ‘–ğ‘—,ğ‘(â„)
ğ‘–ğ‘—
âˆ‘ï¸
ğ‘–,ğ‘—,â„
ğ‘(â„)
ğ‘–ğ‘—ğœŒğ‘–ğ‘—
ğ‘ (â„)
ğ‘–ğ‘—(ğ‘˜(â„)
ğ‘–ğ‘—)
subject to
âˆ‘ï¸
ğ‘–,ğ‘—,â„
ğ‘(â„)ğ‘(â„)
ğ‘–ğ‘—ğœŒğ‘–ğ‘—ğ‘˜(â„)
ğ‘–ğ‘—
ğ‘ (â„)
ğ‘–ğ‘—(ğ‘˜(â„)
ğ‘–ğ‘—)
â‰¤ğ‘
âˆ‘ï¸
â„âˆˆğ»
ğ‘(â„)
ğ‘–ğ‘—
= 1,
âˆ€ğ‘–, ğ‘—
ğ‘˜(â„)
ğ‘–ğ‘—
â‰¥1,
ğ‘(â„)
ğ‘–ğ‘—
âˆˆ[0, 1]
(7)
Here, ğœŒğ‘–ğ‘—= ğœ†ğ‘–E[ğ‘‹ğ‘–ğ‘—] is the load contributed by epoch ğ‘—of type-ğ‘–jobs. The budget constraint
now captures the weighted sum of GPU-hours across different machine types, where each type â„is
weighted by its specific cost ğ‘(â„). This remains a convex optimization problem through a change of
variables similar to the one presented in Appendix B.
