--- Page 1 ---
Solver-in-the-Loop: MDP-Based Benchmarks for
Self-Correction
and Behavioral Rationality in Operations Research
Ruicheng Ao∗
David Simchi-Levi†
Xinshang Wang‡
Abstract
Operations Research practitioners routinely debug infeasible models through an
iterative process: analyzing Irreducible Infeasible Subsystems (IIS), identifying con-
straint conflicts, and systematically repairing formulations until feasibility is achieved.
Yet existing LLM benchmarks evaluate OR as one-shot translation—given a problem
description, generate solver code—ignoring this diagnostic loop entirely. We introduce
two benchmarks that place the solver in the evaluation loop. OR-Debug-Bench
evaluates iterative self-correction through 5,000+ problems spanning 9 error types;
each repair action triggers solver re-execution and IIS recomputation, providing de-
terministic, verifiable feedback.
OR-Bias-Bench evaluates behavioral rationality
through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic
deviations from closed-form optimal policies. Across 26 models and 12,000+ samples,
we find that domain-specific RLVR training enables an 8B model to surpass frontier
APIs: 95.3% vs 86.2% recovery rate (+9.1%), 62.4% vs 47.8% diagnostic accuracy
(+14.6%), and 2.25 vs 3.78 steps to resolution (1.7× faster). On OR-Bias-Bench,
curriculum training achieves the only negative ID→OOD bias drift among models
evaluated (-9.6%), reducing systematic bias by 48% (from 20.0% to 10.4%). These
results demonstrate that process-level evaluation with verifiable oracles enables targeted
training that outperforms scale.
Keywords: Operations Research, Large Language Models, Self-Correction, Reinforcement
Learning, Benchmark
1
Introduction
1.1
From Translation to Debugging
When a linear program returns Infeasible, the real work begins. An analyst must examine
the Irreducible Infeasible Subsystem (IIS)—the minimal subset of constraints that cannot be
∗Institute for Data, Systems, and Society (IDSS), Massachusetts Institute of Technology.
Email:
aorc@mit.edu
†Institute for Data, Systems, and Society (IDSS), Operations Research Center (ORC), and Department of
Civil and Environmental Engineering (CEE), Massachusetts Institute of Technology. Email: dslevi@mit.edu
‡Alibaba Group. Email: xinshang.w@alibaba-inc.com
1
arXiv:2601.21008v2  [cs.LG]  8 Feb 2026


--- Page 2 ---
Figure 1: Evaluation paradigms compared. Top: Static translation benchmarks evaluate
one-shot code generation with no execution feedback. Bottom: Our solver-in-the-loop
approach enables iterative self-correction through IIS feedback.
simultaneously satisfied—diagnose the root cause, and systematically repair the formulation.
This iterative debugging loop is where OR expertise manifests.
Yet existing benchmarks evaluate LLMs on Operations Research as one-shot translation:
given a problem description, generate solver code. This paradigm ignores the debugging
process central to real OR practice. Unlike generic error messages, IIS provides a minimal
certificate of infeasibility, enabling targeted, interpretable repairs—precisely the kind of
structured feedback that should enable targeted self-correction.
1.2
Self-Correction in Structured Domains
Recent work showed that 64.5% of LLM errors result when models fail to self-correct [Tie
et al., 2025]. However, CorrectBench focuses on general programming tasks and omits
the OR domain, where structured self-correction is uniquely suited. First, solvers provide
deterministic feedback—precise, verifiable signals such as IIS, slack values, and objective
bounds.
Second, verifiable ground truth enables mathematical checking of optimal
solutions. Third, the interpretable process means the diagnostic reasoning chain admits
structured evaluation.
This combination—deterministic oracle, verifiable outcomes, structured process—makes
OR an ideal testbed for studying self-correction. The solver-in-the-loop paradigm forces
agents to reason from IIS feedback, enabling systematic hypothesis refinement rather than
blind trial-and-error.
1.3
Behavioral Rationality in Operations
While debugging addresses upstream formulation errors, operational decisions face a distinct
downstream challenge. Concurrent work on AIM-Bench [Zhao et al., 2025] revealed systematic
behavioral biases in LLM inventory managers: a “pull-to-center” tendency where models over-
2


--- Page 3 ---
Table 1: Key results summary.
Finding
Metric
Result
8B surpasses frontier APIs
RR@5
95.3% vs 86.2%
Diagnostic accuracy gain
DA
62.4% vs 47.8%
Efficiency improvement
Steps
2.25 vs 3.78
OOD generalization
Bias drift
-9.6% (only negative)
Bias reduction
Bias diff
20.0%→10.4%
order when optimal quantity is low and under-order when it is high. This bias persists across
model scales, raising concerns about deploying LLMs in high-stakes operations management.
1.4
Contributions
We introduce two benchmarks and a training framework:
1. Two complementary benchmarks: OR-Debug-Bench (∼5,000 problems, 9 error
types) evaluates iterative debugging with solver-verified ground truth; OR-Bias-Bench
(2,000 instances, ID/OOD splits) evaluates behavioral rationality against closed-form
optimal policies.
2. RLVR training with process rewards: Domain-specific GRPO training with
outcome verification, diagnostic accuracy, and faithfulness penalties enables an 8B
model to surpass frontier APIs: +9.1% RR@5 (95.3% vs 86.2%) and +14.6% DA (62.4%
vs 47.8%).
3. Curriculum learning for generalization: Staged training achieves the only negative
ID→OOD bias drift (-9.6%) among models evaluated, reducing systematic bias by 48%
(from 20.0% to 10.4%).
4. Evaluation spanning 26 models: 12,000+ samples across frontier APIs and local
variants, with public release of benchmarks, training pipelines, and evaluation code.
2
Benchmark Setup
2.1
OR-Debug-Bench: Data Organization & Metrics
Each problem consists of: (1) a natural language description paired with sabotaged Gurobi
code producing Infeasible, (2) ground truth including original code, IIS constraints, and
target error type, and (3) Gurobi 11.0 IIS computation as the oracle. Figure 3 illustrates
a complete episode where the agent diagnoses an IIS conflict and repairs the model in two
steps.
3


--- Page 4 ---
Figure 2: Two-phase benchmark framework. Phase I (OR-Debug-Bench): Iterative
debugging where the agent receives Gurobi IIS feedback and repairs infeasible code. Phase
II (OR-Bias-Bench): Inventory decision-making verified against the closed-form optimal
policy Q∗= F −1(CR).
Table 2: Benchmark positioning.
OR-Debug-Bench is the first to evaluate iterative
debugging with deterministic oracle feedback.
Benchmark
Year
Task
Oracle
Multi-step
Self-Corr.
NL4Opt [Ramamonjison et al., 2023]
2022
NL→LP
–
–
–
OptiBench [Yang et al., 2024]
2024
Formulation
–
–
–
MAMO [Huang et al., 2024]
2024
Complex LP
–
–
–
ORLM [Huang et al., 2025]
2025
Formulation
–
–
–
AIM-Bench [Zhao et al., 2025]
2025
Inventory
Closed-form
–
–
SWE-bench [Jimenez et al., 2024]
2024
Code Debug
Unit Tests
✓
Limited
CorrectBench [Tie et al., 2025]
2025
Self-Corr.
–
–
General
OR-Debug-Bench
2026
Debugging
Gurobi IIS
✓
✓
OR-Bias-Bench
2026
Decision
Closed-form
–
✓
Training-Evaluation Split. Training uses <20% of total problems (996 of ∼5,000);
evaluation samples are drawn from a held-out pool excluding all training IDs. For OR-Bias-
Bench, training uses 900 samples across three curriculum stages with CR range [0.3, 0.7],
while evaluation covers broader ranges: ID [0.05, 0.95] and OOD [0.10, 0.89].
Metrics. RR@k (Recovery Rate) measures percentage reaching Optimal within k
steps. DA (Diagnostic Accuracy) = |D ∩IIS|/|IIS| measures root cause identification. OP
(Optimality Preservation) = 1 −|∆obj|/|objorig| measures solution quality.
2.2
OR-Bias-Bench: Data Organization & Metrics
OR-Bias-Bench comprises 2,000 newsvendor instances (1,000 ID + 1,000 OOD) evaluating
inventory decisions where ground truth Q∗= F −1(CR) is a closed-form analytical solution—no
4


--- Page 5 ---
Table 3: OR-Debug-Bench benchmark statistics and data splits.
Attribute
Value
Dataset Structure
Total Problems
∼5,000
Error Types
9 (A–I)
IIS Range
1–11 constraints
Difficulty Levels
Easy / Medium / Hard
Data Splits
Training (SFT)
696 trajectories
Training (RL)
300 prompts
Evaluation (Stratified)
450 (50 per type)
Max Steps per Episode
50
Table 4: OR-Bias-Bench data splits used in experiments. The benchmark contains 2,000
instances (1,000 ID + 1,000 OOD); we evaluate on stratified subsets.
Attribute
Train
ID Eval
OOD Eval
Samples
900
400
200
CR Range
[0.3, 0.7]
[0.05, 0.95]
[0.10, 0.89]
Demand Dist.
N(µ, σ)
N(µ, σ)
N(µ, σ)
µ Range
[50, 200]
[50, 200]
[50, 200]
σ Range
[10, 50]
[10, 50]
[10, 50]
solver interaction occurs.
Metrics. Rationality measures valid response percentage. Bias Diff = |E[Q/Q∗|CR >
0.5] −E[Q/Q∗|CR < 0.5]| measures pull-to-center. ID→OOD ∆measures generalization.
2.3
Evaluation Protocol
For OR-Debug-Bench, each episode proceeds: (1) agent receives initial state (problem,
sabotaged code, Infeasible, IIS), (2) agent outputs action, (3) environment executes and
returns new state, (4) repeat until Optimal or max steps. For OR-Bias-Bench, the agent
outputs a single order quantity Q.
We evaluate 26 models on 450 stratified OR-Debug-Bench samples per model and 600
OR-Bias-Bench samples (400 ID + 200 OOD) spanning frontier APIs and local variants.
3
Benchmark Construction
3.1
Saboteur-based Problem Generation
We design a “saboteur” pipeline that injects controlled errors into valid linear programs while
maintaining verifiable ground truth. Each generated error must: (1) produce a verifiable
5


--- Page 6 ---
Figure 3: Example OR-Debug-Bench episode. Left: The agent receives a sabotaged LP
where minimum requirements (60 + 50 + 0 = 110) exceed capacity (100). Right: The agent
(1) attempts optimization, (2) computes IIS, (3) reasons about the conflict, (4) relaxes the
key constraint, and (5) achieves Optimal status in 2 repair steps.
Table 5: Comparison of the two benchmarks: OR-Debug-Bench targets upstream model
repair through iterative solver interaction, while OR-Bias-Bench evaluates downstream
decision-making against analytical ground truth.
Aspect
OR-Debug-Bench
OR-Bias-Bench
Domain
Mathematical Programming
Operations Management
Task
Debug infeasible code
Inventory decision
Oracle
Gurobi IIS feedback
Closed-form Q∗= F −1(CR)
Interaction
Multi-step (up to 20)
Single-shot
LLM Challenge
Error parsing, code repair
Cognitive bias mitigation
Key Metrics
RR@k, DA, OP
Rationality, Bias Diff
Infeasible status, (2) yield a non-empty IIS containing the sabotaged constraint, and (3)
have a unique ground-truth fix restoring Optimal status.
Generation Pipeline. The pipeline operates in four stages: (1) source selection from
a feasible LP pool, (2) sabotage application using type-specific corruption, (3) verification
via Gurobi IIS computation, and (4) oracle labeling for evaluation. Of the initial candidate
pool, 87% pass all validation checks on first generation; the remainder require at most two
iterations. Full algorithmic details appear in Appendix C.1.
Robust Injection. Reliable error injection uses adaptive methods: Type A (direction
flip) uses slack-based constraint selection to improve success from 30% to 95%; Type C (upper
bound conflict) uses a 4-tier fallback strategy achieving 72% success. Complete algorithms
appear in Appendix C.8.
6


--- Page 7 ---
Table 6: Error type taxonomy for OR-Debug-Bench.
Type
Name
Difficulty
A
Direction Flip
Hard
B
Variable Type Error
Easy
C
Coefficient Modification
Easy
D
Contradicting Constraint
Hard
E
Multi-Constraint Conflict
Hard
F
Hidden Dependency
Hard
G
Cascading Conflict
Hard
H
IIS-Incomplete
Medium
I
Optimal Selection
Medium
Table 7: Difficulty calibration for OR-Debug-Bench. Levels are defined by baseline API
model performance.
Level
Error Types
Baseline RR@5
Easy
B, C
≥85%
Medium
H, I
70–85%
Hard
A, D, E, F, G
<70%
3.2
Anti-Pattern Measures
Three mechanisms prevent pattern-matching shortcuts. First, randomized naming in
Types G–I uses UUID-based identifiers to prevent models from exploiting semantic name
patterns. Second, hidden dependencies (Type F) create scenarios where the IIS reveals a
symptom constraint while the root cause lies elsewhere. Third, cascading conflicts (Type G)
require multi-step reasoning: fixing the primary conflict reveals a secondary one. Additional
details appear in Appendix C.10.
3.3
Newsvendor Problem Generation
For OR-Bias-Bench, we generate newsvendor scenarios with controlled critical ratios:
Q∗= µ + σ · Φ−1(CR),
CR = p −c
p −s
(1)
where Φ−1 is the standard normal inverse CDF, µ is mean demand, and σ is standard
deviation.
Stratified Sampling. We stratify by CR buckets: ID covers [0.05, 0.95] with 100+
samples per bucket; OOD covers [0.10, 0.89] testing intermediate-to-extreme values. The
4-level curriculum is detailed in Appendix C.3.
3.4
MDP Formulation
We formalize both benchmarks as Markov Decision Processes.
7


--- Page 8 ---
Table 8: Curriculum levels for OR-Bias-Bench. Each level targets specific bias phenomena
with controlled CR ranges and prompt complexity.
Level
CR Range
Prompt Style
Target Phenomenon
L1
[0.4, 0.6]
Clean
Foundation (neutral)
L2
[0.05, 0.2) ∪(0.8, 0.95]
Clean
Bias trigger (extreme)
L3
[0.3, 0.7]
+ Distractors
Robustness test
L4
[0.1, 0.9]
+ Censored
Expert inference
OR-Debug-Bench MDP. The state, action, and reward are:
S = (problemNL, code, status, iis, history, t)
A = {Get IIS, Check Slack, Relax,
Drop, Rewrite, Submit}
(2)
R = 0.5 · Routcome + 0.3 · Rdiagnosis + 0.2 · Refficiency
Actions are organized hierarchically: Diagnostic (information gathering), Repair (code
modification), and Meta (episode control). The oracle is Gurobi IIS computation providing
deterministic state transitions. Full specifications appear in Appendix D.1.
OR-Bias-Bench MDP. The bias evaluation uses a single-step MDP where A = {Q ∈
R+ : Q ≥0} and the oracle is the closed-form solution in Eq. (1).
Diagnostic Accuracy (DA). We measure alignment between diagnosed constraints
and ground truth:
DA = |diagnosed ∩IISGT|
|IISGT|
(3)
High RR@5 with low DA indicates “lucky” solutions that fix problems without understanding
root causes.
4
Training Methods
Figure 4 illustrates our two-track training approach. Both tracks start from Qwen3-8B and
apply SFT, but diverge in their RL phases: OR-Debug-Bench uses GRPO with solver-
verified rewards, while OR-Bias-Bench uses curriculum learning to overcome systematic
bias. The following subsections detail each component.
4.1
Foundation Model Selection
We selected Qwen3-8B-Instruct based on a pilot study (100 samples) showing +41.9%
post-SFT improvement headroom.
Qwen3-8B achieved 93.1% RR@5 after SFT with efficient token usage (2,100 tokens/episode).
See Appendix I for methodology.
8


--- Page 9 ---
Figure 4: Training pipeline overview. Track 1 trains the OR-Debug-Bench model: SFT on
teacher trajectories followed by GRPO with composite reward and optional PRM supervision.
Track 2 trains the OR-Bias-Bench model: SFT on rational responses followed by a three-
stage curriculum (Extreme →Boundary →Full) that targets pull-to-center bias.
Table 9: Pilot study: foundation model screening on OR-Debug-Bench validation.
Model
Base RR@5
+SFT RR@5
∆
Qwen3-8B
51.2%
93.1%
+41.9%
4.2
SFT Data Collection
We collect successful debugging trajectories from three teacher models—GPT-5.2-chat (40%),
o4-mini (35%), and DeepSeek-R1 (25%)—chosen for diverse reasoning strategies. Filtering
retains high-quality trajectories:
success = True ∧steps ≤5 ∧DA ≥0.5
(4)
This yields 696 of 1,247 trajectories (55.8% acceptance) averaging 2.3 steps with 68% diagnostic
accuracy. For OR-Bias-Bench, we collect 500 rational responses. Training examples are
shown in Appendix D.10.
4.3
GRPO Training with Composite Reward
Following DeepSeek-R1 [DeepSeek-AI, 2025], we use Group Relative Policy Optimization
with KL removal (β = 0), asymmetric clipping [0.2, 0.28], and LoRA (r = 16, α = 32).
Composite Reward Function. The reward balances outcome verification, diagnostic
quality, and efficiency:
R = 0.5 · Routcome + 0.3 · Rdiagnosis + 0.2 · Refficiency
(5)
where Routcome = +100 for Optimal and −50 otherwise, Rdiagnosis = DA·100, and Refficiency =
−1 per step. The 30% diagnostic weight addresses trial-and-error solutions achieving Optimal
9


--- Page 10 ---
Table 10: Three-stage curriculum for OR-Bias-Bench.
Stage
Focus
Samples
CR Distribution
1
Direction Learning
200
Extreme (0.1, 0.9)
2
Boundary Refinement
300
Near-boundary
3
Full Distribution
400
[0.2, 0.8]
without correct reasoning. A faithfulness penalty (−20) discourages repairs targeting non-IIS
constraints—without this penalty, models often achieve Optimal through indirect fixes that
mask the root cause. Training converges after 4 epochs with RR@5=95.0%. Full training
curves appear in Appendix D.5.
4.4
Process Reward Model (PRM)
Outcome-based rewards provide sparse feedback. We train a PRM (Qwen3-8B with LoRA)
to provide step-level supervision:
label =









1.0
if next status = Optimal or IIS shrinks
0.5
if diagnosis ∩IISGT ̸= ∅
0.2
if diagnostic action
0.0
otherwise
(6)
The PRM achieves AUC-ROC of 0.94 on held-out labels and improves DA by +4.7%
(68.0%→72.7%) at a cost of 3% RR@5 reduction. Details appear in Appendix D.4.
4.5
Curriculum Learning for Bias Mitigation
For OR-Bias-Bench, we use a three-stage curriculum targeting the pull-to-center bias:
Stage 1 (Direction Learning): Extreme CR values (0.1, 0.9) teach directional sensitivity.
Stage 2 (Boundary Refinement): Near-boundary values ([0.15, 0.25] and [0.75, 0.85])
refine magnitude estimation. Stage 3 (Full Distribution): Coverage across [0.2, 0.8]
consolidates learning.
This staged approach achieves 48% bias reduction (20.0%→10.4%) on OOD scenarios. Crit-
ically, curriculum training uniquely achieves negative ID→OOD drift (−9.6%), demonstrating
genuine generalization rather than memorization. Analysis appears in Appendix D.6.
5
Experiments
5.1
Experimental Setup
We evaluate models in two categories: (1) trained local models—8B-parameter models
fine-tuned using our SFT→GRPO pipeline, and (2) API baselines—22 frontier API models
10


--- Page 11 ---
Table 11: Trained local models on OR-Debug-Bench (450 stratified samples). All models
trained using the same SFT→GRPO pipeline on 696 expert trajectories.
Model
RR
RR@5
DA
Steps
Qwen3-8B
GRPO
100%
95.3%
62.4%
2.25
Curriculum
100%
94.0%
61.7%
2.22
DAPO
100%
93.8%
60.4%
2.31
SFT
99.8%
93.1%
60.8%
2.34
Llama-3.1-8B
GRPO
100%
97.3%
60.3%
1.82
SFT
100%
97.1%
60.4%
1.85
DeepSeek-R1-Distill-Qwen3-8B
GRPO
99.6%
88.9%
52.2%
3.30
SFT
99.3%
88.7%
52.2%
3.30
Table 12: API model baselines on OR-Debug-Bench (no training of LLM models). Full
results for all 22 API models in Appendix E.
Model
RR
RR@5
DA
Steps
o4-mini
97.8%
86.2%
47.8%
3.15
claude-sonnet-4
100%
86.2%
50.1%
3.71
o1
99.8%
82.9%
47.8%
3.78
gpt-5.2-chat
99.8%
81.8%
40.9%
3.72
gemini-2.5-flash
84.2%
70.7%
19.2%
3.23
Llama-3.3-70B
93.8%
60.9%
46.9%
4.81
DeepSeek-V3.2
99.3%
58.9%
44.8%
4.86
DeepSeek-R1
99.1%
56.7%
34.5%
5.08
without additional training. Local models include Qwen3-8B, Llama-3.1-8B, and DeepSeek-R1-
Distill-Qwen3-8B, all trained using the same pipeline on 696 expert trajectories. Experiments
run on 2×A100 80GB with SGLang inference (TP=2, concurrency=16).
5.2
Trained Local Models
Table 11 presents results for trained local models. All three model families (Qwen, Llama,
DeepSeek) benefit from the same training pipeline: supervised fine-tuning on expert trajec-
tories followed by GRPO reinforcement learning with solver-verified rewards. Llama-3.1-
8B-GRPO achieves the highest RR@5 (97.3%) with fastest repair (1.82 steps). Qwen3-8B-
GRPO achieves the highest DA (62.4%), indicating more accurate root-cause identification.
DeepSeek-R1-Distill-Qwen3-8B, starting from a reasoning-distilled base, shows competitive
performance (88.9% RR@5).
Training Effectiveness: Across all model families, GRPO training consistently improves
over SFT: +0.2–2.2% RR@5 improvement. The consistent gains across diverse architectures
11


--- Page 12 ---
Table 13: OR-Bias-Bench results (400 ID / 200 OOD samples). Bias = difference from
rational ordering.
Model
ID Bias
OOD Bias
∆
Status
claude-haiku-4.5
0.0%
3.6%
+3.6%
Best ID
Qwen3-8B-OM-SFT
4.9%
11.5%
+6.6%
OK
o4-mini
6.7%
7.7%
+1.0%
OK
Qwen3-8B-Curriculum
20.0%
10.4%
-9.6%
Best OOD
gpt-4.1
11.8%
15.4%
+3.6%
OK
Llama-3.3-70B
19.2%
12.5%
-6.7%
OK
gpt-5-mini
1.2%
53.3%
+52.1%
Degraded
demonstrate the generalizability of our training approach.
5.3
API Model Baselines
Table 12 presents API model baselines evaluated without any training. The best API models
(o4-mini, claude-sonnet-4) achieve 86.2% RR@5, substantially below the trained 8B models.
Key Finding: Domain-specific 8B models outperform all 22 API models. The best
trained model (Llama-3.1-8B-GRPO) achieves +11.1% RR@5 (97.3% vs 86.2% for best
API). Trained models complete repairs in 1.82–2.25 steps vs 3.15–5.08 for API models
(1.4–2.8× efficiency gain).
The trained models use a “diagnose once, repair correctly” pattern: 1.3 diagnostic actions
per episode vs 2.1 for API models, then targeted repairs. This reflects fundamentally different
reasoning—systematic elimination vs trial-and-error.
Per-Error-Type Performance. Domain-specific training provides larger gains on
harder problems (A, D–G): +9.6% average (94.4% vs 84.8%). Easy types (B, C) show smaller
gains (+3.0%) as baselines already exceed 95%. Medium types (H, I) improve by +14.0%
(95.0% vs 81.0%). Full breakdown appears in Appendix E.2.
Cost-Performance Trade-off. Local models achieve top performance at zero API cost.
Training cost (∼8 GPU-hours on 2×A100) amortizes across 10,000+ evaluations; at this
scale, local deployment reduces costs 50–100× vs API-based evaluation.
5.4
Bias Evaluation Results
Key Finding: Curriculum training achieves the best OOD generalization with −9.6% drift
(20.0%→10.4%), demonstrating genuine generalization. Most API models show increased
OOD bias (+3.6% to +15.5%); gpt-5-mini degrades from 7.2% to 22.7%. While claude-haiku-
4.5 achieves lowest ID bias (0%), curriculum training shows improved OOD performance
(−9.6% drift vs −6.7% for Llama-3.3-70B).
5.5
Inference Scaling Analysis
Figure 5 shows RR@k scaling. Qwen3-8B-GRPO at k = 3 (92.1%) already surpasses
o4-mini at k = 10 (90.7%). Token efficiency is 1.87× better: 5,370 tokens per success vs
12


--- Page 13 ---
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Steps (k)
30
40
50
60
70
80
90
100
Recovery Rate (%)
95% threshold
Recovery Rate at k Steps
Qwen3-8B-GRPO (Local)
Qwen3-8B-Curriculum (Local)
Qwen3-8B-SFT (Local)
o4-mini (API)
o1 (API)
gpt-5.2-chat (API)
Figure 5: Recovery Rate vs. attempt budget k. Qwen3-8B-GRPO achieves at k = 5
(95.3%) what the best API model requires k > 10 to approach.
Table 14: Ablation study on OR-Debug-Bench (200-sample validation set).
Configuration
RR@5
DA
∆RR@5
SFT Baseline
91.5%
68.0%
–
+ GRPO
92.0%
66.0%
+0.5%
+ Curriculum
95.0%
68.0%
+3.5%
+ PRM
92.0%
72.7%
+0.5%
Curriculum + GRPO (best)
95.3%
62.4%
+3.8%
10,016 for o4-mini. Hard problems show steeper scaling: +26.8% from k = 1 to k = 5 vs
+9.8% for Easy problems. Full analysis appears in Appendix H.
5.6
Ablation Study
Key Ablation Findings: (1) Curriculum pre-training provides +3.5% RR@5, the largest
single improvement; (2) PRM improves DA by +4.7% but trades off against RR@5 (−3%);
(3) Curriculum synergizes with GRPO (+3.8% over SFT), providing favorable initialization
for hard problems.
6
Related Work
OR Benchmarks. Table 2 compares OR-Debug-Bench and OR-Bias-Bench with
existing OR benchmarks. NL4Opt [Ramamonjison et al., 2023] introduced the task of trans-
lating natural language to linear program formulations. Subsequent work explored both
prompt-based approaches using in-context learning [Xiao et al., 2024, AhmadiTeshnizi et al.,
2024, Bertsimas and Margaritis, 2024] and learning-based methods fine-tuning on domain
13


--- Page 14 ---
data [Yang et al., 2024, Huang et al., 2025]. Other recent efforts include ORQA [Mosta-
jabdaveh et al., 2025] for OR reasoning evaluation, LLMOPT [Jiang et al., 2024], and
agent-based systems [Zhang and Luo, 2025, Thind et al., 2025]. OptMATH [Lu et al., 2025]
proposed bidirectional data synthesis for scalable training data generation. MAMO [Huang
et al., 2024] further contributed benchmarks spanning easy and complex LP instances with
solver integration. PILOT-Bench [Ao et al., 2026c] evaluates LLM workflow execution under
probabilistic tool failures and varying instruction quality. However, all existing benchmarks
evaluate static, one-shot formulation accuracy: a model produces code, and evaluation
checks correctness without iterative refinement or solver feedback. Recent work has extended
formulation benchmarks to dynamic programming: DP-Bench [Zhou et al., 2025] introduces
132 textbook-level DP problems and shows that even SOTA models struggle with stochas-
tic transitions, achieving only 59.1% accuracy. Our work addresses a complementary gap:
evaluating how models respond to solver feedback and iteratively correct their formulations,
focusing on LP infeasibility debugging rather than one-shot formulation.
Self-Correction in LLMs.
While Chain-of-Thought prompting [Wei et al., 2022]
improved LLM reasoning through step-by-step decomposition, it does not address error
correction when intermediate steps fail. CorrectBench [Tie et al., 2025] provided the first
systematic study of LLM self-correction, documenting that 64.5% of errors stem from
correction failures rather than initial mistakes. However, CorrectBench focuses on general
programming tasks and explicitly excludes the OR domain, where feedback is deterministic and
mathematically verifiable. SWE-bench [Jimenez et al., 2024] evaluates code debugging through
unit test feedback, but software testing fundamentally differs from mathematical optimization
verification: unit tests sample behavior while solvers provide complete, deterministic feedback.
Self-Refine [Madaan et al., 2023] demonstrated iterative refinement with self-generated
feedback, and STaR [Zelikman et al., 2022] showed that models can bootstrap reasoning
by learning from their own correct solutions. Kumar et al. [2024] further demonstrated
that RL can explicitly train models for iterative self-correction. However, these approaches
rely on heuristic quality signals rather than formal verification. Our benchmarks leverage
Gurobi’s IIS computation as a noise-free oracle, enabling precise evaluation of diagnostic
reasoning. For multi-agent debugging scenarios, AgentGit [Li et al., 2025] provides version
control abstractions that could complement our single-agent evaluation framework.
RLVR and Process Supervision. Reinforcement Learning with Verifiable Rewards
(RLVR) has emerged as a powerful paradigm for training reasoning models. Building on
Proximal Policy Optimization (PPO) [Schulman et al., 2017] and instruction tuning with
human feedback [Ouyang et al., 2022], alternative approaches include Direct Preference
Optimization (DPO) [Rafailov et al., 2023] for offline alignment and Group Relative Policy
Optimization (GRPO) [Shao et al., 2024] for online training. DeepSeek-R1 [DeepSeek-AI,
2025] demonstrated that GRPO with outcome verification can induce sophisticated reasoning
without explicit chain-of-thought supervision. DAPO [Yu et al., 2025] further improved
RLVR systems with KL-penalty removal and asymmetric clipping. T¨ulu 3 [Lambert et al.,
2025] extended this to a broader post-training pipeline. Recent analysis [Chu et al., 2025]
reveals that while SFT tends to memorize training patterns, RL promotes generalization—a
finding that motivates our two-stage training approach. For process supervision, “Let’s
Verify Step by Step” [Lightman et al., 2024] introduced human-annotated step-level rewards,
later automated by Math-Shepherd [Wang et al., 2024] through Monte Carlo estimation.
14


--- Page 15 ---
BiPRM [Zhang et al., 2025a] proposed bidirectional verification for mathematical reasoning.
PAVs [Setlur et al., 2025] formalized Process Advantage Verifiers for efficient alignment.
Our PRM training builds on these foundations, adapting process supervision to the OR
debugging domain where step-level progress can be automatically verified through IIS size
reduction. Concurrent work OR-R1 [Ding et al., 2025] also explores test-time RL for OR
modeling, though focusing on formulation rather than iterative debugging.
Verifiable Feedback in OR. Prior self-correction work relies on either self-generated
feedback, which can be unreliable, or heuristic metrics like test pass rates, which sample
rather than verify. Solver feedback differs fundamentally: IIS computation provides complete
and deterministic information about the infeasibility structure. We adapt RLVR to the OR
domain where: (1) the oracle provides verifiable rewards without human annotation, (2)
step-level progress is measurable through IIS size reduction, and (3) diagnostic accuracy can
be computed against ground-truth constraint labels—enabling fully automated training and
evaluation pipelines.
Test-Time Scaling and Inference Compute. Recent work explores scaling inference
compute through repeated sampling and verification. AlphaCode [Li et al., 2022] demon-
strated that pass@k metrics reveal headroom beyond single-attempt accuracy, with competi-
tive programming performance improving through sampling. DeepSeek-R1 [DeepSeek-AI,
2025] showed that extended reasoning chains—a form of implicit test-time scaling—improve
mathematical problem-solving. For code generation, best-of-n sampling with execution
feedback [Chen et al., 2021] remains a simple but effective strategy, and scheduling-based
approaches can further reduce inference costs [Ao et al., 2026b]. Our inference scaling analysis
(Section 5, Appendix H) contributes domain-specific findings: OR debugging exhibits similar
scaling behavior to code generation (+17% from k = 1 to k = 5), but domain-specific models
achieve superior sample efficiency, requiring 1.87× fewer tokens per successful solution.
Behavioral Rationality in LLMs. Recent work has examined LLMs as decision-makers
in OM contexts, including demand simulation [Zhang et al., 2025b] and sequential evaluation
under limited human feedback [Ao et al., 2026a]. AIM-Bench [Zhao et al., 2025] documented
the “pull-to-center” phenomenon in LLM inventory decisions, where models regress toward
mean predictions regardless of the optimal decision. This finding echoes classical behavioral
OR on human decision-making under uncertainty [Schweitzer and Cachon, 2000]. Our OR-
Bias-Bench benchmark extends AIM-Bench with explicit ID/OOD splits and demonstrates
that curriculum learning can mitigate these biases, achieving 48% bias reduction on OOD
scenarios.
Connection to Human Behavioral Biases. The pull-to-center bias in LLMs mirrors
documented human biases in newsvendor decisions [Schweitzer and Cachon, 2000, Kremer
et al., 2010]. Our curriculum training, which explicitly teaches directional sensitivity, can be
viewed as a cognitive debiasing intervention for LLMs, analogous to decision support systems
designed for human operators.
7
Conclusion
We introduced OR-Debug-Bench and OR-Bias-Bench, complementary benchmarks
evaluating iterative self-correction and behavioral rationality rather than one-shot formulation.
15


--- Page 16 ---
Through experiments on 12,000+ samples across 26 models, we demonstrate:
1. Domain-specific training enables smaller models to outperform frontier APIs:
8B Qwen models achieve +9.1% RR@5 (95.3% vs 86.2%) and +14.6% DA (62.4% vs
47.8%) at 1.7× efficiency.
2. Process evaluation reveals coincidentally correct repairs: DA distinguishes
correct diagnostic reasoning from trial-and-error success.
3. Curriculum learning enables genuine generalization: Staged training is the only
approach showing improved OOD performance (−9.6% bias drift).
4. PRM improves diagnostic quality: Step-level supervision increases DA by +4.7%.
Broader Significance.
Although our tasks are structurally simple, the Action →
Feedback →Plan Update loop is foundational for reliable agent deployment. Domain-specific
training within deterministic verification loops demonstrates a scalable path toward reliable
decision-making in mission-critical domains.
Future Work. Extensions include MIP/MINLP debugging, multi-period operations,
RAG integration with OR knowledge bases, and multi-agent collaboration.
Impact Statement
This paper presents work whose goal is to advance the field of Machine Learning. There are
many potential societal consequences of our work, none which we feel must be specifically
highlighted here.
References
Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. Optimus: Scalable optimization
modeling with (MI)LP solvers and large language models. arXiv preprint arXiv:2402.10172,
2024.
Ruicheng Ao, Hongyu Chen, Siyang Gao, Hanwei Li, and David Simchi-Levi. Best arm
identification with LLM judges and limited human. arXiv preprint arXiv:2601.21471,
2026a.
Ruicheng Ao, Gan Luo, David Simchi-Levi, and Xinshang Wang. Optimizing LLM inference:
Fluid-guided online scheduling with memory constraints. arXiv preprint arXiv:2504.11320,
2026b.
Ruicheng Ao, Zeping Min, Tingyu Zhu, Wotao Yin, and Xinshang Wang. PILOT-Bench:
Probabilistic interaction for LLM operations in tool-driven scenarios. In International
Conference on Learning Representations, 2026c. URL https://openreview.net/forum?
id=KTZ56LG7jZ.
16


--- Page 17 ---
Dimitris Bertsimas and Georgios Margaritis. Robust and adaptive optimization under a large
language model lens. arXiv preprint arXiv:2501.00568, 2024.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-
ating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans,
Quoc V Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: A comparative
study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025.
DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement
learning. arXiv preprint arXiv:2501.12948, 2025.
Zezhen Ding, Zhen Tan, Jiheng Zhang, and Tianlong Chen. OR-R1: Automating modeling
and solving of operations research optimization problem via test-time reinforcement learning.
arXiv preprint arXiv:2511.09092, 2025.
Chenyu Huang et al. ORLM: A customizable framework in training large models for automated
optimization modeling. Operations Research, 2025.
Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, and Benyou Wang. MAMO: A
mathematical modeling benchmark with solvers. arXiv preprint arXiv:2405.13144, 2024.
Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, and Yang Yu.
LLMOPT: Learning to define and solve general optimization problems from scratch. arXiv
preprint arXiv:2410.13213, 2024.
Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and
Karthik Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues?
In International Conference on Learning Representations, 2024. arXiv:2310.06770.
Mirko Kremer, Stefan Minner, and Luk N Van Wassenhove. Do random errors explain
newsvendor behavior? Manufacturing & Service Operations Management, 12(4):673–681,
2010. doi: 10.1287/msom.1100.0294.
Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate
Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to
self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze
Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu,
Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind
Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi,
and Hannaneh Hajishirzi. T¨ulu 3: Pushing frontiers in open language model post-training.
arXiv preprint arXiv:2411.15124, 2025.
17


--- Page 18 ---
Yang Li, Siqi Ping, Xiyu Chen, Xiaojian Qi, Zigan Wang, Ye Luo, and Xiaowei Zhang.
AgentGit: A version control framework for reliable and scalable LLM-powered multi-agent
systems. arXiv preprint arXiv:2511.00628, 2025.
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code
generation with AlphaCode. Science, 378(6624):1092–1097, 2022.
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy
Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step.
In International Conference on Learning Representations, 2024. arXiv:2305.20050.
Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, and Zaiwen Wen. OptMATH:
A scalable bidirectional data synthesis framework for optimization modeling. arXiv preprint
arXiv:2502.11102, 2025.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegr-
effe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bod-
hisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and
Peter Clark. Self-refine: Iterative refinement with self-feedback. In Advances in Neural
Information Processing Systems, volume 36, 2023. arXiv:2303.17651.
Mahdi Mostajabdaveh, Timothy Tin Long Yu, Samarendra Chandan Bindu Dash, Rindra
Ramamonjison, Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou, and Yong Zhang.
Evaluating LLM reasoning in the operations research domain with ORQA. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 39, pages 24902–24910, 2025.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language
models to follow instructions with human feedback. Advances in Neural Information
Processing Systems, 35:27730–27744, 2022.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward
model. arXiv preprint arXiv:2305.18290, 2023.
Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini, Bissan
Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi, Zirui Zhou, and
Yong Zhang. NL4Opt competition: Formulating optimization problems based on their
natural language descriptions. In Proceedings of the NeurIPS 2022 Competitions Track,
volume 220 of Proceedings of Machine Learning Research, pages 189–203. PMLR, 2023.
arXiv:2303.08233.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Maurice E Schweitzer and G´erard P Cachon. Decision bias in the newsvendor problem with a
known demand distribution: Experimental evidence. Management Science, 46(3):404–420,
2000. doi: 10.1287/mnsc.46.3.404.12070.
18


--- Page 19 ---
Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh
Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar.
Rewarding progress:
Scaling automated process verifiers for LLM reasoning. In International Conference on
Learning Representations, 2025. Spotlight. arXiv:2410.08146.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. DeepSeekMath: Pushing the limits of mathematical
reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Introduces
GRPO algorithm.
Raghav Thind, Youran Sun, Ling Liang, and Haizhao Yang. OptimAI: Optimization from
natural language using LLM-powered AI agents. arXiv preprint arXiv:2504.16918, 2025.
Guiyao Tie, Zenghui Yuan, Zeli Zhao, Chaoran Hu, Tianhe Gu, Ruihang Zhang, Sizhe Zhang,
Junran Wu, Xiaoyue Tu, Ming Jin, Qingsong Wen, Lixing Chen, Pan Zhou, and Lichao
Sun. Can LLMs correct themselves? a benchmark of self-correction in LLMs. In Advances
in Neural Information Processing Systems, volume 38, 2025. NeurIPS 2025 Datasets and
Benchmarks Track. arXiv:2510.16062.
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and
Zhifang Sui. Math-Shepherd: Verify and reinforce LLMs step-by-step without human anno-
tations. In Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 9426–9439, 2024. arXiv:2312.08935.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language
models.
Advances in Neural Information Processing Systems, 35:24824–24837, 2022.
arXiv:2201.11903.
Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han,
Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, et al. Chain-of-experts: When LLMs
meet complex operations research problems. In International Conference on Learning
Representations, 2024. OpenReview:HobyL1B9CZ.
Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang
Feng, Linqi Song, Xiaodan Liang, and Jing Tang. OptiBench meets ReSocratic: Measure
and improve LLMs for optimization modeling. arXiv preprint arXiv:2407.09887, 2024.
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan,
Gaohong Liu, Lingjun Liu, Xin Liu, et al. DAPO: An open-source LLM reinforcement
learning system at scale. arXiv preprint arXiv:2503.14476, 2025.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning
with reasoning. In Advances in Neural Information Processing Systems, volume 35, pages
15476–15488, 2022.
Bowen Zhang and Pengcheng Luo. OR-LLM-Agent: Automating modeling and solving of
operations research optimization problem with reasoning large language model. arXiv
preprint arXiv:2503.10009, 2025.
19


--- Page 20 ---
Lingyin Zhang et al. The bidirectional process reward model. arXiv preprint arXiv:2508.01682,
2025a.
Runze Zhang, Xiaowei Zhang, and Mingyang Zhao. Predicting effects, missing distributions:
Evaluating LLMs as human behavior simulators in operations management. arXiv preprint
arXiv:2510.03310, 2025b.
Xuhua Zhao et al. AIM-Bench: Evaluating decision-making biases of agentic LLM as inventory
manager. arXiv preprint arXiv:2508.11416, 2025.
Chenyu Zhou, Jingyuan Yang, Linwei Xin, Yitian Chen, Ziyan He, and Dongdong Ge. Auto-
formulating dynamic programming problems with large language models. arXiv preprint
arXiv:2507.11737, 2025.
Appendix Overview
This appendix provides supporting material organized as follows:
• Appendix A: Execution examples comparing trained and baseline models
• Appendix B: Task parameters, prompts, and evaluation criteria
• Appendix C: Benchmark construction algorithms and validation
• Appendix D: MDP formulation and training procedures
• Appendix E: Complete evaluation results
• Appendix H: Token efficiency and difficulty analysis
• Appendix I: Base model selection and negative results
A
Execution Examples
This appendix provides execution traces illustrating the difference between trained and
baseline models on OR-Debug-Bench problems.
A.1
Success Case: Type C Error (Coefficient Modification)
Problem Description.
A production planning LP where minimum requirements on
individual products exceed the total capacity constraint:
m = gp.Model("production")
x = m.addVars(3, name="x")
m.addConstr(x[0] + x[1] + x[2] <= 100, name="c1_total")
m.addConstr(x[0] >= 60, name="c2_min_0")
m.addConstr(x[1] >= 50, name="c3_min_1")
# KEY
m.addConstr(x[2] >= 0, name="c4_min_2")
20


--- Page 21 ---
The minimum sum (60 + 50 + 0 = 110) exceeds capacity (100), causing infeasibility.
Initial State.
• Status: Infeasible
• IIS: {c1 total, c2 min 0, c3 min 1}
• Ground Truth Fix: Relax c3 min 1 to x[1] ≥40
Qwen-GRPO Trajectory (2 steps).
1. Step 1: Diagnostic Action
• Action: Get IIS
• Response: “IIS contains c1 total, c2 min 0, c3 min 1. The minimum sum (60 +
50 + 0 = 110) exceeds capacity (100). Key constraint: c3 min 1 (x[1] ≥50) is
over-restrictive.”
• Diagnosis: [c3 min 1] ✓(matches ground truth)
2. Step 2: Repair Action
• Action: Relax(c3 min 1, x[1] ≥40)
• Response: “Modified constraint allows sum = 60+40+0 = 100 = capacity. Model
is now feasible.”
• Status: Optimal ✓
SFT Baseline Trajectory (5 steps).
1. Get IIS →[c1, c2, c3]
2. Check Slack(c1) →“c1 is binding”
3. Relax(c2) →Still Infeasible
4. Relax(c1) →Status changes but objective degraded
5. Rewrite(c3) →Finally Optimal
Analysis. The GRPO-trained model directly identifies the key constraint (c3 min 1)
and applies a minimal fix in 2 steps. The SFT baseline explores multiple constraints through
trial-and-error, taking 5 steps and initially relaxing the wrong constraint (c2).
21


--- Page 22 ---
Table 15: Model behavior comparison on composite error case.
Model
Steps
DA
Success
Analysis
Qwen-GRPO
7
58%
Yes
Systematic decomposition
o4-mini
12
42%
Yes
Trial-and-error
gpt-5.2-chat
15
33%
Yes
Excessive exploration
gpt-4.1-mini
20
17%
No
Error cascade
A.2
Challenge Case: Optimal Selection (Type I)
Problem Description. A composite error with 12 constraints in the IIS, involving resource
allocation, capacity, and flow balance conflicts simultaneously.
Model Behavior Comparison.
Error Cascade Pattern (gpt-4.1-mini).
• Steps 1–5: Correct diagnosis of constraints c3, c7
• Step 6: Incorrect fix of c3 introduces new conflict
• Steps 7–12: Attempts to fix cascading errors
• Steps 13–18: Reverts and retries with different approach
• Steps 19–20: Timeout without resolution
Analysis. API models often fail to reason about constraint interactions, leading to fixes
that introduce new conflicts. The trained model learns to decompose complex IIS sets and
address constraints in dependency order.
A.3
Available Actions
Action
Description
Modifies State
Get IIS
Compute Irreducible Infeasible Subsystem
No
Check Slack
Get constraint slack values
No
Check Bound
Get variable bound status
No
Relax(c, δ)
Modify constraint RHS
Yes
Drop(c)
Remove constraint entirely
Yes
Rewrite(c, expr)
Replace constraint expression
Yes
Submit
Submit current model for evaluation
Yes
A.4
Additional Success Examples
Example: Type E (Multi-Constraint Conflict). A transportation problem where
demand exceeds supply:
22


--- Page 23 ---
# Problem setup
m = gp.Model("transport")
supply = m.addVars(3, name="s")
demand = m.addVars(4, name="d")
# Supply constraints (total = 100)
m.addConstr(supply[0] <= 40, "s0_cap")
m.addConstr(supply[1] <= 35, "s1_cap")
m.addConstr(supply[2] <= 25, "s2_cap")
# Demand constraints (total = 115, infeasible!)
m.addConstr(demand[0] >= 30, "d0_min")
m.addConstr(demand[1] >= 35, "d1_min")
# KEY
m.addConstr(demand[2] >= 25, "d2_min")
m.addConstr(demand[3] >= 25, "d3_min")
# Flow balance
m.addConstr(quicksum(supply) == quicksum(demand))
IIS: {s0 cap, s1 cap, s2 cap, d0 min, d1 min, flow balance}
Qwen3-8B-GRPO Solution (2 steps):
1. Get IIS: “Total supply (100) is less than total demand (115). The largest demand
constraint (d1 min = 35) should be reduced.”
2. Relax(d1 min, -15): “Reducing demand 1 from 35 to 20 balances supply and demand.”
→Optimal
gpt-4.1 Solution (6 steps):
1. Get IIS →Lists all constraints
2. Relax(s0 cap, +10) →Still Infeasible
3. Relax(s1 cap, +10) →Still Infeasible
4. Check Slack(all) →Sees all slacks are 0
5. Relax(d3 min, -10) →Still Infeasible
6. Relax(d1 min, -15) →Optimal
Analysis: The trained model directly identifies that demand constraints are easier
to adjust than capacity constraints (a domain-specific heuristic) and targets the largest
demand. The API model attempts supply increases first, which don’t resolve the fundamental
imbalance.
23


--- Page 24 ---
A.5
OR-Bias-Bench Examples
ID Scenario: CR = 0.5 (Balanced).
Price: $60, Cost: $30, Salvage: $0
Mean demand: 100, Std: 20
CR = (60-30)/(60-0) = 0.5
Q* = 100 + 20 * Phi^{-1}(0.5) = 100
Model responses:
• gpt-5-mini: Q = 100 (Correct, bias = 0%)
• Qwen3-8B-Curriculum: Q = 100 (Correct)
OOD Scenario: CR = 0.1 (Low margin).
Price: $55, Cost: $50, Salvage: $5
Mean demand: 100, Std: 20
CR = (55-50)/(55-5) = 0.1
Q* = 100 + 20 * Phi^{-1}(0.1) = 74.4
Model responses:
• gpt-5-mini: Q = 95 (Over-order, bias = +28%)
• Qwen3-8B-Curriculum: Q = 76 (Correct, bias = +2%)
Analysis: At extreme CR values, gpt-5-mini exhibits severe pull-to-center bias, ordering
near the mean despite the optimal quantity being 32% below. The curriculum-trained model
correctly adjusts its order downward, demonstrating learned sensitivity to the critical ratio.
B
Task Function Components
This appendix details the task parameters, prompts, and evaluation criteria used in OR-
Debug-Bench.
B.1
Task and Tool Details
Table 16 lists the key parameters governing the OR-Debug-Bench environment.
Table 17 describes the available actions and their properties. Diagnostic actions gather
information without modifying the model state, while repair actions apply changes.
24


--- Page 25 ---
Table 16: Task parameters for OR-Debug-Bench.
Parameter
Value
Description
max steps
50
Maximum MDP steps be-
fore timeout
timeout
10s
Per-step solver timeout
iis method
minimal
Gurobi
IIS
computation
method
Table 17: Action space for OR-Debug-Bench.
Action
Description
Modifies State
Get IIS
Compute Irreducible Infeasi-
ble Subsystem
No
Check Slack
Get constraint slack values
No
Check Bound
Get variable bound status
No
Relax(c, δ)
Modify constraint RHS by δ
Yes
Drop(c)
Remove constraint entirely
Yes
Rewrite(c, expr)
Replace constraint expression
Yes
Submit
Submit current model for eval-
uation
Yes
B.2
Prompt Templates
We use three prompt variants in our experiments.
Baseline Prompt. The minimal prompt provides the problem description, code, and
IIS without additional guidance:
You are an OR debugging assistant. Given an
infeasible linear program, analyze the IIS
and suggest fixes to restore feasibility.
Problem: {problem_nl}
Code: {code}
Status: INFEASIBLE
IIS: {iis_constraints}
Provide your diagnosis and suggested action.
Chain-of-Thought (CoT) Prompt. The CoT prompt adds explicit reasoning steps:
You are an OR debugging assistant. Follow
this reasoning process:
1. ANALYZE: Examine each IIS constraint’s
role in the problem formulation
25


--- Page 26 ---
2. IDENTIFY: Determine the root cause
constraint causing infeasibility
3. PROPOSE: Suggest a minimal fix that
preserves problem semantics
4. VERIFY: Explain why the fix resolves
the conflict
Problem: {problem_nl}
Code: {code}
Status: INFEASIBLE
IIS: {iis_constraints}
Optimal Workflow Prompt. Used for SFT data collection, this prompt encodes expert
heuristics:
You are an expert OR debugger. Your task is:
1. Always get the IIS first
2. Identify the single most restrictive
constraint in the IIS
3. Apply minimal relaxation (prefer relax
over drop when possible)
4. Preserve original problem semantics
Problem: {problem_nl}
Code: {code}
Status: INFEASIBLE
IIS: {iis_constraints}
B.3
Tool Result Simulator
For training without solver access, we provide a deterministic simulator that estimates action
outcomes based on ground truth labels:
B.4
Task Result Evaluation
We categorize episode outcomes into three classes:
• Full Success: Status reaches Optimal and the Optimality Preservation score OP >
0.95 (objective value within 5% of original).
• Partial Success: Status reaches Optimal but the objective is degraded, with 0.8 <
OP ≤0.95.
• Failure: The model remains Infeasible after max steps, or OP ≤0.8 indicating the
fix changed the objective by >20%.
The RR@k metric counts full successes achieved within k steps. Partial successes are
counted for the overall Recovery Rate (RR) but not for RR@k to reward efficient diagnosis.
26


--- Page 27 ---
Algorithm 1 Tool Result Simulator
Require: Action a, State s, Ground truth G
Ensure: Simulated next state s′
1: if a.type ∈{GET IIS, CHECK SLACK} then
2:
s′ ←s {Info gathering, no change}
3: else if a.target ∈G.key constraints then
4:
if a.type = RELAX then
5:
s′.status ←OPTIMAL w.p. 0.9
6:
else if a.type = DROP then
7:
s′.status ←OPTIMAL w.p. 0.7
8:
end if
9: else if a.target ∈G.IIS then
10:
Reduce |IIS| by 1 w.p. 0.5
11: else
12:
s′.status ←INFEASIBLE {Wrong target}
13: end if
14: return s′
B.5
Gurobi Configuration
We use Gurobi 11.0.0 with the following configuration:
Table 18: Gurobi solver configuration.
Parameter
Value
IISMethod
1 (minimal IIS)
TimeLimit
10 seconds
OutputFlag
0 (suppress output)
Threads
4
MIPGap
0.01
Why Minimal IIS? Gurobi offers several IIS computation methods. We use the minimal
method (IISMethod=1) because:
• It produces the smallest possible IIS, making diagnosis more focused.
• It is deterministic across runs.
• It completes within reasonable time (typically <1s) for our problem sizes.
B.6
Evaluation Protocol
The evaluation follows a standardized multi-attempt protocol:
Stratified Sampling. To ensure balanced evaluation across difficulty levels (50 samples
per error type):
27


--- Page 28 ---
Algorithm 2 Multi-Attempt Evaluation Protocol
Require: Problem set P, model M, max attempts K, max steps T
Ensure: Metrics {RR@1, . . . , RR@K, DA, Steps}
1: for each problem p ∈P do
2:
success ←False
3:
first success step ←∞
4:
for k = 1 to K do
5:
Run episode with model M on problem p
6:
if episode reaches Optimal at step t then
7:
success ←True
8:
first success step ←min(first success step, t)
9:
end if
10:
end for
11:
Record success, first success step, DA, total steps
12: end for
13: Compute aggregate metrics
14: return Metrics
• 100 samples from Types B, C (Easy)
• 100 samples from Types H, I (Medium)
• 250 samples from Types A, D, E, F, G (Hard)
Total: 450 stratified samples per model evaluation.
B.7
Reproducibility Checklist
✓Code and data will be released upon publication
✓Random seeds fixed for all experiments
✓Hardware and software versions documented
✓Training hyperparameters fully specified
✓Evaluation protocol standardized across models
✓Gurobi configuration deterministic
C
Benchmark Construction Details
This appendix details the algorithms and validation procedures used to construct OR-Debug-
Bench and OR-Bias-Bench.
28


--- Page 29 ---
C.1
Saboteur Generation
The Saboteur generates controlled infeasibilities by applying targeted corruptions to feasible
LP instances. We describe two representative error types.
Type A: Direction Flip. This error reverses the sense of an inequality constraint,
turning a minimum requirement into a maximum limit or vice versa:
Algorithm 3 Sabotage: Direction Flip (Type A)
Require: Feasible model M, target constraint c
Ensure: Infeasible model M ′, ground truth (c, “flip”)
1: M ′ ←copy(M)
2: if c.sense = “ ≥” then
3:
c′.sense ←“ ≤”
4: else
5:
c′.sense ←“ ≥”
6: end if
7: Replace c with c′ in M ′
8: return M ′, (c, “flip”)
Example.
Original: x + y ≥10 (minimum production requirement).
Sabotaged:
x + y ≤10 (maximum limit, conflicts with other minimums).
Type E: Multi-Constraint Conflict. This error creates interlocked constraints that
both must be fixed:
Algorithm 4 Sabotage: Multi-Constraint Conflict (Type E)
Require: Feasible model M, demand constraints D, factor α > 1
Ensure: Infeasible model M ′, ground truth
1: M ′ ←copy(M)
2: c∗←arg maxc∈D c.rhs {Largest demand}
3: c∗.rhs ←c∗.rhs × α
4: assert P
c∈D c.rhs > capacity
5: return M ′, (c∗, “reduce rhs”)
Example. Original demands sum to 90 with capacity 100. Sabotaged demands sum to
117 (α = 1.3), exceeding capacity.
C.2
Newsvendor Generation
For OR-Bias-Bench, we generate newsvendor scenarios with controlled Critical Ratio (CR)
distributions.
The unit cost c derives from the target CR using the relationship CR = (p −c)/(p −s).
This ensures the generated scenario has the desired CR while maintaining realistic cost
structures.
29


--- Page 30 ---
Algorithm 5 Newsvendor Scenario Generation
Require: Target CR range [CRmin, CRmax]
Ensure: Scenario parameters (p, c, s, µ, σ, Q∗)
1: CR ∼Uniform(CRmin, CRmax)
2: p ∼Uniform(10, 100) {Unit price}
3: s ∼Uniform(0, 0.3p) {Salvage value}
4: c ←p −CR · (p −s) {Derive cost from CR}
5: µ ∼Uniform(50, 200) {Demand mean}
6: σ ∼Uniform(10, 50) {Demand std}
7: Q∗←µ + σ · Φ−1(CR) {Optimal quantity}
8: return (p, c, s, µ, σ, Q∗)
C.3
Newsvendor Curriculum Design
The 4-level curriculum for OR-Bias-Bench is designed to progressively test rationality
under increasing complexity.
Level Design Rationale.
• L1 (Foundations): CR ∈[0.4, 0.6] produces Q∗≈µ (within ±0.25σ), minimizing
pull-to-center effects. This establishes baseline formula application capability.
• L2 (Bias Traps): CR ∈[0.05, 0.2) yields Q∗< µ −σ (order less than mean minus
one standard deviation), while CR ∈(0.8, 0.95] yields Q∗> µ + σ. These extremes
maximally trigger the pull-to-center bias documented in behavioral operations research.
• L3 (Robustness): Distractors test whether models can filter irrelevant information.
Five distractor types were selected based on common supply chain context that does
not affect the single-period newsvendor decision.
• L4 (Expert): Censored demand requires inferring µ and σ from percentiles using
µ = P50 and σ = (P75 −P25)/1.35 for normal distributions. This tests parameter
inference capability beyond formula application.
Distractor Types. Table 19 lists the five distractor categories injected in Level 3
scenarios. Each distractor provides contextually plausible but decision-irrelevant information.
Table 19: Distractor types injected in Level 3 scenarios. None affect the optimal newsvendor
quantity.
Distractor Type
Example
Why Irrelevant
Warehouse capacity
“Storage limit: 500 units”
No capacity constraint in model
Competitor pricing
“Competitor sells at $45”
Single-firm decision
Shelf life
“Product expires in 30 days”
Single-period model
Historical trends
“Sales grew 10% last year”
Already reflected in µ, σ
Seasonal factors
“Holiday season approaching”
Already in demand parameters
30


--- Page 31 ---
Censored Demand (L4) Algorithm. Level 4 scenarios present demand information as
percentiles rather than distribution parameters. The generation algorithm:
1. Generate true parameters (µ, σ) from standard ranges
2. Compute percentiles: P25 = µ + σ · Φ−1(0.25), P50 = µ, P75 = µ + σ · Φ−1(0.75)
3. Present scenario using only (P25, P50, P75)
4. Models must infer: ˆµ = P50, ˆσ = (P75 −P25)/1.35
The constant 1.35 is the interquartile range of a standard normal distribution (Φ−1(0.75) −
Φ−1(0.25) ≈1.35).
C.4
Stratification and ID/OOD Design
In-Distribution (ID) Set. The 400-sample ID evaluation set contains 100 samples from
each curriculum level (L1–L4), ensuring balanced coverage:
• CR distribution spans the full range [0.05, 0.95]
• Prompt complexity ranges from clean (L1–L2) to noisy (L3) to censored (L4)
• Enables per-level performance analysis to diagnose specific failure modes
Out-of-Distribution (OOD) Set. The 200-sample OOD set contains only L3 and L4
scenarios (100 each), testing:
• Robustness to distractors (L3): Can models filter irrelevant context?
• Parameter inference capability (L4): Can models derive (µ, σ) from percentiles?
• Generalization beyond clean prompts: No L1–L2 samples in OOD
Stratification Procedure. Scenarios are stratified by CR bucket to ensure no bucket is
over- or under-represented:
1. Partition scenarios by curriculum level (L1–L4)
2. Within each level, bin by CR: very low (<0.2), low (0.2–0.4), neutral (0.4–0.6), high
(0.6–0.8), very high (>0.8)
3. Sample proportionally from each bin to achieve target distribution
4. Verify final CR histogram matches target uniform distribution
Dataset Scale. The complete newsvendor generator produces 57,000+ scenarios across
all curriculum levels and CR ranges. The released benchmark contains 2,000 instances (1,000
ID + 1,000 OOD); we evaluate on stratified subsets (400 ID + 200 OOD) ensuring:
• Statistical power: 100+ samples per level provides reliable performance estimates
• Diversity: All CR buckets represented to detect systematic biases
• Discriminative power: OOD tests generalization beyond training distribution
31


--- Page 32 ---
C.5
Quality Verification
Every benchmark instance passes a four-fold validation pipeline to ensure quality.
Algorithm 6 Four-Fold Validation Pipeline
Require: Original model M, sabotaged model M ′, fix f
Ensure: Boolean: instance passes validation
1: {Check 1: Original feasibility}
2: M.optimize()
3: if M.status ̸= OPTIMAL then
4:
return false
5: end if
6: {Check 2: Sabotaged infeasibility}
7: M ′.optimize()
8: if M ′.status ̸= INFEASIBLE then
9:
return false
10: end if
11: {Check 3: IIS validity}
12: M ′.computeIIS()
13: I ←{c : c.IISConstr}
14: if |I| = 0 or f.target /∈I then
15:
return false
16: end if
17: {Check 4: Fix effectiveness}
18: M ′′ ←apply fix(copy(M ′), f)
19: M ′′.optimize()
20: if M ′′.status ̸= OPTIMAL then
21:
return false
22: end if
23: return true
Validation Statistics. Out of 1,200 candidate OR-Debug-Bench instances generated:
• 98.2% passed Check 1 (original feasibility)
• 95.7% passed Check 2 (sabotaged infeasibility)
• 92.4% passed Check 3 (IIS contains target)
• 89.1% passed Check 4 (fix restores optimality)
The final benchmark contains 900 instances (75% acceptance rate) that pass all four
checks, ensuring every problem has a verified ground truth solution.
C.6
Error Type Examples
We provide concrete code examples for each error type, showing the original feasible formula-
tion and the sabotaged infeasible version.
32


--- Page 33 ---
Type A: Direction Flip.
# Original (feasible)
m.addConstr(x + y >= 10, "min_production")
# Requires at least 10 units
# Sabotaged (infeasible)
m.addConstr(x + y <= 10, "min_production")
# Contradicts other minimum requirements
The direction flip creates a contradiction when combined with other constraints that require
x + y > 10.
Type B: Variable Type Error.
# Original (feasible)
x = m.addVar(vtype=GRB.INTEGER, ub=10, name="x")
# Sabotaged (infeasible)
x = m.addVar(vtype=GRB.BINARY, name="x")
m.addConstr(x >= 2, "forcing")
# Binary variable cannot be >= 2
The variable type change combined with a forcing constraint creates infeasibility.
Type C: Coefficient Modification.
# Original (feasible)
m.addConstr(2*x + 3*y <= 100, "capacity")
# Sabotaged (infeasible)
m.addConstr(20*x + 30*y <= 100, "capacity")
# Scaled coefficients make constraint unsatisfiable
The modified coefficients make the constraint impossible to satisfy with existing bounds.
Type D: Contradicting Constraint.
# Original (feasible)
m.addConstr(x + y <= 100, "upper")
# Sabotaged (infeasible)
m.addConstr(x + y >= 150, "conflicting")
# Directly contradicts upper bound
The added constraint directly contradicts existing constraints.
Type E: Multi-Constraint Conflict.
# Sabotaged (infeasible)
m.addConstr(x + y <= 50, "e1")
m.addConstr(x + y >= 100, "e2")
# Both constraints cannot be satisfied
33


--- Page 34 ---
Interlocked constraints require fixing multiple constraints to restore feasibility.
Type F: Hidden Dependency.
# Sabotaged (infeasible)
aux = m.addVar(name="aux")
m.addConstr(aux == x + y, "def_aux")
m.addConstr(aux >= 200, "root_cause")
# Hidden
m.addConstr(x + y <= 100, "symptom")
# Shows in IIS
The root cause (aux ≥200) is not directly visible in the IIS.
Type G: Cascading Conflict.
# Sabotaged (infeasible)
m.addConstr(x <= 30, "g1")
# Initial IIS
m.addConstr(x >= 50, "g2")
# Hidden until g1 fixed
m.addConstr(x <= 100, "g3")
# Original bound
Fixing the first conflict reveals another; requires understanding the cascade.
Type H: IIS-Incomplete.
# Sabotaged (infeasible)
x.LB = 80
# Root cause (bound)
m.addConstr(x + y <= 50, "symptom")
# IIS shows symptom constraint, not the bound
The IIS shows the symptom constraint, but the root cause is a variable bound.
Type I: Optimal Selection.
# Sabotaged (infeasible)
m.addConstr(x >= 60, "lower")
m.addConstr(x <= 40, "upper")
# Multiple fixes possible, different OP impacts
Multiple repairs restore feasibility, but only one preserves the original optimal objective.
C.7
Dataset Statistics
Table 20 provides detailed statistics for both benchmarks.
C.8
Robust Injection Methods
Basic injection methods often fail when the seed problem structure does not align with the
corruption strategy. We develop robust methods that adaptively select targets based on
problem characteristics. This section details two representative robust methods.
Type A Robust: Slack-Based Constraint Selection. Rather than randomly selecting
a constraint to flip, we solve the original problem and rank constraints by slack magnitude.
Constraints with minimal slack are closest to their bounds and most likely to create infeasibility
when flipped.
34


--- Page 35 ---
Table 20: Dataset statistics for OR-Debug-Bench and OR-Bias-Bench.
Metric
OR-Debug-Bench
OR-Bias-Bench
Size
Total pool
∼5,000
2,000
Evaluation samples
450
600
(ID / OOD)
–
400 / 200
Distribution
Types A–D
44%
–
Types E–G
33%
–
Types H–I
22%
–
CR range
–
[0.05, 0.95]
Complexity
Avg constraints
9.9
1
Avg variables
8.4
1
Avg IIS size
4.3
–
This approach improves Type A injection success from 30% to 95%. Tightly-bound
constraints have minimal slack and are most sensitive to direction changes.
Type C Robust: 4-Tier Fallback Strategy. Type C (upper bound conflict) is
particularly challenging because it requires creating a conflict between an upper bound and
existing constraints. We implement a cascaded fallback strategy:
The 4-tier approach increases Type C success from 0% (when Tier 1 alone fails) to 72%
overall. Tier 4 provides a guaranteed fallback but produces simpler infeasibilities, so earlier
tiers are preferred.
Success Rate Comparison. Table 21 compares basic and robust injection methods
across all error types.
Table 21: Injection success rates: basic vs. robust methods.
Type
Basic
Robust
Robust Method
A
30%
95%
Slack-based selection
B
95%
98%
RHS sensitivity analysis
C
0%
72%
4-tier fallback
D
85%
96%
Bound gap targeting
E
70%
88%
Capacity utilization analysis
F
65%
85%
Bottleneck identification
G
60%
82%
Flow balance verification
H
45%
75%
Constraint interaction graph
I
40%
70%
Composite strategy selection
C.9
Rejection and Regeneration Statistics
Each benchmark instance must pass a four-fold validation pipeline. Table 22 shows pass rates
at each validation phase, broken down by error type.
Failure Analysis. The primary failure modes vary by error type:
35


--- Page 36 ---
Algorithm 7 Robust Type A Injection: Slack-Based Selection
Require: Feasible model M, num candidates k = 10
Ensure: Infeasible model M ′, ground truth (c∗, “flip”)
1: Solve M, extract slack values {sc} for all constraints
2: Sort constraints by |sc| ascending (tightest first)
3: for c in top-k candidates do
4:
M ′ ←flip direction(M, c)
5:
M ′.optimize()
6:
if M ′.status = Infeasible then
7:
M ′.computeIIS()
8:
if c ∈IIS(M ′) then
9:
return M ′, (c, “flip”)
10:
end if
11:
end if
12: end for
13: return failure
• Types A, C: Phase 2 failures occur when the flipped constraint does not interact with
other constraints to create infeasibility.
• Types E–G: Phase 3 failures occur when the IIS contains related but not the exact
target constraint.
• Types H–I: Phase 4 failures occur when the ground-truth fix does not fully restore
feasibility due to cascading effects.
Regeneration Iterations. Problems failing validation are regenerated with a different
seed problem or sabotage target. Table 23 shows the distribution of regeneration iterations
required.
87% of problems pass on first generation. Problems requiring ≥3 iterations (1%) are
typically Type C or H, where finding a valid sabotage target is difficult.
C.10
Anti-Gaming Design Rationale
Benchmarks can inadvertently reward pattern matching over genuine reasoning. We implement
several mechanisms to prevent gaming.
Why Randomized Naming Matters. In preliminary experiments with semantically-
named constraints (e.g., c key capacity, c target demand), we observed that models
achieved 15% higher apparent accuracy by learning to target constraints with “key” or
“target” in their names. This correlation existed because our ground-truth labeling naturally
assigned such names to important constraints.
By switching to UUID-based naming (e.g., c 53e476 ub, c 8a2f91 eq), we eliminate this
shortcut. The naming pattern for Types G, H, and I uses:
name = f"c_{uuid.uuid4().hex[:6]}_{sense_suffix}"
36


--- Page 37 ---
Algorithm 8 Robust Type C Injection: 4-Tier Fallback
Require: Feasible model M
Ensure: Infeasible model M ′, ground truth
1: {Tier 1: High dual value targeting}
2: Solve M, extract dual values {πc}
3: for c in constraints with c.sense = “ ≥” sorted by |πc| desc do
4:
Remove positive coefficient terms from c
5:
if results in infeasibility with c ∈IIS then
6:
return success
7:
end if
8: end for
9: {Tier 2: Coefficient sign flip}
10: for c in constraints with c.sense = “ ≤” do
11:
Flip signs of positive coefficients in c
12:
if results in infeasibility with c ∈IIS then
13:
return success
14:
end if
15: end for
16: {Tier 3: Coefficient scaling}
17: for c in all inequality constraints do
18:
Scale all coefficients in c by factor 10
19:
if results in infeasibility with c ∈IIS then
20:
return success
21:
end if
22: end for
23: {Tier 4: Guaranteed fallback}
24: Select variable x with largest feasible range
25: Add tight bounds: x ≤x∗, x ≥x∗+ ϵ
26: return success (guaranteed)
where sense suffix encodes only the constraint type (ub/lb/eq), not its semantic role.
Hidden Dependency Design (Type F). Type F problems are constructed so that the
IIS reveals a symptom constraint cs, but the root cause is a bound modification on variable
x elsewhere:
1. Original: x ≤100 with constraint cs : P
i aixi ≤b depending on x
2. Sabotaged: x ≤40 (hidden modification) causes cs to become infeasible
3. IIS contains cs but not the bound on x
Models that blindly relax cs fail because the real issue is the tightened bound on x. Solving
Type F requires reasoning about variable dependencies.
Cascading Conflict Design (Type G). Type G problems include two conflicts where
the second is masked until the first is resolved:
37


--- Page 38 ---
Table 22: Validation pass rates by error type across four phases (1,200 candidate instances).
Phase 1: original feasibility; Phase 2: sabotaged infeasibility; Phase 3: IIS contains target;
Phase 4: fix restores optimality.
Type
Phase 1
Phase 2
Phase 3
Phase 4
Final
A
100%
95%
92%
95%
82%
B
100%
100%
100%
100%
100%
C
100%
72%
68%
95%
62%
D
100%
98%
96%
98%
92%
E
100%
85%
80%
90%
72%
F
100%
82%
78%
88%
68%
G
100%
78%
75%
85%
64%
H
100%
75%
72%
82%
60%
I
100%
80%
78%
90%
70%
Overall
100%
85%
82%
91%
75%
Table 23: Regeneration iterations required to pass validation.
Iterations
0 (first try)
1
2
≥3
Percentage
87%
9%
3%
1%
Cumulative
87%
96%
99%
100%
1. Primary conflict: Constraint c1 conflicts with c2 (appears in initial IIS)
2. Secondary conflict: After fixing c1, constraint c3 conflicts with c4
The benchmark includes 15% of problems with such structures. Models that stop after one
fix fail; those that iterate diagnosis succeed.
Optimal Selection Challenge (Type I). Type I problems have multiple valid fixes,
but only one preserves the optimal objective value:
• Fix A: Relax c1 by 10% →Feasible, objective drops 5%
• Fix B: Relax c2 by 5% →Feasible, objective drops 15%
• Fix C: Modify c3 expression →Feasible, objective preserved
The ground truth labels Fix C as correct. This tests whether models consider solution quality,
not just feasibility.
C.11
Difficulty Calibration Procedure
We calibrate difficulty levels through iterative testing against baseline API models. The
procedure ensures each difficulty level provides meaningful differentiation.
Calibration Process.
38


--- Page 39 ---
1. Initial grouping: Group error types by semantic complexity and expected reasoning
requirements.
2. Baseline evaluation: Run API models on 50 problems per error type.
3. Target verification: Check if average RR@5 falls within target range for each group.
4. Group adjustment: Reassign error types between difficulty levels based on observed
performance.
5. Iteration: Repeat until stable groupings (typically 2–3 iterations).
Calibration Results. Table 24 shows the final calibrated parameters and observed SFT
performance.
Table 24: Difficulty calibration based on baseline API model performance.
Level
Error Types
Target RR@5
Observed RR@5
Easy
B, C
≥85%
90.5%
Medium
H, I
70–85%
78.5%
Hard
A, D, E, F, G
<70%
59.0%
Why These Ranges. The target ranges were chosen based on benchmark utility:
• ≥85% (Easy): Ensures baseline models can solve simple problems, establishing floor
performance. Types B and C fall in this category.
• 70–85% (Medium): Moderate difficulty with room for improvement. Types H and I
fall here.
• <70% (Hard): Challenges models significantly while remaining tractable. Types A,
D, E, F, and G require multi-step reasoning.
• <45%: Rejected as too difficult—problems often have ambiguous fixes or require
domain knowledge beyond general OR competence.
Difficulty calibration was based on empirical baseline API model performance rather than
theoretical metrics, as we found accuracy correlates more strongly with error type semantics
than with other factors.
D
MDP-Based Training Details
This appendix provides complete specifications of the MDP formulation and training proce-
dures.
39


--- Page 40 ---
D.1
State Space
The OR-Debug-Bench environment maintains a structured state representation with eight
components:
• Problem description: Natural language specification of the optimization problem
• Code: Current Gurobi/Pyomo model code
• Solver status: One of OPTIMAL, INFEASIBLE, UNBOUNDED, or ERROR
• IIS log: List of constraint names in the current Irreducible Infeasible Subsystem
• Slack values: Constraint slack values (populated after CHECK SLACK)
• Bound status: Variable bound information (populated after CHECK BOUND)
• History: Sequence of previous actions taken in the episode
• Step counter: Current step number (0 to max steps)
The state is serialized to a prompt string for the LLM, including the problem description,
current code, solver status, and relevant diagnostic information based on previous actions.
D.2
Action Space
Actions follow a hierarchical structure separating information gathering from model modifica-
tion:
Diagnostic actions (no state change, zero cost):
• GET IIS: Compute the Irreducible Infeasible Subsystem
• CHECK SLACK: Retrieve constraint slack values
• CHECK BOUND: Retrieve variable bound status
Repair actions (modify model, increment step counter):
• RELAX(constraint, delta): Increase or decrease the right-hand side by delta
• DROP(constraint): Remove the specified constraint from the model
• REWRITE(constraint, expr): Replace the constraint with a new expression
Meta actions:
• SUBMIT: Submit the current model for final evaluation
• RESTART: Reset to the initial sabotaged state
Diagnostic actions do not count toward the step limit for RR@k computation; repair and
meta actions increment the step counter.
40


--- Page 41 ---
Algorithm 9 Compute Reward
Require: State s, action a, next state s′, ground truth G
Ensure: Reward r ∈R
1: r ←0
2: {Outcome Reward (50%)}
3: if s′.status = OPTIMAL then
4:
r ←r + 0.5 × 100
5: else if s′.status = INFEASIBLE then
6:
r ←r + 0.5 × (−50)
7: end if
8: {Diagnostic Accuracy Reward (30%)}
9: if a contains diagnosis D then
10:
DA ←|D ∩G.IIS|/|G.IIS|
11:
r ←r + 0.3 × (DA × 100)
12: end if
13: {Efficiency Reward (20%)}
14: η ←max(0, (50 −s.step)/50)
15: r ←r + 0.2 × (η × 50)
16: {Faithfulness Penalty}
17: if a.type ∈{RELAX, DROP, REWRITE} then
18:
if a.target /∈s′.IIS then
19:
r ←r −20 {Penalize off-target fixes}
20:
end if
21: end if
22: return r
D.3
Reward Function
The composite reward function balances outcome, diagnostic accuracy, and efficiency:
The 50%/30%/20% weighting was determined through ablation (see Appendix G). The
faithfulness penalty discourages repairs that do not address the identified infeasibility source.
D.4
PRM Training Details
The Process Reward Model (PRM) provides step-level supervision for GRPO training.
Label Generation. We assign labels to each step in a trajectory based on progress
indicators:
Training Configuration.
The PRM achieves AUC-ROC of 0.94 on held-out step labels (309 test samples from 1,548
total labels), with correlation metrics: Pearson r=0.87 (p¡1e-90), Spearman r=0.82 (p¡1e-70).
This indicates good discrimination between productive (avg score 0.72) and unproductive
steps (avg score 0.50).
41


--- Page 42 ---
Algorithm 10 Generate Step Labels for PRM
Require: Trajectory τ = [(s0, a0), . . . , (sT, aT)], ground truth G
Ensure: Labels [y0, . . . , yT]
1: for t = 0 to T do
2:
if st+1.status = OPTIMAL then
3:
yt ←1.0 {Problem solved}
4:
else if t > 0 and |st+1.IIS| < |st.IIS| then
5:
yt ←1.0 {IIS shrinking}
6:
else if at.diagnosis ∩G.IIS ̸= ∅then
7:
yt ←0.5 {Correct diagnosis}
8:
else if at.type ∈{GET IIS, CHECK SLACK} then
9:
yt ←0.2 {Information gathering}
10:
else
11:
yt ←0.0 {No progress}
12:
end if
13: end for
14: return [y0, . . . , yT]
Table 25: PRM training hyperparameters.
Parameter
Value
Base model
Qwen/Qwen3-8B
Method
LoRA (r = 8, α = 16)
Epochs
3
Batch size
8
Learning rate
2 × 10−5
Warmup ratio
0.1
Metric for best model
AUC-ROC
D.5
GRPO Training Curves
Table 26 shows the evolution of key metrics during GRPO training.
Convergence Analysis.
• Reward variance decreases from 28.3 to 16.9, indicating policy stabilization as the
model converges to consistent behavior.
• RR@5 plateaus after epoch 4, with diminishing returns beyond this point. We select
the epoch-4 checkpoint for evaluation.
• DA improves more slowly than RR@5, confirming that learning accurate diagnosis is
harder than achieving feasibility. This motivates the 30% weight on diagnostic accuracy
in the reward function.
42


--- Page 43 ---
Table 26: GRPO training metrics by epoch.
Epoch
Mean Reward
Std Reward
RR@5 (val)
DA (val)
1
45.2
28.3
92.0%
56.1%
2
62.8
22.1
93.5%
58.4%
3
71.4
18.7
94.5%
60.8%
4
74.2
16.9
95.0%
62.1%
D.6
Curriculum Training Configuration
For OR-Bias-Bench, we use a three-stage curriculum over the Critical Ratio (CR) distribu-
tion:
Table 27: Curriculum stages for OR-Bias-Bench training.
Stage
CR Range
Samples
Purpose
1
{0.1, 0.9}
200
Learn extreme directions
2
[0.15, 0.25] ∪[0.75, 0.85]
300
Calibrate boundaries
3
[0.2, 0.8]
400
Full distribution coverage
Stage 1 trains on extreme CR values where the optimal direction is unambiguous (high
CR →order more, low CR →order less). Stage 2 refines decision boundaries. Stage 3 ensures
generalization across the full distribution.
This curriculum achieves -9.6% ID→OOD drift, the only method with negative drift
among compared approaches, demonstrating genuine generalization rather than in-distribution
memorization.
D.7
Complete Hyperparameter Tables
Table 28 and Table 29 provide complete training configurations.
D.8
Hardware and Software Environment
All experiments were conducted on the following setup:
D.9
Inference Configuration
For evaluation, we use SGLang for efficient batched inference:
D.10
Training Data Examples
We provide additional examples of the training data format.
SFT Data Format. Each SFT example contains an instruction, input state, and target
output:
43


--- Page 44 ---
Table 28: SFT training hyperparameters.
Parameter
Value
Base model
Qwen/Qwen3-8B
Method
LoRA (r = 16, α = 32)
Epochs
3
Batch size
4 (per GPU)
Gradient accumulation
4
Learning rate
2 × 10−5
LR scheduler
Cosine
Warmup ratio
0.03
Max sequence length
4096
Weight decay
0.01
Optimizer
AdamW (β1 = 0.9, β2 = 0.999)
Table 29: GRPO training hyperparameters.
Parameter
Value
Base model
Qwen3-8B-SFT (from SFT)
Method
LoRA (r = 16, α = 32)
Epochs
4
Group size
4 (samples per prompt)
Learning rate
5 × 10−6
KL coefficient β
0 (removed)
Clip range ϵ
[0.2, 0.28] (asymmetric)
Reward components
50%/30%/20%
Max steps per episode
50
GPU memory
2 × A100 80GB
{
"instruction": "Debug the infeasible
optimization model.",
"input": "Problem: mip_typeA_008
Status: INFEASIBLE
IIS: [’c0’, ’c1’, ’c_key_upper’]",
"output": "<think>
IIS contains 3 constraints.
c_key_upper limits x <= 50 but
c0 and c1 require x >= 80.
</think>
Action: RELAX(c_key_upper, 30)"
}
GRPO Prompt Format. GRPO training uses prompts without target outputs:
{
44


--- Page 45 ---
Table 30: Hardware and software configuration.
Component
Specification
GPU
2 × NVIDIA A100 80GB
CPU
AMD EPYC 7V13 (64 cores)
Memory
256 GB
Storage
NVMe SSD
CUDA
12.9.1
PyTorch
2.9.1
Transformers
4.44.0
TRL
0.24.0
SGLang
0.3.5
Gurobi
11.0.0
Table 31: Inference configuration.
Parameter
Value
Tensor parallel size
2
Batch size
16 (concurrent requests)
Max new tokens
2048
Temperature
0.0 (deterministic)
Attention backend
Triton
Sampling backend
PyTorch
"prompt": "Debug the infeasible model.
Problem: mip_typeE_042
Status: INFEASIBLE
IIS: [’supply’, ’d1’, ’d2’, ’d3’]",
"ground_truth": {
"key_constraint": "d2",
"expected_fix": "RELAX(d2, 15)"
}
}
The model generates completions, which are scored using the composite reward function.
OR-Bias-Bench Training Format. Newsvendor scenarios include all parameters:
{
"scenario": {
"price": 50, "cost": 35,
"salvage": 10, "mean": 100,
"std": 20, "CR": 0.375
},
"optimal_Q": 93.6,
"stage": 1
# Curriculum stage
45


--- Page 46 ---
}
E
OR-Debug-Bench: Complete Results
E.1
Main Results
Table 32 shows complete OR-Debug-Bench results for all 26 models evaluated. All models
report RR@5 and DA from multi-attempt evaluation.
Table 32: Complete OR-Debug-Bench results (450 stratified samples per model). All
models evaluated with multi-attempt protocol.
Model
Type
RR
RR@5
DA
Steps
Qwen3-8B-GRPO
Local
100%
95.3%
62.4%
2.25
Qwen3-8B-Curriculum
Local
100%
94.0%
61.7%
2.22
Qwen3-8B-DAPO
Local
100%
93.8%
60.4%
2.31
Qwen3-8B-SFT
Local
99.8%
93.1%
60.8%
2.34
claude-sonnet-4
API
100%
86.2%
50.1%
3.71
claude-haiku-4.5
API
99.3%
86.0%
53.1%
3.89
o4-mini
API
97.8%
86.2%
47.8%
3.15
o1
API
99.8%
82.9%
47.8%
3.78
gpt-5.2-chat
API
99.8%
81.8%
40.9%
3.72
qwen2.5-7b
API
97.8%
77.8%
40.1%
4.36
claude-opus-4
API
94.2%
76.9%
49.0%
3.92
o3
API
96.7%
75.8%
50.9%
4.23
DeepSeek-V3.2
API
99.3%
58.9%
44.8%
4.86
gpt-4.1
API
94.4%
71.6%
36.2%
4.41
gemini-2.5-flash
API
84.2%
70.7%
19.2%
3.23
Llama-3.3-70B
API
93.8%
60.9%
46.9%
4.81
gpt-5-mini
API
100%
66.9%
37.6%
4.74
gemini-2.5-pro
API
62.9%
62.7%
52.5%
0.83
qwen2.5-32b
API
98.9%
61.1%
32.0%
4.98
DeepSeek-R1
API
99.1%
56.7%
34.5%
5.08
qwen2.5-max
API
99.1%
54.9%
42.6%
5.97
qwen2.5-14b
API
88.9%
53.6%
35.4%
6.32
gemini-2.0-flash
API
85.6%
52.4%
18.3%
5.63
gpt-4.1-mini
API
93.1%
49.8%
26.0%
6.04
kimi-k2
API
55.3%
40.4%
25.6%
2.48
claude-3.7-sonnet
API
98.9%
31.6%
46.8%
8.06
E.2
Per-Error-Type Analysis
Per-error-type analysis is available upon request. Table 32 provides results for all 26 models.
46


--- Page 47 ---
E.3
Token Efficiency
Table 33 shows representative token usage for selected models.
Table 33: Token usage per episode on OR-Debug-Bench (representative models).
Model
Tokens
Steps
Tok/Success
Qwen3-8B-GRPO
2,011
2.25
2,109
Qwen3-8B-SFT
2,103
2.34
2,259
claude-sonnet-4
5,417
3.71
6,283
o4-mini
6,246
3.36
7,985
gpt-4.1
7,260
4.78
11,503
gpt-5-mini
10,544
5.12
18,110
Efficiency Observations.
• Token usage ranges from 2,000 (local) to 17,300 (claude-3.7-sonnet)—up to 8.6× gap.
• Tokens-per-success shows larger gaps (2,109 vs 54,847): up to 26× advantage for
Qwen3-8B-GRPO.
• Step count correlates with token usage (r = 0.79), but response length per step varies
by model.
E.4
RAG Ablation
Table 34 presents the complete RAG ablation across retrieval strategies and k values.
Table 34: RAG ablation on OR-Debug-Bench (200 samples).
Configuration
RR
RR@5
DA
Steps
No RAG (baseline)
99.8%
83.0%
80.0%
3.26
quick fix (k=3)
99.5%
80.0%
66.6%
2.58
reasoning (k=3)
100%
93.5%
51.8%
1.60
by type (k=1)
99.5%
86.5%
80.0%
2.19
by type (k=3)
100%
94.5%
82.0%
1.51
by type (k=5)
100%
96.5%
85.0%
1.62
by type (k=7)
100%
97.0%
85.0%
1.53
Retrieval Strategy Comparison.
• by type: Best overall. Retrieves cases with similar error types, providing relevant
examples without giving away the solution.
• reasoning: High RR@5 but lower DA. Provides complete reasoning chains that models
copy, achieving correct fixes without learning to diagnose.
47


--- Page 48 ---
• quick fix: Worst performance. Too shallow for complex errors, often omitting required
diagnostic steps.
k Value Analysis.
Performance improves from k=1 to k=5, then plateaus.
We
recommend k=5 as the default, balancing accuracy (+13.5% over baseline) with retrieval
cost.
E.5
Failure Analysis
Common Error Patterns in Failed Episodes. We analyzed 100 randomly sampled
failures from Qwen3-8B-GRPO:
Table 35: Failure pattern distribution for Qwen3-8B-GRPO.
Pattern
Count
Example
Wrong constraint identified
22
Relaxed c3 instead of c5
Insufficient relaxation
18
Relaxed by 5, needed 10
Cascading failure
42
Fix c1 →new IIS with c7
Timeout on complex IIS
11
12+ constraints in IIS
Objective degradation
7
Fix valid but OP < 0.8
Cascade Failure Analysis. The most common failure mode (42%) involves cascading
errors where fixing one constraint reveals another. These occur predominantly on Type H–I
problems:
Step 1: IIS = {c1, c3, c5}
Action: RELAX(c1, 10)
Step 2: IIS = {c3, c7, c9}
# New conflict!
Action: RELAX(c7, 5)
Step 3: IIS = {c5, c9, c11}
# Another new conflict
...continues until timeout
This pattern suggests the need for lookahead reasoning about constraint dependencies.
F
OR-Bias-Bench: Complete Results
Table 36 provides complete OR-Bias-Bench results for all 24 evaluated models, reporting
both rationality (valid numerical responses) and bias (deviation from rational ordering)
metrics across ID and OOD splits.
F.1
Rationality Analysis
Most models achieve >99% rationality, consistently producing valid numerical orderings. Two
exceptions stand out: o3 (93.1% ID) and claude-sonnet-4 (89.0% ID). These reasoning-heavy
models occasionally produce malformed outputs when over-analyzing simple ranking tasks.
48


--- Page 49 ---
Table 36: Complete OR-Bias-Bench results with ID/OOD breakdown (24 models, sorted
by ID Bias).
Rationality
Bias
Model
ID
OOD
ID
OOD
∆
claude-haiku-4.5
99.9%
99.9%
0.0%
3.6%
+3.6%
o3
93.1%
97.7%
0.4%
24.5%
+24.1%
qwen2.5-max
99.4%
98.5%
0.5%
25.0%
+24.5%
gpt-5-mini
99.6%
99.7%
1.2%
53.3%
+52.1%
claude-sonnet-4
89.0%
93.5%
1.5%
7.7%
+6.2%
gpt-4.1-mini
99.6%
99.9%
4.1%
12.1%
+8.0%
Qwen3-8B-OM-SFT
99.8%
99.6%
4.9%
11.5%
+6.6%
gemini-2.0-flash
97.6%
98.7%
5.9%
0.0%
-5.9%
qwen2.5-14b
99.1%
99.8%
5.9%
10.7%
+4.8%
o4-mini
98.5%
99.4%
6.7%
7.7%
+1.0%
qwen2.5-7b
98.1%
99.3%
7.4%
8.2%
+0.8%
kimi-k2
92.8%
97.6%
8.9%
2.3%
-6.6%
claude-3.7-sonnet
99.8%
99.4%
11.3%
18.1%
+6.8%
gpt-4.1
99.9%
100.0%
11.5%
14.6%
+3.0%
qwen2.5-32b
95.9%
99.8%
15.4%
5.0%
-10.4%
DeepSeek-V3.2
100.0%
99.9%
18.2%
11.3%
-6.9%
gemini-2.5-flash
96.2%
98.1%
18.2%
71.2%
+53.0%
claude-opus-4
92.5%
94.8%
19.0%
15.9%
-3.1%
Llama-3.3-70B
92.0%
96.9%
19.2%
12.5%
-6.7%
Qwen3-8B-OM-Curriculum
99.9%
99.6%
20.0%
10.4%
-9.6%
o1
98.2%
98.6%
23.2%
43.6%
+20.3%
DeepSeek-R1
98.6%
99.6%
44.1%
40.9%
-3.2%
Qwen3-8B-OM-GRPO
96.1%
98.4%
48.0%
33.8%
-14.2%
gemini-2.5-pro
98.2%
99.6%
97.9%
100.0%
+2.1%
F.2
ID Bias Patterns
ID bias spans from 0.0% (claude-haiku-4.5) to 97.9% (gemini-2.5-pro):
• Near-zero bias (<2%): claude-haiku-4.5, o3, qwen2.5-max, gpt-5-mini—these models
correctly apply EOQ/newsvendor logic on ID distributions.
• Moderate bias (5–20%): Most API models fall here, having partially but not fully
internalized OR principles.
• High bias (>40%): DeepSeek-R1 (44.1%), Qwen3-8B-OM-GRPO (48.0%), gemini-
2.5-pro (97.9%)—these models systematically deviate from rational orderings.
F.3
OOD Generalization
The ID→OOD shift reveals three distinct patterns:
49


--- Page 50 ---
Catastrophic Degradation. gpt-5-mini shows the most severe drift (+52.1%, from 1.2%
to 53.3%): low ID bias does not guarantee OOD generalization. gemini-2.5-flash degrades
similarly (+53.0%). These models likely memorize ID patterns rather than learn underlying
principles.
Stable Performance. o4-mini (+1.0%), qwen2.5-7b (+0.8%), and gpt-4.1 (+3.0%)
maintain consistent bias across distributions. These models appear to have internalized OR
principles more robustly.
OOD Improvement. Curriculum training achieves the only substantial OOD improve-
ment among trained models (−9.6%, from 20.0% to 10.4%). Other improving models include
kimi-k2 (−6.6%), gemini-2.0-flash (−5.9%), and qwen2.5-32b (−10.4%).
F.4
Local Model Comparison
The three Qwen3-8B-OM variants show distinct trade-offs:
• SFT: Best ID bias (4.9%) among local models, moderate OOD drift (+6.6%).
• Curriculum: Higher ID bias (20.0%) but best OOD performance (10.4%, −9.6%
improvement).
• GRPO: Highest bias on both splits (48.0% ID, 33.8% OOD). Outcome-focused RL
does not transfer well to bias mitigation.
This pattern confirms that curriculum training prioritizes generalization over ID memorization—
critical for real-world OR applications where distribution shift is common.
G
Training Ablation Studies
G.1
Reward Weight Ablation (OR-Debug)
Table 37 shows the effect of varying the diagnostic reward weight in the composite reward
function.
Table 37: Reward weight ablation on OR-Debug-Bench validation set.
Diagnostic Weight
RR@5
DA
Steps
20%
94.8%
54.2%
2.18
30%†
95.1%
58.6%
2.21
40%
95.3%
62.4%
2.25
50%
94.6%
64.1%
2.42
60%
93.2%
65.3%
2.78
†Selected for final training based on the RR@5/DA trade-off. The 30–40% range achieves
optimal balance; we use 30% in our final 50%/30%/20% (outcome/diagnosis/efficiency)
configuration.
50


--- Page 51 ---
Reducing diagnostic weight below 30% leads to repairs that achieve feasibility without
correctly identifying the root cause (low DA). Weights above 50% slow convergence by
over-penalizing exploratory actions, as shown by increased step counts.
G.2
Curriculum Stage Ablation (OR-Bias)
Table 38 compares different curriculum configurations for OR-Bias-Bench.
Table 38: Curriculum ablation on OR-Bias-Bench OOD set.
Configuration
OOD Bias
ID→OOD ∆
No curriculum (SFT only)
11.5%
+6.6%
Stage 1 only (extreme CR)
15.2%
+2.1%
Stages 1+2
12.1%
-3.4%
Full curriculum (1+2+3)
10.4%
-9.6%
The full three-stage curriculum achieves the best OOD generalization, with each stage
contributing to the final performance.
H
Token Efficiency and Difficulty Analysis
This appendix presents analysis of token efficiency and problem difficulty patterns across
models.
H.1
Token Efficiency
Token efficiency analysis is summarized in Table 33 (Appendix E.3).
Key Observations.
• Token efficiency: Local models require 2,000–2,100 tokens per episode compared
to 2,900–17,300 for API models. Tokens-per-success ranges from 2,109 (Qwen3-8B-
GRPO) to 54,847 (claude-3.7-sonnet).
• Step efficiency: Local models solve problems in 2.2–2.3 steps on average, while API
models range from 0.7 (DeepSeek-R1) to 8.1 steps (claude-3.7-sonnet).
• Correlation: Step count correlates with token usage (r = 0.79), though response
length per step varies widely across models (500–3,000 tokens).
H.2
Test-Time Compute Analysis
Token efficiency data is summarized in Table 33 (Appendix E.3). Key findings:
Efficiency Metrics.
51


--- Page 52 ---
• Tokens per episode: Local models average 2,000–2,100 tokens per episode, while API
models range from 5,400 (claude-sonnet-4) to 8,000+ (qwen2.5-7b). This 2.5–4× gap
reflects both shorter responses and fewer steps required.
• Tokens per success: Qwen3-8B-GRPO requires 2,109 tokens per successful episode,
compared to 6,283 for claude-sonnet-4 and 7,985 for o4-mini—representing 3.0–3.8×
efficiency advantages.
• Cost-adjusted performance: At equivalent token budgets, Qwen3-8B-GRPO can
attempt approximately 3× more problems than top API models, compounding the
per-problem accuracy advantage.
H.3
Scaling with Problem Difficulty
Based on per-error-type results from the main evaluation, we group error types by empirical
difficulty:
Table 39: Difficulty grouping based on type-averaged RR@5 across all 26 models.
Difficulty
Error Types
Avg RR@5
Characteristics
Easy
B (95%), C (86%)
90.5%
Clear diagnostic signals
Medium
H (82%), I (75%)
78.5%
Multi-step constraint interactions
Hard
A (61%), D (63%), E (69%), F (49%), G (53%)
59.0%
Semantic reasoning required
Observations.
• Type B is easiest: RHS miscalculation (Type B) produces clear diagnostic signals with
95% average success. Type A (direction flip), despite appearing simple, averages only
61%—flipping constraint directions creates conflicts with multiple existing constraints.
• Types F and G are hardest: Types F (capacity violation, 49%) and G (flow
imbalance, 53%) require reasoning about multiple interacting constraints, explaining
the difficulty gap.
• Local models excel uniformly: Qwen3-8B variants achieve >86% on all types
including the hardest (F: 94–100%, G: 86–98%), while API models show type-specific
weaknesses.
H.4
Recommendations for Practitioners
Based on our analysis, we provide the following recommendations:
1. Apply difficulty-adaptive resources: Allocate more compute to Hard types (A, D,
E, F, G; 59% avg RR@5) than Easy types (B, C; 90.5% avg).
2. Account for token efficiency: When comparing models, normalize by tokens-per-
success rather than raw accuracy. At 2,109 tokens/success vs 6,283–10,401 for API
models, local trained models offer 3–5× cost advantages.
52


--- Page 53 ---
3. Prefer local models for high-volume deployment: The combination of higher
RR@5 (95.3% vs 86.2%) and lower token cost (3× efficiency) makes trained local models
strongly preferable for production workloads.
I
Base Model Selection Study
This appendix documents our pilot study for selecting the foundation model for domain-
specific training, and provides analysis of why standard prompting approaches fail on the
OR debugging task.
I.1
Candidate Model Screening
We evaluated Qwen3-8B-Instruct as the foundation model for domain-specific training.
Selection criteria included: (1) base performance on OR debugging, (2) improvement potential
with SFT, and (3) inference efficiency.
Table 40: Foundation model screening on OR-Debug-Bench validation set (100 samples).
Model
Params
Base RR@5
+SFT RR@5
∆
Tokens/ep
Qwen3-8B-Instruct
8B
51.2%
93.1%
+41.9%
2,100
Key Findings.
• Base performance: Qwen3-8B achieves 51.2% RR@5 without any domain-specific
training, demonstrating reasonable out-of-the-box capability for structured reasoning.
• SFT improvement: Qwen3-8B improves by +41.9% with SFT, indicating high
receptivity to domain adaptation.
• Efficiency: Qwen3-8B generates 2,100 tokens per episode, providing efficient inference
for iterative debugging.
Selection Rationale. We selected Qwen3-8B-Instruct as the foundation model based on
three factors:
1. Strong post-SFT performance: 93.1% RR@5 after SFT demonstrates successful
domain adaptation.
2. Good improvement potential: +41.9% delta suggests the model effectively learns
from demonstration data.
3. Practical efficiency: Reasonable token footprint reduces training and inference costs.
I.2
Negative Result: Standard Prompting Approaches Fail
Before investing in domain-specific training, we evaluated whether standard prompting
approaches could achieve competitive performance on OR-Debug-Bench. This section
documents these negative results.
53


--- Page 54 ---
I.2.1
Zero-Shot Chain-of-Thought
We evaluated zero-shot CoT prompting with the instruction: “Let’s think step by step about
how to debug this infeasible model.”
Table 41: Zero-shot CoT performance on OR-Debug-Bench (200 samples).
Model
RR@5
DA
Avg Steps
Notes
gpt-5.2-chat + CoT
38.5%
22.1%
6.8
Verbose, unfocused
o4-mini + CoT
41.2%
28.4%
5.9
Better structure
Qwen3-8B + CoT
23.0%
15.6%
7.2
Often loops
Why Zero-Shot CoT Fails.
• No feedback loop: CoT generates a single reasoning chain without iterating based
on solver output. Models attempt repairs without verifying whether they resolved the
infeasibility.
• Generic reasoning patterns: CoT prompting elicits general problem-solving steps
(“identify the issue, propose a solution, verify”) that lack domain-specific diagnostic
actions like GET IIS.
• Premature commitment: Models commit to repair strategies early in the chain
without exploring the constraint structure, leading to suboptimal fixes.
I.2.2
Few-Shot In-Context Learning
We evaluated 1-shot and 3-shot ICL with curated examples of successful debugging trajectories.
Table 42: Few-shot ICL performance on OR-Debug-Bench (200 samples).
Configuration
RR@5
DA
Avg Steps
gpt-5.2-chat (0-shot)
38.5%
22.1%
6.8
gpt-5.2-chat (1-shot)
52.3%
35.2%
4.9
gpt-5.2-chat (3-shot)
58.1%
41.6%
4.2
Qwen3-8B (0-shot)
23.0%
15.6%
7.2
Qwen3-8B (1-shot)
31.4%
24.3%
6.1
Qwen3-8B (3-shot)
38.7%
29.8%
5.4
Why Few-Shot ICL Is Insufficient.
• Limited generalization: 3-shot ICL improves performance by +19.6% for gpt-5.2-chat
but still falls far short of SFT (+41.9% for Qwen3-8B).
• Context length constraints: Each debugging trajectory requires 500–1000 tokens.
With 3 examples, the prompt consumes 1.5–3K tokens, limiting remaining context for
the actual problem.
54


--- Page 55 ---
• Example selection sensitivity: Performance depends on example choice. Random
examples achieve only 48.2% RR@5, while carefully selected examples reach 58.1%—but
this selection requires domain expertise.
I.2.3
Comparison Summary
Table 43 summarizes why domain-specific training outperforms prompting approaches by
54+ percentage points.
Table 43: Comparison of approaches on OR-Debug-Bench (Qwen3-8B base).
Approach
RR@5
DA
Gap to SFT
Zero-shot
18.4%
12.3%
-74.7%
Zero-shot + CoT
23.0%
15.6%
-70.1%
1-shot ICL
31.4%
24.3%
-61.7%
3-shot ICL
38.7%
29.8%
-54.4%
SFT
93.1%
60.8%
—
SFT + GRPO
95.3%
62.4%
+2.2%
Implication. The 54-point gap between 3-shot ICL (38.7%) and SFT (93.1%) demon-
strates that OR debugging cannot be solved through prompting alone. The task requires:
1. Iterative interaction: Learning to use solver feedback across multiple turns.
2. Domain-specific actions: Acquiring the diagnostic vocabulary (GET IIS, CHECK SLACK).
3. Strategy patterns: Learning when to diagnose vs when to repair, and how to calibrate
repair magnitudes.
These capabilities cannot be induced through few-shot examples alone.
I.3
Analysis: What Makes OR Debugging Hard for Prompting?
We identify three structural properties of OR debugging that make it resistant to prompting-
based solutions:
1. Multi-Turn Dependency. Unlike single-turn tasks where CoT can decompose
reasoning, OR debugging requires acting on solver feedback across multiple turns. The
optimal action at step t depends on the solver response at step t −1, which cannot be
simulated within a single prompt.
2. Precise Action Syntax. The action space requires exact syntax (e.g., RELAX(c key upper,
30)). Small errors in constraint names or numeric values lead to failed repairs. This precision
requirement exceeds what few-shot examples can reliably demonstrate.
3. State-Dependent Strategy. The optimal strategy varies with problem structure:
• Small IIS (2–3 constraints): Direct repair often succeeds.
• Medium IIS (4–7 constraints): Diagnosis before repair improves success rate.
55


--- Page 56 ---
• Large IIS (8+ constraints): Systematic decomposition is required.
Few-shot prompting cannot convey these conditional strategies without extensive examples
that exceed context limits.
I.4
Implications for Future Work
Our negative results suggest several directions for improving prompting-based approaches:
1. Tool-augmented prompting: Providing models with explicit solver interfaces (rather
than expecting them to generate action syntax) may reduce syntax errors.
2. Retrieval-augmented generation: Our RAG experiments (Appendix E.4) show that
retrieving similar solved cases improves RR@5 by +13.5%, partially closing the gap to
SFT.
3. Multi-turn demonstration: Future work could explore demonstration formats
that explicitly show the feedback loop across turns, though this faces context length
challenges.
These approaches address symptoms rather than the fundamental issue: prompting
cannot instill the procedural knowledge that SFT provides through gradient-based learning
on hundreds of examples.
56
