--- Page 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
1
SCRec: A Scalable Computational Storage System
with Statistical Sharding and Tensor-train
Decomposition for Recommendation Models
Jinho Yang, Graduate Student Member, IEEE, Ji-Hoon Kim, Graduate Student Member, IEEE,
Joo-Young Kim, Senior Member, IEEE,
Abstract—Deep Learning Recommendation Models (DLRMs)
play a crucial role in delivering personalized content across
web applications such as social networking and video streaming.
However, with improvements in performance, the parameter size
of DLRMs has grown to terabyte (TB) scales, accompanied by
memory bandwidth demands exceeding TB/s levels. Furthermore,
the workload intensity within the model varies based on the
target mechanism, making it difficult to build an optimized
recommendation system. In this paper, we propose SCRec, a
scalable computational storage recommendation system that can
handle TB-scale industrial DLRMs while guaranteeing high
bandwidth requirements. SCRec utilizes a software framework
that features a mixed-integer programming (MIP)-based cost
model, efficiently fetching data based on data access patterns
and adaptively configuring memory-centric and compute-centric
cores. Additionally, SCRec integrates hardware acceleration cores
to enhance DLRM computations, particularly allowing for the
high-performance reconstruction of approximated embedding
vectors from extremely compressed tensor-train (TT) format.
By combining its software framework and hardware accelera-
tors, while eliminating data communication overhead by being
implemented on a single server, SCRec achieves substantial
improvements in DLRM inference performance. It delivers up
to 55.77× speedup compared to a CPU-DRAM system with no
loss in accuracy and up to 13.35× energy efficiency gains over a
multi-GPU system.
Index Terms—deep learning recommendation model, near-data
processing, statistical sharding, tensor-train decomposition.
I. INTRODUCTION
R
RECOMMENDATION systems are widely used in so-
cial network services and video streaming platforms to
provide personalized and preferred content to consumers as
described in Fig.1. They are also employed in search engines
to offer differentiated search services [1]–[5]. Supporting rec-
ommendation services for such a large number of consumers
demands significant computation resources. For example, more
than 80% of Meta’s data center resources are allocated to
recommendation system inference, while over 50% are utilized
for training these systems [6].
Traditional recommendation systems relied on collaborative
filtering techniques, such as content filtering using matrix
factorization [7]–[10]. However, with advancements in deep
neural networks (DNNs), deep learning recommendation mod-
els (DLRMs) that combine embedding tables (EMBs) and
This work was supported by Samsung Electronics Co., Ltd..
Manuscript received MM dd, yyyy; revised MM dd, yyyy.
Application
Music
Music
Movie
Movie
Game
Game
Sport
Sport
Sport
Shop
Shop
Food
Food
Music
Movie
Game
Sport
Shop
Food
Recommendation
System
Feedback
Rec. List
User
Like/Dislike
Watch
Buy
Fig. 1. Overview of recommendation system.
multi-layer perceptron (MLP) layers have become a dom-
inant approach in recommendation systems. These models
are widely adopted in data centers, with recent focuses on
both software-level and hardware-level optimizations [11]–
[17]. DLRMs process categorical data like website visit
histories through EMBs, and numerical data such as user
age through MLP layers. This combination has demonstrated
superior recommendation performance, making DLRM the
industry standard in recommendation systems. However, their
model size and memory bandwidth requirements have grown
rapidly with the recommendation performance. Specifically,
the number of EMBs, which accounts for the majority of a
DLRM’s size, has increased to multiple terabytes (TBs) of
parameters in memory devices, with bandwidth requirement
rising to TB/s level [18]–[20].
In addition to the inherent characteristics of the DLRM
algorithm, the workload intensity within the model varies
depending on how it is applied in the recommendation system,
making it difficult to build the optimized system [21]. There
are two primary mechanisms in recommendation systems:
content filtering and content ranking. The former is the pro-
cess of selecting a list of recommended content for users
where embedding layer processing dominates, resulting in a
memory-centric workload. In contrast, the latter determines
the order in which the recommended content is displayed
to users where MLP layer processing dominates, leading to
a compute-centric workload. As a result, it is crucial to
configure optimized recommendation systems with focusing
on the targeting mechanism of the DLRM, to achieve high
utilization of the system resources. Since traditional CPU-
DRAM systems have struggled to meet the demands of modern
DLRMs, it highlights the need for systems that can adaptively
handle extensive memory and computational requirements.
Research has been conducted to address the memory and
computational challenges of DLRM using solid-state drives
(SSDs). Recent studies [22]–[25] have explored in-storage
arXiv:2504.00520v1  [cs.AR]  1 Apr 2025


--- Page 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
2
processing (ISP) and near-memory processing (NMP) to per-
form embedding layer computations within standalone SSDs.
This approach facilitates the storage of TBs of EMBs and
accelerates embedding layer operations by utilizing the inter-
nal memory bandwidth of SSDs, providing significantly faster
performance compared to conventional SSD usage. However,
the internal bandwidth of SSDs remains substantially lower
than that of volatile memory, such as high-bandwidth memory
(HBM), rendering them insufficient to meet the growing
memory bandwidth demands of DLRMs. Furthermore, the
high computational complexity of the MLP layer poses sig-
nificant challenges for SSD-based ISP, making it impractical
to implement a complete DLRM system.
Another approach to addressing both memory capacity
and bandwidth is the use of multi-GPU systems. Recent
works [26]–[29] have implemented data parallelism and model
parallelism to distribute the MLP and EMBs of the DLRM
across multiple GPUs. Although this approach achieves highly
performant systems, it also comes with a few challenging
issues. The first issue is the large number of GPUs required
to run a single DLRM. For instance, constructing a DLRM
system with TBs of parameters would require dozens of
NVIDIA H100 GPUs, each equipped with 80 GB of HBM,
resulting in enormous system costs. The second issue is the
underutilization of GPU compute resources for embedding-
dominant DLRM applications, leading to inefficiency in the
entire system. Lastly, multiple GPU nodes must be intercon-
nected to run the entire DLRM system at TB scale in parallel,
leading to increased network complexity and overhead. The
overall DLRM performance degradation is inevitable due to
complex network configuration among GPU nodes, with data
communication overhead among them. To be precise, all-to-
all communication is required during model inference, where
each GPU requests embedding vectors from different GPUs,
causing significant system bottlenecks.
To mitigate these challenges, recent work [19] has statisti-
cally analyzed DLRM characteristics and proposed a sharding
technique that stores frequently accessed embedding vectors
in the GPU’s HBM, while placing rarely accessed embed-
ding vectors in the host’s DRAM. This approach effectively
maintains a comparable system memory bandwidth while
reducing the required number of GPUs to run the DLRM.
However, challenges such as high system cost, computing
resource underutilization, and complexity of network topology
still persist in the GPU-based DLRM systems.
To address these challenges, we propose SCRec, a scalable
computational storage system that can handle TB-scale indus-
trial DLRMs while providing high bandwidth requirements.
SCRec leverages SmartSSD devices, which integrate an SSD
with a field-programmable gate array (FPGA) for near-data
processing (NDP), effectively utilizing the SSD’s large capac-
ity alongside the high bandwidth of DRAM and block RAM
(BRAM) on the FPGA. The detailed contributions of our work
are as follows.
• SCRec utilizes a software framework with two pri-
mary features—statistical sharding and adaptive accel-
eration core mapping—leveraging a mixed-integer pro-
gramming (MIP)-based cost model. By analyzing data
 
 
 
 
 
EMB_1
EMB_N
 
 
 
 
Embedding Layer
Input Data
 
 
Prediction
Feature Interaction Layer
Bottom MLP
Emb. Vector
Emb. Vector
Emb. Vector
Pooling
Pooling
Pooling
Sparse
Features
View History
...
Dense
Features
Region
Age
...
Top MLP
 
 
Emb. Vector
Pooling
EMB_3
EMB_2
Item Rate
Fig. 2. Architecture of deep learning recommendation model.
access patterns of DLRM input features, SCRec optimizes
SmartSSD memory bandwidth by placing hot data in
high-bandwidth memory devices such as BRAM and
DRAM, and cold data in SSD. Additionally, it deter-
mines the optimal configuration of memory-centric and
compute-centric acceleration cores based on the work-
load intensity analysis for each layer, ensuring a high-
utilization DLRM system.
• We designed custom acceleration cores capable of boost-
ing up EMB and MLP computations in DLRM and
implemented them onto the FPGA chip of the SmartSSD
device. Especially, the EMB core utilizes the tensor-train
(TT) format to compress the GB-sized EMBs signifi-
cantly into the megabyte (MB)-level, enabling the high-
performance reconstruction of approximated embedding
vectors. This also complements the device’s DRAM ca-
pacity limitations while enhancing the overall system’s
memory bandwidth.
• We integrated the software framework and hardware
accelerators to develop a multi-SmartSSD system capable
of running the entire DLRM operations. The system is
configured within a single server, eliminating the need
for complex network setup and eliminating data com-
munication overhead. Additionally, within the server, it
minimizes host communication overhead through peer-to-
peer (P2P) data transfer between the SSD and FPGA chip.
Therefore, SCRec can significantly reduce both inter-
node and intra-node communication overhead.
Our evaluation demonstrates that SCRec achieves signifi-
cant improvements in inference performance, achieving up to
55.77× speed-up compared to a CPU-DRAM system with no
loss in accuracy and up to 13.35× improvement in energy
efficiency over a multi-GPU system.
The remainder of this paper is organized as follows: Section
II provides background information on the DLRM and TT-
format; Section III describes SCRec, including its software
framework and hardware architecture; Section IV presents the
evaluation results of SCRec; Section V discusses related works
on memory caching, sharding, and TT decomposition; and
Section VI concludes the paper.


--- Page 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
3
Den 1
Den 2
...
Den N
Den 1
Den 2
...
Den N
Dense
Features
bs
bs/N
Spar 1
Spar 2
...
Spar M
Spar 1
Spar 2
...
Spar M
Sparse
Features
bs
Input Data
MLP Param.
Bot MLP
Layer 0
Bot MLP
Layer 0
EMB Param.
Embedding
Table 0
Embedding
Table 0
MLP Param.
Top MLP
Layer 0
Top MLP
Layer 0
All-to-all
Feature
Interact.
Device 1
Device 1
Device 1
Feat 1
Top MLP
Feature
Interact.
Device 1
Feat 1
Top MLP
Feature
Interact.
Device 2
Device 2
Device 2
Feat 2
Top MLP
Feature
Interact.
Device 2
Feat 2
Top MLP
Feature
Interact.
Device N
Device N
Device N
Feat N
Top MLP
Feature
Interact.
Device N
Feat N
Top MLP
Predict 1
Data
Parallelism
Data
Parallelism
Data
Parallelism
Model
Parallelism
Model
Parallelism
Model
Parallelism
Den 1
Bot MLP
Device 1
Device 1
Device 1
Spar 4
EMB 4
Spar 4
EMB 4
Den 1
Bot MLP
Device 1
Spar 4
EMB 4
Den 2
Bot MLP
Device 2
Device 2
Device 2
Spar 8
EMB 8
Spar 8
EMB 8
Den 2
Bot MLP
Device 2
Spar 8
EMB 8
Den N
Bot MLP
Device N
Device N
Device N
Spar M
EMB M
Spar M
EMB M
Den N
Bot MLP
Device N
Spar M
EMB M
Data
Parallelism
Data
Parallelism
Data
Parallelism
Predict 2
Predict N
Fig. 3. Hybrid-parallel DLRM processing.
II. BACKGROUND
A. DLRM Architecture and Hybird-parallel Processing
DLRM is a DNN-based recommendation model widely
utilized across various industries, as depicted in Fig.2. It
processes user and content data as input to predict user
interactions with specific content, aiming to estimate the
click-through rate (CTR). The model primarily consists of
an embedding layer with multiple EMBs, MLP layers, and
a feature interaction layer. In the DLRM, dense features and
sparse features serve as inputs to the MLP layer and the
embedding layer, respectively. Dense features, also referred
to as continuous features, represent numerical data such as
a user’s age or geographic location. Sparse features, also
known as categorical features, include data like website visit
history or content category IDs. Sparse features are specifically
represented as one-hot or multi-hot binary vectors, which are
used to retrieve latent vectors from the EMBs. These latent
vectors are then aggregated through a pooling operation to
produce a single vector. The outputs of the bottom MLP layer
and the embedding layer are integrated through concatenation
in the feature interaction layer, followed by computation in
the top MLP layer to produce the final prediction. A key
strength of DLRM is its ability to use multiple EMBs to map
discrete, categorical features into their corresponding vector
representations, allowing it to effectively capture semantically
meaningful representations of the target features [30].
The need for parallel processing across multiple devices in
the DLRM arises from the model’s inherent characteristics.
First, EMBs, which account for the majority of DLRM pa-
rameters, can number hundreds, with the total size of the em-
bedding layer exceeding several TBs. Consequently, memory
devices utilizing DRAM and HBM must split and store the
EMBs across multiple devices. Second, as the MLP layer is
compute-bound and the embedding layer is memory-bound,
optimizing the system with respect to workload intensity is
essential to prevent hardware underutilization. To address these
challenges, hybrid-parallel processing is applied to optimize
DLRM execution across multiple devices, as depicted in Fig.3.
Data parallelism is used for the MLP layer, whereas model
parallelism is applied to the embedding layer. Specifically,
dense features are divided among devices using a batch-wise
split, and EMBs are configured with a table-wise split. In this
setup, an all-to-all operation ensures that each device holds all
Decompose
Decompose
Decompose
EMB Table
Reshape
Reshape
Reshape
Reshaped Table
TT Core 0
TT Core 1
TT Core 2
Row [ I1 × I2 × I3 ]
Row [ I1 × I2 × I3 ]
Dim [ J1 × J2 × J3 ]
Dim [ J1 × J2 × J3 ]
TT dim = # of TT cores = 3
TT ranks [R] = 4
TT dim = # of TT cores = 3
TT ranks [R] = 4
 I3 × J3
 I3 × J3
 I1 × J1
 I1 × J1
 I1 × J1
 I1 × J1
 R1 
 R1 
 R1
 R1
 I2 × J2
 I2 × J2
 I3 × J3
 I3 × J3
 R3
Fig. 4. Process of representing an embedding table in TT-format.
pooled latent vectors, as the feature interaction layer requires
pooling results from all EMBs. Subsequently, the feature
interaction layer and top MLP layer are processed using data
parallelism, finally producing the CTR results. Hybrid-parallel
processing in DLRM is a powerful approach that reduces the
computational intensity of MLP operations while overcoming
memory capacity limitations and increasing effective memory
bandwidth.
B. Tensor-train Decomposition
TT
decomposition
[31]
is
a
low-rank
approximation
method that decomposes high-dimensional tensors into lower-
dimensional representations, enabling them to be approxi-
mated with significantly less data. The decomposition process
involves the following steps: A d-dimensional tensor T
∈
RI1×I2×···×Id, where Ik is the size of dimension k, can be
represented as a sequential product of tensors G with lower
dimension d, using the following equation 1.
T (i1, i2, . . . , id) = G1(:, i1, :)G2(:, i2, :) . . . Gd(:, id, :)
(1)
The decomposed tensor Gk ∈RRk−1×Ik×Rk is referred to
as a TT-core, has a TT-rank Rk, where the first and last TT-
cores have TT-rank of 1 to ensure that the sequential product
of TT-cores results in a scalar, meaning R0 = Rk = 1.
A matrix-form EMB can be represented using TT-format, as
illustrated in Fig.4. If the EMB is represented as E ∈RI×J ,
where I = Qd
k=1 ik refers to the row length and J = Qd
k=1 jk
refers to embedding dimension. the EMB can be reshaped into
a d-dimensional tensor T ∈R(i1j1)×(i2j2)×···×(idjd). This re-
shaped tensor can then be decomposed into multiple TT-cores.
Consequently, every element of the EMB can be approximated
as a value obtained through the sequential product of TT-cores,
which can be computed using Equation 2:
E(i, j) = T (m1, m2, . . . , md)
= G1(:, m1, :)G2(:, m2, :) . . . Gd(:, md, :)
(2)
where the EMB row is defined as i = Pd
k=1 ik
Qd
l=k+1 Il,
the EMB dimension value by j = Pd
k=1 jk
Qd
l=k+1 Jl, and
the reshaped tensor index by md = Id · id + jd, with the
ranges 0 ≤id ≤Id −1 and 0 ≤jd ≤Jd −1 [32].
The use of TT decomposition to compress DNN model
parameters has significantly reduced the need for large mem-
ory capacity. This method has proven particularly effective,
achieving exceptionally high compression ratios—up to thou-
sands of times—for DLRMs, especially for EMBs, which
range from hundreds of GBs to severals TBs. Additionally,


--- Page 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
4
MLP & TT-core matmul. operation acceleration 
(enhancing DLRM throughput 
)
Runtime
Device
Host
Remapping
layer
Host
Remapping
layer
SmartSSD Cluster
EMB SmartSSD N
DRAM
NAND
flash
(4TB)
SSD ctlr.
PCIe switch
FPGA chip
BRAM
EMB core
EMB S.SSD 1
EMB param.
MLP S.SSD M
MLP param.
MLP S.SSD 1
MLP param.
Random Sampling
Dense feat.
Sparse feat.
Dataset
Model Config.
# of EMB table
EMB dimension
# of EMB rows
Software Framework
# of MLP layers
MLP dimension
DLRM
Top MLP layer
Bot MLP layer
EMB layer
Adaptive hardware resource mapping 
(optimizing system based on DLRM workload intensity 
)
Statistics
CDF
PF
TT CR
Latency
Statistics
CDF
PF
TT CR
Latency
Scalable Resource Manager
EMB S.SSD 1
EMB 4
Row % [DRAM, TT, SSD]
EMB 4
Row % [DRAM, TT, SSD]
EMB S.SSD N
EMB J
Row % [DRAM, TT, SSD]
EMB J
Row % [DRAM, TT, SSD]
1, ..., M
1, ..., N
EMB S.SSD idx.
MLP S.SSD idx.
1, ..., M
1, ..., N
EMB S.SSD idx.
MLP S.SSD idx.
SmartSSD Spec.
# of devices
Memory size
Memory BW.
SmartSSD Spec.
# of devices
Memory size
Memory BW.
Core Performance Simulator 
EMB Core
TT matmul.
MLP Core
Top MLP
Bot. MLP
Top MLP
Bot. MLP
Address
Remapper
P2P communication (reducing host communication overhead 
)
Inst. Compiler
Param. Mapper
Core Mapper
EMB core
MLP core
MLP core
Data Statistic
Analyzer
Fig. 5. Overview of SCRec.
fetching parameters in TT-format to on-chip memory reduces
the off-chip memory footprint, thereby enhancing the model
throughput.
III. SCREC
A. Overview of SCRec
SCRec integrates a software framework that configures
an optimized DLRM system based on DLRM’s workload
intensity, along with a custom hardware accelerator to enhance
model performance. SCRec consists of three main pipelines in
the offline phase: the Data Statistic Analyzer (DSA), the scal-
able Resource Manager (SRM), and the Address Remapper.
Detailed descriptions of each stage are provided as follows:
Data Statistic Analyzer DSA identifies the statistical
characteristics of a dataset and uses them as parameters for
the cost model in the subsequent SRM task, providing the
necessary information for analyzing workload. Specifically,
the DSA utilizes a randomly subsampled input dataset, the
configuration of the DLRM, and the hardware specifications
of the SmartSSD to derive key metrics, as explained in Section
III-B. SCRec simulates hardware acceleration logic mapped to
the SmartSSD using a cycle-accurate performance simulator.
This simulation measures the latency required to retrieve
embedding vectors in an EMB core and the latency of MLP
operations in an MLP core while considering the off-chip
memory footprint. These metrics are also used as parameters
to build the cost model in the SRM.
Scalable Resource Manager Based on the statistics ob-
tained from the DSA, the SRM applies a MIP-based cost
model to enable adaptive hardware core allocation and model
parameter mapping on the SmartSSDs, based on the workload
intensity of DLRM. A key feature of the SRM is its three-
level sharding strategy, which stores hot embedding vectors
with high access frequency in FPGA DRAM and BRAM,
while cold embedding vectors with low access frequency
are stored in the SSD. This approach leverages the large
memory capacity of SSDs and the high memory bandwidth
of DRAM. Notably, by storing EMBs in TT-format within
FPGA BRAM, even large EMBs can be accommodated in on-
chip memory, effectively overcoming the capacity limitations
of FPGA DRAM, as discussed in Section III-C. Additionally,
the SRM enables scalable, hardware-based latency optimizing
by adaptively allocating MLP and EMB cores based on the
workload intensity of the DLRM. Using the results generated
by the SRM, heterogeneous computation cores are mapped
to the SmartSSD, and DLRM parameters are fetched. Simul-
taneously, the instruction compiler generates instructions for
multi-device execution.
Addresss Remapper In the address remapper, a remapping
layer is implemented to translate logical memory addresses,
enabling access to sharded EMBs stored in corresponding
devices. During DLRM execution, this remapping layer is
loaded into host DRAM at runtime, facilitating memory ac-
cess when the host requests embedding layer access from
SmartSSDs. The implementation details of the remapping
layer are discussed in Section III-D.
Hardware Accelerator On the hardware side, the MLP
core and EMB core are designed to accelerate computations
in the MLP layer and embedding layer, respectively. In par-
ticular, the primary function of the EMB core is to generate
approximated embedding vectors represented in the TT-format
while storing them on-chip, as detailed in Section III-E. Addi-
tionally, P2P communication in SmartSSD is implemented via
a PCIe switch to directly connect the SSD and FPGA chip,
minimizing host communication overhead through NDP.
B. Data Statistic Analyzer
The goal of DSA is to derive the parameters required for
the cost model used in the SRM. This is achieved using sub-
sampled data, the model configuration, and a core performance
simulator. The DSA computes essential parameters for con-
structing the cost model, including the cumulative distribution
function (CDF), pooling factor (PF), TT compression ratio
(CR), and layer operation latency. In SCRec, statistics for
the Meta Embedding Lookup Synthetic (MELS) dataset [33]
from 2021 and 2022, which simulates the embedding layer
of Meta’s industrial DLRM, were analyzed. The results of
this data profiling are presented in Fig.6(a) and Fig.6(b),
respectively. Detailed explanations of each statistical metric
are provided below:


--- Page 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
5
CDF [%]
Pooling Factor
TT Compression Ratio
Cumulative Rows [%]
(a)
(b)
Cumulative Rows [%]
EMB Index
CDF [%]
Pooling Factor
TT Compression Ratio
EMB Index
EMB Index
EMB Index
CDF [%]
Pooling Factor
TT Compression Ratio
Cumulative Rows [%]
(a)
(b)
Cumulative Rows [%]
EMB Index
CDF [%]
Pooling Factor
TT Compression Ratio
EMB Index
EMB Index
EMB Index
Fig. 6. Characteristic of sparse featrues in Meta embedding lookup synthetic
dataset. (a) Year 2021 (b) Year 2022
Cumulative Distribution Function CDF is used to describe
how frequently different rows in EMBs are accessed within the
data sample, as shown in the left graph of Fig.6. Specifically,
we analyzed the 856 EMBs from the 2021 dataset and the
788 EMBs from the 2022 dataset, plotting graphs of the
access statistics. These graphs reveal that the EMB access
pattern follows a flipped power-law distribution, where a
small portion of the EMB accounts for the majority of total
accesses. This observation suggests that employing a multi-
level memory hierarchy—with hot data placed in smaller,
high-bandwidth memory and cold data stored in larger, low-
bandwidth memory—can significantly improve EMB access
performance.
Pooling Factor PF refers to the number of embedding
vectors accessed for a particular sparse feature in a given
data sample. PF directly impacts memory bandwidth demand,
as a higher PF requires accessing more EMB rows, thereby
increasing the memory load. The middle graphs in Fig.6
illustrate the average PF, showing a wide range from 0 to 118
in the 2021 dataset and from 0 to 175 in the 2022 dataset. Due
to the significant variance in PF across EMBs, PF becomes a
critical factor in effectively partitioning EMBs across devices.
TT Compression Ratio TT CR refers to the ratio that
quantifies how much the data can be compressed when an
EMB is represented in TT-format. It is expressed as CR =
EMB size/TT core size. Calculating the CR in the DSA
is crucial because TT-cores must fit within the limited capacity
of BRAM, which is only a few MBs. Additionally, the CR
depends on various factors, including the rank of the TT-core,
the dimensions of the EMBs, and the percentage of rows in
the EMB selected for compression. For example, the graph
on the right side of Fig.6 shows the CR when the entire
EMB is compressed with an embedding dimension of 64 and
a TT-rank of 4. In some EMBs, the TT-represented EMB
surpasses the original size, while in others, the CR reaches
values in the thousands. This variation arises because larger
EMBs generally achieve higher CR. Given this characteristic,
it is crucial to construct a cost model that incorporates the
TABLE I
DESCRIPTIONS OF SRM COST MODEL PARAMETERS
Parameter
Description
M
Number of SmartSSDs
J
Number of EMBs
BS
Batch Size
BS mini
Mini Batch Size of MLP Core
Capbram
FPGA BRAM Capacity per SmartSSD
Capdram
FPGA DRAM Capacity per SmartSSD
Capssd
SSD Capacity per SmartSSD
ttt
TT-core Matrix multiplication Latency
tdram
DRAM Read Latency
tssd
SSD Read Latency
tmlp top
Top MLP Layer Processing Latency
tmlp bot
Bottom MLP Layer Processing Latency
ICDFj
Inverse CDF of EMB j
stepj
Step Size for Piecewise Linear Interpolation of EMB j
tt cmj
TT-cores Size of EMB j
pfj
Average Pooling Factor of EMB j
row lenj
Row Length of EMB j
dimj
Embedding Dimension of EMB j
embj
Overall Data Size of EMB j
dfj
Data Format Byte Size of EMB j
hot thrj
Hot Data Threshold Percentage of EMB j
TT CR of each EMB and its resulting compressed size. This
ensures that TT-format EMBs can be effectively selected and
compressed to fit within the FPGA’s on-chip memory in the
SmartSSDs using the cost model, thereby optimizing memory
usage and access.
Layer Operation Latency The latency of generating ap-
proximated embedding vectors from TT-cores, along with the
processing latency of the top and bottom MLP layers, is
obtained through the core performance simulator and used as
constraints in the cost model. Decoupling the cost model from
the latency measurement simulator reduces the computational
overhead of the cost model, while the cycle-accurate design
of the simulator ensures precise latency estimations. Notably,
when calculating the latency of MLP layers, the off-chip
memory footprint is also considered, resulting in even more
accurate latency measurements.
C. Scalable Resource Manager
SRM aims to achieve configuring optimized DLRM system
through a MIP-based cost model, leveraging the characteristics
of the DLRM profiled by the DSA. The core concept of
SRM is to minimize EMB access latency by utilizing a three-
level memory hierarchy and the TT-format. Specifically, EMBs
requiring high bandwidth are fetched into the DRAM and
BRAM of the FPGA, while those with lower bandwidth
requirements are stored in the SSD. This strategy enables the
embedding layer parameters to take advantage of the large
capacity of SSDs while benefiting from the high bandwidth
of DRAM and BRAM. Moreover, by storing EMBs approxi-
mated in the highly compressed TT-format within the BRAM,
it becomes possible to fit GB-sized EMBs into the MB-
sized on-chip memory, effectively increasing the embedding
layer’s effective bandwidth. We refer to this technique as
three-level sharding, and the following explanation details the
implementation of the cost model used to apply this sharding
scheme.


--- Page 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
6
Objective Function To optimize the latency of DLRM
using multiple SmartSSD devices, we define an objective
function to minimize the cost, as described in Equation 3,
with the parameters outlined in Table I.
minimize
C
subject to
cfnt + cmlp top ≤C
(3)
cfnt refers to the cost of the layer preceding the feature
interaction layer, while cmlp top represents the cost of the top
MLP layer. Since the bottom MLP layer and the embedding
layer, which together determine cfnt, are processed in parallel,
the cost function is designed to minimize the sum of the
maximum cost between these two independently processed
layers and the cost of the top MLP layer.
Device Allocation The constraints for assigning cores ca-
pable of computing the MLP and embedding layers are as
follows:
1 ≤
X
m
dm ≤M −1
∀m ∈M
(4)
dm ∈{0, 1}
∀m ∈M
(5)
dm represents the type of hardware core to be mapped onto
the FPGA chip of the SmartSSD. Specifically, when dm = 0,
the constraint ensures that an MLP core is mapped, whereas
when dm = 1, it enforces the mapping of an EMB core.
Equation 4 controls the allocation of heterogeneous cores,
preventing all SmartSSDs from being exclusively assigned to
either EMB cores or MLP cores. This allocation ensures that
all layers of the DLRM can be effectively accelerated within
the multi-SmartSSD system.
Embedding Table Allocation To enable model paral-
lelism by partitioning the EMB parameters across SmartSSDs
mapped to EMB cores, the following constraints are defined:
X
m
pmj = 1
∀j ∈J
(6)
X
j
pmj = dm ∗
X
j
pmj
∀m ∈M
(7)
pmj ∈{0, 1}
∀m ∈M
(8)
pmjis a binary variable that indicates whether EMB j is
assigned to SmartSSD m. In Equation 6, a constraint is
imposed to ensure that each specific EMB is assigned to
its corresponding SmartSSD. Equation 7 ensures that EMBs
are only assigned to SmartSSDs that have been allocated
EMB cores, serving as a constraint to prevent the incorrect
assignment of EMBs to SmartSSDs with MLP cores. By
utilizing these constraints, SCRec achieves model parallelism
in the embedding layer through table-wise splitting of EMBs
in the SRM.
Three-level Sharding The EMBs assigned to SmartSSDs
are split row-wise based on their access frequency patterns
and are fetched into the FPGA’s DRAM, BRAM, and SSD
memory devices. The constraints are structured as follows to
minimize EMB access latency:
X
i
x dramji ∗ICDFj(i) ∗row lenj
∗dimj ∗dfj = mem dramj
∀j ∈J
(9)
X
i
x dramji ∗
i
stepj
= pct dramj
∀j ∈J
(10)
X
i
x dramji = 1
∀j ∈J
(11)
x dramji ∈{0, 1}
i = 0, . . . , stepj
∀j ∈J
(12)
The above four equations constrain which rows of EMB j
are fetched into the FPGA DRAM and determine the size of
those rows. The variable x dram serves as a binary variable
to compute the pct dram variable, which determines , which
specifies the percentage of rows from EMB j are stored
in DRAM, as described in Equation 10. At this point, the
step size for EMB j is set to min(row lenj, 100). For the
ICDF, the CDF obtained from the DSA is inverted and the
interpolation interval is determined by stepj using piecewise
linear interpolation, reducing the computational complexity
of the cost model. Equation 9, formulated using the ICDF,
calculates the size of EMB j to be fetched into DRAM, with
mem dramj serving as a constraint on the FPGA DRAM
capacity of the SmartSSD.
X
i
x ptr ttji ∗ICDFj(i) ∗row lenj ∗dimj ∗dfj
−mem dramj = mem ttj
∀j ∈J
(13)
X
i
x dramji ∗
i
stepj
+
X
i
x ttji ∗
i
stepj
=
X
i
x ptr ttji ∗
i
stepj
∀j ∈J
(14)
X
i
x ttji ∗
i
stepj
= pct ttj
∀j ∈J
(15)
X
i
x ttji =
X
i
x ptr ttji = 1
∀j ∈J
(16)
x ttji ∈{0, 1}
i = 0, . . . , stepj
∀j ∈J
(17)
x ptr ttji ∈{0, 1}
i = 0, . . . , stepj
∀j ∈J
(18)
The above six equations define constraints for determin-
ing which rows of EMB j are fetched into BRAM in the
compressed TT-format and the size of those rows before
compression. The variable x tt is a binary variable used
to calculate the percentage of rows from EMB j stored in
BRAM, as described in Equation 15, while x ptr tt is a
binary variable used to calculate the size of the EMB rows
allocated to BRAM. Unlike Equations 9–12, the EMB rows
stored in TT-format are positioned between the rows stored in
DRAM and SSD. Thus, the starting and ending row indices
of the EMB stored in TT-format can be derived using the
x dram and x ptr tt binary variables via Equations 14 and
15. Additionally, the size of EMB j before compression into
TT-format, denoted as mem ttj, is calculated using the ICDF
in Equation 13, and this variable serves as a constraint on the
BRAM capacity of the SmartSSD.


--- Page 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
7
X
i
x ptr ttji ∗ICDFj −
X
i
x dramji ∗ICDFj
=
X
i
x row ttji ∗
i
stepj
∀j ∈J
(19)
X
i
x row ttji = 1
∀j ∈J
(20)
x row ttji ∈{0, 1}
i = 0, . . . , stepj
∀j ∈J
(21)
The above three equations define constraints for calculating
the x row tt binary variable, which is necessary to determine
the size of EMB j when represented across multiple TT-cores.
Using x row tt, the cost model calculate the size of the
EMBs compressed into TT-format, enabling the formulation
of BRAM capacity constraints.
pct dramj + pct ttj ≤hot thrj
∀j ∈J
(22)
Equation 22 imposes constraints to prevent excessive TT-
format compression, ensuring that less frequently accessed
embedding vectors are stored in the SSD. The hot thrj is
assigned a value for each EMB. A value of 1 is used when
the EMB row size is relatively small, whereas a value less
than 1 is applied for significantly larger row sizes, ensuring
that EMB parameters from larger tables are fetched from the
SSD.
Memory Capacity The constraints in the EMB allocation
process determine how each EMB j is split at the row level
and the extent of memory capacity it occupies when assigned
to a SmartSSD. Additionally, a condition must be imposed to
ensure that the size of the EMBs fetched to the device does
not exceed the device’s memory capacity. The constraints are
defined as follows:
row lenj ∗dimj ∗dfj = embj
∀j ∈J
(23)
X
j
pmj ∗mem dramj ≤CapDRAM
∀m ∈M (24)
X
j
pmj ∗(embj −mem dramj −mem ttj)
≤CapSSD
∀m ∈M
(25)
X
j
x row ttij ∗tt cmj(i) ∗dfj = tt capj
∀j ∈J
(26)
X
i
pmj ∗tt capj ≤CapBRAM
∀m ∈M (27)
Equations 23–25 limit the capacity of EMBs allocated to
the DRAM and SSD of SmartSSD m, while Equations 26 and
27 restrict the capacity of EMBs compressed in TT-format. In
these constraints, the tt cmj used in Equation 26 refers to the
compressed size, which depends on the percentage of the total
EMB rows compressed in TT-format. Similar to the ICDF, this
value is obtained through piecewise linear interpolation during
DSA stage.
Embedding Vector Access Latency To optimize processing
overhead, constraints can be formulated to link the embedding
vector access latency of each memory device to the batch size.
This is achieved utilizing the PF obtained from the DSA and
the variables derived from the three-level sharding process.
The resulting equations are as follows:
(pfj ∗BS) ∗(pct dramj ∗tdram)
= c dramj
∀j ∈J
(28)
(pfj ∗BS) ∗(pct ttj ∗ttt) = c ttj
∀j ∈J
(29)
(pfj ∗BS) ∗((1 −pct dramj −pct ttj) ∗tssd)
= c ssdj
∀j ∈J
(30)
X
j
pmj ∗c dramj = c dramm
∀m ∈M
(31)
X
j
pmj ∗c ttj = c ttm
∀m ∈M
(32)
X
j
pmj ∗c ssdj = c ssdm
∀m ∈M
(33)
Equations 28–30 calculate latency cost of EMB j allocated
to DRAM, BRAM, and SSD. Additionally, Equations 31–
33 define the constraints for determining the latency of each
memory tier in the SmartSSD, using the pmj derived earlier in
the EMB allocation process. By applying these constraints, a
precise cost model for the embedding layer can be constructed,
taking the specific characteristics of DLRM into account.
MLP Layer Latency In addition to the embedding layer’s
cost constraints, the latency cost of the top MLP layer and the
bottom MLP layer can also be formulated with the following
constraints:
tmlp top ∗(BS/BS mini)
= cmlp top ∗
X
m
(1 −dm)
∀m ∈M
(34)
tmlp bot ∗(BS/BS mini)
= cmlp bot ∗
X
m
(1 −dm)
∀m ∈M
(35)
In the Equations 34 and 35, tmlp top and tmlp bot are values
obtained from the cycle-accurate MLP core simulator, and
BS mini refers to the batch size that can be processed by the
MLP core in a single tile operation. The value of BS mini
is configurable, as the MLP core is equipped with multiple
computing cores. The details of the hardware architecture will
be discussed in Section III-E.
Cost Definition To define the objective function for opti-
mizing the DLRM, constraints for the previously mentioned
variables were formulated. Using these constraints, the equa-
tions for the corresponding layers of the DLRM were estab-
lished as follows:
{c dramm, c ttm, c ssdm} ≤cemb
∀m ∈M
(36)
{cmlp bot, cemb} ≤cfnt
(37)
The constraint in Equation 36 defines the cost of the
embedding layer, cemb, while the constraint in Equation 37
defines cfnt which represents the maximum cost between the
bottom MLP layer and the embedding layer. Using cfnt and
cmlp top, the objective function for Equation 3 in the SRM
can be formulated.


--- Page 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
8
(a)
Top Controller
Vector Pooling Unit
EMB Core
DMA
TT Computation Unit
Array Ctlr.
TT_AMem
TT_BMem
TT_CMem
PE
PE
PE
PE
PE
PE
PE Array
32
16
Reshaper
Main Pool.
DRAM
SSD
TT
Post-pooling
Top Controller
Interconnect
Controller
MLP Core
Interconnect
Interconnect
Interconnect
(b)
WMem
BMem
IOMem
Shared Mem.
DMA
Array Ctlr.
MLP Computation Unit
PE
PE
PE
PE
PE
PE
PE Array
16
8
Bias Adder
Act. Func.
Fig. 7.
Hardware architecture of computing cores. (a) EMB core (b) MLP
core
By constructing the cost model as described above, embed-
ding layer partitioning and custom hardware core allocation
can be achieved, enabling the implementation of an optimized
hybrid-parallel processing system based on DLRM workload
intensity.
D. Address Remapping
Address remapping is essential step for facilitating access to
memory-allocated EMBs, which are partitioned by the SRM,
using EMB access indices derived from input sparse features.
The length of the remapping table generated in this stage
matches the row length of the EMB. When the sparse feature
index accesses the remapping table, a 32-bit remapped address
is retrieved, which is then decoded to read the embedding
vectors allocated in the corresponding memory devices. In
detail, the remapped address is composed of {device id[1:0],
emb idx[29:0]}, where a device id of 0 indicates the embed-
ding vector is fetched from DRAM, 1 from BRAM, and 2
from SSD. By utilizing the remapping table loaded into a host,
SCRec enables three-level sharding to be performed in a fine-
grained manner.
E. Hardware Accelerator Allocation
The objective of the core mapper is to allocate either an
MLP core for processing MLP layers or an EMB core for
processing embedding layers based on dm, derived from solv-
ing the cost model of SRM. The core mapper’s applicability
within SCRec is closely tied to the hardware architecture
of the SmartSSD, which is equipped with an FPGA chip
featuring reconfigurable hardware. This architecture enables
adaptive programming of hardware configurations optimized
for specific tasks prior to runtime. By leveraging this reconfig-
urable capability, hardware accelerators tailored to the DLRM
workload can be mapped onto the SmartSSD.
EMB Core EMB core fetches embedding vectors stored
in TT-format, as well as those stored in DRAM and SSD,
to perform pooling operations. It operates based on a 32-
bit floating-point (FP) processing element (PE), as shown
in Fig.7(a). The key module of the EMB core is the TT
Algorithm 1 TT-core Matrix-multiplication Sequence
Input: Gk (TT-core of EMB table)
Output: V (Approximated EMB vector)
Parameter: K (TT-dim.), R (TT-rank), J (EMB dim.)
1: /* Initialization */
2: for k = 1 to K do
3:
for i = 1 to Ik do
4:
if k = 1 then
5:
Uk[i] = Unfold(Gk[:, i, :, :], (−1, R))
6:
TT AMem = Fetch(Uk[i])
7:
else
8:
Uk[i] = Unfold(Gk[:, i, :, :], (R, −1))
9:
TT CMem = Fetch(Uk[i])
10:
end if
11:
end for
12: end for
13: /* Runtime computation */
14: for k = 1 to K do
15:
// Matrix multiplication
16:
if k = 1 then
17:
Tk = MatMul(Uk[ik], Uk+1[ik+1])
18:
else
19:
Tk = MatMul(Rk, Uk+1[ik+1])
20:
end if
21:
// Matrix reshaping
22:
if k ̸= K then
23:
Rk+1 = Reshape(Tk, (−1, rank))
24:
TT BMem = Fetch(Rk+1)
25:
else
26:
V = Reshape(Tk, (J))
27:
end if
28: end for
computation unit (CU), which accelerates sequential matrix
multiplication operations within the TT-core to generate ap-
proximated embedding vectors. While Equations 1 and 2
in Section II describe the calculation of a single element
of an embedding vector, the TT CU computes the entire
embedding vector simultaneously by processing the product
of a 3-dimensional tensor using Equation 38:
E(i, :) = G1(:, i1, :, :)G2(:, i2, :, :) . . . Gd(:, id, :, :)
(38)
where TT-core Gk ∈RRk−1×Ik×Jk×Rk with R0 = Rk = 1,
the EMB row is defined as i = Pd
k=1 ik
Qd
l=k+1 Il with the
ranges 0 ≤id ≤Id −1. To perform 3D tensor multiplications
on the EMB core, which is implemented with a 2D PE array,
the 3D TT-core is unfolded into a 2D matrix and loaded into
the on-chip memory during initialization. During sequential
TT-core matrix multiplications, intermediate reshaping is per-
formed to ensure conformability, as detailed in Algorithm 1.
Specifically, the multiply-accumulate (MAC) operations in the
EMB core are executed using an output-stationary systolic
array, processing the entire matrix in 16 × 32-sized output
tiles iteratively. The input memory for the PE array is de-
signed with dual channels in the column direction, effectively


--- Page 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
9
TABLE II
EXPERIMENT SETUP
Server
CPU
Intel Xeon Silver 4310 @ 2.1GHz
GPU
Nvidia A40 GDDR6 48GB
DRAM
DDR4 SDRAM 256GB @ 3200Mbps
SSD Simulator (MQSim)
Mem. Device
Samsung 860 EVO 3.84 TB
NAND Flash
Samsung V-NAND V4 (TLC, 64-layer)
PCIe Interface
Single Port PCIe Gen3 × 4
DRAM Cache
LPDDR4 SDRAM 2GB @ 1866Mbps
Read Time (tR)
45µs
Page Size
16KB
DRAM Simulator (Ramulator)
Mem. Device
DDR4 SDRAM 4GB @ 2400Mbps
FPGA Chip
Name
AMD Kintex™Ultrascale+ KU15P
On-chip Mem. Cap.
BRAM - 4.325MB / URAM - 4.5MB
# of DSP Slices
1968
Core Freq.
200MHz
hiding the reshaping overhead within the memory read latency.
Additionally, the vector pooling unit (VPU) independently
performs pooling operations for each memory device, and the
final pooled embedding vector is produced through the post-
pooling module. The proposed EMB core not only accelerates
TT computations but also supports pooling operations on
embedding vectors stored across multi-tier memory devices,
enabling complete embedding layer computations within the
SmartSSD.
MLP Core The MLP core is designed to accelerate both the
top and bottom MLP layers of the DLRM, utilizing a 32-bit
FP PE, as illustrated in Fig.7(b). The MLP CU consists of an 8
× 16 PE array, performing matrix multiplication operations in
tiles using output-stationary computation. Input values for the
MLP layer are read from IOMem and flow into the PE array in
the column direction, while weight parameters are read from
WMem and flow into the PE array in the row direction. Once
partial outputs are computed in the PE array, the bias adder
module adds bias values from BMem, and the activation func-
tion module applies a rectified linear unit (ReLU) operation
before writing the results back to IOMem. The MLP core
contains four CUs in total, with configurable input and weight
data distribution from the interconnect, enabling the system
to select between latency-optimized or throughput-optimized
computation. Specifically, if input data is broadcast to all CUs
and weight parameters are split so that each CU receives
different weights, the MLP layer is processed in a latency-
optimized manner. Conversely, if different input data is fed to
each CU while weight parameters are broadcast to all CUs,
the MLP layer is processed in a throughput-optimized manner.
As a result, the MLP core in SCRec not only accelerates MLP
operations but also supports configurable layer processing,
significantly enhancing MLP processing performance.
IV. EVALUATION
A. Experiment Setup
We established an experimental environment using a GPU
server and cycle-accurate simulators to evaluate the effective-
ness of implementing DLRM in SCRec, with detailed specifi-
TABLE III
DLRM EVALUATION DATASET SPECIFICATION
Dataset
Criteo Kaggle
Meta Synthetic
2021
2022
MLP Layer
O
X
X
# of EMB Table
26
856
788
Avg. EMB Rows
1,298,560
2,720,716
4,841,017
Avg. PF
1
8.34
13.6
# of Data Sample
45,840,617
65,536
131,072
cations outlined in Table II. DLRM training and inference were
conducted on a server equipped with Nvidia A40 GPUs to
assess accuracy based on Meta DLRM [34] when applying the
TT-format to the embedding layer. Additionally, we utilized
Ramulator [35] and MQSim [36], simulators designed to
evaluate the performance of DRAM and SSD, respectively,
to measure processing performance during SCRec’s runtime
operation. To compare SCRec with a multi-GPU system, we
measured the power consumption of SSD and DRAM using
SimpleSSD [37] and VAMPIRE [38]. FPGA power consump-
tion was profiled using the Xilinx Vivado power analysis
tool to evaluate the power of the implemented core logic.
We also designed an in-house cycle-accurate core simulator,
similar to the memory simulators, to measure performance for
simulating the hardware design implemented on the FPGA
chip. To synchronize the memory simulators and the core
simulator, modifications were made to enable transmission and
reception of control signals and data between simulators via
inter-process communication (IPC) using the TCP protocol
[39]. This synchronization ensured accurate emulation of real-
time system behavior by aligning the simulators at each cycle.
Regarding the datasets used for evaluation, large-scale in-
dustrial DLRM datasets are not publicly available. Therefore,
we utilized the Criteo Display Advertising (CDA) dataset [40]
and the MELS dataset [33]. As the characteristics of these
datasets differ, as shown in Table III, the appropriate dataset
was selected based on the evaluation metrics. Specifically, the
CDA dataset supports the evaluation of the entire DLRM,
including both the embedding and MLP layers. However,
due to significant differences in the size and access patterns
of its embedding layer compared to industrial DLRMs, it
was primarily used for assessing DLRM accuracy and the
performance of acceleration cores in our evaluation. Con-
versely, while the MELS dataset is limited to evaluating the
embedding layer without the MLP layers, it offers insights
into addressing key challenges currently faced by industrial
DLRMs. Consequently, we focused on using the MELS dataset
to evaluate the energy efficiency and performance of the
industrial DLRM implementation, with a specific emphasis on
the embedding layer.
To evaluate the SCRec software framework, we derived
statistical parameters for the SRM from the DSA using sub-
sampled data from the CDA and MELS datasets. For the CDA
dataset, a 1% randomly subsampled portion was used to extract
these parameters. In contrast, for the MELS dataset, given its
smaller data sample size, we increased the subsampling rate to
10% to enhance statistical reliability. In the SRM, the Gurobi
optimizer [41] was employed to construct the MIP-based cost


--- Page 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
10
LUT
LUTRAM
FF
BRAM
URAM
DSP
PE Array
209.5K
(40.08%)
30.7K
(19.08%)
406.9K
(38.93%)
0
(0%)
0
(0%)
1536
(78.05%)
Reshaper
413
(0.08%)
0
(0%)
935
(0.09%)
0
(0%)
0
(0%)
0
(0%)
Arr Ctlr.
248
(0.05%)
0
(0%)
299
(0.03%)
0
(0%)
0
(0%)
0
(0%)
13
(0.01%)
1.3K
(0.79%)
5.2K
(0.49%)
114
(11.59%)
128
(100%)
0
(0%)
18.2K
(3.49%)
1.3K
(0.79%)
32.5K
(3.11%)
0
(0%)
0
(0%)
198
(10.06%)
627
(0.12%)
0
(0%)
570
(0.05%)
0
(0%)
0
(0%)
0
(0%)
229.0K
(43.82%)
33.3K
(20.66%)
446.5K
(42.70%)
114
(11.59%)
128
(100%)
1734
(88.11%)
    Vector Pooling Unit
Etc.
Total
EMB Core
    TT CU
TT_Mem
LUT
LUTRAM
FF
BRAM
URAM
DSP
PE Array
215.8K
(41.30%)
29.3K
(18.20%)
417.2K
(39.91%)
0
(0%)
0
(0%)
1536
(78.05%)
Bias Adder
5.1K
(0.98%)
416
(0.26%)
2.5K
(0.24%)
0
(0%)
0
(0%)
64
(3.25%)
Act Func.
520
(0.10%)
0
(0%)
247
(0.02%)
0
(0%)
0
(0%)
0
(0%)
Array Ctlr.
2.2K
(0.44%)
0
(0%)
145
(0.01%)
0
(0%)
0
(0%)
0
(0%)
4.8K
(0.86%)
0
(0%)
2.8K
(0.27%)
488
(49.59%)
128
(100%)
0
(0%)
153
(0.03%)
0
(0%)
219
(0.02%)
16
(1.63%)
0
(0%)
0
(0%)
228.4K
(43.69%)
29.7K
(18.46%)
423.1K
(40.47%)
504
(51.22%)
128
(100%)
1600
(81.30%)
MLP Core
MLP CU
(#0 ~ #3)
    Shared Mem
Etc.
Total
(a)
(b)
Fig. 8. FPGA layout and resource utilization on SmartSSD. (a) EMB core
(b) MLP core
model. Specifically, the hot thr was empirically set to 1 if its
value was below 0.01% of the largest EMB row in the entire
EMB and to 0.99 if it exceeded this threshold. To generate
TT-formatted EMBs, the t3nsor library [42] was utilized,
applying a TT-rank of 4 to achieve higher data compression
performance.
B. Hardware Implementation Results
To verify the feasibility of implementing the hardware cores
of SCRec on a SmartSSD, we synthesized and performed
place-and-route operations for the computation kernels of
the EMB core and MLP core at a target frequency of 200
MHz. The implementation details and results are shown in
Fig.8(a) and Fig.8(b), respectively. Both cores demonstrated
the highest utilization in the CU module, primarily due to
the hardware computation data format being based on single-
precision FP. To achieve this, the PE modules instantiate FP
multipliers and FP accumulators synthesized using Xilinx’s
FP Intellectual Property (IP). This design optimizes hardware
by utilizing approximately 78% of the DSP slices, leveraging
look-up tables (LUTs) and flip-flops (FFs). For the EMB
core, the vector pooling unit (VPU) pools embedding vectors
read from each memory device using FP adders, resulting in
about 10% DSP slice utilization. Additionally, the post-pooling
unit within the VPU computes the average pooling vector
using an FP divider. In the MLP core, the bias adder module
performs vector addition between the matrix multiplication
result and the bias parameter using the FP adder module,
which utilizes approximately 3% of the DSP slices. The
difference in DSP slice utilization between non-CU modules
and the CU modules arises from their differing workloads;
non-CU modules perform element-wise operations, resulting
60
50
40
30
20
10
0
0
1
2
3
4
5
6
7
RM0
RM1
RM2
RM3
Relative Performance (times)
# of EMB Smart SSDs
Embedding Dimension (     : 16     : 32     : 64     : 128     : 256)
: # of EMB SmartSSDs
RM0
RM1
RM2
RM3
Bottom MLP
13-64-32-D
13-128-64-D
13-256-128-D
13-512-256-D
Top MLP
X-64-16-1
X-128-32-1
X-256-64-1
X-512-128-1
D: embedding dimension   /   X: (26+1) * embedding dimension
(a)
(b)
Fig. 9. Performance comparison between CPU-DRAM system and SCRec.
(a) DLRM configuration (b) Relative performance and EMB core allocation
in a lower computational load. As a result, these modules are
designed to consume relatively less hardware resources.
C. Performance
We evaluated SCRec using the CDA dataset by analyzing
core allocation variations based on the layer workload in
the DLRM and comparing its performance to a CPU-DRAM
system. This evaluation utilized the inference per second (IPS)
metric with a batch size of 128, using 8 SmartSSDs. The
DLRM configuration applied four different setups, as de-
scribed in Fig.9(a), with each model containing an embedding
layer composed of 26 EMBs. In the DLRM, the input layer
for the top MLP layer was determined by concatenating the
outputs of the 26 EMBs and the bottom MLP output, following
the constraints of the Meta DLRM [34]. Analyzing the graph
in Fig.9(b), SCRec significantly outperforms the CPU-DRAM
system when the embedding layer workload exceeds that of
the MLP layers. Specifically, for an embedding dimension
of 16, SCRec shows a performance improvement of 10.37×-
55.77× across the four recommendation models (RMs). This
enhancement is attributed to SCRec’s ability to leverage
multiple SmartSSD devices, enabling model parallelism in
processing the embedding layer and thereby increasing pro-
cessing bandwidth compared to the CPU-DRAM system.
Within each RM, as the embedding dimension increases, the
relative performance of SCRec declines. This is because a
larger embedding dimension increases the processing overhead
of the top MLP layer, indicating that SCRec is more effective
for embedding-dominant DLRM workloads than for MLP-
dominant ones. Regarding core allocation for the SmartSSDs,
as the workload of the MLP layer grows relative to that of
the embedding layer, the number of SmartSSDs assigned to
EMB cores decreases, while the number assigned to MLP
cores increases. This demonstrates that by optimizing the cost
model in its software stack, SCRec effectively mitigates low


--- Page 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
11
0
Relative Energy Efficiency (times)
64
128
256
512
64
128
256
512
Embedding Dimension
# of GPUs
2
4
6
8
10
12
14
280
240
200
160
120
80
40
0
MELS 2021
MELS 2022
: # of GPUs
System Type (     : Multi-GPU      : SCRec [Ours])
System Type (     : Multi-GPU      : SCRec [Ours])
Fig. 10.
Energy efficiency comparison between multi-GPU system and
SCRec.
core utilization caused by imbalances in processing overheads
between the embedding and MLP layers. These results indicate
that SCRec can optimize the system based on workload
intensity across various DLRM configurations and improve
processing performance through hybrid-parallel processing.
D. Energy Efficiency
We evaluated the energy efficiency of SCRec, deployed
on 8 SmartSSDs, against a multi-GPU system by varying
embedding dimensions and utilizing the MELS 2021 and 2022
datasets. The batch size was set to 1024, and model parallelism
was applied similarly in both systems, with EMBs split table-
wise and distributed to GPUs in a round-robin manner based
on their index. Due to the large size of the model parameters
in MELS, which could not fit into a single server, a hash
table was applied to the EMBs during measurement to limit
the maximum index of the EMBs. Energy efficiency was
measured using the IPS per watt (IPS/W) metric to assess rel-
ative performance. As shown in Fig.10, SCRec demonstrated
superior energy efficiency compared to the multi-GPU system
across both datasets, except when the embedding dimension
was set to 64. As the embedding dimension increased, the
improvement in energy efficiency became more pronounced.
Specifically, with an embedding dimension of 512, SCRec
achieved energy efficiency improvements of 6.14× for the
2021 dataset and 13.35× for the 2022 dataset. This improve-
ment stems from the fact that as the embedding dimension
increases, the number of GPUs required to store DLRM
parameters grows significantly. In contrast, SCRec can fully
accommodate DLRM parameters within the 32 TB capacity
of the SmartSSDs. Moreover, SCRec compensates for hot
embedding vectors that cannot be fetched to DRAM due to
capacity limitations by compressing them using the TT-format,
enabling GB/s-level bandwidth. These findings highlight that
SCRec provides an energy-efficient solution for the embedding
layers of industrial-scale DLRMs compared to multi-GPU
systems. Unlike systems requiring dozens or even hundreds
of GPUs with complex network topologies, SCRec achieves
energy efficiency by utilizing multiple SmartSSD devices
within a single server, eliminating network communication
overhead.
100
Inference Per Second (IPS)
MELS 2021
101
102
103
104
1
2
4
8
1
2
4
8
# of SmartSSDs
MELS 2022
Sharding strategy (     : one-level      : two-level      : three-level [SCRec])
Fig. 11.
Performance-based ablation study using Meta embedding lookup
synthetic dataset.
E. Ablation Study
We evaluated the performance changes in SCRec based on
the sharding levels applied, varying the number of SmartSSDs
and using the MELS 2021 and 2022 datasets, which in-
clude only embedding layer access patterns. An embedding
dimension of 256 and a batch size of 1024 were used. For
one-level sharding, only SSD was utilized, while for two-
level sharding, both SSD and DRAM were employed. As
shown in Fig.11, increasing the sharding level by incorporating
additional memory devices to store EMBs, while maintaining
the same number of SmartSSDs, leads to improved inference
performance. For the 2021 dataset, the three-level sharding
scheme applied in SCRec achieved performance improvements
of 141.34× and 10.11× over one-level and two-level sharding,
respectively, when 8 SmartSSDs were used. With a single
SmartSSD, the improvements were 6.55× and 3.02×, respec-
tively. Similarly, for the 2022 dataset, three-level sharding
delivered performance gains of 161.77× and 15.48× over one-
level and two-level sharding, respectively, with 8 SmartSSDs,
and 6.31× and 3.14× with a single SmartSSD. The variation in
performance improvements with higher sharding levels, while
keeping the number of SmartSSDs constant, can be attributed
to several factors. Transitioning from one-level to two-level
sharding allows frequently accessed embedding vectors to be
fetched from DRAM, mitigating the performance degradation
caused by the lower bandwidth of SSDs. Advancing from
two-level to three-level sharding compensates for DRAM’s
capacity limitations by storing highly compressed embedding
vectors in TT-format. This approach effectively hides SSD
access latency by overlapping it with the access latencies
of DRAM and BRAM, resulting in superior performance
with three-level sharding. Moreover, increasing the number of
SmartSSDs in SCRec’s three-level sharding further enhances
performance due to the application of model parallelism in
the embedding layer. As the number of devices grows, the
effective bandwidth increases proportionally. Notably, for both
datasets, the most significant performance improvements occur
when the number of SmartSSDs increases from 1 to 2, with
gains of 17.64× and 11.46×, respectively. This is because
fewer SmartSSDs exacerbate the capacity limitations when


--- Page 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
12
4
8
16
32
16
78.02 (-0.78)
78.14 (-0.66)
78.23 (-0.57)
78.40 (-0.40)
32
78.08 (-0.73)
78.26 (-0.55)
78.35 (-0.46)
78.47 (-0.34)
64
77.95 (-0.84)
78.17 (-0.62)
78.27 (-0.52)
78.45 (-0.34)
16
78.84 (+0.04)
78.86 (+0.06)
78.85 (+0.05)
78.85 (+0.05)
32
78.89 (+0.08)
78.89 (+0.08)
78.90 (+0.09)
78.90 (+0.09)
64
78.97 (+0.18)
78.94 (+0.15)
78.99 (+0.20)
78.97 (+0.18)
[%]
EMB dim.
TT-rank
All-TT
SCRec
(Ours)
Fig. 12. Accuracy comparison. (Numbers in parentheses represent accuracy
difference from vanilla-DLRM)
fetching hot embedding vectors into high-bandwidth memory
devices. These results demonstrate that three-level sharding in
SCRec not only addresses DRAM capacity limitations but also
effectively hides SSD access latency, significantly enhancing
the performance of the embedding layer in industrial-scale
DLRMs.
F. Accuracy
Using low-rank approximation techniques such as TT de-
composition significantly increases the data CR. However,
applying the TT-format introduces information loss, and as
the TT-rank decreases, the quality of data representation
deteriorates, resulting in lower model accuracy. Therefore,
minimizing accuracy drop becomes a critical focus when
compressing the embedding layer in the DLRM using the
TT-format. To evaluate accuracy when part of the EMBs
is represented in the TT-format with SCRec, we varied the
embedding dimension and rank, as shown in Fig.12. For
this evaluation, we used the RM3 configuration described in
Fig.9, based on the CDA dataset. When all embedding layers
were replaced with the TT-format, and inference accuracy was
measured after training, the accuracy drop was highest at a
TT-rank of 4 across all embedding dimensions, ranging from -
0.84% to -0.73%. In contrast, the accuracy drop was lowest at a
TT-rank of 32, ranging from -0.40% to -0.34%, demonstrating
that higher TT-ranks improve the data quality of EMBs and
reduce accuracy loss. SCRec, on the other hand, exhibited
a slight accuracy increase of +0.04% to +0.20% across all
experiments. This improvement occurs because SCRec does
not apply the TT-format to all EMBs. Instead, it stores most
frequently accessed embedding vectors in DRAM, while less
frequently accessed ones are stored in the TT-format. Conse-
quently, only a small portion of embedding vector accesses
are impacted by accuracy loss. These findings indicate that in
SCRec, using a TT-rank of 4 achieves high compression rates
while maintaining high-throughput access to the embedding
layer without any accuracy drop. Beyond the CDA dataset,
SCRec facilitates partial compression of EMBs in large-scale
industrial DLRMs using the TT-format, suggesting its potential
for effective application in these scenarios while mitigating
accuracy loss.
V. RELATED WORKS
A. Multi-tier Memory Caching and Sharding
Due to the massive size of the embedding layer and
the irregular access patterns with a power-law distribution
in DLRM, many studies have focused on leveraging multi-
tier memory and caching schemes to reduce EMB access
latency. Scratch-Pipe [43] utilizes a 2-level memory hierarchy
consisting of host CPU memory and GPU memory, caching
embedding vectors in GPU memory to overcome the limited
memory capacity of GPUs. This approach enables the embed-
ding layer to be trained at GPU memory speed. AIBox [20]
integrates GPU and SSD devices to store large embedding
layers, implementing an SSD cache management system to
provide low-latency access to embedding layers stored in
SSDs. This allows industrial-scale recommendation models to
be trained on a single node.
In addition to caching methods, sharding schemes that
split model parameters across multiple devices have also
been extensively researched. RecShard [19] leverages multiple
GPUs, placing frequently accessed data in the GPU’s HBM
and less frequently accessed data in unified virtual memory
(UVM), which encompasses host DRAM and GPU HBM. By
employing a cost model before runtime, RecShard effectively
partitions the DLRM across GPU devices, significantly en-
hancing training throughput. Similarly, AutoShard [44] uti-
lizes a cost model to estimate table computation costs and
incorporates deep reinforcement learning (DRL) to address the
EMB partitioning challenge across multiple GPUs, achieving
balanced sharding among GPUs.
B. Tensor-train Decomposition
In addition to the challenges posed by large EMB sizes in
DLRM, the substantial vocabulary size in transformer models
presents a significant hurdle in natural language processing
(NLP). To address this, TT-embedding [42] was introduced
to efficiently compress EMBs using tensor-train (TT) decom-
position [31]. This method enables seamless integration into
any deep learning framework and supports training via back-
propagation. TT-Rec [32] applies the TT-embedding format to
effectively compress EMBs in DLRM, introducing a scheme
that initializes TT-core weights based on a sampled Gaussian
distribution. It demonstrated negligible accuracy loss when
tested on the CDA and Criteo Terabyte datasets. Similarly,
EL-Rec [45] utilized the TT-format to compress the embedding
layer and developed a TT-based training pipeline system. This
approach mitigated host communication latency between the
CPU and GPU, improving training performance on a single
GPU.
VI. CONCLUSION
In this paper, we present SCRec, a scalable computational
storage system with statistical sharding and TT decomposition
for recommendation models. On the software side, SCRec
leverages three-level statistical sharding to meet the high
bandwidth requirements of DLRM by storing hot data in high-
bandwidth memory devices and cold data in a low-bandwidth
memory device. Additionally, SCRec can adaptively configure
memory-centric and compute-centric cores based on DLRM
workload intensity, thereby configuring optimized system. On
the hardware side, SCRec implements custom hardware accel-
eration cores to enhance DLRM computations. In particular, it


--- Page 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
13
enables the high-performance reconstruction of approximated
embedding vectors from significantly compressed TT-format,
complementing memory capacity limitations while enhancing
memory bandwidth. By integrating the software framework
with the hardware accelerators into our system using mul-
tiple SmartSSDs, SCRec achieved up to 55.77× inference
performance improvement compared to a CPU-DRAM system
and up to 13.35× energy efficiency improvement compared
to a multi-GPU system. These results validate that large-
scale DLRMs can be implemented on a single server with
high performance and low energy costs, eliminating data
communication overhead.
REFERENCES
[1] P. Resnick and H. R. Varian, “Recommender systems,” Communications
of the ACM, vol. 40, no. 3, pp. 56–58, 1997.
[2] S. K. Raghuwanshi and R. K. Pateriya, “Recommendation systems:
techniques, challenges, application, and evaluation,” in Soft Computing
for Problem Solving: SocProS 2017, Volume 2.
Springer, 2019, pp.
151–164.
[3] Z. Z. Darban and M. H. Valipour, “Ghrs: Graph-based hybrid recom-
mendation system with application to movie recommendation,” Expert
Systems with Applications, vol. 200, p. 116850, 2022.
[4] P. Covington, J. Adams, and E. Sargin, “Deep neural networks for
youtube recommendations,” in Proceedings of the 10th ACM conference
on recommender systems, 2016, pp. 191–198.
[5] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,
G. Anderson, G. Corrado, W. Chai, M. Ispir et al., “Wide & deep
learning for recommender systems,” in Proceedings of the 1st workshop
on deep learning for recommender systems, 2016, pp. 7–10.
[6] M. Naumov, J. Kim, D. Mudigere, S. Sridharan, X. Wang, W. Zhao,
S. Yilmaz, C. Kim, H. Yuen, M. Ozdal et al., “Deep learning training in
facebook data centers: Design of scale-up and scale-out systems,” arXiv
preprint arXiv:2003.09518, 2020.
[7] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry, “Using collaborative
filtering to weave an information tapestry,” Communications of the ACM,
vol. 35, no. 12, pp. 61–70, 1992.
[8] Y. Hu, Y. Koren, and C. Volinsky, “Collaborative filtering for implicit
feedback datasets,” in 2008 Eighth IEEE international conference on
data mining.
Ieee, 2008, pp. 263–272.
[9] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for
recommender systems,” Computer, vol. 42, no. 8, pp. 30–37, 2009.
[10] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collabo-
rative filtering recommendation algorithms,” in Proceedings of the 10th
international conference on World Wide Web, 2001, pp. 285–295.
[11] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman,
J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini et al., “Deep
learning recommendation model for personalization and recommenda-
tion systems,” arXiv preprint arXiv:1906.00091, 2019.
[12] H.-J. M. Shi, D. Mudigere, M. Naumov, and J. Yang, “Compositional
embeddings using complementary partitions for memory-efficient rec-
ommendation systems,” in Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining,
2020, pp. 165–175.
[13] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks,
B. Cottel, K. Hazelwood, M. Hempstead, B. Jia et al., “The architectural
implications of facebook’s dnn-based personalized recommendation,” in
2020 IEEE International Symposium on High Performance Computer
Architecture (HPCA).
IEEE, 2020, pp. 488–501.
[14] A. Firoozshahian, J. Coburn, R. Levenstein, R. Nattoji, A. Kamath,
O. Wu, G. Grewal, H. Aepala, B. Jakka, B. Dreyer et al., “Mtia:
First generation silicon targeting meta’s recommendation systems,” in
Proceedings of the 50th Annual International Symposium on Computer
Architecture, 2023, pp. 1–13.
[15] S. Agarwal, C. Yan, Z. Zhang, and S. Venkataraman, “Bagpipe: Ac-
celerating deep recommendation model training,” in Proceedings of the
29th Symposium on Operating Systems Principles, 2023, pp. 348–363.
[16] Y. Kwon, Y. Lee, and M. Rhu, “Tensordimm: A practical near-memory
processing architecture for embeddings and tensor operations in deep
learning,” in Proceedings of the 52nd Annual IEEE/ACM International
Symposium on Microarchitecture, 2019, pp. 740–753.
[17] V. Gupta, D. Choudhary, P. Tang, X. Wei, X. Wang, Y. Huang, A. Ke-
jariwal, K. Ramchandran, and M. W. Mahoney, “Training recommender
systems at scale: Communication-efficient model and data parallelism,”
in Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining, 2021, pp. 2928–2936.
[18] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li, “Distributed
hierarchical gpu parameter server for massive scale deep learning ads
systems,” Proceedings of Machine Learning and Systems, vol. 2, pp.
412–428, 2020.
[19] G. Sethi, B. Acun, N. Agarwal, C. Kozyrakis, C. Trippel, and C.-
J. Wu, “Recshard: statistical feature-based memory optimization for
industry-scale neural recommendation,” in Proceedings of the 27th ACM
International Conference on Architectural Support for Programming
Languages and Operating Systems, 2022, pp. 344–358.
[20] W. Zhao, J. Zhang, D. Xie, Y. Qian, R. Jia, and P. Li, “Aibox: Ctr
prediction model training on a single node,” in Proceedings of the
28th ACM International Conference on Information and Knowledge
Management, 2019, pp. 319–328.
[21] U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.-
H. S. Lee, D. Brooks, and C.-J. Wu, “Deeprecsys: A system for
optimizing end-to-end at-scale neural recommendation inference,” in
2020 ACM/IEEE 47th Annual International Symposium on Computer
Architecture (ISCA).
IEEE, 2020, pp. 982–995.
[22] X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, “Rm-
ssd: In-storage computing for large-scale recommendation inference,” in
2022 IEEE International Symposium on High-Performance Computer
Architecture (HPCA).
IEEE, 2022, pp. 1056–1070.
[23] M. Kim and S. Lee, “Reducing tail latency of dnn-based recommender
systems using in-storage processing,” in Proceedings of the 11th ACM
SIGOPS Asia-Pacific Workshop on Systems, 2020, pp. 90–97.
[24] M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks,
and G.-Y. Wei, “Recssd: near data processing for solid state drive
based recommendation inference,” in Proceedings of the 26th ACM
International Conference on Architectural Support for Programming
Languages and Operating Systems, 2021, pp. 717–729.
[25] H. Wan, X. Sun, Y. Cui, C.-L. Yang, T.-W. Kuo, and C. J. Xue,
“Flashembedding: storing embedding tables in ssd for large-scale recom-
mender systems,” in Proceedings of the 12th ACM SIGOPS Asia-Pacific
Workshop on Systems, 2021, pp. 9–16.
[26] Z. Wang, Y. Wei, M. Lee, M. Langer, F. Yu, J. Liu, S. Liu, D. G. Abel,
X. Guo, J. Dong et al., “Merlin hugectr: Gpu-accelerated recommender
system training and inference,” in Proceedings of the 16th ACM Con-
ference on Recommender Systems, 2022, pp. 534–537.
[27] Y. Xiao, S. Zhao, Z. Zhou, Z. Huan, L. Ju, X. Zhang, L. Wang,
and J. Zhou, “G-meta: Distributed meta learning in gpu clusters for
large-scale recommender systems,” in Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management,
2023, pp. 4365–4369.
[28] D. Mudigere, Y. Hao, J. Huang, Z. Jia, A. Tulloch, S. Sridharan, X. Liu,
M. Ozdal, J. Nie, J. Park et al., “Software-hardware co-design for
fast and scalable training of deep learning recommendation models,” in
Proceedings of the 49th Annual International Symposium on Computer
Architecture, 2022, pp. 993–1011.
[29] Z. Wang, Y. Wang, J. Deng, D. Zheng, A. Li, and Y. Ding, “Rap:
Resource-aware automated gpu sharing for multi-gpu recommendation
model training and input preprocessing,” in Proceedings of the 29th ACM
International Conference on Architectural Support for Programming
Languages and Operating Systems, Volume 2, 2024, pp. 964–979.
[30] Y. Kwon, Y. Lee, and M. Rhu, “Tensor casting: Co-designing algorithm-
architecture for personalized recommendation training,” in 2021 IEEE
International Symposium on High-Performance Computer Architecture
(HPCA).
IEEE, 2021, pp. 235–248.
[31] I. V. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scien-
tific Computing, vol. 33, no. 5, pp. 2295–2317, 2011.
[32] C. Yin, B. Acun, C.-J. Wu, and X. Liu, “Tt-rec: Tensor train compression
for deep learning recommendation models,” Proceedings of Machine
Learning and Systems, vol. 3, pp. 448–462, 2021.
[33] Facebook Research, “Embedding lookup synthetic dataset,” 2021,
accessed:
2021-12-09.
[Online].
Available:
https://github.com/
facebookresearch/dlrm datasets
[34] Facebook
Research,
“An
implementation
of
a
deep
learning
recommendation model (dlrm),” 2019, accessed: 2019-03-21. [Online].
Available: https://github.com/facebookresearch/dlrm
[35] Y. Kim, W. Yang, and O. Mutlu, “Ramulator: A fast and extensible
dram simulator,” IEEE Computer architecture letters, vol. 15, no. 1, pp.
45–49, 2015.


--- Page 14 ---
JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY
14
[36] A. Tavakkol, J. G´omez-Luna, M. Sadrosadati, S. Ghose, and O. Mutlu,
“{MQSim}: A framework for enabling realistic studies of modern
{Multi-Queue}{SSD} devices,” in 16th USENIX Conference on File
and Storage Technologies (FAST 18), 2018, pp. 49–66.
[37] M. Jung, J. Zhang, A. Abulila, M. Kwon, N. Shahidi, J. Shalf, N. S. Kim,
and M. Kandemir, “Simplessd: Modeling solid state drives for holistic
system simulation,” IEEE Computer Architecture Letters, vol. 17, no. 1,
pp. 37–41, 2017.
[38] S. Ghose, A. G. Yaglikc¸i, R. Gupta, D. Lee, K. Kudrolli, W. X. Liu,
H. Hassan, K. K. Chang, N. Chatterjee, A. Agrawal et al., “What
your dram power models are not telling you: Lessons from a detailed
experimental study,” Proceedings of the ACM on Measurement and
Analysis of Computing Systems, vol. 2, no. 3, pp. 1–41, 2018.
[39] V. Cerf and R. Kahn, “A protocol for packet network intercommunica-
tion,” IEEE Transactions on communications, vol. 22, no. 5, pp. 637–
648, 1974.
[40] Crtieo
AI
Lab,
“Kaggle
display
advertising
dataset,”
2014,
accessed:
2014-06-25.
[Online].
Available:
https://go.criteo.net/
criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz
[41] Gurobi Optimization, “Gurobi optimizer reference manual,” 2024. [On-
line]. Available: https://docs.gurobi.com/projects/optimizer/en/current/
[42] O. Hrinchuk, V. Khrulkov, L. Mirvakhabova, E. Orlova, and I. Oseledets,
“Tensorized embedding layers for efficient model compression,” arXiv
preprint arXiv:1901.10787, 2019.
[43] Y. Kwon and M. Rhu, “Training personalized recommendation systems
from (gpu) scratch: Look forward not backwards,” in Proceedings of the
49th Annual International Symposium on Computer Architecture, 2022,
pp. 860–873.
[44] D. Zha, L. Feng, B. Bhushanam, D. Choudhary, J. Nie, Y. Tian, J. Chae,
Y. Ma, A. Kejariwal, and X. Hu, “Autoshard: Automated embedding
table sharding for recommender systems,” in Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
2022, pp. 4461–4471.
[45] Z. Wang, Y. Wang, B. Feng, D. Mudigere, B. Muthiah, and Y. Ding,
“El-rec: Efficient large-scale recommendation model training via tensor-
train embedding table,” in SC22: International Conference for High
Performance Computing, Networking, Storage and Analysis.
IEEE,
2022, pp. 1–14.
Jinho Yang Jinho Yang (Graduate Student Mem-
ber, IEEE) received the B.S. degree in electrical
engineering from Hanyang University, Seoul, South
Korea, in 2022, and the M.S. degree in electrical en-
gineering from Korea Advanced Institute of Science
and Technology (KAIST), Daejeon, South Korea,
in 2025, where he is currently pursuing the Ph.D.
degree.
His research interests include a hardware accel-
erator for machine learning, hardware-software co-
design, and a near-memory processing system.
Ji-Hoon
Kim
Ji-Hoon Kim (Graduate Student
Member, IEEE) received the B.S. degree in electrical
engineering from Kyung-Hee University, Suwon,
South Korea, in 2017 and the M.S. and Ph.D. degree
in electrical engineering from the Korea Advanced
Institute of Science and Technology (KAIST), Dae-
jeon, South Korea, in 2019 and 2024, respectively.
His research interests include AI/ML hardware
accelerator (ASIC & FPGA) design, efficient AI/ML
system design, energy-efficient processing-in/near-
memory architecture, and hardware/software code-
sign for DNN processing.
Joo-Young Kim Joo-Young Kim (Senior Member,
IEEE) received the B.S., M.S., and Ph. D degrees in
electrical engineering from Korea Advanced Insti-
tute of Science and Technology (KAIST), Daejeon,
South Korea, in 2005, 2007, and 2010, respectively.
He is currently an Associate Professor in the School
of Electrical Engineering, KAIST, and the Director
of the AI Semiconductor Systems Research Center,
KAIST. His research interests span various aspects
of hardware design, including chip design, com-
puter architecture, domain-specific accelerators, and
hardware/software co-design. Before joining KAIST, he was a Hardware
Engineering Leader at Microsoft Azure, Redmond, WA, USA, working on
hardware acceleration for cloud services such as machine learning, data
storage, and networking.
He founded an AI fabless startup, HyperAccel, in Jan 2023 to build innova-
tive AI processors/solutions for large-language-model(LLM)-based generative
AI, making it sustainable for everyone.
