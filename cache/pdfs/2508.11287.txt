--- Page 1 ---
Journal of Communications and Information Networks
Research paper
CSGO: Generalized Optimization for Cold Start in
Wireless Collaborative Edge LLM Systems
Xuran Liu, Nan Xue, Rui Bao, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Shuguang Cui
Abstractâ€” While deploying large language models on
edge devices promises low-latency and privacy-preserving
AI services, it is hindered by limited device resources.
Although
pipeline
parallelism
facilitates
distributed
inference, existing approaches often ignore the cold-start
latency caused by on-demand model loading.
In this
paper, we propose a latency-aware scheduling framework
that overlaps model loading with computation and com-
munication to minimize total inference latency. Based on
device and model parameters, the framework dynamically
adjusts layer partitioning and allocation to effectively hide
loading time, thereby eliminating as many idle periods as
possible. We formulate the problem as a Mixed-Integer
Non-Linear Program and design an efficient dynamic
programming algorithm to optimize model partitioning
and device assignment. Experimental results show that
the proposed method significantly reduces cold-start
latency compared to baseline strategies.
Keywordsâ€” large language models, mobile edge com-
puting, cold start latency, pipeline parallelism
X.R. Liu, N.Xue, R. Bao, Z.Y. Chen, M.X. Tao. Cooperative Medi-
anet Innovation Center, Shanghai Jiao Tong University, Shanghai 200240,
China.
(e-mail: {3232794836, nan.xue, 851756936, zhiyongchen, mx-
tao}@sjtu.edu.cn).
Y.P. Sun. Department of Broadband Communication, Pengcheng Labo-
ratory, Shenzhen 518000, China and Future Network of Intelligent Institute
(FNii), the Chinese University of Hong Kong (Shenzhen), Shenzhen 518172,
China. (e-mail: sunyp@pcl.ac.cn).
X.D. Xu. Beijing University of Posts and Telecommunications, Beijing
100876, China and Department of Broadband Communication, Pengcheng
Laboratory, Shenzhen 518000, China. (e-mail: xuxiaodong@bupt.edu.cn).
S.G. Cui.
School of Science and Engineering (SSE) and Future Net-
work of Intelligent Institute (FNii), the Chinese University of Hong Kong
(Shenzhen), Shenzhen 518172, China and Department of Broadband Com-
munication, Pengcheng Laboratory, Shenzhen 518000, China.
(e-mail:
shuguangcui@cuhk.edu.cn).
1
INTRODUCTION
Recent years have witnessed a paradigm shift in artifi-
cial intelligence (AI), driven by the advent of Large Lan-
guage Models (LLMs) and generative diffusion models[1] .
These models exhibit unprecedented capabilities, revolution-
izing domains from natural language processing to creative
content generation. As their influence extends across diverse
aspects of digital lives, there is a growing demand to move
beyond cloud-centric deployments and embed these powerful
AI functionalities directly into personal computing, particu-
larly onto mobile devices such as smartphones. On-device
inference holds the promise of a new wave of applications, of-
fering advantages such as real-time responsiveness, enhanced
user privacy through local data processing[2], and highly per-
sonalized user experiences.
However, realizing this vision poses a fundamental chal-
lenge: the massive computational and memory requirements
of state-of-the-art models sharply contrast with the limited re-
sources of mobile devices. A single large model can easily
exceed the available RAM and processing power of a typical
smartphone, rendering local execution impractical. To address
this mismatch, distributed inference has emerged as a promis-
ing paradigm[3] . The key idea is to partition a monolithic
model into a smaller segments, or â€œshardsâ€, and distribute
them across a network of collaborating mobile devices, as de-
picted in Fig. 1. By organizing these shards into a pipeline
parallel workflow, the system can process inference requests
in an assembly-line fashion, leveraging the aggregate capa-
bilities of multiple devices to overcome individual hardware
constraints.
1.1
Related Work
Pipeline Parallelism (PP)[4] , a fundamental model paral-
lelism strategy, has been widely adopted for both the training
and inference of Deep Neural Networks (DNNs).
In cloud-centric server environments, a variety of dis-
arXiv:2508.11287v1  [cs.IT]  15 Aug 2025


--- Page 2 ---
2
Journal of Communications and Information Networks
Figure 1
Pipeline-parallel model deployment on edge wireless networks
with heterogeneous devices. The system adopts a star topology, where all
devices communicate via a single access point (AP). The model is partitioned
into sequential stages and distributed across devices for pipelined execution.
tributed frameworks have been developed to support large-
scale model deployment,
including GPipe,
PipeDream,
GraphPipe, ZeRO, and PipeFusion[5-9] . These systems par-
tition models across multiple compute devices to overcome
single-device memory limitations and accelerate computa-
tion. However, they are designed with the assumption of de-
ployment in data centers, where abundant computational and
memory resources, along with stable network connectivity,
are readily available. In such settings, models are typically
preloaded into memory and kept resident to serve a high vol-
ume of continuous requestsâ€“a model we refer to as â€œalways-
onâ€ service.
This paradigm contrasts sharply with edge deployment sce-
narios, particularly on personal devices like smartphones or
laptops, where memory, compute, and energy are highly con-
strained.
To address these challenges, several lightweight
pipeline parallel frameworks have emerged, such as PipeEdge,
EdgePipe, EPipe, and WDMoE[10-13] . These frameworks aim
to optimize throughput for continuous inference tasks and are
generally targeted at edge servers like smart cameras or IoT
gateways, where models are loaded and executed over ex-
tended periods. However, they are less suited for personal
edge devices, where inference requests tend to be sporadic
and infrequent, and resources may not permit persistent model
residency.
In addition, model compression methods such as model
quantization[14-15] , pruning[16-17] , and distillation[18-19] are
also widely used to improve the performance of lightweight
models suitable for deployment on edge devices.
However,
these methods do not address the often-
overlooked cold start problem. On personal devices, where
long idle periods between inference requests are typical, each
task may require loading the model from storage into mem-
ory from scratch. This results in substantial latency, which
becomes a crucial bottleneck that directly impacts user expe-
rience. In this paper, we aim to address this overlooked yet
crucial challenge.
1.2
Contributions
To address this cold-start challenge in wireless distributed
collaborative inference, this paper proposes a latency-aware
pipeline scheduling algorithm tailored for edge environ-
ments. The key idea is to overlap model loading with ongoing
computation and communication phases within the pipeline.
By strategically scheduling the on-demand loading of down-
stream shards in parallel with the execution of upstream
stages, the proposed method effectively hides the loading la-
tency, reduces pipeline stalls, and minimizes end-to-end infer-
ence latency.
The main contributions of this work are summarized as fol-
lows:
â€¢ We systematically identify and analyze the cold-start la-
tency issue in mobile distributed cooperative inference scenar-
ios. Building upon a pipeline-parallel framework, we mathe-
matically model the latency introduced by the cold start of
large models.
â€¢ We design and implement a dynamic programming
(DP)-based scheduling algorithm that jointly determines opti-
mal layer partitioning and devices assignment for both loading
and computation, with the goal of minimizing the end-to-end
inference latency.
â€¢ We validate our approach through extensive experi-
ments, demonstrating that the proposed method substantially
reduces user-perceived latency and improves pipeline effi-
ciency compared to conventional strategies that decouple
loading from computation.
The rest of this paper is organized as follows. Section II
presents the system model of collaborative edge LLMs infer-
ence. Section III formulates the optimization problem. Sec-
tion IV proposes DP algorithm tailored for the latency opti-
mization. Experimental results are shown in Section V and
conclusions are drawn in Section VI.
2
SYSTEM MODEL
2.1
Large Model Details
We aim to deploy a large Transformer-based model[20]
onto a wireless network illustrated in Fig. 1. In the model, we
focus exclusively on the Transformer Blocks, omitting the em-
bedding and output layers. Let the model consists of L Trans-


--- Page 3 ---
CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems
3
former Block layers, indexed by the set L â‰œ{1,2,...,L}.
In the edge deployment scenarios of interest, the number of
model layers typically exceeds the number of available de-
vices, i.e., L â‰¥K. Each layer l âˆˆL is characterized by a
triple Ï†l = {Wl,Al,Dl}, where Wl is the computational work-
load in floating point operations (FLOPs), Al is the size of
the activations in megabytes (MB), Dl is the parameter size in
MB.
The
model
is
a
Transformer-based
LLM
that
em-
ploys a multi-head attention mechanism with Grouped
Query Attention (GQA)[21] and a Swish-Gated Linear Unit
(SwiGLU)[22] -based feed-forward network (FFN). Both ac-
tivations and parameters are represented using the bfloat16
(bf16) format.
The input representation has dimensionality dmodel. The at-
tention layer includes hq query heads and hk = hv key-value
heads, each with dimensionality dhead. The intermediate di-
mensionality of the feed-forward network is denoted by dff.
To enable the calculation of concrete metrics such as compu-
tational cost and activation size, we define the input sequence
length as t.
Given the above definitions, the metrics are calculated as
follows:
W ATTN
l
= 4tdhead(dmodelhq +dmodelhk +thq),
(1)
W FFN
l
= 6tdmodeldff,
(2)
Wl = W ATTN
l
+W FFN
l
.
(3)
Here,W ATTN
l
includes the FLOPs associated with the query,
key, and value (Q/K/V) projections, the context computa-
tion, and output projection.
Similarly, W FFN
l
accounts for
the FLOPs from the up-projection, gate-projection, and down-
projection within the FFN. Both additions and multiplications
involved in matrix operations are included in the FLOP count.
However, overheads from nonlinear activations functions and
element-wise operations are omitted.
Similarly, based on the definitions of GQA and SwiGLU,
the activation size and the parameter size of the model layer
can be computed using the following formulas:
Al = 2tdmodel,
(4)
PATTN
l
= 4dmodeldhead(hq +hk),
(5)
PFFN
l
= 6dmodeldff,
(6)
Pl = PATTN
l
+PFFN
l
.
(7)
Here, PATTN
l
includes weights for the Q/K/V projections and
the output projection.
PFFN
l
includes weights for the up-
projection, gate-projection, and down-projection.
2.2
System Resource Modeling
We consider a wireless network consisting of a single AP
and K mobile devices, indexed by the set K â‰œ{1,2,...,K}.
These devices and the AP form a star topology, where com-
munication between any tow devices occurs via the AP. Typi-
cal deployment scenarios include indoor Wi-Fi networks and
cellular networks. Each device k âˆˆK is characterized by a
profile Ïˆk = {ck,bu
k,bd
k,rk,mk}, where ck is the computational
capability in FLOPS, bu
k is the uplink bandwidth from mobile
devices to the AP in Mbps, bd
k is the downlink bandwidth from
the AP, measured in Mbps, rk is the disk read speed in MB/s,
and mk is the GPU memory capacity in GB.
2.2.1
Computational Resources
We focus on low-workload inference scenarios, which are
typical in edge deployments. In such setting, the actual uti-
lization of a mobile device is often substantially lower than
its theoretical peak performance or the average utilization ob-
served during large-scale training[23]. Depending on factors
such as batch size and input sequence length, utilization rates
can vary widely, typically ranging from 1% to 40%. As a
result, it is important to account for workload size when eval-
uating the effective computational capability.
To capture this behavior, we model utilization using a satu-
rating exponential function:
U(t) = a(1âˆ’eâˆ’bt),
(8)
where U(t) denotes the utilization rate as a function of the
workload t, a represents the maximum achievable utilization,
and b controls the growth rate. The parameters a and b are de-
rived from empirical observations in preliminary experiments.
Let câˆ—
k denote the theoretical peak computational capability of
device k. The effective computational capability, accounting
for workload-dependent utilization, is then given by:
ck = câˆ—
kU(t).
(9)
2.2.2
I/O Resources
During model cold start phase, the primary I/O bottleneck
is the disk read speed[24] . As a result, we use the disk read
speed as a proxy for the deviceâ€™s overall I/O performance.
Since this metric is generally stable and the underlying-
hardware-level read process lies outside the scope of our opti-
mization, we abstract it using standardized, canonical values.
The specific values adopted in our analysis are provided in
Section V.
2.2.3
Communication Resources
Let Bk denote the channel bandwidth allocated to device k,
and Pu
k and Pd
k represent its uplink and downlink transmission
power, respectively. Let dk be the distance from the device
to the AP and N0 be the noise power spectral density. Fur-
thermore, to reflect practical constraints, an efficiency factor


--- Page 4 ---
4
Journal of Communications and Information Networks
Âµ is applied to account for protocol overheads and practical
limitations[25-26] .
The achievable uplink and downlink data rates for device k
are formulated as follows[27]:
bu
k = ÂµBk log2 (1+ Pu
k gk
N0Bk
),
(10)
bd
k = ÂµBk log2 (1+ Pd
k gk
N0Bk
),
(11)
where gk represents the channel gain, which is modeled using
a path loss model as follows[28] :
gk = Î²0(dk
d0
)âˆ’Î¶
(12)
where Î²0 is the channel gain at a reference distance d0, and
Î¶ is the path loss exponent. For simplicity, we consider the
channel gain gk is symmetric for both uplink and downlink
transmissions.
2.3
Deployment framework
This paper adopts a pipeline parallelism strategy to deploy
the model across K heterogeneous mobile devices, as depicted
in Fig.2. The large modelâ€™s L layers are partitioned into N
contiguous, non-overlapping segments (hereafter referred to
as partitions), where 1 â‰¤N â‰¤K. Each partition is assigned
exclusively to a single device for execution.
To formalize the partitioning scheme, we define a triplet
{kn,sn,en} for each partition n âˆˆN â‰œ{1,2,...,N}. Here,
kn âˆˆK denotes the index of the device assigned to the n-th
partition, while sn âˆˆL and en âˆˆL represent the starting and
ending layer indices of the partition, respectively.
The allocation scheme must satisfy the following con-
straints. First, devices assigned to different partitions must
be unique:
âˆ€n,nâ€² âˆˆN ,n Ì¸= nâ€² â‡’kn Ì¸= knâ€².
(13)
Second, the partitioning must be contiguous and compre-
hensive, covering all layers of the model:
s1 = 1,
eN = L,
(14)
âˆ€n âˆˆN ,
sn â‰¤en,
(15)
âˆ€n âˆˆN \{1},
sn = enâˆ’1 +1.
(16)
Before the inference task begins, device kn stores the model
parameters for its assigned layers (from sn to en) in its local
storage. When an inference request arrives, it triggers a cold
start process consisting of three main stages: model loading,
activation transfer, and forward computation.
During pipeline execution, the start time tstart
kn
for device
kn depends on two factors: the completion of its own model
loading and the computation finish time tfinish
knâˆ’1 of the preced-
ing device knâˆ’1. Due to the limited I/O bandwidth of mobile
devices, we consider that model loading and transmission are
performed sequentially, that is, a device cannot begin commu-
nicating until it has completed loading the model. The model
loading start time and the output transmission completion time
of device kn are given by:
tstart
kn
= max(tload
kn ,tfinish
knâˆ’1 ),
(17)
tfinish
kn
= tstart
kn
+tcomm
kn
+tcomp
kn
.
(18)
Here, tfinish
k0
is defined as 0.
The terms tload
kn , tcomm
kn
and
tcomp
kn
represent the model loading latency, activation transfer
latency, and computation latency of device kn, respectively.
These latencies are calculated as follows:
tload
kn
= âˆ‘en
l=sn Pl
rkn
,
(19)
tcomm
kn
=
Asnâˆ’1
min(bu
knâˆ’1,bd
kn),
(20)
tcomp
kn
= âˆ‘en
l=sn Wl
ckn
.
(21)
Additionally, to account for device memory constraints, we
have:
mkn â‰¥max
snâ‰¤lâ‰¤en Al +
en
âˆ‘
l=sn
Pl.
(22)
3
PROBLEM FORMULATION
In this section, we first highlight the necessity of optimiza-
tion by analyzing the performance bottlenecks of a baseline
deployment strategy. We then formulate a combinatorial op-
timization problem aimed at minimizing the total cold-start
latency:
T = tfinish
kN
.
(23)
To illustrate the core idea behind the proposed method, we
first consider a baseline method in which the model is evenly
partitioned into K segments and deployed across K homoge-
neous devices. As shown in Fig. 2, this strategy parallelizes
model loading across devices but fails to exploit pipelining op-
portunities between model loading and computation. Specif-
ically, each device kn must wait for its predecessor knâˆ’1 to
complete both its computation and activation transfer before
beginning its own computation. This results in idle time which
can be quantified as:
twait
kn
= max(0,tfinish
knâˆ’1 âˆ’tload
kn ).
(24)
To eliminate this bottleneck, the proposed algorithm aims
to align the computation completion time of a predecessor


--- Page 5 ---
CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems
5
Figure 2
Gantt charts illustrating different allocation strategies on homogeneous devices, covering the entire cold start process including model loading,
communication, and computation (prefill). There are two kinds of bubbles: obstructive and non-obstructive. The bubble after loading is an inevitable idle time
while the bubble after computation can be filled by other requests. The former is what we aim to eliminate. In all strategy, the pipeline proceeds from top to
bottom. Ideal Single Device refers to assigning all model layers to the most powerful device in the system, assuming no RAM constraints. Even denotes a
uniform (even) distribution.
device (tfinish
knâˆ’1 ) with the model loading time of its successor
(tload
kn ) through fine-grained layer allocation optimization. Ide-
ally, this alignment ensures that twait
kn
â‰ˆ0, enabling an efficient
pipeline between the predecessorâ€™s computation and the suc-
cessorâ€™s loading phase, thereby reducing end-to-end cold-start
latency. This core principle extends naturally to more com-
plex, heterogeneous environments. Even when devices differ
in computational capacity (ck) and communication resources
(rk,bk), effective overlap between computation and loading
can still be achieved by identifying an optimal, non-uniform
model partitioning.
Based on the above analysis, we formalize the cold-start
latency minimization problem as follows:
P0:
min
N,k,s,e
T
s.t.
N = {1,2,...,N}
(c1)
n Ì¸= nâ€² â‡’kn Ì¸= knâ€²,
âˆ€n,nâ€² âˆˆN
(c2)
s1 = 1
(c3)
eN = L
(c4)
sn â‰¤en,
âˆ€n âˆˆN
(c5)
sn = enâˆ’1 +1,
âˆ€n âˆˆN \{1}
(c6)
mkn â‰¥max
snâ‰¤lâ‰¤en Al +
en
âˆ‘
l=sn
Pl,
âˆ€n âˆˆN
(c7)
where k = (k1,k2,...,kN),
s = (s1,s2,...,sN) and e =
(e1,e2,...,eN) denote the vectors of the assigned device, start-
ing layer indices, and ending layer indices, respectively.
The optimization problem (P0) is a complex Mixed-Integer
Non-Linear Program (MINLP). Its complexity arises primar-
ily from two factors: (1) the presence of integer decision vari-
ables, including the number of partitions N, the layer bound-
aries sn,en, and the combinatorial device assignments kn; and
(2) the nonlinearities introduced by the use of max and min
operators in the timing constraints. Overall, P0 is an NP-hard
problem.
Despite its theoretical intractability, we observe that in
practical deployment scenarios, the problem size is often mod-
erate, e.g., the number of available devices K and model lay-
ers L typically satisfy K â‰¤20,L â‰¤100). More importantly,
the problem exhibits a clear optimal substructure property,
meaning that an optimal solution to the full problem can be
built from optimal solutions to its subproblems. This property
makes the problem well-suited for a DP approach. Accord-
ingly, we design a DP algorithm capable of computing the ex-
act optimal solution within a reasonable runtime, as detailed
in the following section.
4
PROPOSED DP ALGORITHM
In the previous section, we formulated the problem as a
MINLP, which involves jointly determining the model parti-
tioning and computational task assignment. Due to the pres-
ence of nonlinear and non-convex operators, the problem can-
not be efficiently solved using brute-force methods, which re-
quire exhaustive search with a computational complexity of


--- Page 6 ---
6
Journal of Communications and Information Networks
Algorithm 1 Optimal Pipeline Scheduling
Require: Number of devices K, number of model layers
L, device parameters {Ïˆ1,Ïˆ2,...,ÏˆK}, model parameters
{Ï†1,Ï†2,...,Ï†L}
Ensure: Minimum cold-start latency T, allocation plan plan
1: // Preprocessing
2: Compute and store all M(i, j), Tload(i, j,d), Tcomp(i, j,d),
and Tcomm(dâ€²,d,i) values.
3: Initialize DP(S, j,d) â†âˆfor all states.
4: Initialize path(S, j,d) â†null for backtracking.
5: // Base Cases
6: DP(S, j,d) â†Tload(1, j,d) + Tcomp(1, j,d) for all j â‰¤L,
d â‰¤K, S = (1 â‰ª(d âˆ’1)).
7: path(S, j,d) â†(0,0) for all j â‰¤L, d â‰¤K, S = (1 â‰ª(d âˆ’
1)).
8: // DP Iterations
9: for j â†2 to L do
10:
for S â†1 to ((1 â‰ªK)âˆ’1) do
11:
if population count(S) > 1 then
12:
for d â†to K do
13:
if ((S â‰«(d âˆ’1))&1) = 1 then
14:
Sâ€² â†SâŠ•(1 â‰ª(d âˆ’1))
15:
for i â†1 to j âˆ’1 do
16:
if M(i, j) â‰¤md then
17:
for dâ€² â†1 to K do
18:
if ((Sâ€² â‰«(dâ€² âˆ’1))&1) = 1 then
19:
tprev â†DP(Sâ€²,i,dâ€²)
20:
if tprev < âˆthen
21:
tfinish
â†
max(Tload(i
+
1, j,d),tprev) + Tcomm(dâ€²,d,i) +
Tcomp(i+1, j,d)
22:
if tfinish < DP(S,L,d) then
23:
DP(S, j,d) â†tfinish
24:
path(S, j,d) â†(i,dâ€²)
25:
end if
26:
end if
27:
end if
28:
end for
29:
end if
30:
end for
31:
end if
32:
end for
33:
end if
34:
end for
35: end for
36: Find (Sâˆ—,dâˆ—) = argminS,d{DP(S,L,d)}
37: T â†DP(Sâˆ—,L,dâˆ—)
38: Reconstruct plan by backtracking from path(Sâˆ—,L,dâˆ—).
39: return T, plan
O((KL)K). This level of complexity makes the approache in-
feasible for practical scenarios. Fortunately, the problem ex-
hibits two key properties that make it well-suited for a DP
solution:
â€¢ Optimal Substructure: A globally optimal solution for
partitioning and scheduling L layers across K devices neces-
sarily includes optimal solutions to its subproblems, such as
scheduling the first j layers (where j < L) across a subset of
devices S âŠ‚K.
â€¢ Overlapping Subproblems: In the recursive construc-
tion of the global solution, certain subproblems, e.g., comput-
ing the minimum completion time for scheduling, the first j
layers on a given subset of devices recur multiple times. DP
mitigates redundant computations by storing and reusing these
intermediate results.
4.1
DP State Definition
We begin by defining the following utility functions for
time cost and memory usage, which can be precomputed prior
to executing the main DP algorithm to enhance efficiency:
Tload(i, j,d) = âˆ‘j
l=i Pl
rd
,
(25)
Tcomp(i, j,d) = âˆ‘j
l=iWl
cd
,
(26)
Tcomm(dprev,dcurr, j) =
Aj
min(bu
dprev,bd
dcurr),
(27)
M(i, j) = max
iâ‰¤lâ‰¤jAl +
j
âˆ‘
l=i
Pl.
(28)
To formulate the subproblems and define the state tran-
sitions in our DP approach, we represent each state as a
triplet (S, j,d), where S âŠ‚K is the set of devices used so far,
j âˆˆ{1,2,...,L} is the index of the last layer assigned. d âˆˆS
is the device to which the final segment (ending at layer j) is
assigned. In our implementation, the set S is represented by a
bitmask, an integer where the k-th (binary) bit is 1 if device d
is in the set, and 0 otherwise.
The DP state value, denoted as DP(S, j,d), represents
the minimum total completion time required to partition and
schedule the first j layers (i.e., layers 1 to j) onto the device
set S, with the final segment processed by device d.
4.2
State Transition Recurrence
To compute the value of the DP state DP(S, j,d), we con-
sider all possible predecessor states corresponding to valid
partitions of the first j layers. Suppose the final segment as-
signed to device d begins at layer i+1, where 1 â‰¤i < j). This
implies that the first i layers (i.e., layer 1 through i) have al-
ready been scheduled on the reduced device set Sâ€² = S \ {d},


--- Page 7 ---
CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems
7
Table 1
Simulation Parameters.
Parameters
Computation
Load
Communication
câˆ—(FLOPS)
a
b
rk (MB/s)
mk (GB)
Âµ
Bk (MHz)
Pu
k (dBm)
Pd
k (dBm)
N0 (dBm/Hz)
dk (m)
d0 (m)
Î¶
Î²0 (dB)
Device 1
165
0.4
5.1E-04
5000
20
0.5
160
20
25
-174
1
1
3
-47.2
Device 2
70
0.7
8.7E-04
4000
10
18
3
Device 3
30
0.8
1.1E-03
3000
8
15
5
Device 4
20
0.8
1.8E-03
2000
8
15
7
with the final segment in that subproblem executed on some
device dâ€² âˆˆSâ€². The state transition recurrence can be written
as:
DP(S, j,d) = min
1â‰¤i<j,
dâ€²âˆˆSâ€²

max
 Tload(i+1, j,d),DP(Sâ€²,i,dâ€²)

+Tcomm(dâ€²,d,i)+Tcomp(i+1, j,d)
	
.
(29)
This recurrence captures the dependencies between parti-
tion boundaries, device assignments, and timing, ensuring that
model loading on device d and computation on the previous
device dâ€² are appropriately overlapped while respecting com-
munication delays.
4.3
Base Case
The base case of the recurrence corresponds to the scenario
where the entire pipeline consists of a single segment, where
all layers from 1 to j are assigned to a single device d. In this
case, there is no predecessor device, and therefore no inter-
device communication. The set of used devices is S = {d}.
The corresponding base case is defined as:
DP(1 â‰ª(d âˆ’1), j,d) = Tload(1, j,d)+Tcomp(1, j,d),
âˆ€d âˆˆK , j âˆˆL ,
M(1, j) â‰¤md.
(30)
4.4
Reconstructing the Optimal Schedule
To recover the optimal scheduling plan after computing the
DP table, we store, for each state DP(S,L,d), the choice of
split point i and predecessor device dâ€² that led to the minimum
value. Once the DP table is fully populated, the optimal total
makespan is given by
T =
min
SâŠ‚K ,SÌ¸=/0

min
dâˆˆS(DP(S,L,d))

.
(31)
To enable reconstruction of the complete solution, we
maintain a backtracking table path path(S, j,d) = (i,dâ€²),
where i is the starting layer of the final segment assigned to
device dâ€². By recursively following these stored paths start-
ing from the final state, we can reconstruct the entire layer-to-
device assignment and scheduling order.
The full procedure is formally described in the accompany-
ing pseudocode. The DP iteration involves five nested loops,
resulting in a space complexity of O(K Â· L Â· 2K) and a time
complexity of O(K2Â·L2Â·2K). While the algorithm remains ex-
ponential in K, this complexity is significantly more tractable
than the factorial complexity of brute-force enumeration, and
is acceptable for practical values of K and L.
5
NUMERICAL RESULTS
5.1
Experiment Setting
To evaluate the performance of the proposed algorithm, we
conduct a series of numerical simulations. The simulation
environment is configured as a typical personal Wi-Fi net-
work, consisting of one AP and four heterogeneous comput-
ing devices. For model parameters, we adopt the settings of
the Qwen3-14B[29] model. The detailed simulation configu-
rations, including the computational, I/O characteristics, and
communication bandwidths of each device, are summarized
in Tab 1.
5.2
Performance Evaluation
To comprehensively validate the effectiveness of the pro-
posed algorithm, we compare it against three baseline strate-
gies:
â€¢ Ideal Single Device: An idealized reference in which
all model layers are deployed on the most powerful device
(Device 1), assumed to have unlimited GPU memory.
â€¢ Even: A naive strategy that evenly partitions model lay-
ers among the four devices, ignoring performance heterogene-
ity. Allocation priority is given to stronger devices.
â€¢ Heuristic: A performance-aware strategy that allocates
layers based on a metric defined as the harmonic mean of a
deviceâ€™s compute capability and disk read speed. Layers are
proportionally distributed according to this metric, again pri-
oritizing stronger devices.
As shown in Fig.3, the proposed DP algorithm consis-
tently achieves the lowest cold-start latency across all eval-
uated token lengths (256 to 8192), significantly outperform-
ing all baseline methods. Fig.3(b) further indicates that our


--- Page 8 ---
8
Journal of Communications and Information Networks



	


	

 "




	


 " " "% !
 ! 
  $
$
# !"
(a)
	
	




# " "&





	
&") '&#" 
 
! "! ( (% 
(" (% 
'$%& (% 
( %& (% 
(b)
Figure 3
Comparison of four strategies. (a) Cold start delay changes with load; (b) Performance improvement percentage of our algorithm compared to baseline
strategies.








 
" 
" 
" 
" 	
 
!  
! 
!
 ! " !
(a)







 
" 
" 
" 
" 	
 
!  
! 
!
 ! " !
(b)



	



 
" 
" 
" 
" 
 
!  
! 
!
 ! " !
(c)



	


 
" 
" 
" 
" 
 
!  
! 
!
 ! " !
(d)
Figure 4
Gantt chart of Even and Heuristic strategies. (a) Even strategy for 256 token length; (b) Even strategy for 8196 token length; (c) Heuristic strategy for
256 token length; (d) Heuristic strategy for 8196 token length.
method reduces latency by 8% to 50% compared to the base-
lines. Even against the best-performing baseline at each token
length, it delivers an average improvement of 17.43%. The
following analysis reveals a key trade-off underlying these re-
sults: I/O parallelization versus computational heterogeneity.
1) Limitations of Baseline Strategies: In short-token sce-
narios (e.g., â‰¤2048 tokens), the Even and Heuristic strate-
gies perform reasonably well by parallelizing model loading
during the I/O-intensive phase, as illustrated in Fig. 4(a) and
Fig. 4(c). However, as token length increases, computation
becomes the dominant bottleneck, revealing the limitations of
these static allocation strategies as depicted in Fig. 4(b) and
Fig. 4(d). Because they fail to consider computational hetero-
geneity, the weakest device often becomes a pipeline â€œstrag-
glerâ€, significantly increasing overall latency. In contrast, the
Ideal Single Device exhibits the opposite behavior: although
it suffers from higher latency in the I/O phase due to lack
of parallelism, its strong computational power enables it to
outperform the imbalanced Even and Heuristic strategies in
compute-intensive phases.
2) Adaptive Advantage of the Proposed DP: The Pro-
posed DP algorithm effectively addresses the trade-off be-


--- Page 9 ---
CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems
9






 
" 
" 	
" 
" 
 
!  
! 
!
 ! " !
(a)








 
" 	
" 
" 
" 
 
!  
! 
!
 ! " !
(b)





	


  
# 
# 
# 
# 
 
"!!
"!
"
 !"!# "
(c)
Figure 5
Gantt chart of our proposed DP algorithm. (a) 256 token length; (b) 2048 token length; (c) 8196 token length.
tween I/O and computation. Rather than using a fixed alloca-
tion scheme, it dynamically optimizes layer deployment based
on the input token length, which determines the expected com-
putational workload. Under low-load conditions, the algo-
rithm favors a more balanced layer distribution to maximize
I/O parallelism, as shown in Fig. 5(a).
Conversely, under
high-load conditions, it strategically assigns more compute-
intensive layers to the most powerful devices, preventing
weaker devices from becoming bottlenecks and fully lever-
aging the systemâ€™s heterogeneous computing resources, as il-
lustrated in Fig.5(c). This shift in allocation strategy with in-
creasing load is clearly visible in Fig. 5: as the load grows, the
distribution becomes more skewed, with a larger proportion of
layers assigned to stronger devices.
In summary, by accurately modeling and dynamically op-
timizing computation, communication, and I/O overheads,
the proposed DP algorithm effectively adapts to device het-
erogeneity, enabling it to achieve near-optimal performance
across diverse workload conditions.
6
CONCLUSION
AND
FUTURE
WORK
In this paper, we tackle the challenge of inference cold-start
latency in wireless networks composed of heterogeneous de-
vices. We propose a novel dynamic programmingâ€“based layer
allocation algorithm to minimize latency. By carefully mod-
eling computation, communication, and I/O overheads, our
approach effectively balances these factors to fully leverage
the parallel processing capabilities of multiple devices. Ex-
perimental results demonstrate that our algorithm significantly
outperforms several baseline strategies in reducing end-to-end
latency.
Looking forward, a key direction for future work is the
seamless integration of our cold-start optimization with strate-
gies tailored for the steady-state, high-throughput inference
phase. Designing a dynamic and adaptive transition mecha-
nism between these phases is a critical challenge for achieving
comprehensive, lifecycle-aware latency minimization.
Our
future research will focus on developing a unified optimiza-
tion framework to address this challenge.
References
[1]
YANG L, ZHANG Z, SONG Y, et al. Diffusion models:
A comprehensive survey of methods and applications[J].
ACM computing surveys, 2023, 56(4): 1-39.
[2]
LIU Y, HUANG J, LI Y, et al. Generative ai model pri-
vacy: a survey[J]. Artificial Intelligence Review, 2024,
58(1): 33.
[3]
RODRIGUEZ-CONDE
I,
CAMPOS
C,
FDEZ-
RIVEROLA F.
Horizontally distributed inference of
deep neural networks for ai-enabled iot[J].
Sensors,
2023, 23(4): 1911.
[4]
LIU S, TAO X, CAO W, et al. Parallelization techniques
for large language models: A review from training to
inference[C]//International Conference on Wireless Ar-
tificial Intelligent Computing Systems and Applications.
Springer, 2025: 307-317.
[5]
HUANG Y, CHENG Y, BAPNA A, et al. Gpipe: Ef-
ficient training of giant neural networks using pipeline
parallelism[J]. Advances in neural information process-
ing systems, 2019, 32.
[6]
NARAYANAN D, HARLAP A, PHANISHAYEE A,
et al. Pipedream: Generalized pipeline parallelism for
dnn training[C]//Proceedings of the 27th ACM sympo-
sium on operating systems principles. 2019: 1-15.
[7]
JEON B, WU M, CAO S, et al. Graphpipe: Improving
performance and scalability of dnn training with graph
pipeline parallelism[C]//Proceedings of the 30th ACM
International Conference on Architectural Support for
Programming Languages and Operating Systems, Vol-
ume 1. 2025: 557-571.
[8]
QI P, WAN X, HUANG G, et al. Zero bubble pipeline
parallelism[A]. 2023.


--- Page 10 ---
10
Journal of Communications and Information Networks
[9]
FANG J, PAN J, WANG J, et al. Pipefusion: Patch-level
pipeline parallelism for diffusion transformers inference
[A]. 2024.
[10] HU Y, IMES C, ZHAO X, et al.
Pipeedge: Pipeline
parallelism for large-scale model inference on hetero-
geneous edge devices[C]//2022 25th Euromicro Confer-
ence on Digital System Design (DSD).
IEEE, 2022:
298-307.
[11] YOON J, BYEON Y, KIM J, et al.
Edgepipe: Tai-
loring pipeline parallelism with deep neural networks
for volatile wireless edge devices[J]. IEEE Internet of
Things Journal, 2021, 9(14): 11633-11647.
[12] XIONG Y, LIU W, ZHANG R, et al. Epipe: Pipeline
inference framework with high-quality offline paral-
lelism planning for heterogeneous edge devices[C]//
Proceedings of the 43rd IEEE/ACM International Con-
ference on Computer-Aided Design. 2024: 1-10.
[13] XUE N, SUN Y, CHEN Z, et al. Wdmoe: Wireless dis-
tributed large language models with mixture of experts
[C]//GLOBECOM 2024-2024 IEEE Global Communi-
cations Conference. IEEE, 2024: 2707-2712.
[14] YAO Z, DONG Z, ZHENG Z, et al. Hawq-v3: Dyadic
neural network quantization[C]//International Confer-
ence on Machine Learning. PMLR, 2021: 11875-11886.
[15] LIN J, TANG J, TANG H, et al. Awq: Activation-aware
weight quantization for on-device llm compression and
acceleration[J].
Proceedings of machine learning and
systems, 2024, 6: 87-100.
[16] FANG G, MA X, SONG M, et al.
Depgraph: To-
wards any structural pruning[C]//Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition. 2023: 16091-16101.
[17] GAN Z, CHEN Y C, LI L, et al. Playing lottery tickets
with vision and language[C]//Proceedings of the AAAI
Conference on Artificial Intelligence: Vol. 36.
2022:
652-660.
[18] GU Y, DONG L, WEI F, et al. Minillm: Knowledge
distillation of large language models[A]. 2023.
[19] SAUER A, LORENZ D, BLATTMANN A, et al. Ad-
versarial diffusion distillation[C]//European Conference
on Computer Vision. Springer, 2024: 87-103.
[20] VASWANI A, SHAZEER N, PARMAR N, et al. Atten-
tion is all you need[J]. Advances in neural information
processing systems, 2017, 30.
[21] AINSLIE J, LEE-THORP J, DE JONG M, et al. Gqa:
Training generalized multi-query transformer models
from multi-head checkpoints[C]//Proceedings of the
2023 Conference on Empirical Methods in Natural Lan-
guage Processing. 2023: 4895-4901.
[22] SHAZEER N.
Glu variants improve transformer[A].
2020.
[23] KIRK D B, WEN-MEI W H. Programming massively
parallel processors: a hands-on approach[M]. Morgan
kaufmann, 2016.
[24] HENNESSY J L, PATTERSON D A. Computer archi-
tecture: a quantitative approach[M]. Elsevier, 2011.
[25] LANANTE L, UWAI H O T, NAGAO Y, et al. Perfor-
mance analysis of the 802.11 ax ul ofdma random access
protocol in dense networks[C]//2017 IEEE international
conference on communications (ICC). IEEE, 2017: 1-6.
[26] QU Q, LI B, YANG M, et al.
Survey and perfor-
mance evaluation of the upcoming next generation wlans
standard-ieee 802.11 ax[J]. Mobile Networks and Appli-
cations, 2019, 24(5): 1461-1474.
[27] SHANNON C E. A mathematical theory of communi-
cation[J]. The Bell system technical journal, 1948, 27
(3): 379-423.
[28] RAPPAPORT T S.
Wireless communications: Prin-
ciples and practice, 2/e[M]. Pearson Education India,
2010.
[29] YANG A, LI A, YANG B, et al. Qwen3 technical report
[A]. 2025.
About the Authors
Xuran Liu was born in Anhui Province, China in 2003. He is currently a
senior undergraduate student at the Shanghai Jiao Tong University, Shang-
hai, majoring in Information Engineering. His research interests focus on the
efficient inference and deployment of large-scale models at the wireless edge.
Nan Xue [corresponding author] received the B.E. degree in Telecommu-
nication Engineering from Xidian University, Xiâ€™an, China, in 2023.
He
is currently pursuing the Ph.D degree with the Department of Information
and Communication Engineering. His research interests include wireless net-
works for large language models.
Rui Bao received his B.S. degree in Information Engineering from the IEEE
Honor Class at Shanghai Jiao Tong University in 2025. He will pursue a Ph.D.
in Information and Communication Engineering at SJTU under the supervi-
sion of Prof. Zhiyong Chen. His research interests include large language
models, agents system, and wireless communication networks.


--- Page 11 ---
CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems
11
Yaping Sun received the B.E. degree and the Ph.D. degree from Xidian Uni-
versity and Shanghai Jiao Tong University, in 2015 and 2020, respectively.
From 2018 to 2019, she was a Visiting Scholar with University of Washing-
ton, USA. From 2020 to 2022, She was a Post-Doctoral Research Fellow with
the Future Network of Intelligent Institute (FNii), the Chinese University of
Hong Kong (CUHK), Shenzhen, China. She is currently an Assistant Re-
searcher with the Department of Broadband, Peng Cheng Laboratory, Shen-
zhen, and also with the FNii-Shenzhen, the CUHK, Shenzhen, China. Her
research interests include mobile 3C networks and semantic communication.
Zhiyong Chen received the Ph.D. degree from the School of Information and
Communication Engineering, Beijing University of Posts and Telecommuni-
cations (BUPT), Beijing, China, in 2011. From 2009 to 2011, he was a vis-
iting Ph.D. Student at the Department of Electronic Engineering, University
of Washington, Seattle, USA. He is currently a Professor with the Cooper-
ative Medianet Innovation Center, Shanghai Jiao Tong University (SJTU),
Shanghai, China.
His research interests include mobile communications-
computing-caching (3C) networks and mobile AI systems.
He served as
the Student Volunteer Chair for the IEEE ICC 2019, the Publicity Chair for
the IEEE/CIC ICCC 2014 and a TPC member for major international con-
ferences. He was the recipient of the IEEE Asia-Pacific Outstanding Paper
Award in 2019.
Meixia Tao is a Distinguished Professor with the Department of Electronic
Engineering at Shanghai Jiao Tong University, China. She received the B.S.
degree from Fudan University, Shanghai, China, in 1999, and the Ph.D. de-
gree from the Hong Kong University of Science and Technology in 2003.
Her current research interests include wireless edge learning, semantic com-
munications, integrated communication-computing-sensing, AI-based chan-
nel modeling and beamforming.
Dr. Tao receives the 2019 IEEE Marconi Prize Paper Award, the 2013
IEEE Heinrich Hertz Paper Award, and a number of IEEE conference best
paper awards. She also receives the 2009 IEEE ComSoc Asia-Pacific Out-
standing Young Researcher award. Dr. Tao is a member of IEEE ComSoc
GLOBECOM/ICC Technical Content (GITC) Committee. She has served for
IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS for many
years, first as an associate editor, then as a member of the Executive Editorial
Committee, and currently as a member of the Steering Committee. She was
also on the editorial board as an editor or guest editor of a number of other
journals, including the IEEE TRANSACTIONS ON OMMUNICATIONS,
IEEE TRANSACTIONS ON INFORMATION THEORY, PROCEEDINGS
OF THE IEEE, and IEEE JOURNAL ON SELECTED AREAS IN COM-
MUNICATIONS. She has served as the TPC Co-Chair of IEEE ICC 2023
and Symposia Oversight Chair of IEEE ICC 2019.
Xiaodong Xu (Sâ€™06-Mâ€™07-SMâ€™18) received his B.S degree and Masterâ€™s De-
gree both from Shandong University in 2001 and 2004 separately. He re-
ceived his Ph.D. degree in Beijing University of Posts and Telecommunica-
tions (BUPT) in 2007. He is currently a professor of BUPT, a research fellow
of the Department of Broadband Communication of Peng Cheng Laboratory
and a member of IMT-2030 (6G) Experts Panel. He has coauthored nine
books/chapters and more than 130 journal and conference papers. He is also
the inventor or co-inventor of 73 granted patents. His research interests cover
semantic communications, intellicise networks, trial system and network.
Shuguang Cui received his Ph.D. from Stanford in 2005. He is now a X.Q.
Deng Presidential Chair Professor at The Chinese University of Hong Kong,
Shenzhen, China. His current research interest is data driven large-scale infor-
mation analysis and system design. He was selected as the Thomson Reuters
Highly Cited Researcher and listed in the Worldsâ€™ Most Influential Scientific
Minds by ScienceWatch in 2014. He was the recipient of the IEEE SP So-
ciety 2012 and ComSoc 2023 Marconi Best Paper Awards. He is an IEEE
Fellow, Member of Both Royal Society of Canada and Canadian Academy of
Engineering.
